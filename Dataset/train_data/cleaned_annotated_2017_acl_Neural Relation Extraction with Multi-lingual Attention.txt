Neural B-TaskName
Relation I-TaskName
Extraction I-TaskName
with O
Multi O
- O
lingual O
Attention O
Yankai O
Lin1 O
, O
Zhiyuan O
Liu1∗ O
, O
Maosong O
Sun1,2 O
1Department O
of O
Computer O
Science O
and O
Technology O
, O
State O
Key O
Lab O
on O
Intelligent O
Technology O
and O
Systems O
, O
National O
Lab O
for O
Information O
Science O
and O
Technology O
, O
Tsinghua O
University O
, O
Beijing O
, O
China O
2Jiangsu O
Collaborative O
Innovation O
Center O
for O
Language O
Competence O
, O
Jiangsu O
, O
China O
Abstract O
Relation B-TaskName
extraction I-TaskName
has O
been O
widely O
used O
for O
ﬁnding O
unknown O
relational O
facts O
from O
the O
plain O
text O
. O

Most O
existing O
methods O
fo- O
cus O
on O
exploiting O
mono O
- O
lingual O
data O
for O
relation O
extraction O
, O
ignoring O
massive O
in- O
formation O
from O
the O
texts O
in O
various O
lan- O
guages O
. O

To O
address O
this O
issue O
, O
we O
intro- O
duce O
a O
multi O
- O
lingual O
neural O
relation O
ex- O
traction O
framework O
, O
which O
employs O
mono- O
lingual O
attention O
to O
utilize O
the O
information O
within O
mono O
- O
lingual O
texts O
and O

further O
pro- O
poses O
cross O
- O
lingual O
attention O
to O
consider O
the O
information O
consistency O
and O
comple- O
mentarity O
among O
cross O
- O
lingual O
texts O
. O

Ex- O
perimental O
results O
on O
real O
- O
world O
datasets O
show O
that O
our O
model O
can O
take O
advan- O
tage O
of O
multi O
- O
lingual O
texts O
and O
consistently O
achieve O
signiﬁcant O
improvements O
on O
re- B-TaskName
lation I-TaskName
extraction I-TaskName
as O
compared O
with O
base- O
lines O
. O

The O
source O
code O
of O
this O
paper O
can O
be O
obtained O
from O
https://github O
. O
com O
/ O
thunlp O
/ O
MNRE O
1 O
Introduction O
People O
build O
many O
large O
- O
scale O
knowledge O
bases O
( O
KBs O
) O
to O
store O
structured O
knowledge O
about O
the O
real O
world O
, O
such O
as O
Wikidata1and O
DBpedia2 O
. O

KBs O
are O
playing O
an O
important O
role O
in O
many O
AI O
and O
NLP O
applications O
such O
as O
information O
retrieval O
and O
question O
answering O
. O

The O
facts O
in O
KBs O
are O
typically O
organized O
in O
the O
form O
of O
triplets O
, O
e.g. O
, O
( O
New O
York O
, O
CityOf O
, O
United O
States O
) O
. O

Since O
ex- O
isting O
KBs O
are O
far O
from O
complete O
and O
new O
facts O
are O
growing O
inﬁnitely O
, O
meanwhile O
manual O
anno- O
tation O
of O
these O
knowledge O
is O
time O
- O
consuming O
and O
human-intensive O
, O
many O
works O
have O
been O
devoted O
to O
automated O
extraction O
of O
novel O
facts O
from O
vari- O
ous O
Web O
resources O
, O
where O
relation B-TaskName
extraction I-TaskName
( O
RE B-TaskName
) O
from O
plain O
texts O
is O
one O
the O
most O
important O
knowl- O
edge O
sources O
. O

Among O
various O
methods O
for O
relation B-TaskName
extraction I-TaskName
, O
distant O
supervision O
is O
the O
most O
promising O
approach O
( O
Mintz O
et O
al O
. O
, O
2009 O
; O
Riedel O
et O
al O
. O
, O
2010 O
; O
Hoffmann O
et O
al O
. O
, O
2011 O
; O
Surdeanu O
et O
al O
. O
, O
2012 O
) O
, O
which O
can O
au- O
tomatically O
generate O
training O
instances O
via O
aligning O
KBs O
and O
texts O
to O
address O
the O
issue O
of O
lacking O
super- O
vised O
data O
. O

As O
the O
development O
of O
deep O
learning O
, O
Zeng O
et O
al O
. O
( O
2015 O
) O
introduce O
neural O
networks O
to O
ex- O
tract O
relations O
with O
automatically O
learned O
features O
from O
training O
instances O
. O

To O
address O
the O
wrong O
labelling O
issue O
of O
distant O
supervision O
data O
, O
Lin O
et O
al O
. O

( O
2016 O
) O
further O
employ O
sentence O
- O
level O
atten- O
tion O
mechanism O
in O
neural B-TaskName
relation I-TaskName
extraction I-TaskName
, O
and O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
. O

However O
, O
most O
RE O
systems O
concentrate O
on O
ex- O
tracting O
relational O
facts O
from O
mono O
- O
lingual O
data O
. O

In O
fact O
, O
people O
describe O
knowledge O
about O
the O
world O
using O
various O
languages O
. O

And O
people O
speaking O
different O
languages O
also O
share O
similar O
knowledge O
about O
the O
world O
due O
to O
the O
similarities O
of O
human O
experiences O
and O
human O
cognitive O
systems O
. O

For O
in- O
stance O
, O
though O
New O
York O
andUnited O
States O
are O
ex- O
pressed O
as O
纽约 O
and美国 O
respectively O
in O
Chinese O
, O
both O
Americans O
and O
Chinese O
share O
the O
fact O
that O
“ O
New O
York O
is O
a O
city O
of O
USA O
. O
” O

It O
is O
straightforward O
to O
build O
mono O
- O
lingual O
RE O
systems O
separately O
for O
each O
single O
language O
. O

But O
if O
so O
, O
it O
wo O
n’t O
be O
able O
to O
take O
full O
advantage O
of O
di- O
verse O
information O
hidden O
in O
the O
data O
of O
various O
lan- O
guages O
. O

Multi O
- O
lingual O
data O
will O
beneﬁt O
relation O
ex- O

traction O
for O
the O
following O
two O
reasons O
: O
1 O
. O
Consis- O

tency O
. O

According O
to O
the O
distant O
supervision O
data O
in O
our O
experiments3 O
, O
we O
ﬁnd O
that O
over O
half O
of O
Chinese O
and O
English O
sentences O
are O
longer O
than O
20words O
, O
in O
which O
only O
several O
words O
are O
related O
to O
the O
re- O
lational O
facts O
. O

Take O
Table O
1for O
example O
. O

The O
ﬁrst O
Chinese O
sentence O
has O
over O
20words O
, O
in O
which O
only O
“ O
纽约 O
” O
( O
New O
York O
) O
and O
“ O
ᱥ美国ㅢжཝค O
ᐸ O
” O
( O
is O
the O
biggest O
city O
in O
the O
United O
States O
) O
ac- O
tually O
directly O
reﬂect O
the O
relational O
fact O
CityOf O
. O

It O
is O
thus O
non O
- O
trivial O
to O
locate O
and O
learn O
these O
rela- O
tional O
patterns O
from O
complicated O
sentences O
for O
re- B-TaskName
lation I-TaskName
extraction I-TaskName
. O

Fortunately O
, O
a O
relational O
fact O
is O
usually O
expressed O
with O
certain O
patterns O
in O
various O
languages O
, O
and O
the O
correspondence O
of O
these O
pat- O
terns O
among O
languages O
is O
substantially O
consistent O
. O

The O
pattern O
consistency O
among O
languages O
provides O
us O
augmented O
clues O
to O
enhance O
relational O
pattern O
learning O
for O
relation B-TaskName
extraction I-TaskName
. O

2 O
. O
Complementarity O
. O

From O
our O
experiment O
data O
, O
we O
also O
ﬁnd O
that O
42.2 O
% O
relational O
facts O
in O
English O
data O
and O
41.6 O
% O
ones O
in O
Chinese O
data O
are O
unique O
. O

Moreover O
, O
for O
nearly O
half O
of O
relations O
, O
the O
number O
of O
sentences O
expressing O
relational O
facts O
of O
these O
relations O
varies O
a O
lot O
in O
different O
languages O
. O

It O
is O
thus O
straightforward O
that O
the O
texts O
in O
differ- O
ent O
languages O
can O
be O
complementary O
to O
each O
other O
, O
especially O
from O
those O
resource O
- O
rich O
languages O
to O
resource O
- O
poor O
languages O
, O
and O
improve O
the O
overall O
performance O
of O
relation O
extraction O
. O

To O
take O
full O
consideration O
of O
these O
issues O
, O
we O
propose O
Multi B-MethodName 
- I-MethodName
lingual I-MethodName
Attention I-MethodName
- I-MethodName
based I-MethodName
Neural I-MethodName
Relation I-MethodName
Extraction I-MethodName
( O
MNRE B-MethodName
) O
. O

We O
ﬁrst O
employ O
a O
convolutional O
neural O
network O
( O
CNN O
) O
to O
embed O
the O
relational O
patterns O
in O
sentences O
into O
real O
- O
valued O
vectors O
. O

Afterwards O
, O
to O
consider O
the O
complemen- O
tarity O
of O
all O
informative O
sentences O
in O
various O
lan- O
guages O
and O
capture O
the O
consistency O
of O
relational O
patterns O
, O
we O
apply O
mono O
- O
lingual O
attention O
to O
select O
the O
informative O
sentences O
within O
each O
language O
and O
propose O
cross O
- O
lingual O
attention O
to O
take O
advan- O
tages O
of O
pattern O
consistency O
and O
complementarity O
among O
languages O
. O

Finally O
, O
we O
classify O
relations O
according O
to O
the O
global O
vector O
aggregated O
from O
all O
sentence O
vectors O
weighted O
by O
mono O
- O
lingual O
atten- O
tion O
and O
cross O
- O
lingual O
attention O
. O

In O
experiments O
, O
we O
build O
training O
instances O
via O
distant O
supervision O
by O
aligning O
Wikidata O
with O
Chi- O
nese O
Baidu O
Baike O
and O
English O
Wikipedia O
articles O
θ O
and O
evaluate O
the O
performance O
of O
relation O
extraction O
in O
both O
English O
and O
Chinese O
. O

The O
experimental O
results O
show O
that O
our O
framework O
achieves O
signif- O
icant O
improvement O
for O
relation O
extraction O
as O
com- O
pared O
to O
all O
baseline O
methods O
including O
both O
mono- O
lingual O
and O
multi O
- O
lingual O
ones O
. O

It O
indicates O
that O
our O
framework O
can O
take O
full O
advantages O
of O
sentences O
in O
different O
languages O
and O
better O
capture O
sophisti- O
cated O
patterns O
expressing O
relations O
. O

2 O
Related O
Work O
Recent O
years O
KBs O
have O
been O
widely O
used O
on O
var- O
ious O
AI O
and O
NLP O
applications O
. O

As O
an O
impor- O
tant O
approach O
to O
enrich O
KBs O
, O
relation B-TaskName
extraction I-TaskName
from O
plain O
text O
has O
attracted O
many O
research O
in- O
terests O
. O

Relation B-TaskName
extraction I-TaskName
typically O
classiﬁes O
each O
entity O
pair O
into O
various O
relation O
types O
ac- O

cording O
to O
supporting O
sentences O
that O
the O
both O
enti- O
ties O
appear O
, O
which O
needs O
human O
- O
labelled O
relation- O
speciﬁc O
training O
instances O
. O

Many O
works O
have O
been O
invested O
to O
relation B-TaskName
extraction I-TaskName
including O
kernel- O
based O
model O
( O
Zelenko O
et O
al O
. O
, O
2003 O
) O
, O
embedding- O
based O
model O
( O
Gormley O
et O

al O
. O
, O
2015 O
) O
, O
CNN O
- O
based O
models O
( O
Zeng O
et O
al O
. O
, O
2014 O
; O
dos O
Santos O
et O
al O
. O
, O
2015 O
) O
, O
and O
RNN O
- O
based O
model O
( O
Socher O
et O
al O
. O
, O
2012 O
) O
. O

Nevertheless O
, O
these O
RE O
systems O
are O
insufﬁ- O
cient O
due O
to O
the O
lack O
of O
training O
data O
. O

To O
ad- O
dress O
this O
issue O
, O
Mintz O
et O
al O
. O

( O
2009 O
) O
align O
plain O
text O
with O
Freebase O
to O
automatically O
generate O
train- O
ing O
instances O
following O
the O
distant O
supervision O
assumption O
. O

To O
further O
alleviate O
the O
wrong O
la- O
belling O
problem O
, O
Riedel O
et O
al O
. O

( O
2010 O
) O
model O
dis- O
tant O
supervision O
for O
relation B-TaskName
extraction I-TaskName
as O
a O
multi- O
instance O
single O
- O
label O
learning O
problem O
, O
and O
Hoff- O
mann O
et O

al O
. O

( O
2011 O
) O
; O
Surdeanu O
et O
al O
. O

( O
2012 O
) O
regard O
it O
as O
a O
multi O
- O
instance O
multi O
- O
label O
learning O
problem O
. O

Recently O
, O
Zeng O
et O
al O
. O
( O
2015 O
) O
attempt O
to O
connect O
neural O
networks O
with O
distant O
supervision O
follow- O
ing O
the O
expressed O
- O
at O
- O
least O
- O
once O
assumption O
. O

Lin O
et O
al O
. O

( O
2016 O
) O
further O
utilize O
sentence O
- O
level O
attention O
mechanism O
to O
consider O
all O
informative O
sentences O
jointly O
. O

Most O
existing O
RE O
systems O
are O
absorbed O
in O
ex- O
tracting O
relations O
from O
mono O
- O
lingual O
data O
, O
ignor- O
ing O
massive O
information O
lying O
in O
texts O
from O
mul- O
tiple O
languages O
. O

In O
this O
area O
, O
Faruqui O
and O
Kumar O
( O
2015 O
) O
present O
a O
language O
independent O
open O
do- O
main O
relation O
extraction O
system O
, O
and O
Verga O
et O

al O
. O

( O
2015 O
) O
further O
employ O
Universal O
Schema O
to O
com- O
bine O
OpenIE O
and O
link O
- O
prediction O
perspective O
for O
multi O
- O
lingual O
relation O
extraction O
. O

Both O
the O
works O
focus O
on O
multi O
- O
lingual O
transfer O
learning O
and O
learn O
a O
predictive O
model O
on O
a O
new O
language O
for O
existing O
KBs O
, O
by O
leveraging O
uniﬁed O
representation O
learn- O
ing O
for O
cross O
- O
lingual O
entities O
. O

Different O
from O
these O
works O
, O
our O
framework O
aims O
to O
jointly O
model O
the O
texts O
in O
multiple O
languages O
to O
enhance O
relation O
ex- O

traction O
with O
distant O
supervision O
. O

To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
ﬁrst O
effort O
to O
multi B-TaskName
- I-TaskName
lingual I-TaskName
neural I-TaskName
relation I-TaskName
extraction I-TaskName
. O

The O
scope O
of O
multi O
- O
lingual O
analysis O
has O
been O
widely O
considered O
in O
many O
tasks O
besides O
relation B-TaskName
extraction I-TaskName
, O
such O
as O
sentiment O
analysis O
( O
Boiy O
and O
Moens O
, O
2009 O
) O
, O
cross O
- O
lingual O
document O
summa- O
rization O
( O
Boudin O
et O
al O
. O
, O
2011 O
) O
, O
information O
retrieval O
in O
Web O
search O
( O
Dong O
et O
al O
. O
, O
2014 O
) O
and O
so O
on O
. O

3 O
Methodology O

In O
this O
section O
, O
we O
describe O
our O
proposed O
MNRE B-MethodName
framework O
in O
detail O
. O

The O
key O
motivation O
of O
MNRE B-MethodName
is O
that O
, O
for O
each O
relational O
fact O
, O
the O
relation O
pat- O
terns O
in O
sentences O
of O
different O
languages O
should O
be O
substantially O
consistent O
, O
and O
MNRE B-MethodName
can O
utilize O
the O
pattern O
consistency O
and O
complementarity O
amonglanguages O
to O
achieve O
better O
results O
for O
relation B-TaskName
ex- I-TaskName
traction I-TaskName
. O

Formally O
, O
given O
two O
entities O
, O
their O
correspond- O
ing O
sentences O
in O
mdifferent O
languages O
are O
de- O
ﬁned O
as O
T={S1 O
, O
S2 O
, O
. O
. O
. O

, O
S O
m O
} O
, O
where O
Sj= O
{ O
x1 O
j O
, O
x2 O
j O
, O
. O
. O
. O

, O
xnj O
j}corresponds O
to O
the O
sentence O
set O
in O
the O
jth O
language O
with O
njsentences O
. O

Our O
model O
measures O
a O
score O
f(T O
, O
r)for O
each O
relation O
r O
, O
which O
is O
expected O
to O
be O
high O
when O
ris O
the O
valid O
one O
, O
oth- O
erwise O
low O
. O

The O
MNRE B-MethodName
framework O
contains O
two O
main O
components O
: O
1 O
. O
Sentence O
Encoder O
. O

Given O
a O
sentence O
xand O
two O
target O
entities O
, O
we O
employ O
CNN O
to O
encode O
re- O
lation O
patterns O
in O
xinto O
a O
distributed O
representation O
x. O

The O
sentence O
encoder O
can O
also O
be O
implemented O
with O
GRU O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
or O
LSTM O
( O
Hochre- O
iter O
and O
Schmidhuber O
, O
1997 O
) O
. O

In O
experiments O
, O
we O
ﬁnd O
CNN O
can O
achieve O
a O
better O
trade O
- O
off O
between O
computational O
efﬁciency O
and O
performance O
effec- O
tiveness O
. O

Thus O
, O
in O
this O
paper O
, O
we O
focus O
on O
CNN O
as O
the O
sentence O
encoder O
. O

2 O
. O
Multi O
- O
lingual O
Attention O
. O

With O
all O
sentences O
in O
various O
languages O
encoded O
into O
distributed O
vec- O
tor O
representations O
, O
we O
apply O
mono O
- O
lingual O
and O
cross O
- O
lingual O
attentions O
to O
capture O
those O
infor- O
mative O
sentences O
with O
accurate O
relation O
patterns O
. O

MNRE B-MethodName
further O
aggregates O
these O
sentence O
vectors O
with O
weighted O
attentions O
into O
global O
representa- O
tions O
for O
relation O
prediction O
. O

We O
introduce O
the O
two O
components O
in O
detail O
as O
follows O
. O

3.1 O
Sentence O
Encoder O
The O
sentence O
encoder O
aims O
to O
transform O
a O
sentence O
xinto O
its O
distributed O
representation O
xvia O
CNN O
. O

First O
, O
it O
embeds O
the O
words O
in O
the O
input O
sentence36 O

into O
dense O
real O
- O
valued O
vectors O
. O

Next O
, O
it O
employs O
convolutional O
, O
max O
- O
pooling O
and O
non O
- O
linear O
trans- O
formation O
layers O
to O
construct O
the O
distributed O
repre- O
sentation O
of O
the O
sentence O
, O
i.e. O
, O
x. O
3.1.1 O
Input O
Representation O
Following O
( O
Zeng O
et O
al O
. O
, O
2014 O
) O
, O
we O
transform O
each O
input O
word O
into O
the O
concatenation O
of O
two O
kinds O
of O
representations O
: O
( O
1 O
) O
a O
word O
embedding O
which O
cap- O
tures O
syntactic O
and O
semantic O
meanings O
of O
the O
word O
, O
and O
( O
2 O
) O
a O
position O
embedding O
which O
speciﬁes O
the O
position O
information O
of O
this O
word O
with O
respect O
to O
two O
target O
entities O
. O

In O
this O
way O
, O
we O
can O
repre- O
sent O
the O
input O
sentence O
as O
a O
vector O
sequence O
w= O
{ O
w1,w2 O
, O
. O
. O

.}with O

wi∈Rd O
, O
where O
d O
= O
da+db×2 O
. O

( O
daanddbare O
the O
dimensions O
of O
word O
embeddings O
and O
position O
embeddings O
respectively O
) O
3.1.2 O
Convolution O
, O
Max O
- O
pooling O
and O
Non O
- O
linear O
Layers O
After O
encoding O
the O
input O
sentence O
, O
we O
use O
a O
con- O
volutional O
layer O
to O
extract O
the O
local O
features O
, O
max- O
pooling O
, O
and O
non O
- O
linear O
layers O
to O
merge O
all O
local O
features O
into O
a O
global O
representation O
. O

First O
, O
the O
convolutional O
layer O
extracts O
local O
fea- O
tures O
by O
sliding O
a O
window O
of O
length O
l B-HyperparameterName
over O
the O
sen- O
tence O
and O
perform O
a O
convolution O
within O
each O
slid- O
ing O
window O
. O

Formally O
, O
the O
output O
of O
convolutional O
layer O
for O
the O
ith O
sliding O
window O
is O
computed O
as O
: O

pi O
= O
Ww O
i−l+1 O
: O
i+b O
, O
( O
1 O
) O
where O
wi−l+1 O
: O
iindicates O
the O
concatenation O
of O
l O
word O
embeddings O
within O
the O
i O
- O
th O
window O
, O
W∈ O
Rdc×(l×d)is O
the O
convolution O
matrix O
and O
b∈Rdc O
is O
the O
bias O
vector O
. O

( O
dcis O
the O
dimension O
of O
output O
embeddings O
of O
the O
convolution O
layer O
) O

After O
that O
, O
we O
combines O
all O
local O
features O
via O
a O
max O
- O
pooling O
operation O
and O
apply O
a O
hyperbolic O
tan- O
gent O
function O
to O
obtain O
a O
ﬁxed O
- O
sized O
sentence O
vec- O
tor O
for O
the O
input O
sentence O
. O

Formally O
, O
the O
ith O
ele- O
ment O
of O
the O
output O
vector O
x∈Rdcis O
calculated O
as O
: O
[ O
x]j O
= O
tanh O
( O
max O
i(pij O
) O
) O
. O

( O
2 O
) O
The O
ﬁnal O
vector O
xis O
expected O
to O
efﬁciently O
en- O
code O
relation O
patterns O
about O
target O
entities O
from O
the O
input O
sentence O
. O

Here O
, O
instead O
of O
max O
pooling O
operation O
, O
we O
can O
use O
piecewise O
max O
pooling O
operation O
adopted O
by O
PCNN O
( O
Zeng O
et O
al O
. O
, O
2015 O
) O
which O
is O
a O
variation O
of O
CNN O
to O
better O
capture O
the O
relation O
patterns O
in O
the O
input O
sentence.3.2 O
Multi O
- O
lingual O
Attention O
To O
exploit O
the O
information O
of O
the O
sentences O
from O
all O
languages O
, O
our O
model O
adopts O
two O
kinds O
of O
at- O
tention O
mechanisms O
for O
multi B-TaskName
- I-TaskName
lingual I-TaskName
relation I-TaskName
ex- I-TaskName
traction I-TaskName
, O
including O
: O
( O
1 O
) O
the O
mono O
- O
lingual O
atten- O
tion O
which O
selects O
the O
informative O
sentences O
within O
one O
language O
and O
( O
2 O
) O
the O
cross O
- O
lingual O
attention O
which O
measures O
the O
pattern O
consistency O
among O
languages O
. O

3.2.1 O
Mono O
- O
lingual O
Attention O
To O
address O
the O
wrong O
- O
labelling O
issue O
in O
distant O
su- O
pervision O
, O
we O
follow O
the O
idea O
of O
sentence O
- O
level O
at- O
tention O
( O
Lin O
et O
al O
. O
, O
2016 O
) O
and O
set O
mono O
- O
lingual O
at- O
tention O
for O
MNRE B-MethodName
. O

It O
is O
intuitive O
that O
each O
hu- O
man O
language O
has O
its O
own O
characteristics O
. O

Hence O
we O
adopt O
different O
mono O
- O
lingual O
attentions O
to O
de- O
emphasize O
those O
noisy O
sentences O
within O
each O
lan- O
guage O
. O

More O
speciﬁcally O
, O
for O
the O
j O
- O
th O
language O
and O
the O
sentence O
set O
Sj O
, O
we O
aim O
to O
aggregate O
all O
sentence O
vectors O
into O
a O
real O
- O
valued O
vector O
Sjfor O
relation O
pre- O
diction O
. O

The O
mono O
- O
lingual O
vector O
Sjis O
computed O
as O
a O
weighted O
sum O
of O
those O
sentence O
vectors O
xi O

j O
: O
Sj=∑ O
iαi O
jxi O
j O
, O
( O
3 O
) O
where O
αi O
jis O
the O
attention O
score O
of O
each O
sentence O
vector O
xi O
j O
, O
deﬁned O
as O
: O
αi O
j O
= O
exp(ei O
j O
) O
∑ O
kexp(ek O
j O
) O
, O
( O
4 O
) O
where O
ei O
jis O
referred O
as O
a O
query O
- O
based O
function O
which O
scores O
how O
well O
the O
input O
sentence O
xi O
jre- O
ﬂects O
its O
labelled O
relation O
r. O

There O
are O
many O
ways O
to O
obtain O
ei O
j O
, O
and O
here O
we O
simply O
compute O
eias O
the O
inner O
product O
: O
ei O
j O
= O
xi O
j·rj O
. O

( O
5 O
) O
Here O
rjis O
the O
query O
vector O
of O
the O
relation O
rwith O
respect O
to O
the O
j O
- O
th O
language O
. O

3.2.2 O
Cross O
- O
lingual O
Attention O
Besides O
mono O
- O
lingual O
attention O
, O
we O
propose O
cross- O
lingual O
attention O
for O
neural B-TaskName
relation I-TaskName
extraction I-TaskName
to O
better O
take O
advantages O
of O
multi O
- O
lingual O
data O
. O

The O
key O
idea O
of O
cross O
- O
lingual O
attention O
is O
to O
em- O
phasize O
those O
sentences O
which O
have O
strong O
con- O
sistency O
among O
different O
languages O
. O

On O
the O
basis O
of O
mono O
- O
lingual O
attention O
, O
cross O
- O
lingual O
attention37 O

is O
capable O
of O
further O
removing O
unlikely O
sentences O
and O
resulting O
in O
more O
concentrated O
and O
informa- O
tive O
sentences O
, O
with O
the O
factor O
of O
consistent O
cor- O
respondence O
of O
relation O
patterns O
among O
different O
languages O
. O

Cross O
- O
lingual O
attention O
works O
similar O
to O
mono- O
lingual O
attention O
. O

Suppose O
jindicates O
a O
language O
andkis O
a O
another O
language O
( O
k̸=j O
) O
. O

Formally O
, O
the O
cross O
- O
lingual O
representation O
Sjkis O
deﬁned O
as O
a O
weighted O
sum O
of O
those O
sentence O
vectors O
xi O
jin O
the O
jth O
language O
: O
Sjk=∑ O
iαi O
jkxi O
j O
, O
( O
6 O
) O
where O
αi O
jkis O
the O
cross O
- O
lingual O
attention O
score O
of O
each O
sentence O
vector O
xi O
jwith O
respect O
to O
the O
kth O
lan- O
guage O
. O

The O
cross O
- O
lingual O
attention O
αi O
jkis O
deﬁned O
as O
: O
αi O
jk O
= O
exp(ei O
jk O
) O
∑ O
kexp(ek O
jk O
) O
, O
( O
7 O
) O
where O
ei O
jkis O
referred O
as O
a O
query O
- O
based O
function O
which O
scores O
the O
consistency O
between O
the O
input O
sentence O
xi O
jin O
the O
jth O
language O
and O
the O
relation O
patterns O
in O
the O
kth O
language O
for O
expressing O
the O
se- O
mantic O
meanings O
of O
the O
labelled O
relation O
r. O
Similar O
to O
the O
mono O
- O
lingual O
attention O
, O
we O
compute O
ei O
jkas O
follows O
: O
ei O
jk O
= O
xi O
j·rk O
, O
( O
8) O
where O
rkis O
the O
query O
vector O
of O
the O
relation O
rwith O
respect O
to O
the O
kth O
language O
. O

Note O
that O
, O
for O
convenience O
, O
we O
denote O
those O
mono O
- O
lingual O
attention O
vectors O
SjasSjjin O
the O
re- O
mainder O
of O
this O
paper O
. O

3.3 O
Prediction O
For O
each O
entity O
pair O
and O
its O
corresponding O
sentence O
setTinmlanguages O
, O
we O
can O
obtain O
m×mvec- O
tors{Sjk|j O
, O
k∈ O
{ O
1 O
, O
. O
. O
. O

, O
m O
} O
} O
from O
the O
neural O
net- O
works O
with O
multi O
- O
lingual O
attention O
. O

Those O
vectors O
with O
j O
= O
kare O
mono O
- O
lingual O
attention O
vectors O
, O
and O
those O
with O
j̸=kare O
cross O
- O
lingual O
attention O
vec- O
tors O
. O

We O
take O
all O
vectors O
{ O
Sjk}together O
and O
deﬁne O
the O
overall O
score O
function O
f(T O
, O
r)as O
follows O
: O
f(T O
, O
r O
) O
= O
∑ O
j O
, O
k∈{1, O
... O
,m}logp(r|Sjk O
, O
θ),(9 O
) O
where O
p(r|Sjk O
, O
θ)is O
the O
probability O
of O
predicting O
the O
relation O
rconditional O
on O
Sjk O
, O
computed O
usinga O
softmax O
layer O
as O
follows O
: O
p(r|Sjk O
, O
θ O
) O

= O
softmax O
( O
MS O
jk+d O
) O
, O
( O
10 O
) O
where O
d∈Rnris O
a O
bias O
vector O
, O
nris O
the O
number O
of O
relation O
types O
and O
M∈Rnr×Rcis O
a O
global O
relation O
matrix O
initialized O
randomly O
. O

To O
better O
consider O
the O
characteristics O
of O
each O
hu- O
man O
language O
, O
we O
further O
introduce O
Rkas O
the O
spe- O
ciﬁc O
relation O
matrix O
of O
the O
kth O
language O
. O

Here O
we O
simply O
deﬁne O
Rkas O
composed O
by O
rkin O
Eq O
. O

( O
8) O
. O

Hence O
, O
Eq O
. O

( O
10 O
) O
can O
be O
extended O
to O
: O
p(r|Sjk O
, O
θ O
) O
= O
softmax O

[ O
( O
Rk+M)Sjk+d],(11 O
) O
where O
Mencodes O
global O
patterns O
for O
predicting O
relations O
and O
Rkencodes O
those O
language O
- O
speciﬁc O
characteristics O
. O

Note O
that O
, O
in O
the O
training O
phase O
, O
the O
vectors O
{ O
Sjk}are O
constructed O
using O
Eq O
. O

( O
3 O
) O
and O
( O
6 O
) O
using O
the O
labelled O
relation O
. O

In O
the O
testing O
phase O
, O
since O
the O
relation O
is O
not O
known O
in O
advance O
, O
we O
will O
construct O
different O
vectors O
{ O
Sjk}for O
each O
possible O
relation O
r O
to O
compute O
f(T O
, O
r)for O
relation O
prediction O
. O

3.4 O
Optimization O
Here O
we O
introduce O
the O
learning O
and O
optimization O
details O
of O
our O
MNRE B-MethodName
framework O
. O

We O
deﬁne O
the O
objective O
function O
as O
follows O
: O
J(θ O
) O
= O
s∑ O
i=1f(Ti O
, O
ri O
) O
, O
( O
12 O
) O
where O
sindicates O
the O
number O
of O
all O
entity O
pairs O
with O
each O
corresponding O
to O
a O
sentence O
set O
in O
dif- O
ferent O
languages O
, O
and O
θindicates O
all O
parameters O
of O
our O
framework O
. O

To O
solve O
the O
optimization O
problem O
, O
we O
adopt O
mini O
- O
batch O
stochastic O
gradient O
descent O
( O
SGD O
) O
to O
minimize O
the O
objective O
function O
. O

For O
learning O
, O
we O
iterate O
by O
randomly O
selecting O
a O
mini O
- O
batch O
from O
the O
training O
set O
until O
converge O
. O

4 O
Experiments O
We O
ﬁrst O
introduce O
the O
datasets O
and O
evaluation O
met- O
rics O
used O
in O
the O
experiments O
. O

Next O
, O
we O
use O
a O
vali- O
dation O
set O
to O
determine O
the O
best O
model O
parameters O
and O
choose O
the O
best O
model O
via O
early O
stopping O
. O

Af- O
terwards O
, O
we O
show O
the O
effectiveness O
of O
our O
frame- O
work O
of O
considering O
pattern O
complementarity O
and O
consistency O
for O
multi O
- O
lingual O
relation O
extraction O
by O
quantitative O
and O
qualitative O
analysis O
. O

Finally O
, O
we O
compare O
the O
effect O
of O
two O
kinds O
of O
relation O
matri- O
ces O
in O
Eq O
. O

( O
11 O
) O
used O
for O
prediction.38 O

4.1 O
Datasets O
and O
Evaluation O
Metrics O
We O
generate O
a O
new O
multi O
- O
lingual O
relation O
extrac- O
tion O
dataset O
to O
evaluate O
our O
MNRE B-MethodName
framework O
. O

Without O
loss O
of O
generality O
, O
the O
experiments O
fo- O
cus O
on O
relation B-TaskName
extraction I-TaskName
from O
two O
languages O
in- O

cluding O
English O
and O
Chinese O
. O

In O
this O
dataset O
, O
the O
Chinese O
instances O
are O
generated O
by O
aligning O
Chinese O
Baidu O
Baike O
with O
Wikidata O
, O
and O
the O
En- O
glish O
instances O
are O
generated O
by O
aligning O
English O
Wikipedia O
articles O
with O
Wikidata O
. O

The O
relational O
facts O
of O
Wikidata O
in O
this O
dataset O
are O
divided O
into O
three O
parts O
for O
training O
, O
validation O
and O
testing O
re- O
spectively O
. O

There O
are O
176relations O
including O
a O
spe- O
cial O
relation O
NA O
indicating O
there O
is O
no O
relation O
be- O
tween O
entities O
. O

And O
we O
set O
both O
validation O
and O
test- O
ing O
sets O
for O
Chinese O
and O
English O
parts O
contain O
the O
same O
facts O
. O

We O
list O
the O
statistics O
about O
the O
dataset O
in O
Table O
2 O
. O

Dataset O
# O
Rel O
# O
Sent O
# O
Fact O
Train O
1,022,239 O
47,638 O
English O
Valid O
176 O
80,191 O
2,192 O
Test O
162,018 O
4,326 O
Train O
940,595 O
42,536 O
Chinese O
Valid O

176 O
82,699 O
2,192 O
Test O
167,224 O
4,326 O
Table O
2 O
: O
Statistics O
of O
the O
dataset O
. O

We O
follow O
previous O
works O
( O
Mintz O
et O
al O
. O
, O
2009 O
) O
and O
investigate O
the O
performance O
of O
RE O
systems O
us- O

ing O
the O
held O
- O
out O
evaluation O
, O
by O
comparing O
the O
re- O
lational O
facts O
discovered O
by O
RE O
systems O
from O
the O
testing O
set O
with O
those O
facts O
in O
KB O
. O

The O
evaluation O
method O
assumes O
that O
if O
a O
RE O
system O
accurately O
ﬁnds O
more O
relational O
facts O
in O
KBs O
from O
the O
test- O
ing O
set O
, O
it O
will O
achieve O
better O
performance O
for O
rela- B-TaskName
tion I-TaskName
extraction I-TaskName
. O

The O
held O
- O
out O
evaluation O
provides O
an O
approximate O
measure O
of O
RE B-TaskName
performance O
with- O
out O
time O
- O
consuming O
human O
evaluation O
. O

In O
experi- O
ments O
, O
we O
report O
the O
precision B-MetricName
/ O
recall B-MetricName
curves O
as O
the O
evaluation O
metric O
. O

4.2 O
Experimental O
Settings O
We O
tune O
the O
parameters O
of O
our O
MNRE B-MethodName
framework O
by O
grid O
searching O
using O
validation O
set O
. O

For O
train- O
ing O
, O
we O
set O
the O
iteration B-HyperparameterName
number O
over O
all O
the O
train- O
ing O
data O
as O
15 B-HyperparameterValue
. O

The O
best O
models O
were O
selected O
by O
early O
stopping O
using O
the O
evaluation O
results O
on O
the O
validation O
set O
. O

In O
Table O
3we O
show O
the O
best O
setting O
of O
all O
parameters O
used O
in O
our O
experiments O
. O

4.3 O
Effectiveness O
of O
Consistency O
To O
demonstrate O
the O
effectiveness O
of O
considering O
pattern O
consistency O
among O
languages O
, O
we O
empir- O
ically O
compare O
different O
methods O
through O
held O
- O
out O
evaluation O
. O

We O
select O
CNN O
proposed O
in O
( O
Zeng O
et O
al O
. O
, O
2014 O
) O
as O
our O
sentence O
encoder O
and O
imple- O
ment O
it O
by O
ourselves O
which O
achieves O
comparable O
results O
as O
the O
authors O
reported O
on O
their O
experimen- O
tal O
dataset O
NYT104 B-DatasetName
. O

And O
we O
compare O
the O
perfor- O
mance O
of O
our O
framework O
with O
the O
[ B-MethodName
P]CNN I-MethodName
model O
trained O
with O
only O
English O
data O
( O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
) O
, O
only O
Chinese O
data O
( O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
) O
, O
a O
joint O
model O
( O
[ B-MethodName
P]CNN+joint I-MethodName
) O
which O
predicts O
using O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
and O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
jointly O
, O
and O
another O
joint O
model O
with O
shared O
embeddings O
( O
[ B-MethodName
P]CNN+share I-MethodName
) O
which O
trains O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
and O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
with O
common O
relation O
embedding O
matrices O
. O

From O
Fig O
. O

2 O
, O
we O
have O
the O
following O
observa- O
tions O
: O
( O
1 O
) O
Both O
[ B-MethodName
P]CNN+joint I-MethodName
and O
[ B-MethodName
P]CNN+share I-MethodName
achieve O
better O
performances O
as O
compared O
to O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
and O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
. O

It O
indicates O
that O
uti- O
lizing O
Chinese O
and O
English O
sentences O
jointly O
is O
beneﬁcial O
to O
extracting O
novel O
relational O
facts O
. O

The O
reason O
is O
that O
those O
relational O
facts O
that O
are O
discov- O
ered O
from O
multiple O
languages O
are O
more O
reliable O
to O
be O
true O
. O

( O
2 O
) O
CNN+share B-MethodName
only O
has O
similar O
performance O
as O
compared O
to O
CNN+joint B-MethodName
, O
even O
through O
a O
bit O
worse O
when O
recall B-MetricName
ranges O
from O
0.1 B-MetricValue
to O
0.2 B-MetricValue
. O

Besides O
, O
PCNN+share B-MethodName
performs O
worse O
than O
PCNN+joint B-MethodName
nearly O
over O
the O
entire O
range O
of O
recall B-MetricName
. O

It O
demon- O
strates O
that O
a O
simple O
combination O
of O
multiple O
lan- O
guages O
by O
sharing O
relation O
embedding O
matrices O
can O
not O
further O
capture O
more O
implicit O
correlations O
among O
various O
languages O
. O

( O
3 O
) O
Our O
MNRE B-MethodName
model O
achieves O
the O
highest O
pre- O
cision O
over O
the O
entire O
range O
of O
recall O
as O
com- O
pared O
to O
other O
methods O
including O
[ B-MethodName
P]CNN+joint I-MethodName
and O
[ B-MethodName
P]CNN+share I-MethodName
models O
. O

By O
grid O
searching O
of O
parameters O
for O
these O
baseline O
models O
, O
we O
can O
ob- O
serve O
that O
both O
[ B-MethodName
P]CNN+joint I-MethodName
and O
[ B-MethodName
P]CNN+share I-MethodName
can O
not O
achieve O
competitive O
results O
compared O
to O
MNRE B-MethodName
even O
when O
increasing O
the O
size O
of O
the O
output O
layer O
. O

This O
indicates O
that O
no O
more O
useful O
informa- O
tion O
can O
be O
captured O
by O
simply O
increasing O
model O
size O
. O

On O
the O
contrary O
, O
our O
proposed O
MNRE B-MethodName
model O
can O
successfully O
improve O
multi B-TaskName
- I-TaskName
lingual I-TaskName
relation I-TaskName
ex- I-TaskName
traction I-TaskName
by O
considering O
pattern O
consistency O
among O
languages O
. O

We O
further O
give O
an O
example O
of O
cross O
- O
lingual O
at O
- O
tention O
in O
Table O
4 O
. O

It O
shows O
four O
sentences O
hav- O
ing O
the O
highest O
and O
lowest O
Chinese O
- O
to O
- O
English O
and O
English O
- O
to O
- O
Chinese O
attention O
weights O
respectively O
with O
respect O
to O
the O
relation O
PlaceOfBirth O
in O
MNRE B-MethodName
. O

We O
highlight O
the O
entity O
pairs O
in O
bold O
face O
. O

For O
comparison O
, O
we O
also O
show O
their O
attention O
weights O
from O
CNN+Zh B-MethodName
and O
CNN+En B-MethodName
. O

From O
the O
table O
we O
ﬁnd O
that O
, O
although O
all O
of O
the O
four O
sentences O
actually O
express O
the O
fact O
that O
Barzun O
was O
born O
in O
France O
, O
the O
ﬁrst O
and O
third O
sentences O
contain O
much O
more O
noisy O
information O
that O
may O
confuse O
RE B-TaskName
sys- O
tems O
. O

By O
considering O
pattern O
consistency O
between O
sentences O
in O
two O
languages O
with O
cross O
- O
lingual O
at- O
tention O
, O
MNRE B-MethodName
can O
identify O
the O
second O
and O
fourth O
sentences O
that O
unambiguously O
express O
the O
relation O
PlaceOfBirth O
with O
higher O
attention O
as O
com- O
pared O
to O
CNN+Zh B-MethodName
and O
CNN+En B-MethodName
. O

4.4 O
Effectiveness O
of O
Complementarity O
To O
demonstrate O
the O
effectiveness O
of O
consider- O
ing O
pattern O
complementarity O
among O
languages O
, O
we O
empirically O
compare O
the O
following O
methods O
through O
held O
- O
out O
evaluation O
: O
MNRE B-MethodName
for I-MethodName
English I-MethodName
( O
MNRE B-MethodName
- I-MethodName
En I-MethodName
) O
and O
MNRE B-MethodName
for I-MethodName
Chinese I-MethodName
( O
MNRE B-MethodName
- I-MethodName
Zh I-MethodName
) O
which O
only O
use O
the O
mono O
- O
lingual O
vectors O
to O
predict O
relations O
, O
and O
[ O
P]CNN O
- O
En O
and O
[ O
P]CNN O
- O
Zh O
mod- O
els O
. O

Fig O
. O

3shows O
the O
aggregated O
precision O
/ O
recall O
curves O
of O
the O
four O
models O
for O
both O
CNN O
and O
PCNN O
. O

From O
the O
ﬁgure O
, O
we O
ﬁnd O
that O
: O
( O
1 O
) O
MNRE B-MethodName
- I-MethodName
En I-MethodName
and O
MNRE B-MethodName
- I-MethodName
Zh I-MethodName
outperform O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
and O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
almost O
in O
entire O
range O
of O
recall B-MetricName
. O

It O
indicates O
that O
by O
jointly O
training O
with O
multi O
- O
lingual O
attention O
, O
both O
Chinese O
and O
English O
relation O
extractors O
are O
beneﬁcial O
from O
those O
sentences O
from O
the O
other O
language O
. O

( O
2 O
) O
Although O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
underperforms O
as O
compared O
to O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
, O
MNRE B-MethodName
- I-MethodName
En I-MethodName
is O
compara- O
ble O
to O
MNRE B-MethodName
- I-MethodName
Zh I-MethodName
by O
jointly O
training O
through O
multi- O
lingual O
attention O
. O

It O
demonstrates O
that O
both O
Chi- O
nese O
and O
English O
relation O
extractors O
can O
take O
full O
advantages O
of O
texts O
in O
both O
languages O
via O
our O
pro- O
pose O
multi O
- O
lingual O
attention O
scheme O
. O

Table O
5shows O
the O
detailed O
results O
( O
in O
preci- B-MetricName
sion I-MetricName
@ O
1 B-MetricValue
) O
of O
some O
speciﬁc O
relations O
of O
which O
the O
training O
instances O
are O
un O
- O
balanced O
on O
English O
and O
Chinese O
sides O
. O

From O
the O
table O
, O
we O
can O
see O
that O
: O
( O
1 O
) O
For O
the O
relation O
Contains O
of O
which O
the O
number O
of O
English O
training O
instances O
is O
only O
1/7 O
of O
Chinese O
ones O
, O
CNN B-MethodName
- I-MethodName
En I-MethodName
gets O
much O
worse O
per- O
formance O
as O
compared O
to O
CNN B-MethodName
- I-MethodName
Zh I-MethodName
due O
to O
the O
lack O
of O
training O
data O
. O

Nevertheless O
, O
by O
jointly O
training O
through O
multi O
- O
lingual O
attention O
, O
MNRE(CNN)- B-MethodName
En I-MethodName
is O
comparable O
to O
and O
slightly O
better O
than O
MNRE(CNN)-Zh B-MethodName
. O

( O
2 O
) O
For O
the O
relation O
HeadquartersLoca- O
tion O
of O
which O
the O
number O
of O
Chinese O
training O
in- O
stances O
is O
only O
1/9of O
English O
ones O
, O
CNN B-MethodName
- I-MethodName
Zh I-MethodName
even O
can O
not O
predict O
any O
correct O
results O
. O

The O
reason O
is O
perhaps O
that O
, O
CNN B-MethodName
- I-MethodName
Zh I-MethodName
of O
the O
relation O
is O
not O
suf- O
ﬁciently O
trained O
because O
there O
are O
only O
210Chi- O
nese O
training O
instances O
for O
this O
relation O
. O

Simi- O
larly O
, O
by O
jointly O
training O
through O
multi O
- O
lingual O
at- O
tention O
, O
MNRE(CNN)-En B-MethodName
and O
MNRE(CNN)-Zh B-MethodName
both O
achieve O
promising O
results O
. O

( O
3 O
) O
For O
the O
relations O
Father O
andCountry- O
OfCitizenship O
of O
which O
the O
sentence O
number O
in O
English O
and O
Chinese O
are O
not O
so O
un O
- O
balanced O
, O
our O
MNRE B-MethodName
can O
still O
improve O
the O
performance O
of O
rela- O
tion O
extraction O
on O
both O
English O
and O
Chinese O
sides O
. O

4.5 O
Comparison O
of O
Relation O
Matrix O
For O
relation O
prediction O
, O
we O
use O
two O
kinds O
of O
re- O
lation O
matrices O
including O
: O
Mthat O
considers O
the O
global O
consistency O
of O
relations O
, O
and O
Rthat O
consid- O
ers O
the O
speciﬁc O
characteristics O
of O
relations O
for O
each O
language O
. O

To O
measure O
the O
effect O
of O
the O
two O
relation O
matrices O
, O
we O
compare O
the O
performance O
of O
MNRE B-MethodName
using O
the O
both O
matrices O
with O
those O
only O
using O
M O
( O
MNRE B-MethodName
- I-MethodName
M I-MethodName
) O
and O
only O
using O
R( O
MNRE B-MethodName
- I-MethodName
R I-MethodName
) O
. O

Fig O
. O

4shows O
the O
precision O
- O
recall O
curves O
for O
each O
method O
. O

From O
the O
ﬁgure O
, O
we O
observe O
that O
: O
t O
( O
1 O
) O
The O
performance O
of O
MNRE B-MethodName
- I-MethodName
M I-MethodName
is O
much O
worse O
than O
both O
MNRE B-MethodName
- I-MethodName
R I-MethodName
and O
MNRE B-MethodName
. O

It O
indicates O
that O
we O
can O
not O
just O
use O
global O
relation O
matrix O
for O
relation O
prediction O
. O

The O
reason O
is O
that O
each O
lan- O
guage O
has O
its O
own O
speciﬁc O
characteristics O
to O
ex- O
press O
relation O
patterns O
, O
which O
can O
not O
be O
well O
in- O
tegrated O
into O
a O
single O
relation O
matrix O
. O

( O
2 O
) O
MNRE(CNN)-R B-MethodName
has O
similar O
performance O
as O
compared O
to O
MNRE(CNN B-MethodName
) I-MethodName
when O
the O
recall O
is O
low O
. O

However O
, O
it O
has O
a O
sharp O
decline O
when O
the O
recall B-MetricName
reaches O
0.25 B-MetricValue
. O

It O
suggests O
there O
also O
exists O
global O
consistency O
of O
relation O
patterns O
among O
languages O
which O
can O
not O
be O
neglected O
. O

Hence O
, O
we O
should O
combine O
both O
MandRtogether O
for O
multi B-TaskName
- I-TaskName
lingual I-TaskName
relation I-TaskName
extraction I-TaskName
, O
as O
proposed O
in O
our O
MNRE B-MethodName
framework O
. O

5 O
Conclusion O
In O
this O
paper O
, O
we O
introduce O
a O
neural B-TaskName
relation I-TaskName
extrac- I-TaskName
tion I-TaskName
framework O
with O
multi O
- O
lingual O
attention O
to O
take O
pattern O
consistency O
and O
complementarity O
among O
multiple O
languages O
into O
consideration O
. O

We O
evalu- O
ate O
our O
framework O
on O
multi B-TaskName
- I-TaskName
lingual I-TaskName
relation I-TaskName
extrac- I-TaskName
tion I-TaskName
task O
, O
and O
the O
results O
show O
that O
our O
framework O
can O
effectively O
model O
relation O
patterns O
among O
lan- O
guages O
and O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
. O

We O
will O
explore O
the O
following O
directions O
as O
fu- O
ture O
work O
: O
( O
1 O
) O
In O
this O
paper O
, O
we O
only O
consider O
sentence O
- O
level O
multi O
- O
lingual O
attention O
for O
relation B-TaskName
extraction I-TaskName
. O

In O
fact O
, O
we O
ﬁnd O
that O
the O
word O
alignment O
information O
may O
be O
also O
helpful O
for O
capturing O
rela- O
tion O
patterns O
. O

Hence O
, O
the O
word O
- O
level O
multi O
- O
lingual O
attention O
, O
which O
may O
discover O
implicit O
alignments O
between O
words O
in O
multiple O
languages O
, O
will O
fur- O
ther O
improve O
multi B-TaskName
- I-TaskName
lingual I-TaskName
relation I-TaskName
extraction I-TaskName
. O

We O
will O
explore O
the O
effectiveness O
of O
word O
- O
level O
multi- O
lingual O
attention O
for O
relation B-TaskName
extraction I-TaskName
as O
our O
fu O
- O
ture O
work O
. O

( O
2 O
) O
MNRE B-MethodName
can O
be O
ﬂexibly O
implemented O
in O
the O
scenario O
of O
multiple O
languages O
, O
and O
this O
pa- O
per O
focuses O
on O
two O
languages O
of O
English O
and O
Chi- O
nese O
. O

In O
future O
, O
we O
will O
extend O
MNRE B-MethodName
to O
more O
lan- O
guages O
and O
explore O
its O
signiﬁcance O
. O

Acknowledgments O
This O
work O
is O
supported O
by O
the O
973 O
Program O
( O
No O
. O
2014CB340501 O
) O
, O
the O
National O
Natu- O
ral O
Science O
Foundation O
of O
China O
( O
NSFC O
No O
. O
61572273 O
, O
61532010 O
) O
, O
and O
the O
Key O
Technologies O
Research O
and O
Development O
Program O
of O
China O
( O
No O
. O
2014BAK04B03 O
) O
. O

This O
work O
is O
also O
funded O
by O
the O
Natural O
Science O
Foundation O
of O
China O
( O
NSFC O
) O
and O
the O
German O
Research O
Foundation(DFG O
) O
in O
Project O
Crossmodal O
Learning O
, O
NSFC O
61621136008 O
/ O
DFC O
TRR-169 O
. O

References O
Erik O
Boiy O
and O
Marie O
- O
Francine O
Moens O
. O

2009 O
. O

A O
machine O
learning O
approach O
to O
sentiment O
analysis O
in O
multilingual O
web O
texts O
. O

Information O
retrieval O
12(5):526–558 O
. O

Florian O
Boudin O
, O
Stéphane O
Huet O
, O
and O
Juan O
- O
Manuel O
Torres O
- O
Moreno O
. O
2011 O
. O

A O
graph O
- O
based O
approach O
to O
cross O
- O
language O
multi O
- O
document O
summarization O
. O

Polibits O
( O
43):113–118 O
. O
Kyunghyun O
Cho O
, O
Bart O
Van O
Merriënboer O
, O
Dzmitry O
Bah- O
danau O
, O
and O
Yoshua O
Bengio O
. O

2014 O
. O

On O
the O
properties O
of O
neural O
machine O
translation O
: O
Encoder O
- O
decoder O
ap- O
proaches O
. O

arXiv O
preprint O
arXiv:1409.1259 O
. O

Meiping O
Dong O
, O
Yong O
Cheng O
, O
Yang O
Liu O
, O
Jia O
Xu O
, O
Maosong O
Sun O
, O
Tatsuya O
Izuha O
, O
and O
Jie O
Hao O
. O
2014 O
. O

Query O
lattice O
for O
translation O
retrieval O
. O

In O
Proceed- O
ings O
of O
COLING O
. O

pages O
2031–2041 O
. O

Cıcero O
Nogueira O
dos O
Santos O
, O
Bing O
Xiang O
, O
and O
Bowen O
Zhou O
. O

2015 O
. O

Classifying O
relations O
by O
ranking O
with O
convolutional O
neural O
networks O
. O

In O
Proceedings O
of O
ACL O
. O

volume O
1 O
, O
pages O
626–634 O
. O

Manaal O
Faruqui O
and O
Shankar O
Kumar O
. O
2015 O
. O

Multilin- O
gual O
open O
relation O
extraction O
using O
cross O
- O
lingual O
pro- O
jection O
. O

arXiv O
preprint O
arXiv:1503.06450 O
. O

Matthew O
R. O
Gormley O
, O
Mo O
Yu O
, O
and O
Mark O
Dredze O
. O
2015 O
. O

Improved O
relation O
extraction O
with O
feature O
- O
rich O
com- O
positional O
embedding O
models O
. O

In O
Proceedings O
of O
EMNLP O
. O

pages O
1774–1784 O
. O

Sepp O
Hochreiter O
and O
Jürgen O
Schmidhuber O
. O

1997 O
. O

Long O
short O
- O
term O
memory O
. O

Neural O
Computation O
pages O
1735–1780.42 O

Raphael O
Hoffmann O
, O
Congle O
Zhang O
, O
Xiao O
Ling O
, O
Luke O
Zettlemoyer O
, O
and O
Daniel O
S O
Weld O
. O

2011 O
. O

Knowledge- O
based O
weak O
supervision O
for O
information O
extraction O
of O
overlapping O
relations O
. O

In O
Proceedings O
of O
ACL O
- O
HLT O
. O

pages O
541–550 O
. O

Yankai O
Lin O
, O
Shiqi O
Shen O
, O
Zhiyuan O
Liu O
, O
Huanbo O
Luan O
, O
and O
Maosong O
Sun O
. O
2016 O
. O

Neural O
relation O
extraction O
with O
selective O
attention O
over O
instances O
. O

In O
Proceed- O
ings O
of O
ACL O
. O

volume O
1 O
, O
pages O
2124–2133 O
. O

Mike O
Mintz O
, O
Steven O
Bills O
, O
Rion O
Snow O
, O
and O
Dan O
Juraf- O
sky O
. O

2009 O
. O

Distant O
supervision O
for O
relation O
extrac- O
tion O
without O
labeled O
data O
. O

In O
Proceedings O
of O
ACL- O
IJCNLP O
. O

pages O
1003–1011 O
. O

Sebastian O
Riedel O
, O
Limin O
Yao O
, O
and O
Andrew O
McCallum O
. O

2010 O
. O

Modeling O
relations O
and O
their O
mentions O
without O
labeled O
text O
. O

In O
Proceedings O
of O
ECML O
- O
PKDD O
. O

pages O
148–163 O
. O

Richard O
Socher O
, O
Brody O
Huval O
, O
Christopher O
D O
Manning O
, O
and O
Andrew O
Y O
Ng O
. O
2012 O
. O

Semantic O
compositionality O
through O
recursive O
matrix O
- O
vector O
spaces O
. O

In O
Proceed- O
ings O
of O
EMNLP O
- O
CoNLL O
. O
pages O
1201–1211 O
. O

Nitish O
Srivastava O
, O
Geoffrey O
Hinton O
, O
Alex O
Krizhevsky O
, O
Ilya O
Sutskever O
, O
and O
Ruslan O
Salakhutdinov O
. O

2014 O
. O

Dropout O
: O
A O
simple O
way O
to O
prevent O
neural O
networks O
from O
overﬁtting O
. O

JMLR O
15(1):1929–1958 O
. O

Mihai O
Surdeanu O
, O
Julie O
Tibshirani O
, O
Ramesh O
Nallapati O
, O
and O
Christopher O
D O
Manning O
. O

2012 O
. O

Multi O
- O
instance O
multi O
- O
label O
learning O
for O
relation O
extraction O
. O

In O
Pro- O
ceedings O
of O
EMNLP O
. O
pages O
455–465 O
. O

Patrick O
Verga O
, O
David O
Belanger O
, O
Emma O
Strubell O
, O
Ben- O
jamin O
Roth O
, O
and O
Andrew O
McCallum O
. O
2015 O
. O

Multi- O
lingual O
relation O
extraction O
using O
compositional O
uni- O
versal O
schema O
. O

arXiv O
preprint O
arXiv:1511.06396 O
. O

Dmitry O
Zelenko O
, O
Chinatsu O
Aone O
, O
and O
Anthony O
Richardella O
. O
2003 O
. O

Kernel O
methods O
for O
relation O
extraction O
. O

JMLR O
3(Feb):1083–1106 O
. O

Daojian O
Zeng O
, O
Kang O
Liu O
, O
Yubo O
Chen O
, O
and O
Jun O
Zhao O
. O
2015 O
. O

Distant O
supervision O
for O
relation O
extraction O
via O
piecewise O
convolutional O
neural O
networks O
. O

In O
Pro- O
ceedings O
of O
EMNLP O
. O

Daojian O
Zeng O
, O
Kang O
Liu O
, O
Siwei O
Lai O
, O
Guangyou O
Zhou O
, O
and O
Jun O
Zhao O
. O

2014 O
. O

Relation O
classiﬁcation O
via O
con- O
volutional O
deep O
neural O
network O
. O

In O
Proceedings O
of O
COLING O
. O

pages O
2335–2344.43 O

