Addressing O
the O
Rare O
Word O
Problem O
in O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName
Minh O
- O
Thang O
Luong† O
∗ O
Stanford O
lmthang@stanford.edu O

Ilya O
Sutskever† O
GoogleQuoc O
V O
. O

Le† O
Google O
{ O
ilyasu O
, O
qvl O
, O
vinyals O
} O
@google.comOriol O
Vinyals O
GoogleWojciech O
Zaremba∗ O

New O
York O
University O
woj.zaremba@gmail.com O

Abstract O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName
( O
NMT B-TaskName
) O
is O
a O
new O
approach O
to O
machine B-TaskName
translation I-TaskName
that O
has O
shown O
promising O
results O
that O
are O
com- O
parable O
to O
traditional O
approaches O
. O

A O
sig- O
niﬁcant O
weakness O
in O
conventional O
NMT O
systems O
is O
their O
inability O
to O
correctly O
trans- O
late O
very O
rare O
words O
: O
end O
- O
to O
- O
end O
NMTs O
tend O
to O
have O
relatively O
small O
vocabularies O
with O
a O
single O
unk O
symbol O
that O
represents O
every O
possible O
out O
- O
of O
- O
vocabulary O
( O
OOV O
) O
word O
. O

In O
this O
paper O
, O
we O
propose O
and O
im- O
plement O
an O
effective O
technique O
to O
address O
this O
problem O
. O

We O
train O
an O
NMT O
system O
on O
data O
that O
is O
augmented O
by O
the O
output O
of O
a O
word O
alignment O
algorithm O
, O
allowing O
the O
NMT O
system O
to O
emit O
, O
for O
each O
OOV O
word O
in O
the O
target O
sentence O
, O
the O
position O
of O
its O
corresponding O
word O
in O
the O
source O
sen- O
tence O
. O

This O
information O
is O
later O
utilized O
in O
a O
post O
- O
processing O
step O
that O
translates O
every O
OOV O
word O
using O
a O
dictionary O
. O

Our O
exper- O
iments O
on O
the O
WMT’14 B-DatasetName
English I-DatasetName
to I-DatasetName
French I-DatasetName
translation O
task O
show O
that O
this O
method O
pro- O
vides O
a O
substantial O
improvement O
of O
up O
to O
2.8 B-MetricValue
BLEU B-MetricName
points O
over O
an O
equivalent O
NMT O
system O
that O
does O
not O
use O
this O
technique O
. O

With O
37.5 B-MetricValue
BLEU B-MetricName
points O
, O
our O
NMT O
sys- O
tem O
is O
the O
ﬁrst O
to O
surpass O
the O
best O
result O
achieved O
on O
a O
WMT’14 B-DatasetName
contest O
task O
. O

1 O
Introduction O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName
( O
NMT B-TaskName
) O
is O
a O
novel O
ap- O
proach O
to O
MT B-TaskName
that O
has O
achieved O
promising O
results O
( O
Kalchbrenner O
and O
Blunsom O
, O
2013 O
; O

Sutskever O
et O
al O
. O
, O
2014 O
; O
Cho O
et O
al O
. O
, O
2014 O
; O
Bahdanau O
et O
al O
. O
, O
2015 O
; O
Jean O
et O
al O
. O
, O
2015 O
) O
. O

An O
NMT O
system O
is O
a O
conceptu- O
ally O
simple O
large O
neural O
network O
that O
reads O
the O
en- O
∗Work O
done O
while O
the O
authors O
were O
in O
Google O
. O

†indicates O
equal O
contribution.tire O
source O
sentence O
and O
produces O
an O
output O
trans- O
lation O
one O
word O
at O
a O
time O
. O

NMT O
systems O
are O
ap- O
pealing O
because O
they O
use O
minimal O
domain O
knowl- O
edge O
which O
makes O
them O
well O
- O
suited O
to O
any O
prob- O
lem O
that O
can O
be O
formulated O
as O
mapping O
an O
input O
sequence O
to O
an O
output O
sequence O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
. O

In O
addition O
, O
the O
natural O
ability O
of O
neural O
networks O
to O
generalize O
implies O
that O
NMT O
systems O
will O
also O
generalize O
to O
novel O
word O
phrases O
and O
sen- O
tences O
that O
do O
not O
occur O
in O
the O
training O
set O
. O

In O
addi- O
tion O
, O
NMT O
systems O
potentially O
remove O
the O
need O
to O
store O
explicit O
phrase O
tables O
and O
language O
models O
which O
are O
used O
in O
conventional O
systems O
. O

Finally O
, O
the O
decoder O
of O
an O
NMT O
system O
is O
easy O
to O
imple- O
ment O
, O
unlike O
the O
highly O
intricate O
decoders O
used O
by O
phrase O
- O
based O
systems O
( O
Koehn O
et O
al O
. O
, O
2003 O
) O
. O

Despite O
these O
advantages O
, O
conventional O
NMT O
systems O
are O
incapable O
of O
translating O
rare O
words O
be- O

cause O
they O
have O
a O
ﬁxed O
modest O
- O
sized O
vocabulary1 O
which O
forces O
them O
to O
use O
the O
unk O
symbol O
to O
repre- O
sent O
the O
large O
number O
of O
out O
- O
of O
- O
vocabulary O
( O
OOV O
) O
words O
, O
as O
illustrated O
in O
Figure O
1 O
. O

Unsurpris- O
ingly O
, O
both O
Sutskever O
et O
al O
. O

( O
2014 O
) O
and O
Bahdanau O
et O

al O
. O

( O
2015 O
) O
have O
observed O
that O
sentences O
with O
many O
rare O
words O
tend O
to O
be O
translated O
much O
more O
poorly O
than O
sentences O
containing O
mainly O
frequent O
words O
. O

Standard O
phrase O
- O
based O
systems O
( O
Koehn O
et O
al O
. O
, O
2007 O
; O
Chiang O
, O
2007 O
; O
Cer O
et O
al O
. O
, O
2010 O
; O
Dyer O
et O
al O
. O
, O
2010 O
) O
, O
on O
the O
other O
hand O
, O
do O
not O
suffer O
from O
the O
rare O
word O
problem O
to O
the O
same O
extent O
because O
they O
can O
support O
a O
much O
larger O
vocabulary O
, O
and O
because O
their O
use O
of O
explicit O
alignments O
and O
phrase O
tables O
allows O
them O
to O
memorize O
the O
translations O
of O
even O
extremely O
rare O
words O
. O

Motivated O
by O
the O
strengths O
of O
standard O
phrase- O
based O
system O
, O
we O
propose O
and O
implement O
a O
novel O
approach O
to O
address O
the O
rare O
word O
problem O
of O
NMTs B-TaskName
. O

Our O
approach O
annotates O
the O
training O
cor- O
pus O
with O
explicit O
alignment O
information O
that O
en- O
ables O
the O
NMT O
system O
to O
emit O
, O
for O
each O
OOV O
word O
, O
a O
“ O
pointer O
” O
to O
its O
corresponding O
word O
in O
the O
source O
sentence O
. O

This O
information O
is O
later O
utilized O
in O
a O
post O
- O
processing O
step O
that O
translates O
the O
OOV O
words O
using O
a O
dictionary O
or O
with O
the O
identity O
trans- O
lation O
, O
if O
no O
translation O
is O
found O
. O

Our O
experiments O
conﬁrm O
that O
this O
approach O
is O
effective O
. O

On O
the O
English B-DatasetName
to I-DatasetName
French I-DatasetName
WMT’14 I-DatasetName
translation O
task O
, O
this O
approach O
provides O
an O
im- O
provement O
of O
up O
to O
2.8 B-MetricValue
( O
if O
the O
vocabulary O
is O
rel- O
atively O
small O
) O
BLEU B-MetricName
points O
over O
an O
equivalent O
NMT O
system O
that O
does O
not O
use O
this O
technique O
. O

Moreover O
, O
our O
system O
is O
the O
ﬁrst O
NMT O
that O
out- O
performs O
the O
winner O
of O
a O
WMT’14 B-DatasetName
task O
. O

2 O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName
A O
neural O
machine O
translation O
system O
is O
any O
neural O
network O
that O
maps O
a O
source O
sentence O
, O
s1 O
, O
. O
. O
. O

, O
s O
n O
, O
to O
a O
target O
sentence O
, O
t1 O
, O
. O

. O
. O

, O
t O
m O
, O
where O
all O
sen- O
tences O
are O
assumed O
to O
terminate O
with O
a O
special O
“ O
end O
- O
of O
- O
sentence O
” O
token O
< O
eos O
> O
. O

More O
con- O
cretely O
, O
an O
NMT O
system O
uses O
a O
neural O
network O
to O
parameterize O
the O
conditional O
distributions O
p(tj|t O
< O
j O
, O
s≤n O
) O
( O
1 O
) O
for1≤j≤m O
. O

By O
doing O
so O
, O
it O
becomes O
pos- O
sible O
to O
compute O
and O
therefore O
maximize O
the O
log O
probability O
of O
the O
target O
sentence O
given O
the O
source O
sentence O
logp(t|s O
) O

= O
m O
/ O
summationdisplay O
j=1logp(tj|t O
< O
j O
, O
s≤n O
) O
( O
2 O
) O
There O
are O
many O
ways O
to O
parameterize O
these O
con- O
ditional O
distributions O
. O

For O
example O
, O
Kalchbrennerand O
Blunsom O
( O
2013 O
) O
used O
a O
combination O
of O
a O
con- O
volutional O
neural O
network O
and O
a O
recurrent O
neural O
network O
, O
Sutskever O
et O
al O
. O

( O
2014 O
) O
used O
a O
deep O
Long O
Short O
- O
Term O
Memory O
( O
LSTM O
) O
model O
, O
Cho O
et O
al O
. O

( O
2014 O
) O
used O
an O
architecture O
similar O
to O
the O
LSTM O
, O
and O
Bahdanau O
et O

al O
. O

( O
2015 O
) O
used O
a O
more O
elabo- O
rate O
neural O
network O
architecture O
that O
uses O
an O
atten- O
tional O
mechanism O
over O
the O
input O
sequence O
, O
similar O
to O
Graves O
( O
2013 O
) O
and O
Graves O
et O

al O
. O
( O
2014 O
) O
. O

In O
this O
work O
, O
we O
use O
the O
model O
of O
Sutskever O
et O
al O
. O

( O
2014 O
) O
, O
which O
uses O
a O
deep O
LSTM O
to O
encode O
the O
input O
sequence O
and O
a O
separate O
deep O
LSTM O
to O
out- O
put O
the O
translation O
. O

The O
encoder O
reads O
the O
source O
sentence O
, O
one O
word O
at O
a O
time O
, O
and O
produces O
a O
large O
vector O
that O
represents O
the O
entire O
source O
sentence O
. O

The O
decoder O
is O
initialized O
with O
this O
vector O
and O
gen- O
erates O
a O
translation O
, O
one O
word O
at O
a O
time O
, O
until O
it O
emits O
the O
end O
- O
of O
- O
sentence O
symbol O
< O
eos O
> O
. O

None O
the O
early O
work O
in O
neural O
machine O
transla- O
tion O
systems O
has O
addressed O
the O
rare O
word O
problem O
, O
but O
the O
recent O
work O
of O
Jean O
et O
al O
. O
( O
2015 O
) O
has O
tack- O
led O
it O
with O
an O
efﬁcient O
approximation O
to O
the O
soft- O
max O
to O
accommodate O
for O
a O
very O
large O
vocabulary O
( O
500 O
K O
words O
) O
. O

However O
, O
even O
with O
a O
large O
vocab- O
ulary O
, O
the O
problem O
with O
rare O
words O
, O
e.g. O
, O
names O
, O
numbers O
, O
etc O
. O
, O
still O
persists O
, O
and O
Jean O
et O
al O
. O

( O
2015 O
) O
found O
that O
using O
techniques O
similar O
to O
ours O
are O
beneﬁcial O
and O
complementary O
to O
their O
approach O
. O

3 O
Rare O
Word O
Models O
Despite O
the O
relatively O
large O
amount O
of O
work O
done O
on O
pure O
neural O
machine O
translation O
systems O
, O
there O
has O
been O
no O
work O
addressing O
the O
OOV O
problem O
in O
NMT O
systems O
, O
with O
the O
notable O
exception O
of O
Jean O
et O
al O
. O

( O
2015 O
) O
’s O
work O
mentioned O
earlier O
. O

We O
propose O
to O
address O
the O
rare O
word O
problem O
by O
training O
the O
NMT O
system O
to O
track O
the O
origins O
of O
the O
unknown O
words O
in O
the O
target O
sentences O
. O

If O
we O
knew O
the O
source O
word O
responsible O
for O
each O
un- O
known O
target O
word O
, O
we O
could O
introduce O
a O
post- O
processing O
step O
that O
would O
replace O
each O
unk O
in O
the O
system O
’s O
output O
with O
a O
translation O
of O
its O
source O
word O
, O
using O
either O
a O
dictionary O
or O
the O
identity O
translation O
. O

For O
example O
, O
in O
Figure O
1 O
, O
if O
the O
model O
knows O
that O
the O
second O
unknown O
token O
in O
the O
NMT O
( O
line O
nn O
) O
originates O
from O
the O
source O
word O
ecotax O
, O
it O
can O
perform O
a O
word O
dictionary O
lookup O
to O
replace O
that O
unknown O
token O
by O
´ O
ecotaxe O
. O

Sim- O

ilarly O
, O
an O
identity O
translation O
of O
the O
source O
word O
Pont O
- O
de O
- O
Buis O
can O
be O
applied O
to O
the O
third O
un- O
known O
token O
. O

We O
present O
three O
annotation O
strategies O
that O
can O
easily O
be O
applied O
to O
any O
NMT O
system O
( O
Kalchbren- O
ner O
and O
Blunsom O
, O
2013 O
; O
Sutskever O
et O
al O
. O
, O
2014 O
; O
Cho O
et O
al O
. O
, O
2014 O
) O
. O

We O
treat O
the O
NMT O
system O
as O
a O
black O
box O
and O
train O
it O
on O
a O
corpus O
annotated O
by O
one O
of O
the O
models O
below O
. O

First O
, O
the O
alignments O
are O
produced O
with O
an O
unsupervised O
aligner O
. O

Next O
, O
we O
use O
the O
alignment O
links O
to O
construct O
a O
word O
dictio- O

nary O
that O
will O
be O
used O
for O
the O
word O
translations O
in O
the O
post O
- O
processing O
step.2If O
a O
word O
does O
not O
ap- O
pear O
in O
our O
dictionary O
, O
then O
we O
apply O
the O
identity O
translation O
. O

The O
ﬁrst O
few O
words O
of O
the O
sentence O
pair O
in O
Fig- O
ure O
1 O
( O
lines O
enandfr O
) O
illustrate O
our O
models O
. O

3.1 O
Copyable B-MethodName
Model O
In O
this O
approach O
, O
we O
introduce O
multiple O
tokens O
to O
represent O
the O
various O
unknown O
words O
in O
the O
source O
and O
in O
the O
target O
language O
, O
as O
opposed O
to O
using O
only O
one O
unk O
token O
. O

We O
annotate O
the O
OOV O
words O
in O
the O
source O
sentence O
with O
unk O
1,unk O
2 O
, O
unk O
3 O
, O
in O
that O
order O
, O
while O
assigning O
repeating O
un- O
known O
words O
identical O
tokens O
. O

The O
annotation O
of O
the O
unknown O
words O
in O
the O
target O
language O
is O
slightly O
more O
elaborate O
: O
( O
a O
) O
each O
unknown O
target O
word O
that O
is O
aligned O
to O
an O
unknown O
source O
word O
is O
assigned O
the O
same O
unknown O
token O
( O
hence O
, O
the O
“ O
copy O
” O
model O
) O
and O
( O
b O
) O
an O
unknown O
target O
word O
that O
has O
no O
alignment O
or O
that O
is O
aligned O
with O
a O
known O
word O
uses O
the O
special O
null O
token O
unk O
∅. O

See O
Figure O
2 O
for O
an O
example O
. O

This O
annotation O
enables O
us O
to O
translate O
every O
non O
- O
null O
unknown O
token O
. O

3.2 O
Positional B-MethodName
All I-MethodName
Model O
( O
PosAll B-MethodName
) O
The O
copyable B-MethodName
model O
is O
limited O
by O
its O
inability O
to O
translate O
unknown O
target O
words O
that O
are O
aligned O
toknown O
words O
in O
the O
source O
sentence O
, O
such O
as O
the O
pair O
of O
words O
, O
“ O
portico O
” O
and O
“ O
portique O
” O
, O
in O
our O
running O
example O
. O

The O
former O
word O
is O
known O
on O
the O
source O
sentence O
; O
whereas O
latter O
is O
not O
, O
so O
it O
is O
labelled O
with O
unk O
∅. O

This O
happens O
often O
since O
the O
source O
vocabularies O
of O
our O
models O
tend O
to O
be O
much O
larger O
than O
the O
target O
vocabulary O
since O
a O
large O
source O
vocabulary O
is O
cheap O
. O

This O
limita- O
tion O
motivated O
us O
to O
develop O
an O
annotation O
model O
that O
includes O
the O
complete O
alignments O
between O
the O
source O
and O
the O
target O
sentences O
, O
which O
is O
straight- O
forward O
to O
obtain O
since O
the O
complete O
alignments O
are O
available O
at O
training O
time O
. O

Speciﬁcally O
, O
we O
return O
to O
using O
only O
a O
single O
universal O
unk O
token O
. O

However O
, O
on O
the O
target O
side O
, O
we O
insert O
a O
positional O
token O
pdafter O
ev- O
ery O
word O
. O

Here O
, O
dindicates O
a O
relative O
position O
( O
d=−7 O
, O
. O
. O
. O

, O
−1,0,1 O
, O
. O
. O

. O
, O
7 O
) O
to O
denote O
that O
a O
tar- O
get O
word O
at O
position O
jis O
aligned O
to O
a O
source O
word O
at O
position O
i O
= O
j−d O
. O

Aligned O
words O
that O
are O
too O
far O
apart O
are O
considered O
unaligned O
, O
and O
unaligned O
words O

rae O
annotated O
with O
a O
null O
token O
pn O
. O

Our O
an- O
notation O
is O
illustrated O
in O
Figure O
3 O
. O
3.3 O
Positional B-MethodName
Unknown I-MethodName
Model O
( O
PosUnk B-MethodName
) O

The O
main O
weakness O
of O
the O
PosAll B-MethodName
model O
is O
that O
it O
doubles O
the O
length O
of O
the O
target O
sentence O
. O

This O
makes O
learning O
more O
difﬁcult O
and O
slows O
the O
speed O
of O
parameter O
updates O
by O
a O
factor O
of O
two O
. O

How- O
ever O
, O
given O
that O
our O
post O
- O
processing O
step O
is O
con- O
cerned O
only O
with O
the O
alignments O
of O
the O
unknown O
words O
, O
so O
it O
is O
more O
sensible O
to O
only O
annotate O
the O
unknown O
words O
. O

This O
motivates O
our O
positional B-MethodName
un- I-MethodName
known I-MethodName
model O
which O
uses O
unkpos O
dtokens O
( O
for O
d O
in−7 O
, O
. O
. O
. O

, O
7or∅ O
) O
to O
simultaneously O
denote O
( O
a)13 O

the O
fact O
that O
a O
word O
is O
unknown O
and O
( O
b O
) O
its O
rela- O
tive O
position O
dwith O
respect O
to O
its O
aligned O
source O
word O
. O

Like O
the O
PosAll O
model O
, O
we O
use O
the O
symbol O
unkpos O
∅for O
unknown O
target O
words O
that O
do O
not O
have O
an O
alignment O
. O

We O
use O
the O
universal O
unk O
for O
all O
unknown O
tokens O
in O
the O
source O
language O
. O

See O
Figure O
4 O
for O
an O
annotated O
example O
. O

It O
is O
possible O
that O
despite O
its O
slower O
speed O
, O
the O
PosAll B-MethodName
model O
will O
learn O
better O
alignments O
because O
it O
is O
trained O
on O
many O
more O
examples O
of O
words O
and O
their O
alignments O
. O

However O
, O
we O
show O
that O
this O
is O
not O
the O
case O
( O
see O
§ O
5.2 O
) O
. O

4 O
Experiments O
We O
evaluate O
the O
effectiveness O
of O
our O
OOV O
mod- O
els O
on O
the O
WMT’14 B-DatasetName
English I-DatasetName
- I-DatasetName
to I-DatasetName
- I-DatasetName
French I-DatasetName
translation O
task O
. O

Translation O
quality O
is O
measured O
with O
the O
BLEU B-MetricName
metric O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
on O
the O
new- B-DatasetName
stest2014 I-DatasetName
test O
set O
( O
which O
has O
3003 O
sentences O
) O
. O

4.1 O
Training O
Data O
To O
be O
comparable O
with O
the O
results O
reported O
by O
pre- O
vious O
work O
on O
neural O
machine O
translation O
systems O
( O
Sutskever O
et O
al O
. O
, O
2014 O
; O
Cho O
et O
al O
. O
, O
2014 O
; O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
, O
we O
train O
our O
models O
on O
the O
same O
training O
data O
of O
12 O
M O
parallel O
sentences O
( O
348 O
M O
French O
and O
304 O
M O
English O
words O
) O
, O
obtained O
from O
( O
Schwenk O
, O
2014 O
) O
. O

The O
12 O
M O
subset O
was O
selected O
from O
the O
full O
WMT’14 B-DatasetName
parallel O
corpora O
using O
the O
method O
proposed O
in O
Axelrod O
et O
al O
. O

( O
2011 O
) O
. O

Due O
to O
the O
computationally O
intensive O
nature O
of O
the O
naive O
softmax O
, O
we O
limit O
the O
French O
vocabulary O
( O
thetarget O
language O
) O
to O
the O
either O
the O
40 O
K O
or O
the O
80 O
K O
most O
frequent O
French O
words O
. O

On O
the O
source O
side O
, O
we O
can O
afford O
a O
much O
larger O
vocabulary O
, O
so O
we O
use O
the O
200 O
K O
most O
frequent O
English O
words O
. O

The O
model O
treats O
all O
other O
words O
as O
unknowns.3 O
We O
annotate O
our O
training O
data O
using O
the O
three O
schemes O
described O
in O
the O
previous O
section O
. O

The O
alignment O
is O
computed O
with O
the O
Berkeley O
aligner O
( O
Liang O
et O
al O
. O
, O
2006 O
) O
using O
its O
default O
settings O
. O

We O
discard O
sentence O
pairs O
in O
which O
the O
source O
or O
the O
target O
sentence O
exceed O
100 O
tokens O
. O

4.2 O
Training O
Details O
Our O
training O
procedure O
and O
hyperparameter O
choices O
are O
similar O
to O
those O
used O
by O
Sutskever O
et O
al O
. O

( O
2014 O
) O
. O

In O
more O
details O
, O
we O
train O
multi O
- O
layer O
deep O
LSTMs O
, O
each O
of O
which O
has O
1000 B-HyperparameterValue
cells O
, O
with O
1000 B-HyperparameterValue
dimensional O
embeddings O
. O

Like O
Sutskever O
et O
al O
. O

( O
2014 O
) O
, O
we O
reverse O
the O
words O
in O
the O
source O
sen- O
tences O
which O
has O
been O
shown O
to O
improve O
LSTM O
memory O
utilization O
and O
results O
in O
better O
transla- O
tions O
of O
long O
sentences O
. O

Our O
hyperparameters O
can O
be O
summarized O
as O
follows O
: O
( O
a O
) O
the O
parameters B-HyperparameterName
are O
initialized O
uniformly O
in O
[ B-HyperparameterValue
-0.08 I-HyperparameterValue
, I-HyperparameterValue
0.08 I-HyperparameterValue
] I-HyperparameterValue
for O
4 B-HyperparameterValue
- O
layer O
models O
and O
[ B-HyperparameterValue
-0.06 I-HyperparameterValue
, I-HyperparameterValue
0.06 I-HyperparameterValue
] I-HyperparameterValue
for O
6 B-HyperparameterValue
- O
layer O
models O
, O
( O
b O
) O
SGD O
has O
a O
ﬁxed O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0.7 B-HyperparameterValue
, O
( O
c O
) O
we O
train O
for O
8 O
epochs O
( O
after O
5 O
epochs O
, O
we O
begin O
to O
halve O
the O
learning O
rate O
every O
0.5 O
epoch O
) O
, O
( O
d O
) O
the O
size B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
mini I-HyperparameterName
- I-HyperparameterName
batch I-HyperparameterName
is O
128 B-HyperparameterValue
, O
and O
( O
e O
) O
we O
rescale O
the O
normalized O
gradient O
to O
ensure O
that O
its O
norm B-HyperparameterName
does O
not O
exceed O
5 B-HyperparameterValue
( O
Pascanu O
et O
al O
. O
, O
2012 O
) O
. O

We O
also O
follow O
the O
GPU O
parallelization O
scheme O
proposed O
in O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
, O
allowing O
us O
to O
reach O
a O
training O
speed O
of O
5.4 O
K O
words O
per O
sec- O
ond O
to O
train O
a O
depth-6 O
model O
with O
200 O
K O
source O
and O
80 O
K O
target O
vocabularies O
; O
whereas O
Sutskever O
et O
al O
. O

( O
2014 O
) O
achieved O
6.3 O
K O
words O
per O
second O
for O
a O
depth-4 O
models O
with O
80 O
K O
source O
and O
target O
vo- O
cabularies O
. O

Training O
takes O
about O
10 O
- O
14 O
days O
on O
an O
8 O
- O
GPU O
machine O
. O

4.3 O
A O
note O
on O
BLEU B-MetricName
scores O
We O
report O
BLEU B-MetricName
scores O
based O
on O
both O
: O
( O
a O
) O
detok- O
enized O
translations O
, O
i.e. O
, O
WMT’14 O
style O
, O
to O
be O
com- O
parable O
with O
results O
reported O
on O
the O
WMT O
web- O
site4and O

( O
b O
) O
tokenized O
translations O
, O
so O
as O
to O
be O
consistent O
with O
previous O
work O
( O
Cho O
et O
al O
. O
, O
2014 O
; O
Bahdanau O
et O
al O
. O
, O
2015 O
; O
Schwenk O
, O
2014 O
; O
Sutskever O
et O
al O
. O
, O
2014 O
; O
Jean O
et O
al O
. O
, O
2015).5 O

The O
existing O
WMT’14 O
state O
- O
of O
- O
the O
- O
art O
system O
( O
Durrani O
et O
al O
. O
, O
2014 O
) O
achieves O
a O
detokenized B-MetricName
BLEU I-MetricName
score O
of O
35.8 B-MetricValue
on O
the O
newstest2014 B-DatasetName
test O
set O
for O
English O
to O
French O
language O
pair O
( O
see O
Table O
2 O
) O
. O

In O
terms O
of O
the O
tokenized B-MetricName
BLEU I-MetricName
, O
its O
performance O
is O
37.0 B-MetricValue
points O
( O
see O
Table O
1 O
) O
. O

4.4 O
Main O
Results O
We O
compare O
our O
systems O
to O
others O
, O
including O
the O
current O
state O
- O
of O
- O
the O
- O
art O
MT O
system O
( O
Durrani O
et O
al O
. O
, O
2014 O
) O
, O
recent O
end O
- O
to O
- O
end O
neural O
systems O
, O
as O
well O
as O
phrase O
- O
based O
baselines O
with O
neural O
com- O
ponents O
. O

The O
results O
shown O
in O
Table O
1 O
demonstrate O
that O
our O
unknown O
word O
translation O
technique O
( O
in O
par- O
ticular O
, O
the O
PosUnk B-MethodName
model O
) O
signiﬁcantly O
improves O
the O
translation O
quality O
for O
both O
the O
individual O
( O
non- O
ensemble O
) O
LSTM O
models O
and O
the O
ensemble O
mod O
- O
els.6For O
40K O
- O
word O
vocabularies O
, O
the O
performance O
gains O
are O
in O
the O
range O
of O
2.3 B-MetricValue
- I-MetricValue
2.8 I-MetricValue
BLEU B-MetricName
points O
. O

With O
larger O
vocabularies O
( O
80 O
K O
) O
, O
the O
performance O
gains O
are O
diminished O
, O
but O
our O
technique O
can O
still O
provide O
a O
nontrivial O
gains O
of O
1.6 B-MetricValue
- I-MetricValue
1.9 I-MetricValue
BLEU B-MetricName
points O
. O

It O
is O
interesting O
to O
observe O
that O
our O
approach O
is O
more O
useful O
for O
ensemble O
models O
as O
compared O
to O
the O
individual O
ones O
. O

This O
is O
because O
the O
useful- O
ness O
of O
the O
PosUnk B-MethodName
model O
directly O
depends O
on O
the O
ability O
of O
the O
NMT O
to O
correctly O
locate O
, O
for O
a O
given O
OOV O
target O
word O
, O
its O
corresponding O
word O
in O
the O
source O
sentence O
. O

An O
ensemble O
of O
large O
models O
identiﬁes O
these O
source O
words O
with O
greater O
accu- O
racy O
. O

This O
is O
why O
for O
the O
same O
vocabulary O
size O
, O
better O
models O
obtain O
a O
greater O
performance O
gain O
our O
post O
- O
processing O
step O
. O

e O

Except O
for O
the O
very O
re- O
cent O
work O
of O
Jean O
et O
al O
. O

( O
2015 O
) O
that O
employs O
a O
sim- O
ilar O
unknown O
treatment O
strategy7as O
ours O
, O
our O
best O
result O
of O
37.5 B-MetricValue
BLEU B-MetricName
outperforms O
all O
other O
NMT O
systems O
by O
a O
arge O
margin O
, O
and O
more O
importanly O
, O
our O
system O
has O
established O
a O
new O
record O
on O
the O
WMT’14 B-DatasetName
English I-DatasetName
to I-DatasetName
French I-DatasetName
translation O
. O

5 O
Analysis O
We O
analyze O
and O
quantify O
the O
improvement O
ob- O
tained O
by O
our O
rare O
word O
translation O
approach O
and O
provide O
a O
detailed O
comparison O
of O
the O
different O
rare O
word O
techniques O
proposed O
in O
Section O
3 O
. O

We O
also O
examine O
the O
effect O
of O
depth O
on O
the O
LSTM O
architectures O
and O
demonstrate O
a O
strong O
correla- O
tion O
between O
perplexities O
and O
BLEU B-MetricName
scores O
. O

We O
also O
highlight O
a O
few O
translation O
examples O
where O
our O
models O
succeed O
in O
correctly O
translating O
OOV O
words O
, O
and O
present O
several O
failures O
. O

5.1 O
Rare O
Word O
Analysis O
To O
analyze O
the O
effect O
of O
rare O
words O
on O
translation O
quality O
, O
we O
follow O
Sutskever O
et O
al O
. O

( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
and O
sort O
sentences O
in O
newstest2014 B-DatasetName
by O
the O
average O
inverse O
frequency O
of O
their O
words O
. O

We O
split O
the O
test O
sentences O
into O
groups O
where O
the O
sentences O
within O
each O
group O
have O
a O
comparable O
number O
of O
rare O
words O
and O
evaluate O
each O
group O
independently O
. O

We O
evaluate O
our O
systems O
before O
and O
after O
translat- O

ing O
the O
OOV O
words O
and O
compare O
with O
the O
stan- O
dard O
MT O
systems O
– O
we O
use O
the O
best O
system O
from O
the O
WMT’14 O
contest O
( O
Durrani O
et O
al O
. O
, O
2014 O
) O
, O
and O
neural O
MT O
systems O
– O
we O
use O
the O
ensemble O
systems O
described O
in O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
and O
Section O
4 O
. O

Rare B-TaskName
word I-TaskName
translation I-TaskName
is O
challenging O
for O
neural O
machine O
translation O
systems O
as O
shown O
in O
Figure O
5 O
. O

Speciﬁcally O
, O
the O
translation O
quality O
of O
our O
model O
before O
applying O
the O
postprocessing O
step O
is O
shown O
by O
the O
green O
curve O
, O
and O
the O
current O
best O
NMT O
sys- O
tem O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
is O
the O
purple O
curve O
. O

While O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
produces O
better O
translations O
for O
sentences O
with O
frequent O
words O
( O
the O
left O
part O
of O
the O
graph O
) O
, O
they O
are O
worse O
than O
best O
system O
( O
red O
curve O
) O
on O
sentences O
with O
many O
rare O
words O
( O
the O
right O
side O
of O
the O
graph O
) O
. O

When O
applying O
our O
unknown O
word O
translation O
technique O
( O
purple O
curve O
) O
, O
we O
signiﬁcantly O
improve O
the O
translation O
quality O
of O
our O
NMT O
: O
for O
the O
last O
group O
of O
500 O
sen- O
tences O
which O
have O
the O
greatest O
proportion O
of O
OOV O
words O
in O
the O
test O
set O
, O
we O
increase O
the O
BLEU B-MetricName
score O
of O
our O
system O
by O
4.8 B-MetricValue
BLEU B-MetricName
points O
. O

Overall O
, O
our O
rare O
word O
translation O
model O
interpolates O
between O
the O
SOTA O
system O
and O
the O
system O
of O
Sutskever O
et O
al O
. O

( O
2014 O
) O
, O
which O
allows O
us O
to O
outperform O
the O
win- O
ning O
entry O
of O
WMT’14 O
on O
sentences O
that O
consist O
predominantly O
of O
frequent O
words O
and O
approach O
its O
performance O
on O
sentences O
with O
many O
OOV O
words O
. O

5.2 O
Rare O
Word O
Models O
We O
examine O
the O
effect O
of O
the O
different O
rare O
word O
models O
presented O
in O
Section O
3 O
, O
namely O
: O
( O
a O
) O
Copy- B-MethodName
able I-MethodName
– O
which O
aligns O
the O
unknown O
words O
on O
both O
the O
input O
and O
the O
target O
side O
by O
learning O
to O
copy O
in- O
dices O
, O
( O
b O
) O
the O
Positional B-MethodName
All I-MethodName
( O
PosAll B-MethodName
) O
– O
which O
pre- O
dicts O
the O
aligned O
source O
positions O
for O
every O
target O
word O
, O
and O
( O
c O
) O
the O
Positional B-MethodName
Unknown I-MethodName
( O
PosUnk B-MethodName
) O
– O
which O
predicts O
the O
aligned O
source O
positions O
for O
only O
the O
unknown O
target O
words.8It O
is O
also O
interest- O
ing O
to O
measure O
the O
improvement O
obtained O
when O
no O
alignment O
information O
is O
used O
during O
training O
. O

As O
such O
, O
we O
include O
a O
baseline O
model O
with O
no O
align- O
ment O
knowledge O
( O
NoAlign B-MethodName
) O
in O
which O
we O
simply O
as- O
sume O
that O
the O
ithunknown O
word O
on O
the O
target O
sen- O
tence O
is O
aligned O
to O
the O
ithunknown O
word O
in O
the O
source O
sentence O
. O

From O
the O
results O
in O
Figure O
6 O
, O
a O
simple O
mono- O
tone O
alignment O
assumption O
for O
the O
NoAlign B-MethodName
model O
yields O
a O
modest O
gain O
of O
0.8 B-MetricValue
BLEU B-MetricName
points O
. O

If O
we O
train O
the O
model O
to O
predict O
the O
alignment O
, O
then O
the O
Copyable B-MethodName
model O
offers O
a O
slightly O
better O
gain O
of O
1.0 B-MetricValue
BLEU B-MetricName
. O

Note O
, O
however O
, O
that O
English O
and O
French O
have O
similar O
word O
order O
structure O
, O
so O
it O
would O
be O
interesting O
to O
experiment O
with O
other O
language O
pairs O
, O
such O
as O
English O
and O
Chinese O
, O
in O
which O
the O
word O
order O
is O
not O
as O
monotonic O
. O

These O
harder O
lan- O
guage O
pairs O
potentially O
imply O
a O
smaller O
gain O
for O
the O
NoAlign B-MethodName
model O
and O
a O
larger O
gain O
for O
the O
Copyable B-MethodName
model O
. O

We O
leave O
it O
for O
future O
work O
. O

The O
positional O
models O
( O
PosAll B-MethodName
and O
PosUnk B-MethodName
) O
im- O
prove O
translation O
performance O
by O
more O
than O
2 B-MetricValue
BLEU B-MetricName
points O
. O

This O
proves O
that O
the O
limitation O
of O
the O
copyable B-MethodName
model O
, O
which O
forces O
it O
to O
align O
each O
un- O
known O
output O
word O
with O
an O
unknown O
input O
word O
, O
is O
considerable O
. O

In O
contrast O
, O
the O
positional O
mod- O
els O
can O
align O
the O
unknown O
target O
words O
with O
any O
source O
word O
, O
and O
as O
a O
result O
, O
post O
- O
processing O
has O
a O
much O
stronger O
effect O
. O

The O
PosUnk B-MethodName
model O
achieves O
better O
translation O
results O
than O
the O
PosAll B-MethodName
model O
which O
suggests O
that O
it O
is O
easier O
to O
train O
the O
LSTM O
on O
shorter O
sequences O
. O

5.3 O
Other O
Effects O
Deep O
LSTM O
architecture O
– O
We O
compare O
PosUnk B-MethodName
models O
trained O
with O
different O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
( O
3 B-HyperparameterValue
, O
4 B-HyperparameterValue
, O
and O
6 B-HyperparameterValue
) O
. O

We O
observe O
that O
the O
gain O
obtained O
by O
the O
PosUnk B-MethodName
model O
increases O
in O
tandem O
with O
the O
overall O
accuracy O
of O
the O
model O
, O
which O
is O
consistent O
with O
the O
idea O
that O
larger O
models O
can O
point O
to O
the O
ap- O
propriate O
source O
word O
more O
accurately O
. O

Addition- O
ally O
, O
we O
observe O
that O
on O
average O
, O
each O
extra O
LSTM O
layer O
provides O
roughly O
1.0 B-MetricValue
BLEU B-MetricName
point O
improve- O
ment O
as O
demonstrated O
in O
Figure O
7 O
. O

Perplexity O
and O
BLEU B-MetricName
– O

Lastly O
, O
we O
ﬁnd O
it O
inter- O
esting O
to O
observe O
a O
strong O
correlation O
between O
the O
perplexity O
( O
our O
training O
objective O
) O
and O
the O
transla- O
tion O
quality O
as O
measured O
by O
BLEU B-MetricName
. O

Figure O
8 O
shows O
the O
performance O
of O
a O
4 O
- O
layer O
LSTM O
, O
in O
which O
we O
compute O
both O
perplexity O
and O
BLEU B-MetricName
scores O
at O
dif- O
ferent O
points O
during O
training O
. O

We O
ﬁnd O
that O
on O
aver- O
age O
, O
a O
reduction O
of O
0.5 O
perplexity O
gives O
us O
roughly O
1.0 B-MetricValue
BLEU B-MetricName
point O
improvement. O

5.4 O
Sample O
Translations O
We O
present O
three O
sample O
translations O
of O
our O
best O
system O
( O
with O
37.5 B-MetricValue
BLEU B-MetricName
) O
in O
Table O
3 O
. O

In O
our O
ﬁrst O
example O
, O
the O
model O
translates O
all O
the O
un- O
known O
words O
correctly O
: O
2600 O
, O
orthop O
´ O
ediques O
, O
and O
cataracte O
. O

It O
is O
interesting O
to O
observe O
that O
the O
model O
can O
accurately O
predict O
an O
alignment O
of O
dis- O
tances O
of O
5 O
and O
6 O
words O
. O

The O
second O
example O
highlights O
the O
fact O
that O
our O
model O
can O
translate O
long O
sentences O
reasonably O
well O
and O
that O
it O
was O
able O
to O
correctly O
translate O
the O
unknown O
word O
for O
JP- O
Morgan O
at O
the O
very O
far O
end O
of O
the O
source O
sentence O
. O

Lastly O
, O
our O
examples O
also O
reveal O
several O
penalties O
incurred O
by O
our O
model O
: O
( O
a O
) O
incorrect O
entries O
in O
the O
word O
dictionary O
, O
as O
with O
n´egociateur O
vs.trader O
in O
the O
second O
example O
, O
and O
( O
b O
) O
incorrect O
alignment O
prediction O
, O
such O
as O
when O
unkpos O
3is O
incorrectlyaligned O
with O
the O
source O
word O
was O
and O
not O
with O
abandoning O
, O
which O
resulted O
in O
an O
incorrect O
trans- O
lation O
in O
the O
third O
sentence O
. O

6 O
Conclusion O
We O
have O
shown O
that O
a O
simple O
alignment O
- O
based O
technique O
can O
mitigate O
and O
even O
overcome O
one O
of O
the O
main O
weaknesses O
of O
current O
NMT O
systems O
, O
which O
is O
their O
inability O
to O
translate O
words O
that O
are O
not O
in O
their O
vocabulary O
. O

A O
key O
advantage O
of O
our O
technique O
is O
the O
fact O
that O
it O
is O
applicable O
to O
any O
NMT O
system O
and O
not O
only O
to O
the O
deep O
LSTM O
model O
of O
Sutskever O
et O
al O
. O

( O
2014 O
) O
. O

A O
technique O
like O
ours O
is O
likely O
necessary O
if O
an O
NMT O
system O
is O
to O
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
machine O
translation O
. O

We O
have O
demonstrated O
empirically O
that O
on O
the18 O

WMT’14 B-DatasetName
English I-DatasetName
- I-DatasetName
French I-DatasetName
translation O
task O
, O
our O
technique O
yields O
a O
consistent O
and O
substantial O
im- O
provement O
of O
up O
to O
2.8 B-MetricValue
BLEU B-MetricName
points O
over O
various O
NMT O
systems O
of O
different O
architectures O
. O

Most O
im- O
portantly O
, O
with O
37.5 B-MetricValue
BLEU B-MetricName
points O
, O
we O
have O
estab- O
lished O
the O
ﬁrst O
NMT O
system O
that O
outperformed O
the O
best O
MT O
system O
on O
a O
WMT’14 B-DatasetName
contest O
dataset O
. O

Acknowledgments O
We O
thank O
members O
of O
the O
Google O
Brain O
team O
for O
thoughtful O
discussions O
and O
insights O
. O

The O
ﬁrst O
author O
especially O
thanks O
Chris O
Manning O
and O
the O
Stanford O
NLP O
group O
for O
helpful O
comments O
on O
the O
early O
drafts O
of O
the O
paper O
. O

Lastly O
, O
we O
thank O
the O
an- O
nonymous O
reviewers O
for O
their O
valuable O
feedback O
. O

References O
Amittai O
Axelrod O
, O
Xiaodong O
He O
, O
and O
Jianfeng O
Gao O
. O
2011 O
. O

Domain O
adaptation O
via O
pseudo O
in O
- O
domain O
data O
selection O
. O

In O
EMNLP O
. O

D. O
Bahdanau O
, O
K. O
Cho O
, O
and O
Y O
. O

Bengio O
. O

2015 O
. O

Neural O
machine O
translation O
by O
jointly O
learning O
to O
align O
and O
translate O
. O

In O
ICLR O
. O

D. O
Cer O
, O
M. O
Galley O
, O
D. O
Jurafsky O
, O
and O
C. O
D. O
Manning O
. O

2010 O
. O

Phrasal O
: O
A O
statistical O
machine O
translation O
toolkit O
for O
exploring O
new O
model O
features O
. O

In O
ACL O
, O
Demonstration O
Session O
. O

David O
Chiang O
. O

2007 O
. O

Hierarchical O
phrase O
- O
based O
trans- O
lation O
. O

Computational O
Linguistics O
, O
33(2):201–228 O
. O

Kyunghyun O
Cho O
, O
Bart O
van O
Merrienboer O
, O
Caglar O
Gul- O
cehre O
, O
Fethi O
Bougares O
, O
Holger O
Schwenk O
, O
and O
Yoshua O
Bengio O
. O

2014 O
. O

Learning O
phrase O
representations O
using O
rnn O
encoder O
- O
decoder O
for O
statistical O
machine O
translation O
. O

In O
EMNLP O
. O

Nadir O
Durrani O
, O
Barry O
Haddow O
, O
Philipp O
Koehn O
, O
and O
Kenneth O
Heaﬁeld O
. O

2014 O
. O

Edinburgh O
’s O
phrase O
- O
based O
machine O
translation O
systems O
for O
WMT-14 O
. O

In O
WMT O
. O

Chris O
Dyer O
, O
Jonathan O
Weese O
, O
Hendra O
Setiawan O
, O
Adam O
Lopez O
, O
Ferhan O
Ture O
, O
Vladimir O
Eidelman O
, O
Juri O
Gan- O
itkevitch O
, O
Phil O
Blunsom O
, O
and O
Philip O
Resnik O
. O

2010 O
. O

cdec O
: O
A O
decoder O
, O
alignment O
, O
and O
learning O
framework O
for O
ﬁnite O
- O
state O
and O
context O
- O
free O
translation O
models O
. O

InACL O
, O
Demonstration O
Session O
. O

A. O
Graves O
, O
G. O
Wayne O
, O
and O
I. O
Danihelka O
. O

2014 O
. O

Neural O
turing O
machines O
. O

arXiv O
preprint O
arXiv:1410.5401 O
. O

A. O
Graves O
. O

2013 O
. O

Generating O
sequences O
with O
recurrent O
neural O
networks O
. O

In O
Arxiv O
preprint O
arXiv:1308.0850 O
. O

S O
´ O
ebastien O
Jean O
, O
Kyunghyun O
Cho O
, O
Roland O
Memisevic O
, O
and O
Yoshua O
Bengio O
. O

2015 O
. O

On O
using O
very O
large O
tar- O
get O
vocabulary O
for O
neural O
machine O
translation O
. O

In O
ACL.N. O
Kalchbrenner O
and O
P. O
Blunsom O
. O

2013 O
. O

Recurrent O
continuous O
translation O
models O
. O

In O
EMNLP O
. O

Philipp O
Koehn O
, O
Franz O
Josef O
Och O
, O
and O
Daniel O
Marcu O
. O

2003 O
. O

Statistical O
phrase O
- O
based O
translation O
. O

In O
NAACL O
. O

Philipp O
Koehn O
, O
Hieu O
Hoang O
, O
Alexandra O
Birch O
, O
Chris O
Callison O
- O
Burch O
, O
Marcello O
Federico O
, O
Nicola O
Bertoldi O
, O
Brooke O
Cowan O
, O
Wade O
Shen O
, O
Christine O
Moran O
, O
Richard O
Zens O
, O
et O
al O
. O
2007 O
. O

Moses O
: O
Open O
source O
toolkit O
for O
statistical O
machine O
translation O
. O

In O
ACL O
, O
Demonstration O
Session O
. O

P. O
Liang O
, O
B. O
Taskar O
, O
and O
D. O
Klein O
. O

2006 O
. O

Alignment O
by O
agreement O
. O

In O
NAACL O
. O

Kishore O
Papineni O
, O
Salim O
Roukos O
, O
Todd O
Ward O
, O
and O
Wei O
jing O
Zhu O
. O
2002 O
. O

BLEU O
: O
a O
method O
for O
automatic O
evaluation O
of O
machine O
translation O
. O

In O
ACL O
. O

R. O
Pascanu O
, O
T. O
Mikolov O
, O
and O
Y O
. O

Bengio O
. O

2012 O
. O

On O
the O
difﬁculty O
of O
training O
recurrent O
neural O
networks O
. O

arXiv O
preprint O
arXiv:1211.5063 O
. O

H. O
Schwenk O
. O

2014 O
. O

University O
le O
mans O
. O

http://www-lium.univ-lemans.fr/ O
˜schwenk O
/ O
cslm_joint_paper/ O
. O

[ O
Online O
; O
accessed O
03 O
- O
September-2014 O
] O
. O

I. O
Sutskever O
, O
O. O
Vinyals O
, O
and O
Q. O
V O
. O

Le O
. O
2014 O
. O

Sequence O
to O
sequence O
learning O
with O
neural O
networks O
. O

In O
NIPS O
. O

Wojciech O
Zaremba O
, O
Ilya O
Sutskever O
, O
and O
Oriol O
Vinyals O
. O
2015 O
. O

Recurrent O
neural O
network O
regularization O
. O

In O
ICLR O
.19 O

