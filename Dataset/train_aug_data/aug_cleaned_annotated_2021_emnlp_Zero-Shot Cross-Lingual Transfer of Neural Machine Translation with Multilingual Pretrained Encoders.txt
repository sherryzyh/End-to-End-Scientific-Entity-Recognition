Guanhua O
Chen1 O
, O
Shuming O
Ma2 O
, O
Yun O
Chen3 O
, O
Li O
Dong2 O
, O
Dongdong O
Zhang2 O
, O
Jia O
Pan1 O
, O
Wenping O
Wang4 O
, O
and O
Furu O
Wei2 O
contributed O
to O
the O
research O
of O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName
with O
Multilingual B-MethodName
Pretrained I-MethodName
Encoders I-MethodName
. O

The O
main O
focus O
of O
previous O
work O
has O
been O
on O
improving O
the O
cross O
- O
lingual O
transfer O
for O
tasks O
with O
a O
multilingual O
pretrained O
encoder O
( O
MPE O
) O
, O
or O
on O
improving O
the O
performance O
of O
supervised O
machine O
translation O
with O
BERT O
. O

It O
is O
not O
well O
understood O
whether O
the O
MPE O
can O
help O
improve O
the O
ability O
of O
the O
NMT B-TaskName
model O
to O
be O
used O
across O
different O
languages O
. O

In O
this O
paper O
, O
we O
focus O
on O
a O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
task O
in O
the O
NMT B-TaskName
domain O
. O

We O
propose O
SixT B-MethodName
, O
an O
effective O
model O
that O
is O
both O
simple O
and O
easy O
to O
use O
for O
this O
task O
. O

SixT B-MethodName
uses O
the O
MPE O
with O
a O
two O
- O
stage O
training O
schedule O
and O
improves O
further O
with O
a O
position O
disentangled O
encoder O
and O
a O
capacity O
- O
enhanced O
decoder O
. O

This O
method O
outperforms O
a O
pretrained O
multilingual O
encoder- O
decoder O
model O
with O
an O
average O
improvement O
of O
7.1 B-MetricValue
BLEU B-MetricName
on O
zero O
- O
shot O
any O
- O
to O
- O
English O
test O
sets O
across O
14 O
source O
languages O
. O

With O
much O
less O
training O
computation O
cost O
and O
training O
data O
, O
our O
model O
outperforms O
CRISS B-MethodName
and O
m2m-100 B-MethodName
on O
any O
- O
to O
- O
English O
test O
sets O
. O

Pretrained O
encoders O
that O
are O
multilingual O
( O
MPE O
) O
such O
as O
mBERT B-MethodName
( O
Wu O
and O
Dredze O
, O
2019 O
) O
, O
XLM B-MethodName
( O
Conneau O
and O
Lample O
, O
2019 O
) O
, O
and O
XLM B-MethodName
- I-MethodName
R I-MethodName
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
have O
shown O
to O
be O
remarkably O
effective O
for O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
on O
tasks O
including O
natural B-TaskName
language I-TaskName
understanding I-TaskName
( O
NLU B-TaskName
) O
, O
named B-TaskName
entity I-TaskName
recognition I-TaskName
( O
NER B-TaskName
) O
, O
ques- B-TaskName
tion I-TaskName
answering I-TaskName
( O
QA B-TaskName
) O
, O
and O
natural B-TaskName
language I-TaskName
infer- I-TaskName
ence I-TaskName
( O
NLI B-TaskName
) O
. O

The O
pretrained O
model O
is O
then O
ﬁne O
- O
tuned O
on O
a O
downstream O
task O
using O
labeled O
data O
in O
a O
single O
language O
and O
evaluated O
on O
the O
same O
task O
in O
other O
languages O
. O

Given O
that O
MPE O
has O
been O
successful O
in O
tasks O
like O
cross- B-TaskName
lingual I-TaskName
NLU I-TaskName
, O
it O
's O
worth O
researching O
how O
to O
use O
that O
knowledge O
to O
do O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
in O
the O
NMT B-TaskName
task O
by O
taking O
advantage O
of O
MPE B-MethodName
. O

However O
, O
replacing O
the O
monolingual O
pretrained O
encoder O
with O
MPE O
in O
previous O
studies O
does O
not O
work O
well O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
of O
NMT B-TaskName
, O
as O
seen O
in O
the O
baselines O
in O
Table O
2 O
. O

It O
is O
still O
unclear O
how O
to O
use O
existing O
multilingual O
pretrained O
encoders O
to O
conduct O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
for O
the O
NMT B-TaskName
model O
. O

The O
task O
of O
this O
paper O
is O
MPE O
, O
which O
is O
translating O
multiple O
unseen O
languages O
by O
leveraging O
an O
MPE O
, O
as O
seen O
in O
Figure O
1 O
. O

We O
are O
proposing O
a O
system O
( O
SixT B-MethodName
) O
which O
can O
directly O
translate O
languages O
unseen O
during O
supervised O
training O
. O

We O
start O
with O
the O
encoder O
and O
decoder O
embeddings O
of O
SixT B-MethodName
being O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
suggest O
a O
two O
- O
part O
training O
schedule O
that O
makes O
a O
trade O
- O
off O
between O
supervised O
performance O
and O
transferability O
. O

The O
SixT B-MethodName
model O
, O
which O
is O
trained O
with O
only O
one O
language O
pair O
, O
can O
be O
transferred O
to O
unseen O
languages O
and O
alleviates O
the O
effect O
of O
' O
catastrophic O
forgetting O
' O
. O

SixT B-MethodName
outperforms O
mBART B-MethodName
significantly O
, O
with O
an O
average O
improvement O
of O
7.1 B-MetricValue
BLEU B-MetricName
across O
14 O
source O
languages O
. O

The O
SixT B-MethodName
model O
outperforms O
CRISS B-MethodName
and O
m2m-100 B-MethodName
on O
any O
- O
to O
- O
English O
test O
sets O
, O
with O
less O
training O
computation O
cost O
and O
training O
data O
. O

The O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
NMT I-TaskName
transfer I-TaskName
task O
explores O
ways O
to O
improve O
the O
cross O
- O
lingual O
transferability O
of O
NMT O
models O
. O

In O
contrast O
to O
multilingual B-TaskName
NMT I-TaskName
( O
Johnson O
et O
al O
. O
, O
2017 O
) O
, O
unsupervised B-TaskName
NMT I-TaskName
( O
Lample O
et O
al O
. O
, O
2018 O
) O
or O
zero B-TaskName
- I-TaskName
resource I-TaskName
NMT I-TaskName
through O
pivoting O
( O
Chen O
et O
al O
. O
, O
2017 O
, O
2018 O
) O
, O
neither O
the O
parallel O
nor O
monolingual O
data O
in O
the O
language O
li O
zis O
directly O
accessible O
in O
the O
ZeXT B-TaskName
task O
. O

In O
this O
paper O
, O
we O
use O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
which O
is O
trained O
on O
100 O
languages O
, O
as O
the O
off O
- O
the O
- O
shelf O
MPE O
. O

The O
ZeXT B-TaskName
task O
is O
looking O
for O
ways O
to O
build O
a O
many O
- O
to O
- O
one O
NMT O
model O
efficiently O
, O
that O
can O
translate O
from O
100 O
languages O
supported O
by O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
using O
a O
parallel O
dataset O
of O
only O
one O
language O
pair O
. O

The O
ZeXT B-TaskName
task O
provides O
a O
new O
perspective O
for O
the O
evaluation O
of O
different O
MPEs O
, O
which O
can O
hopefully O
facilitate O
the O
research O
on O
MPEs O
. O

We O
observe O
that O
it O
is O
best O
to O
initialize O
the O
encoder O
embedding O
, O
the O
encoder O
layers O
and O
the O
decoder O
embedding O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
keep O
their O
parameters O
frozen O
, O
while O
randomly O
initializing O
the O
decoder O
layers O
( O
see O
Figure O
2 O
) O
. O

This O
inspires O
us O
to O
introduce O
a O
new O
model O
that O
can O
further O
improve O
on O
zero B-TaskName
- I-TaskName
shot I-TaskName
translations I-TaskName
. O

The O
encoder O
from O
XLM B-MethodName
- I-MethodName
R I-MethodName
has O
strong O
positional O
representations O
that O
correspond O
to O
the O
source O
sentence O
. O

The O
default O
decoder O
configuration O
for O
training O
an O
NMT O
on O
the O
Europarl B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
training O
dataset O
is O
Transformer O
base O
( O
Gu O
et O
al O
. O
, O
2018 O
; O
Currey O
et O
al O
. O
, O
2020 O
) O
. O

The O
model O
capacity O
of O
SixT B-MethodName
is O
smaller O
than O
the O
vanilla O
Transformer O
with O
the O
same O
size O
. O

We O
focus O
on O
the O
any O
- O
to O
- O
English O
translations O
for O
the O
ZeXT B-TaskName
task O
. O

The O
Europarl B-DatasetName
- I-DatasetName
v7 I-DatasetName
German I-DatasetName
and I-DatasetName
English I-DatasetName
is O
used O
for O
training O
. O

We O
use O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
base O
model O
as O
the O
MPE O
that O
is O
ready O
to O
use O
. O

We O
set O
the O
Transformer O
encoder O
to O
the O
same O
size O
as O
the O
XLM- B-MethodName
R I-MethodName
base O
model O
. O

We O
refer O
to O
the O
model O
configuration O
as O
SixT B-MethodName
and O
use O
this O
configuration O
for O
our O
NMT O
models O
throughout O
the O
paper O
unless O
otherwise O
noted O
. O

We O
use O
a O
smaller O
decoder O
, O
denoted O
as O
BaseDec B-MethodName
, O
for O
strategy O
( O
1)–(7 O
) O
where O
decoder O
layers O
are O
trained O
from O
scratch O
. O

This O
model O
configuration O
is O
denoted O
by O
the O
letters O
A O
through O
G O
, O
with O
0 O
representing O
a O
small O
size O
. O

We O
denote O
the O
decoder O
for O
the O
remaining O
strategies O
as O
BigDec B-MethodName
. O

The O
Adam O
optimizer O
with O
1= O
0.9 B-HyperparameterValue
and O
2= O
0.98 B-HyperparameterValue
is O
used O
for O
training O
and O
evaluation O
. O

We O
use O
label B-HyperparameterName
smoothing I-HyperparameterName
to O
represent O
the O
value O
0.1 B-HyperparameterValue
. O

At O
the O
first O
stage O
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
0.0005 B-HyperparameterValue
and O
warmup B-HyperparameterName
step I-HyperparameterName
is O
4000 B-HyperparameterValue
. O

For O
the O
second O
stage O
, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
as O
the O
0.0001 B-HyperparameterValue
and O
do O
not O
use O
warmup O
. O

All O
the O
settings O
are O
set O
to O
0.3 B-HyperparameterValue
. O

We O
use O
eight O
GPUs O
, O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
set O
as O
4096 B-HyperparameterValue
tokens O
per O
GPU O
. O

Maximum B-HyperparameterName
updates I-HyperparameterName
number I-HyperparameterName
is O
the O
first O
stage O
and O
200k B-HyperparameterValue
is O
the O
second O
stage O
. O

We O
use O
beam O
search O
and O
do O
not O
tune O
the O
length O
penalty O
. O

We O
evaluate O
the O
results O
with O
the O
sacrebleu B-MetricName
method O
. O

If O
no O
specific O
checkpoint O
is O
chosen O
, O
the O
best O
one O
will O
be O
selected O
based O
on O
the O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
performance O
on O
the O
validation O
set O
for O
all O
experiments O
. O

We O
compare O
our O
model O
to O
vanilla B-MethodName
Transformer I-MethodName
and O
five O
other O
methods O
that O
use O
a O
pretrained O
Transformer O
encoder O
for O
NMT O
tasks O
. O

The O
encoders O
in O
these O
methods O
are O
replaced O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
base O
for O
a O
fair O
comparison O
. O

The O
encoder O
has O
the O
same O
size O
as O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
base O
, O
while O
the O
decoder O
uses O
the O
size O
of O
the O
BaseDec B-MethodName
. O

The O
encoder O
is O
initialized O
with O
all O
parameters O
being O
trained O
. O

All O
parameters O
except O
those O
of O
the O
cross O
attention O
module O
are O
initialized O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
and I-MethodName
and O
then O
fine O
- O
tuned O
. O

Zhu O
et O
al O
. O
( O
2020 O
) O
found O
that O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
output O
improved O
the O
NMT O
model O
when O
used O
as O
the O
encoder O
input O
, O
and O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
model O
was O
kept O
fixed O
during O
training O
. O

The O
method O
Imamura O
and O
Sumita O
proposed O
in O
2019 O
initializes O
the O
encoder O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
only O
trains O
the O
decoder O
at O
the O
first O
step O
. O
Then O
, O
all O
components O
are O
trained O
at O
the O
second O
step O
. O

According O
to O
Zhu O
et O
al O
. O
( O
2020 O
) O
, O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
output O
is O
fused O
into O
the O
encoder O
and O
decoder O
separately O
using O
an O
attention O
mechanism O
. O

The O
encoder O
embedding O
is O
initialized O
from O
XLM B-MethodName
- I-MethodName
R I-MethodName
to I-MethodName
to O
facilitate O
transfer O
. O

The O
parameters O
of O
XLM B-MethodName
- I-MethodName
R I-MethodName
are I-MethodName
will O
not O
change O
during O
training O
. O

In O
general O
, O
we O
find O
that O
it O
is O
most O
effective O
to O
use O
a O
large O
decoder O
, O
initialize O
the O
decoder O
embedding O
and O
all O
encoder O
parameters O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
and O
train O
the O
decoder O
layers O
from O
scratch O
( O
Strategy O
( O
10 O
) O
) O
. O

In O
order O
to O
verify O
the O
effect O
of O
a O
capacity O
- O
enhanced O
decoder O
in O
the O
ZeXT B-TaskName
task O
, O
we O
train O
two O
models O
, O
vanilla B-MethodName
Trans- I-MethodName
former I-MethodName
and O
BaseDec B-MethodName
, O
using O
the O
same O
size O
of O
Strategy O
( O
7 O
) O
and O
Strategy O
( O
10 O
) O
respectively O
, O
and O
using O
the O
same O
training O
corpus O
. O
The O
vanilla O
Transformer O
model O
with O
BaseDec B-MethodName
and O
BigDec B-MethodName
obtains O
a O
BLEU B-MetricName
score O
of O
23.5 B-MetricValue
and O
22.9 B-MetricValue
on O
the O
De B-DatasetName
- I-DatasetName
En I-DatasetName
test O
set O
, O
respectively O
. O

The O
big O
decoder O
improves O
the O
performance O
of O
SixT B-MethodName
, O
but O
does O
not O
improve O
the O
performance O
of O
vanilla B-MethodName
Trans- I-MethodName
former I-MethodName
. O

This O
demonstrates O
that O
BigDec B-MethodName
can O
improve O
the O
zero B-TaskName
- I-TaskName
shot I-TaskName
translation I-TaskName
performance O
of O
our O
model O
. O

Table O
2 O
compares O
the O
performance O
of O
the O
proposed O
SixT B-MethodName
with O
the O
baselines O
. O

SixT B-MethodName
outperforms O
the O
best O
baseline O
by O
5.4 B-HyperparameterValue
on O
average O
, O
showing O
that O
SixT B-MethodName
is O
able O
to O
learn O
to O
translate O
while O
preserving O
the O
cross O
- O
lingual O
transferability O
of O
XLM B-MethodName
- I-MethodName
R. I-MethodName
. O

SixT B-MethodName
outperforms O
all O
other O
language O
pairs O
in O
terms O
of O
transfer O
scores O
. O

In O
contrast O
, O
vanilla B-MethodName
Transformer I-MethodName
can O
not O
be O
transferred O
well O
to O
distant O
languages O
, O
and O
neither O
can O
other O
baselines O
. O

In O
addition O
to O
performing O
well O
on O
tasks O
for O
which O
it O
has O
not O
been O
trained O
, O
SixT B-MethodName
also O
achieves O
the O
best O
result O
on O
De O
- O
En O
test O
set O
. O

Previous O
work O
( O
Conneau O
et O
al O
. O
, O
2020 O
; O
Hu O
et O
al O
. O
, O
2020 O
) O
mainly O
uses O
XLM B-MethodName
- I-MethodName
R I-MethodName
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
trans- I-TaskName
fer O
on O
NLU O
tasks O
. O

The O
experiments O
demonstrate O
that O
XLM B-MethodName
- I-MethodName
R I-MethodName
can O
also O
be O
utilized O
for O
zero B-TaskName
- I-TaskName
shot I-TaskName
neu- I-TaskName
ral I-TaskName
machine I-TaskName
translation I-TaskName
if O
it O
is O
fine O
- O
tuned O
properly O
. O

We O
conduct O
an O
ablation O
study O
on O
the O
proposed O
SixT B-MethodName
, O
as O
shown O
in O
Table O
3 O
. O

In O
general O
, O
SixT B-MethodName
has O
the O
best O
zero B-TaskName
- I-TaskName
shot I-TaskName
translation I-TaskName
outcomes O
, O
which O
emphasizes O
the O
significance O
of O
all O
three O
elements O
. O

The O
results O
of O
1 O
- O
3 O
show O
that O
TwoStage B-MethodName
and O
BigDec B-MethodName
improve O
the O
zero- B-TaskName
shot I-TaskName
translation I-TaskName
performance O
by O
0.8 B-MetricValue
and O
0.4 B-MetricValue
on O
average O
over O
( O
1 O
) O
. O

However O
, O
combining O
them O
together O
significantly O
improves O
average O
BLEU B-MetricName
over O
( O
1 O
) O
. O

This O
means O
that O
TwoStage B-MethodName
and O
BigDec O
work O
well O
together O
and O
it O
is O
important O
to O
use O
both O
of O
them O
. O

The O
results O
of O
6!5 O
confirm O
our O
claim O
: O
without O
using O
BigDec O
, O
the O
performance O
of O
SixT B-MethodName
drops O
by O
1.8 B-MetricValue
average O
BLEU B-MetricName
. O

We O
observe O
that O
the O
supervised O
task O
improves O
with O
TwoStage B-MethodName
and O
BigDec B-MethodName
while O
it O
degrades O
with O
Resdrop B-MethodName
. O

Since O
Resdrop B-MethodName
helps O
to O
build O
a O
more O
language O
- O
agnostic O
encoder O
, O
this O
is O
expected O
. O

Although O
Res- B-MethodName
drop I-MethodName
has O
a O
negative O
effect O
on O
supervised O
performance O
, O
it O
has O
a O
positive O
effect O
on O
zero B-TaskName
- I-TaskName
shot I-TaskName
translation I-TaskName
. O

The O
performance O
of O
zero O
- O
shot O
translation O
can O
be O
improved O
by O
either O
enhancing O
the O
supervised O
performance O
( O
with O
TwoStage B-MethodName
and O
BigDec B-MethodName
) O
or O
the O
model O
transferability O
( O
with O
Resdrop B-MethodName
) O
. O

In O
this O
section O
, O
we O
compare O
SixT B-MethodName
with O
mBART B-MethodName
, O
CRISS B-MethodName
, O
and O
m2m-100 B-MethodName
on O
any B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
English I-TaskName
test O
sets O
. O

mBART B-MethodName
is O
a O
strong O
multilingual O
encoder O
- O
decoder O
that O
is O
explicitly O
designed O
for O
NMT B-TaskName
. O

We O
fine O
- O
tune O
all O
model O
parameters O
on O
the O
training O
set O
that O
follows O
their O
setting O
. O

The O
CRISS B-MethodName
and O
m2m-100 B-MethodName
models O
are O
the O
most O
advanced O
unsupervised O
and O
supervised O
multilingual O
neural O
machine O
translation O
models O
available O
, O
respectively O
. O

The O
CRISS B-MethodName
model O
is O
trained O
on O
the O
mBART B-MethodName
model O
and O
then O
iteratively O
improved O
on O
1.8 O
billion O
sentences O
from O
90 O
different O
language O
pairs O
. O

m2m-100 B-MethodName
is O
a O
machine O
translation O
system O
that O
has O
been O
trained O
with O
7.5 O
billion O
parallel O
sentences O
across O
2200 O
translation O
directions O
. O

The O
performance O
of O
CRISS B-MethodName
and O
m2m-100 B-MethodName
may O
degrade O
because O
they O
are O
many O
- O
to O
- O
many O
NMT O
models O
that O
compete O
with O
each O
other O
, O
while O
SixT B-MethodName
is O
a O
many B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
one I-TaskName
NMT I-TaskName
model O
. O

The O
ofﬁcial O
m2m-100 B-MethodName
model O
is O
available O
in O
three O
sizes O
: O
small O
( O
418 B-HyperparameterValue
M I-HyperparameterValue
parameters B-HyperparameterName
) O
, O
base O
( O
1.2B B-HyperparameterValue
parameters B-HyperparameterName
) O
and O
large O
( O
12B B-HyperparameterValue
parameters B-HyperparameterName
) O
. O

The O
results O
of O
the O
m2m-100 B-MethodName
( I-MethodName
small I-MethodName
) I-MethodName
model O
are O
being O
reported O
. O

We O
train O
a O
many O
- O
to O
- O
one O
model O
with O
WMT19 B-DatasetName
German- I-DatasetName
English I-DatasetName
training O
data O
, O
which O
only O
consists O
of O
41 O
million O
sentence O
pairs O
, O
to O
compare O
with O
these O
models O
. O

All O
you O
need O
is O
a O
pre O
- O
trained O
XLM B-MethodName
- I-MethodName
R I-MethodName
large I-MethodName
model I-MethodName
, O
and O
it O
does O
n't O
need O
to O
contain O
any O
data O
in O
other O
languages O
. O

After O
the O
self O
- O
attention O
sublayer O
of O
the O
23 B-HyperparameterValue
- I-HyperparameterValue
th I-HyperparameterValue
( O
penultimate O
) O
encoder O
layer O
, O
we O
remove O
the O
residual O
connection O
. O

The O
SixT B-MethodName
large I-MethodName
model O
is O
significantly O
better O
than O
the O
mBART B-MethodName
, O
CRISS B-MethodName
, O
and O
m2m-100 B-MethodName
models O
. O

On O
average O
, O
BLEU B-MetricName
is O
higher O
for O
all O
languages O
than O
7.1 B-MetricValue
, O
0.5 B-MetricValue
, O
and O
1.4 B-MetricValue
, O
respectively O
. O

The O
SixT B-MethodName
model O
is O
larger O
, O
but O
the O
SixT B-MethodName
results O
are O
impressive O
considering O
SixT B-MethodName
does O
n't O
use O
any O
monolingual O
or O
parallel O
texts O
except O
for O
German B-DatasetName
- I-DatasetName
English I-DatasetName
training O
data O
. O

The O
multilingual O
encoder O
outperforms O
mBART B-MethodName
, O
showing O
that O
with O
the O
right O
tuning O
strategy O
, O
it O
can O
produce O
better O
results O
on O
NMT O
tasks O
. O

The O
SixT B-MethodName
model O
translates O
well O
to O
distant O
resource O
- O
poor O
languages O
like O
Ne O
and O
Si O
, O
which O
indicates O
a O
promising O
approach O
to O
translate O
resource O
- O
poor O
languages O
. O

More O
data O
from O
different O
language O
pairs O
could O
potentially O
improve O
the O
performance O
of O
the O
SixT B-MethodName
. O

The O
SixT B-MethodName
models O
are O
trained O
on O
different O
supervised O
language O
pairs O
, O
including O
De O
- O
En O
, O
Es O
- O
En O
, O
Fi O
- O
En O
, O
Hi O
- O
En O
and O
Zh O
- O
En O
. O
These O
models O
are O
then O
applied O
to O
all O
test O
sets O
, O
as O
seen O
in O
Table O
5 O
. O

We O
find O
that O
the O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
performs O
better O
when O
the O
SixT B-MethodName
is O
trained O
on O
source O
languages O
that O
are O
from O
the O
same O
language O
family O
. O

The O
performance O
of O
Hi B-DatasetName
- I-DatasetName
En I-DatasetName
is O
the O
best O
transfer O
performance O
, O
with O
the O
exception O
of O
Ko B-DatasetName
- I-DatasetName
En I-DatasetName
. O

When O
trained O
on O
3.5 O
million O
Hi O
- O
En O
sentence O
pairs O
, O
SixT B-MethodName
obtains O
promising O
results O
on O
the O
Ne O
- O
En O
and O
Si O
- O
En O
translation O
, O
with O
a O
BLEU B-MetricName
score O
of O
16.7 B-MetricValue
and O
9.6 B-MetricValue
, O
respectively O
. O

As O
a O
comparison O
, O
the O
vanilla B-MethodName
Trans- I-MethodName
former I-MethodName
that O
is O
supervised O
with O
the O
FLoRes B-DatasetName
training O
set O
only O
receives O
14.5 B-MetricValue
and O
7.2 B-MetricValue
BLEU B-MetricName
score O
( O
Liu O
et O
al O
. O
, O
2020 O
) O
on O
the O
same O
test O
sets O
. O

A O
different O
way O
to O
translate O
resource O
- O
poor O
languages O
is O
to O
train O
SixT B-MethodName
on O
similar O
high O
- O
resource O
language O
pairs O
. O

We O
train O
vanilla B-MethodName
Transformer I-MethodName
configured O
as O
Transformer B-MethodName
big I-MethodName
without O
MPE O
initialization O
using O
the O
same O
training O
and O
validation O
sets O
for O
comparison O
. O

The O
poor O
performance O
of O
vanilla B-MethodName
Transformer I-MethodName
when O
zero O
- O
shot O
cross O
- O
lingualism O
is O
attempted O
. O

We O
compare O
the O
vanilla B-MethodName
Transformer I-MethodName
big I-MethodName
model O
and O
SixT B-MethodName
model O
to O
see O
if O
the O
SixT B-MethodName
model O
degrades O
performance O
on O
the O
supervised O
language O
pair O
in O
order O
to O
gain O
cross O
- O
lingual O
transfer O
ability O
. O

When O
there O
are O
more O
than O
20 O
million O
parallel O
sentences O
available O
, O
the O
performance O
of O
SixT B-MethodName
is O
lower O
than O
vanilla B-MethodName
Transformer I-MethodName
, O
but O
it O
performs O
better O
with O
fewer O
parallel O
sentences O
. O

The O
Hindi O
- O
to O
- O
English O
translation O
is O
an O
exception O
where O
the O
word O
" O
SixT B-MethodName
" O
has O
a O
lower O
BLEU B-MetricName
score O
. O

The O
SixT B-MethodName
model O
size O
is O
expected O
to O
increase O
when O
large O
amounts O
of O
bi O
- O
text O
data O
are O
given O
in O
order O
to O
fully O
digest O
the O
bi O
- O
text O
. O

If O
we O
replace O
SixT B-MethodName
with O
SixT B-MethodName
large I-MethodName
and O
train O
SixT B-MethodName
large I-MethodName
on O
WMT19 B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
, O
we O
get O
results O
on O
the O
De B-DatasetName
- I-DatasetName
En I-DatasetName
test O
set O
that O
are O
comparable O
to O
those O
obtained O
by O
vanilla B-MethodName
Transformer I-MethodName
( O
see O
Table O
4 O
) O
. O

We O
examine O
the O
relationship O
between O
the O
ability O
to O
transfer O
cross O
- O
lingually O
and O
the O
size O
of O
training O
data O
by O
comparing O
the O
zero O
- O
shot O
BLEU B-MetricName
scores O
of O
SixT O
models B-MethodName
trained O
on O
Eu- B-DatasetName
roparl I-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
and O
WMT19 B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
. O

The O
sentence O
shows O
that O
increasing O
the O
size O
of O
training O
data O
can O
improve O
the O
performance O
of O
zero- B-TaskName
shot I-TaskName
translation I-TaskName
. O

For O
example O
, O
if O
SixT B-MethodName
is O
trained O
with O
WMT19 B-DatasetName
, O
it O
will O
improve O
by O
3.4 B-MetricValue
average O
BLEU B-MetricName
compared O
to O
SixT B-MethodName
being O
trained O
with O
Europarl B-DatasetName
- I-DatasetName
v7 I-DatasetName
. O

We O
train O
two O
models O
, O
one O
on O
WMT16 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
and O
the O
other O
on O
WMT19 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
, O
to O
build O
a O
many O
- O
to O
- O
one O
NMT O
model O
with O
another O
target O
language O
. O

When O
the O
target O
language O
is O
not O
English O
, O
SixT B-MethodName
can O
reasonably O
transfer O
scores O
to O
unseen O
source O
languages O
, O
as O
shown O
in O
Table O
8 O
. O

There O
is O
related O
work O
to O
this O
paper O
in O
the O
form O
of O
Zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
learning I-TaskName
. O


 O
There O
is O
similar O
research O
to O
what O
is O
presented O
in O
this O
paper O
in O
the O
form O
of O
Zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
learning I-TaskName
. O

Multilingual O
pretrained O
models O
have O
been O
successful O
for O
various O
NLP O
tasks O
, O
such O
as O
mBERT B-MethodName
( O
Wu O
and O
Dredze O
, O
2019 O
) O
, O
XLM B-MethodName
- I-MethodName
R I-MethodName
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
, O
mBART B-MethodName
( O
Liu O
et O
al O
. O
, O
2020 O
) O
, O
and O
mT5 B-MethodName
( O
Xue O
et O
al O
. O
, O
2021 O
) O
. O

For O
example O
, O
Liu O
et O
al O
. O
( O
2020 O
) O
propose O
an O
encoder O
- O
decoder O
- O
based O
Transformer O
that O
is O
explicitly O
designed O
for O
NMT O
and O
demonstrates O
that O
it O
can O
be O
fine O
- O
tuned O
for O
supervised O
and O
unsupervised O
tasks O
. O

We O
use O
MPE O
for O
zero- B-TaskName
shot I-TaskName
translation I-TaskName
instead O
of O
supervised O
translation O
, O
which O
is O
different O
from O
what O
they O
do O
. O

We O
focus O
on O
the O
task O
of O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross- I-TaskName
lingual I-TaskName
NMT I-TaskName
transfer I-TaskName
( O
ZeXT B-TaskName
) O
, O
which O
aims O
to O
use O
an O
MPE O
for O
machine O
translation O
while O
preserving O
its O
ability O
to O
cross O
- O
lingual O
transfer O
. O

In O
this O
task O
, O
only O
a O
multilingual O
pretrained O
encoder O
such O
as O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
one O
parallel O
dataset O
such O
as O
German O
- O
English O
are O
available O
. O

We O
suggest O
using O
SixT B-MethodName
for O
this O
task O
, O
which O
enables O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
for I-TaskName
NMT I-TaskName
by O
taking O
advantage O
of O
the O
la- O
belled O
data O
and O
improving O
the O
transferability O
of O
XLM B-MethodName
- I-MethodName
R. I-MethodName
. O

Extensive O
experiments O
have O
shown O
that O
SixT O
is O
an O
effective O
model O
, O
outperforming O
the O
pretrained O
encoder O
- O
decoder O
- O
based O
model O
mBART B-MethodName
that O
was O
specifically O
designed O
for O
NMT O
. O

It O
outperforms O
CRISS B-MethodName
and O
m2m-100 B-MethodName
, O
two O
strong O
multilingual O
NMT O
models O
, O
on O
15 O
any O
- O
to- O
tasks O
. O

The O
following O
sentence O
is O
referencing O
Roee O
Aharoni O
, O
Melvin O
Johnson O
, O
and O
Orhan O
Firat O
's O
2019 O
paper O
Massively B-TaskName
multilingual I-TaskName
neural I-TaskName
machine I-TaskName
translation I-TaskName
, O
which O
can O
be O
found O
in O
the O
Proceedings O
of O
NAACL O
on O
pages O
3874 O
- O
3884 O
. O

Yun O
Chen O
, O
Yang O
Liu O
, O
Yong O
Cheng O
, O
and O
Victor O
OK O
Li O
. O
2017 O
. O
A O
teacher O
- O
student O
framework O
for O
zero- B-TaskName
resource I-TaskName
neural I-TaskName
machine I-TaskName
translation I-TaskName
. O
In O
Proceedings O
of O
ACL O
, O
pages O
1925–1935 O
. O

The O
sentence O
is O
about O
a O
study O
done O
by O
Yun O
Chen O
, O
Yang O
Liu O
, O
and O
Victor O
OK O
Li O
in O
2018 O
. O
The O
study O
is O
titled O
" O
Zero- B-TaskName
resource I-TaskName
neural I-TaskName
machine I-TaskName
translation I-TaskName
with O
multi O
- O
agent O
communication O
game O
. O
" O
The O
study O
was O
presented O
at O
the O
Thirty O
- O
Second O
AAAI O
Conference O
on O
Artificial O
Intelligence O
. O

All O
texts O
are O
tokenized O
with O
the O
same O
XLM O
- O
R O
sentencepiece O
( O
Kudo O
, O
2018 O
) O
model O
to O
be O
compatible O
with O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
model O
. O

The O
< O
bos O
> O
token O
is O
added O
to O
the O
beginning O
of O
each O
source O
sentence O
while O
the O
< O
eos O
> O
token O
is O
appended O
to O
the O
end O
when O
the O
NMT O
model O
initializes O
the O
encoder O
with O
XLM B-MethodName
- I-MethodName
R. I-MethodName
. O

The O
source B-HyperparameterName
sentence I-HyperparameterName
length I-HyperparameterName
is O
restricted O
to O
512 B-HyperparameterValue
tokens I-HyperparameterValue
. O

The O
encoder O
of O
the O
SixT B-MethodName
model O
is O
the O
same O
size O
as O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
model O
. O

The O
number B-HyperparameterName
of I-HyperparameterName
attention I-HyperparameterName
heads I-HyperparameterName
is O
set O
to O
16 B-HyperparameterValue
for O
the O
decoder O
of O
the O
SixT B-MethodName
large I-MethodName
model O
, O
so O
that O
the O
dimension B-HyperparameterName
of I-HyperparameterName
hidden I-HyperparameterName
states I-HyperparameterName
can O
be O
divided O
by O
the O
number B-HyperparameterName
of I-HyperparameterName
attention I-HyperparameterName
heads I-HyperparameterName
. O

The O
source O
vocabulary O
and O
target O
vocabulary O
both O
use O
the O
same O
250k O
vocabulary O
of O
XLM B-MethodName
- I-MethodName
R I-MethodName
. O

In O
Table O
4 O
, O
we O
compare O
SixT B-MethodName
large I-MethodName
with O
CRISS B-MethodName
, O
m2m-100 B-MethodName
, O
and O
mBART B-MethodName
. O

We O
use O
the O
official O
model O
checkpoints O
of O
mBART15 B-MethodName
, O
(611 B-HyperparameterValue
M I-HyperparameterValue
, O
parame- B-HyperparameterName
ters I-HyperparameterName
, O
CRISS16 B-MethodName
, O
(680 B-HyperparameterValue
M I-HyperparameterValue
, O
parameters B-HyperparameterName
, O
and O
m2m-100 B-MethodName
. O
