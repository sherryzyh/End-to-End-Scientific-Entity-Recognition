Probabilistic B-MethodName
FastText I-MethodName
is O
a O
multi O
- O
sense O
word O
embedding O
that O
can O
be O
used O
to O
improve O
the O
accuracy O
of O
your O
word O
embeddings O
. O

We O
introduce O
Probabilistic B-MethodName
FastText I-MethodName
, O
a O
new O
model O
for O
word O
embeddings O
that O
can O
capture O
multiple O
word O
senses O
, O
sub O
- O
word O
structure O
, O
and O
uncertainty O
information O
. O

Probabilistic B-MethodName
FastText I-MethodName
outperforms O
both O
F B-MethodName
ASTTEXT I-MethodName
and O
dictionary B-MethodName
- I-MethodName
level I-MethodName
probabilistic I-MethodName
embeddings I-MethodName
on O
several O
word O
- O
similarity O
benchmarks O
, O
including O
English B-DatasetName
RareWord I-DatasetName
and O
foreign O
language O
datasets O
. O

The O
search O
has O
been O
focused O
on O
finding O
vector O
word O
embeddings O
, O
such O
as O
W B-MethodName
ORD2VEC I-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
, O
where O
words O
with O
similar O
meanings O
are O
mapped O
to O
points O
that O
are O
close O
to O
each O
other O
in O
a O
vector O
space O
. O

Bojanowski O
et O
al O
. O
, O
2016 O
) O
F B-MethodName
ASTTEXT I-MethodName
is O
the O
current O
best O
practice O
for O
character O
- O
level O
embeddings O
. O

In O
the O
sentence O
F B-MethodName
ASTTEXT I-MethodName
, O
each O
word O
is O
represented O
by O
a O
sum O
of O
vectors O
, O
with O
each O
vector O
representing O
an O
n O
- O
gram O
. O

In O
this O
paper O
, O
we O
propose O
a O
method O
( O
PFT B-MethodName
) O
that O
provides O
probabilistic O
character O
- O
level O
representations O
of O
words O
. O

The O
sentence O
states O
that O
PFT B-MethodName
can O
model O
rare O
words O
, O
uncertainty O
information O
, O
hierarchical O
representations O
, O
and O
multiple O
word O
senses O
. O

We O
represent O
each O
word O
with O
a O
Gaussian O
or O
a O
Gaussian O
mixture O
density O
, O
which O
we O
name O
PFT B-MethodName
- I-MethodName
G I-MethodName
and O
PFT B-MethodName
- I-MethodName
GM I-MethodName
, O
respectively O
. O

We O
also O
develop O
an O
efficient O
energy O
- O
based O
max O
- O
margin O
training O
procedure O
for O
PFT B-MethodName
. O

We O
compare O
our O
model O
to O
existing O
density O
word O
embeddings O
F B-MethodName
ASTTEXT I-MethodName
, O
W2 B-MethodName
G I-MethodName
, O
and O
W2GM B-MethodName
. O

Our O
models O
are O
able O
to O
extract O
high O
- O
quality O
semantics O
based O
on O
multiple O
word O
- O
similarity O
benchmarks O
, O
including O
the O
rare B-DatasetName
word I-DatasetName
dataset O
. O

We O
average O
a O
weighted O
improvement O
of O
3.7% B-MetricValue
over O
F B-MethodName
ASTTEXT I-MethodName
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
and O
3.1 B-MetricValue
% I-MetricValue
over O
the O
dictionary B-MethodName
- I-MethodName
level I-MethodName
density I-MethodName
- I-MethodName
based I-MethodName
models I-MethodName
. O

Our O
models O
can O
be O
used O
on O
foreign O
languages O
without O
changing O
any O
hyperparameters O
, O
and O
we O
see O
good O
performance O
, O
outperforming O
F B-MethodName
AST- I-MethodName
TEXT I-MethodName
on O
many O
foreign O
word O
similarity O
datasets O
. O

Our O
models O
outperform O
a O
recent O
density O
embedding O
model O
on O
a O
benchmark O
that O
measures O
the O
ability O
to O
separate O
different O
word O
meanings O
. O

The O
W B-MethodName
ORD2VEC I-MethodName
method O
, O
which O
uses O
a O
log O
- O
linear O
model O
and O
negative O
sampling O
approach O
to O
extract O
rich O
semantics O
from O
text O
, O
was O
developed O
in O
2013 O
. O

Another O
popular O
approach O
, O
G B-MethodName
LOVE I-MethodName
, O
involves O
learning O
word O
embeddings O
by O
factorizing O
co O
- O
occurrence O
matrices O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

The O
sentence O
means O
that O
the O
2014 O
version O
of O
the O
software O
uses O
an O
external O
dataset O
called O
WORDNET B-DatasetName
to O
improve O
its O
ability O
to O
identify O
the O
meaning O
of O
words O
. O

We O
compare O
our O
multimodal O
embeddings O
to O
these O
models O
in O
Section O
4.3 O
. O

We O
represent O
each O
word O
with O
a O
Gaussian O
mixture O
of O
K B-HyperparameterName
Gaussian O
components O
. O

This O
sentence O
states O
that O
the O
structure O
of O
F B-MethodName
ASTTEXT I-MethodName
is O
similar O
to O
that O
of O
the O
author O
's O
study O
; O
however O
, O
the O
author O
's O
study O
uses O
single O
- O
prototype O
determinis- O
tic O
embeddings O
, O
while O
F B-MethodName
ASTTEXT I-MethodName
uses O
a O
multi O
- O
prototype O
probabilistic O
embedding O
. O

Figure O
1b O
and O
1c O
depict O
our O
models O
Gaussian B-MethodName
probabilistic I-MethodName
F I-MethodName
ASTTEXT I-MethodName
and O
PFT- B-MethodName
G I-MethodName
, O
as O
well O
as O
Gaussian B-MethodName
mixture I-MethodName
probabilistic I-MethodName
F I-MethodName
ASTTEXT I-MethodName
and O
PFT B-MethodName
- I-MethodName
GM I-MethodName
. O

If O
words O
are O
represented O
by O
distribution O
functions O
, O
we O
use O
the O
generalized O
dot O
product O
in O
Hilbert O
space O
, O
which O
is O
called O
the O
expected B-MetricName
likelihood I-MetricName
kernel I-MetricName
. O

This O
loss O
function O
, O
in O
combination O
with O
the O
Gaussian O
mixture O
model O
, O
can O
extract O
multiple O
senses O
of O
words O
. O

The O
term O
scale B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
in- I-HyperparameterName
verse I-HyperparameterName
covariance I-HyperparameterName
is O
in O
Equation O
3 O
. O

We O
generate O
a O
context O
word O
for O
a O
given O
word O
by O
picking O
a O
nearby O
word O
within O
a O
context O
window O
. O

We O
have O
proposed O
a O
model O
which O
combines O
the O
flexibility O
of O
subword O
structure O
with O
the O
density O
embedding O
approach O
. O

Our O
quantitative O
evaluation O
in O
Section O
4.3 O
demonstrates O
strong O
performance O
compared O
to O
the O
baseline O
models O
F B-MethodName
ASTTEXT I-MethodName
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
and O
the O
dictionary O
- O
level O
Gaussian O
( O
W2 O
G O
) O
( O
Vilnis O
and O
McCallum O
, O
2014 O
) O
and O
Gaussian O
mixture O
embed- O
dings O
( O
Athiwaratkun O
and O
Wilson O
, O
2017 O
) O
( O
W2GM O
) O
. O

The O
sentence O
is O
saying O
that O
English O
uses O
a O
combination O
of O
UKWAC B-DatasetName
and O
W B-DatasetName
ACKY I-DatasetName
PEDIA I-DatasetName
, O
which O
consists O
of O
3.376 O
billion O
words O
. O

We O
use O
three O
text O
corpora O
consisting O
of O
1.634 O
, O
1.716 O
, O
and O
1.955 O
billion O
words O
respectively O
. O
These O
corpora O
are O
in O
French O
, O
German O
, O
and O
Italian O
( O
Baroni O
et O
al O
. O
, O
2009 O
) O
. O

We O
have O
dictionaries O
with O
the O
following O
word O
counts O
: O
1:3 O
, O
2:7 O
, O
and O
1:4 O
million O
words O
for O
F B-DatasetName
RWAC I-DatasetName
, O
DEWAC B-DatasetName
, O
and O
ITWAC B-DatasetName
. O

The O
adjustable O
parameters O
in O
our O
models O
are O
loss B-HyperparameterName
margin I-HyperparameterName
m B-HyperparameterName
in O
Equation O
4 O
and O
scale B-HyperparameterName
. O

We O
search O
for O
the O
optimal O
hyperparameters O
in O
a O
grid O
of O
m B-HyperparameterName
2f O
0.01 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
1 B-HyperparameterValue
, O
10 B-HyperparameterValue
, O
100 B-HyperparameterValue
, O
g O
and O
. O

5*10-3/1 B-HyperparameterValue
, O
10-3/1 B-HyperparameterValue
, O
2*10-4/1 B-HyperparameterValue
, O
1*10-4/1 B-HyperparameterValue
can O
be O
found O
in O
our O
English O
corpus O
. O

The O
scale O
of O
the O
loss O
function O
affects O
how O
we O
adjust O
the O
learn- B-HyperparameterName
ing I-HyperparameterName
rate I-HyperparameterName
. O

The O
learning B-HyperparameterName
rates I-HyperparameterName
are O
particularly O
used O
. O

The O
sentence O
means O
that O
there O
are O
seven O
letters O
in O
the O
alphabet O
, O
with O
A O
being O
the O
first O
letter O
, O
G O
being O
the O
seventh O
letter O
, O
and O
0 O
, O
1 O
, O
2 O
being O
the O
eighth O
, O
ninth O
, O
and O
tenth O
letters O
, O
respectively O
. O

The O
other O
hyperparameters O
that O
are O
ﬁxed O
include O
K B-HyperparameterName
= O
2 B-HyperparameterValue
, O
con- B-HyperparameterName
text I-HyperparameterName
window I-HyperparameterName
length I-HyperparameterName
` O
= O
10 B-HyperparameterValue
and O
subsampling B-HyperparameterName
threshold I-HyperparameterName
t B-HyperparameterName
= O
10-5 B-HyperparameterValue
. O

We O
use O
n O
- O
grams O
where O
the O
setup O
is O
similar O
to O
F B-MethodName
AST I-MethodName
- I-MethodName
TEXT I-MethodName
. O

To O
estimate O
the O
mean O
vectors O
, O
3 B-HyperparameterValue
, O
4 B-HyperparameterValue
, O
5 B-HyperparameterValue
, O
and O
6 B-HyperparameterValue
are O
used O
. O

We O
evaluate O
our O
word O
embeddings O
on O
several O
standard O
datasets O
, O
including O
SL-999 B-DatasetName
( O
Hill O
et O
al O
. O
, O
2014 O
) O
, O
WS-353 B-DatasetName
( O
Finkelstein O
et O
al O
. O
, O
2002 O
) O
, O
MEN-3k B-DatasetName
( O
Bruni O
et O
al O
. O
, O
2014 O
) O
, O
MC-30 B-DatasetName
( O
Miller O
and O
Charles O
, O
1991 O
) O
, O
RG-65 B-DatasetName
( O
Rubenstein O
and O
Goodenough O
, O
1965 O
) O
, O
YP-130 B-DatasetName
( O
Yang O
and O
Powers O
, O
2006 O
) O
, O
MTurk(-287,-771 B-DatasetName
) I-DatasetName
( O
Radinsky O
et O
al O
. O
, O
2011 O
; O
Halawi O
et O
al O
. O
, O
2012 O
) O
, O
and O
RW-2k B-DatasetName
( O
Luong O
et O
al O
. O
, O
2013 O
) O
. O

We O
note O
that O
the O
dataset O
RW B-DatasetName
focuses O
more O
on O
infrequent O
words O
and O
SimLex-999 B-DatasetName
focuses O
on O
the O
similarity O
of O
words O
rather O
than O
relatedness O
. O

We O
compare O
PFT B-MethodName
- I-MethodName
GM I-MethodName
to O
other O
multi O
- O
prototype O
embeddings O
in O
the O
literature O
using O
. O

A O
word O
similar O
to O
SCWS B-DatasetName
was O
found O
in O
Huang O
et O
al O
. O
, O
2012 O
. O

We O
calculate O
the O
Spearman O
correlation O
coefficient O
between O
the O
labels O
and O
our O
scores O
generated O
by O
the O
embeddings O
. O

The O
Spearman B-MetricName
corre- I-MetricName
lation I-MetricName
is O
a O
correlation O
measure O
that O
ranks O
how O
well O
the O
scores O
describe O
the O
true O
labels O
. O

This O
sentence O
can O
be O
rewritten O
as O
: O
In O
the O
Gaussian O
case O
, O
this O
score O
reduces O
to O
a O
cosine O
similarity O
( O
K B-HyperparameterName
= O
1 B-HyperparameterValue
) O
. O

We O
compare O
our O
models O
with O
the O
dictionary- B-MethodName
level I-MethodName
Gaussian I-MethodName
and I-MethodName
Gaussian I-MethodName
mixture I-MethodName
embed- I-MethodName
dings I-MethodName
in O
Table O
2 O
, O
with O
50 O
- O
dimensional O
and O
300 O
- O
dimensional O
mean O
vectors O
. O

The O
results O
for O
W2G B-MethodName
and O
W2GM B-MethodName
in O
50 O
dimensions O
are O
taken O
directly O
from O
Athiwaratkun O
and O
Wilson O
's O
2017 O
study O
. O

We O
compare O
the O
public O
code3 O
to O
train O
the O
300- O
dimensional O
W2G B-MethodName
and O
W2GM B-MethodName
models O
against O
the O
publicly O
available O
F B-MethodName
ASTTEXT I-MethodName
model4 O
. O

We O
calculate O
Spearman B-MetricName
’s I-MetricName
correlations I-MetricName
for O
each O
word O
similarity O
dataset O
. O

Our O
PFT B-MethodName
- I-MethodName
GM I-MethodName
model O
outperforms O
both O
FASTTEXT B-MethodName
and O
the O
dictionary O
- O
level O
embeddings O
W2G B-MethodName
and O
W2GM B-MethodName
, O
achieving O
the O
highest O
average O
score O
among O
all O
competing O
models O
. O

Our O
unimodal O
model O
outperforms O
the O
dictionary O
- O
level O
counterparts O
W2 B-MethodName
G I-MethodName
and O
F B-MethodName
ASTTEXT I-MethodName
. O

The O
model O
W2GM B-MethodName
appears O
to O
be O
stronger O
than O
PFT B-MethodName
- I-MethodName
GM I-MethodName
, O
based O
on O
the O
results O
in O
Table O
2 O
. O

However O
, O
the O
datasets O
that O
W2GM B-MethodName
performs O
better O
than O
PFT B-MethodName
- I-MethodName
GM I-MethodName
often O
have O
small O
sizes O
, O
such O
as O
MC-30 B-DatasetName
or O
RG-65 B-DatasetName
, O
where O
the O
Spearman B-MetricName
’s I-MetricName
correlations I-MetricName
are O
more O
subject O
to O
noise O
. O

In O
general O
, O
PFT B-MethodName
- I-MethodName
GM I-MethodName
achieves O
better O
results O
than O
W2GM B-DatasetName
in O
300 O
and O
50 O
dimensional O
models O
, O
as O
measured O
by O
3.1% B-MetricValue
and O
8.7% B-MetricValue
. O

In O
addition O
, O
PFT B-MethodName
- I-MethodName
G I-MethodName
and O
PFT B-MethodName
- I-MethodName
GM I-MethodName
outperform O
F B-MethodName
ASTTEXT I-MethodName
by O
1.2% B-MetricValue
and O
3.7% B-MetricValue
, O
respectively O
. O

In O
Table O
3 O
, O
we O
compare O
the O
50- O
and O
300 O
- O
dimensional O
PFT B-MethodName
- I-MethodName
GM I-MethodName
models O
against O
the O
multi O
- O
prototype O
embeddings O
described O
in O
Section O
2 O
and O
the O
existing O
multimodal O
density O
embeddings O
W2GM B-MethodName
. O

The O
word O
similarity O
dataset O
SCWS B-DatasetName
( O
Huang O
et O
al O
. O
, O
2012 O
) O
can O
be O
used O
to O
distinguish O
senses O
by O
containing O
words O
with O
potentially O
many O
meanings O
. O

We O
use O
the O
maximum B-MetricName
similarity I-MetricName
score O
( O
denoted O
as O
M B-MetricName
AXSIM I-MetricName
) O
to O
calculate O
the O
results O
. O

The O
AVESIM B-MetricName
denotes O
the O
average B-MetricName
of I-MetricName
the I-MetricName
similarity I-MetricName
scores I-MetricName
, O
not O
the O
maximum O
. O

We O
outperform O
the O
dictionary O
- O
based O
density O
embeddings O
W2GM B-MethodName
in O
both O
50 O
and O
300 O
dimensions O
, O
demonstrating O
the O
benefits O
of O
subword O
in- O
. O

We O
use O
Italian O
models O
Italian B-DatasetName
W I-DatasetName
ORDSIM353 I-DatasetName
and O
Ital- B-DatasetName
ian I-DatasetName
S I-DatasetName
IMLEX-999 I-DatasetName
( O
Leviant O
and O
Reichart O
, O
2015 O
) O
, O
German O
models O
GUR350 B-DatasetName
and O
GUR65 B-DatasetName
( O
Gurevych O
, O
2005 O
) O
, O
and O
French O
model O
W B-DatasetName
ORD- I-DatasetName
SIM353 I-DatasetName
( O
Finkelstein O
et O
al O
. O
, O
2002 O
) O
. O

We O
use O
the O
results O
reported O
in O
the O
F B-MethodName
ASTTEXT I-MethodName
publication O
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
for O
datasets O
GUR350 B-DatasetName
and O
GUR65 B-DatasetName
. O

We O
train O
F B-MethodName
ASTTEXT I-MethodName
models O
for O
comparison O
using O
the O
public O
code5on O
our O
text O
corpuses O
for O
other O
datasets O
. O

We O
also O
train O
dictionary O
- O
level O
models O
W2 B-MethodName
G I-MethodName
and O
W2GM B-MethodName
for O
comparison O
. O

Table O
4 O
presents O
the O
results O
of O
our O
models O
for O
the O
Spearman B-MetricName
’s I-MetricName
correlation I-MetricName
metrics O
. O

We O
are O
better O
than O
F B-MethodName
ASTTEXT I-MethodName
on O
many O
word O
similarity O
tests O
. O

Our O
results O
are O
significantly O
better O
than O
the O
dictionary O
- O
based O
models O
W2G B-MethodName
and O
W2GM B-MethodName
. O

We O
think O
that O
W2G B-MethodName
and O
W2GM B-MethodName
could O
do O
better O
than O
what O
has O
been O
reported O
so O
far O
, O
if O
the O
words O
were O
pre O
- O
processed O
to O
account O
for O
special O
characters O
like O
accents O
. O

It O
is O
possible O
to O
train O
our O
approach O
with O
a O
mixture O
of O
components O
; O
however O
, O
Athiwaratkun O
and O
Wilson O
( O
2017 O
) O
observe O
that O
dictionary O
- O
level O
Gaussian O
mixtures O
with O
do O
not O
overall O
improve O
word O
similarity O
results O
, O
even O
though O
these O
mixtures O
can O
discover O
distinct O
senses O
for O
certain O
words O
. O

K B-HyperparameterName
and O
> B-HyperparameterValue
2 I-HyperparameterValue
allow O
for O
more O
flexibility O
than O
K B-HyperparameterName
and O
2 B-HyperparameterValue
, O
but O
most O
words O
can O
be O
accurately O
modeled O
with O
a O
mixture O
of O
two O
Gaussians O
. O
This O
leads O
to O
K B-HyperparameterName
and O
2 B-HyperparameterValue
, O
which O
is O
a O
good O
balance O
between O
flexibility O
and O
Occam O
's O
razor O
. O

Even O
a O
model O
with O
a O
single O
meaning O
often O
learns O
richer O
representations O
than O
a O
model O
with O
multiple O
meanings O
. O

We O
observe O
that O
our O
model O
with O
more O
components O
can O
capture O
more O
meanings O
. O

For O
example O
, O
the O
word O
pairs O
( O
" O
cell O
" O
, O
" O
jail O
" O
) O
and O
( O
" O
cell O
" O
, O
" O
biology O
" O
) O
and O
( O
" O
cell O
" O
, O
" O
phone O
" O
) O
will O
all O
have O
positive O
similarity O
scores O
based O
on O
the O
1 B-HyperparameterValue
model O
. O

Other O
work O
that O
needs O
to O
be O
done O
involves O
training O
PFT B-MethodName
on O
many O
languages O
. O
