kruszewski@fb.com O


 O
The O
emergence O
of O
number O
and O
syntax O
units O
in O
LSTM B-TaskName
language O
models O
is O
a O
result O
of O
the O
cognitive O
neuroimaging O
unit O
's O
research O
at O
the O
NeuroSpin O
center O
in O
France O
. O

Recent O
work O
has O
shown O
that O
a O
model O
trained O
on O
a O
generic O
language O
modeling O
objective O
captures O
syntax O
- O
sensitive O
generalizations O
, O
such O
as O
long O
- O
distance O
number O
agreement O
. O

We O
provide O
a O
detailed O
study O
of O
how O
individual O
neurons O
keep O
track O
of O
numbers O
in O
the O
LSTMs B-TaskName
network O
. O

Our O
conclusion O
is O
that O
LSTMs B-TaskName
are O
, O
to O
some O
extent O
, O
using O
genuinely O
syntactic O
processing O
mechanisms O
, O
which O
could O
lead O
to O
a O
more O
general O
understanding O
of O
grammatical O
encoding O
in O
LSTMs B-TaskName
. O

In O
recent O
years O
, O
recurrent O
neural O
networks O
( O
RNNs O
) O
have O
been O
successfully O
applied O
to O
a O
variety O
of O
tasks O
, O
especially O
those O
involving O
long O
- O
term O
dependencies O
. O

So O
far O
, O
this O
debate O
has O
mostly O
been O
based O
on O
" O
behavioral O
" O
evidence O
: O
The O
LSTM B-TaskName
has O
been O
treated O
as O
a O
black O
box O
, O
and O
its O
capabilities O
have O
been O
indirectly O
inferred O
from O
its O
performance O
on O
linguistic O
tasks O
. O

In O
this O
study O
, O
we O
took O
an O
approach O
inspired O
by O
neuroscience O
that O
investigates O
the O
inner O
dynamics O
of O
an O
LSTM B-TaskName
language O
model O
to O
understand O
how O
it O
accomplishes O
the O
task O
of O
agreement O
. O

We O
found O
that O
the O
LSTM B-TaskName
had O
specialized O
two O
“ O
grand- O
mother O
” O
cells O
( O
Bowers O
, O
2009 O
) O
to O
carry O
number O
features O
from O
the O
subject O
to O
the O
verb O
across O
the O
intervening O
material O
. O
Interestingly O
, O
the O
LSTM B-TaskName
also O
specialized O
two O
“ O
grand- O
mother O
” O
cells O
to O
carry O
number O
features O
from O
the O
subject O
to O
the O
verb O
across O
the O
intervening O
material O
. O

Our O
analysis O
provides O
evidence O
that O
LSTMs B-TaskName
, O
which O
is O
trained O
on O
unannotated O
corpus O
data O
and O
lacks O
significant O
linguistic O
priors O
, O
can O
learn O
to O
perform O
structure O
- O
dependent O
linguistic O
operations O
. O

This O
suggests O
that O
mechanisms O
such O
as O
those O
implemented O
in O
LSTMs B-TaskName
, O
which O
take O
in O
raw O
linguistic O
input O
and O
store O
generic O
information O
, O
may O
be O
enough O
to O
induce O
the O
formation O
of O
complex O
grammatical O
rules O
. O

In O
2018 O
, O
al O
. O
carried O
out O
a O
study O
that O
showed O
that O
LSTM B-TaskName
language O
models O
perform O
almost O
as O
well O
as O
humans O
on O
the O
agreement O
task O
. O

Gulordava O
and O
colleagues O
provided O
some O
evidence O
that O
the O
LSTMs B-TaskName
are O
relying O
on O
genuine O
syntactic O
generalizations O
, O
but O
Kuncoro O
et O
al O
. O
( O
2018a O
) O
and O
Linzen O
and O
Leonard O
( O
2018 O
) O
suggested O
that O
the O
LSTM B-TaskName
achievements O
can O
, O
at O
least O
in O
part O
, O
be O
accounted O
for O
by O
superficial O
heuristics O
( O
e.g. O
, O
“ O
percolate O
the O
number O
of O
the O
first O
noun O
in O
a O
sentence O
” O
) O
. O

Kementchedjhieva O
and O
Lopez O
found O
a O
character O
- O
level O
RNN B-TaskName
to O
track O
morpheme O
boundaries O
in O
a O
single O
cell O
. O

We O
are O
not O
aware O
of O
any O
other O
studies O
that O
systematically O
characterize O
the O
processing O
of O
a O
linguistic O
phenomenon O
at O
the O
level O
of O
cell O
dynamics O
, O
as O
is O
attempted O
in O
the O
study O
hereby O
presented O
. O

al O
. O


 O
We O
study O
the O
pretrained O
LSTM B-TaskName
language O
model O
made O
available O
by O
Gu O
- O
lordava O
et O
al O
. O

This O
model O
has O
an O
650 B-HyperparameterValue
- I-HyperparameterValue
dimensional I-HyperparameterValue
embedding B-HyperparameterName
layer I-HyperparameterName
, O
two O
650 B-HyperparameterValue
- I-HyperparameterValue
dimensional I-HyperparameterValue
hidden B-HyperparameterName
layers I-HyperparameterName
, O
and O
an O
output O
layer O
with O
vocabulary B-HyperparameterName
size I-HyperparameterName
50,000 B-HyperparameterValue
. O

The O
model O
was O
trained O
on O
Wikipedia B-DatasetName
data I-DatasetName
without O
focusing O
on O
number O
agreement O
, O
and O
still O
obtained O
perplexity O
close O
to O
state O
of O
the O
art O
in O
the O
experiments O
of O
Gulordava O
et O
al.2 O
Number O
- O
Agreement O
Tasks O
. O
We O
complement O
analysis O
of O
the O
naturalistic O
, O
corpus B-DatasetName
- O
derived O
number O
- O
agreement O
test O
set O
of O
Linzen O
. O

The O
situation O
is O
illustrated O
in O
Table O
2 O
, O
which O
shows O
that O
there O
are O
4 O
answers O
in O
which O
the O
subject O
of O
the O
sentence O
is O
a O
boy O
, O
and O
there O
are O
3 O
answers O
in O
which O
the O
subject O
is O
the O
guy O
. O


 O
2.2.2.2 O
. O
Number O
of O
answers O
and O
subject O


 O
The O
table O
below O
shows O
a O
summary O
of O
the O
answers O
that O
were O
given O
by O
the O
participants O
in O
response O
to O
the O
singular O
sentences O
. O


 O
Table O
2 O
: O
Number O
of O
answers O
according O
to O
the O
subject O
of O
the O
sentence O


 O
Number O
of O
answers O
according O
to O
the O
subject O
of O
the O
sentence O
Subject O
boy O
guy O
Total O
number O
of O
answers O
4 O
3 O
7 O


 O
The O
situation O
is O
illustrated O
in O
Table O
2 O
, O
which O
shows O
that O
there O
are O
4 O
answers O
in O
which O
the O
subject O
of O
the O
sentence O
is O
a O
boy O
, O
and O
there O
are O
3 O
answers O
in O
which O
the O
subject O
is O
the O
guy O
. O


 O
2.2.2.3 O
. O
Number O
of O
answers O
according O
to O
type O
of O
answer O


 O
Table O
3 O
shows O
a O
summary O
of O
the O
answers O
that O
were O
given O
by O
the O
participants O
in O
response O
to O
the O
singular O
sentences O
. O


 O
Table O
3 O
: O
Number O
of O
answers O
according O
to O
the O
type O
of O
answer O


 O
Number O
of O
answers O
according O
to O
the O
type O
of O
answer O
Type O
of O
answer O
Simple O
Adv O
Adv+Adv O
Adv+NounPP O
Adv+NounPP+Adv O
Total O
number O
of O
answers O
1 O
1 O
3 O
1 O
1 O
7 O

Each O
synthetic O
number O
- O
agreement O
task O
( O
e.g. O
NA B-TaskName
- O
task O
) O
instantiates O
a O
fixed O
syntactic O
structure O
with O
varied O
lexical O
material O
, O
in O
order O
to O
probe O
subject O
- O
verb O
number O
agreement O
in O
controlled O
and O
increasingly O
challenging O
setups O
. O
The O
different O
structures O
are O
illustrated O
in O
Table O
1 O
, O
where O
all O
forms O
are O
in O
the O
singular O
. O

For O
every O
task O
NA B-TaskName
, O
we O
generated O
both O
a O
singular O
and O
plural O
version O
of O
each O
sentence O
. O

For O
all O
tasks O
in O
the O
set O
NA B-TaskName
, O
each O
condition O
consisted O
of O
600 O
sentences O
. O
We O
probed O
the O
model O
's O
ability O
to O
implicitly O
parse O
syntax O
by O
testing O
it O
on O
various O
conditions O
. O

The O
final O
data O
set O
contains O
4,033 O
positions O
from O
1,303 O
sentences O
. O
To O
successfully O
perform O
the O
NA B-TaskName
- O
task O
, O
the O
LSTM B-TaskName
should O
: O
( O
1 O
) O
encode O
and O
store O
the O
grammatical O
number O
of O
the O
subject O
; O
and O
( O
2 O
) O
track O
the O
main O
subject O
- O
verb O
syntactic O
dependency O
. O

This O
section O
describes O
the O
' O
neural O
circuit O
' O
that O
encodes O
and O
processes O
information O
in O
the O
LSTM B-TaskName
. O

We O
first O
tested O
the O
performance O
of O
the O
LSTM B-TaskName
on O
the O
Linzen O
's O
data O
and O
on O
the O
NA B-TaskName
- O
tasks O
in O
Table O
1 O
. O

We O
computed O
the O
likelihood O
that O
the O
LSTM B-TaskName
assigns O
to O
the O
main O
verb O
of O
each O
sentence O
given O
the O
preceding O
context O
and O
compared O
it O
to O
the O
likelihood O
it O
assigns O
to O
the O
wrong O
verb O
inflection O
. O

Our O
results O
on O
the O
Linzen O
NA B-TaskName
- O
task O
confirm O
previous O
findings O
by O
Gulordava O
et O
al O
. O
( O
2018 O
) O
. O

The O
results O
of O
the O
other O
NA B-TaskName
tasks O
show O
that O
some O
tasks O
and O
conditions O
are O
more O
difficult O
than O
others O
. O

The O
task O
at O
hand O
is O
better O
than O
the O
one O
before O
it O
, O
which O
is O
better O
than O
the O
one O
before O
that O
. O

Second O
, O
as O
expected O
, O
network O
performance O
is O
reduced O
in O
conditions O
where O
the O
numbers O
do O
n't O
match O
( O
the O
namePP B-TaskName
, O
nounPP B-TaskName
, O
and O
nounPPAdv B-TaskName
conditions O
) O
. O

The O
results O
of O
the O
ablation O
experiments O
are O
shown O
in O
Table O
2 O
. O

The O
percentage O
of O
accuracy O
in O
all O
tasks O
NA B-TaskName
. O

For O
long O
- O
range O
dependencies O
, O
encoding O
a O
singular O
subject O
across O
an O
interfering O
noun O
is O
more O
difficult O
than O
a O
plural O
subject O
: O
for O
both O
nounPP B-TaskName
and O
nounPPAdv B-TaskName
, O
PS O
is O
easier O
than O
SP O
. O

If O
these O
units O
were O
removed O
, O
it O
would O
lead O
to O
a O
significant O
decrease O
in O
performance O
on O
the O
NA B-TaskName
tasks O
. O

We O
tested O
the O
network O
by O
fixing O
the O
activation O
of O
each O
unit O
to O
zero O
, O
one O
at O
a O
time O
, O
and O
tested O
it O
on O
the O
NA B-TaskName
- O
tasks O
. O

The O
columns O
in O
Table O
2 O
labeled O
776 O
and O
988 O
had O
the O
biggest O
impact O
on O
network O
performance O
. O
Without O
those O
columns O
, O
network O
performance O
would O
be O
reduced O
by O
more O
than O
10 B-MetricValue
% I-MetricValue
under O
various O
conditions O
. O
It O
's O
especially O
important O
to O
note O
that O
those O
columns O
were O
the O
only O
ones O
whose O
removal O
consistently O
brought O
network O
performance O
down O
to O
around O
the O
level O
of O
chance O
in O
the O
more O
difficult O
incongruent O
conditions O
of O
the O
namePP B-TaskName
, O
nounPP B-TaskName
, O
and O
nounPPAdv B-TaskName
tasks O
. O

Although O
the O
Linzen O
NA B-TaskName
- O
task O
contained O
mixed O
stimuli O
from O
many O
types O
of O
conditions O
, O
we O
found O
that O
the O
plural O
unit O
had O
a O
substantial O
effect O
on O
average O
network O
performance O
. O

This O
case O
did O
not O
show O
a O
similar O
effect O
with O
singular O
units O
, O
which O
highlights O
the O
importance O
of O
using O
carefully O
crafted O
stimuli O
, O
as O
in O
the O
nounPP B-TaskName
and O
nounPPAdv B-TaskName
tasks O
, O
to O
understand O
network O
dynamics O
. O

The O
task O
we O
focus O
on O
is O
the O
simplest O
task O
that O
includes O
a O
long O
- O
range O
dependency O
with O
an O
interfering O
noun O
, O
in O
both O
SP O
and O
PS O
conditions O
. O

Recall O
the O
standard O
memory O
update O
and O
output O
rules O
LSTM B-TaskName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

We O
do O
not O
make O
precise O
predictions O
for O
gate O
and O
cell O
activity O
at O
time O
points O
unrelated O
to O
solving O
the O
NA B-TaskName
- O
task O
, O
so O
we O
mask O
them O
with O
white O
. O

The O
update O
rule O
for O
the O
LSTM B-TaskName
cell O
has O
two O
terms O
( O
Eq O
. O
1 O
) O
. O
The O
first O
term O
, O
ft*Ct-1 O
, O
the O
forget O
gate O
controls O
whether O
to O
keep O
the O
previous O
cell O
content O
( O
ft=1 O
: O
perfect O
remembering O
) O
or O
forget O
it O
( O
ft=0 O
: O
complete O
forgetting O
) O
. O

The O
performance O
on O
the O
easier O
tasks O
was O
not O
impaired O
by O
the O
single O
- O
unit O
ablations O
. O

We O
trained O
a O
linear O
model O
to O
predict O
the O
grammatical O
number O
of O
the O
subject O
from O
network O
activity O
in O
response O
to O
the O
presentation O
of O
the O
subject O
, O
and O
tested O
its O
prediction O
on O
test O
sets O
from O
all O
time O
points O
( O
King O
and O
Dehaene O
, O
2014 O
) O
, O
in O
incongruent O
conditions O
only O
of O
the O
nounPP B-TaskName
task O
. O

We O
evaluated O
model O
performance O
using O
Area B-MetricName
under I-MetricName
of I-MetricName
Curve I-MetricName
( O
AUC B-MetricName
) O
. O

The O
results O
show O
that O
number O
information O
can O
be O
decoded O
efficiently O
from O
other O
units O
in O
the O
network O
, O
and O
that O
this O
information O
can O
be O
carried O
for O
several O
time O
steps O
up O
to O
the O
second O
determiner O
. O

However O
, O
the O
way O
in O
which O
these O
units O
encode O
number O
is O
sensitive O
to O
the O
last O
encountered O
noun O
, O
decreasing O
from O
AUC B-MetricName
. O

17a O
. O
The O
activity O
of O
syntax O
unit O
1150 O
while O
processing O
various O
syntactic O
structures O
is O
shown O
in O
Figure O
3 O
. O


 O
17b O
. O
The O
activity O
of O
syntax O
unit O
1150 O
while O
processing O
various O
syntactic O
structures O
is O
shown O
in O
Figure O
3 O
. O


 O
17c O
. O
The O
activity O
of O
syntax O
unit O
1150 O
while O
processing O
various O
syntactic O
structures O
is O
shown O
in O
Figure O
3 O
. O


 O
17d O
. O
The O
activity O
of O
syntax O
unit O
1150 O
while O
processing O
various O
syntactic O
structures O
is O
shown O
in O
Figure O
3 O
. O

The O
average O
value O
across O
all O
stimuli O
in O
an O
NA B-TaskName
- O
task O
is O
shown O
, O
with O
error O
bars O
representing O
the O
standard O
deviation O
. O

The O
task O
stimuli O
were O
specifically O
generated O
for O
this O
visualization O
. O

The O
units O
were O
only O
sensitive O
to O
the O
last O
encountered O
noun O
in O
regards O
to O
subject O
number O
and O
activity O
swaps O
. O

To O
validate O
the O
role O
of O
SR O
- O
number O
units O
in O
encoding O
number O
for O
easier O
NA B-TaskName
- O
tasks O
, O
we O
ablated O
both O
SR O
and O
LR O
number O
units O
( O
12 O
in O
total O
) O
or O
SR O
units O
only O
( O
10 O
in O
total O
) O
and O
evaluated O
network O
performance O
on O
these O
NA O
- O
tasks O
. O

Since O
the O
interpretation O
of O
the O
regression O
weights O
may O
be O
affected O
by O
possible O
correlations O
among O
the O
features O
, O
we O
also O
tested O
the O
causal O
effect O
of O
these O
units O
on O
NA B-TaskName
- O
task O
performance O
. O

Removing O
the O
syntax O
units O
together O
resulted O
in O
a O
significant O
decrease O
in O
performance O
for O
tasks O
that O
have O
an O
interfering O
noun O
. O

Linzen O
NA B-TaskName
- O
task O
: O
p O
= O
0:024 O
, O
nounPPAdv- O
. O


 O
This O
sentence O
is O
saying O
that O
Linzen O
NA B-TaskName
- O
task O
: O
p O
= O
0:024 O
is O
a O
noun O
phrase O
that O
modifies O
the O
adverb O
phrase O
" O
nounPPAdv- O
. O
" O

There O
was O
a O
significant O
difference O
between O
the O
two O
groups O
in O
nounPPAdv B-TaskName
, O
nounPP B-TaskName
, O
and O
nounPP B-TaskName
, O
with O
the O
group O
receiving O
the O
intervention O
having O
a O
lower O
p O
- O
value O
in O
all O
three O
cases O
. O

Figures O
3a O
and O
3b O
show O
the O
cell O
activity O
of O
this O
unit O
during O
the O
processing O
of O
stimuli O
from O
the O
2Adv O
and O
nounPP B-TaskName
tasks O
. O

We O
look O
at O
the O
connections O
between O
syntax O
unit O
1150 O
, O
which O
appears O
to O
be O
more O
closely O
involved O
in O
tracking O
subject O
- O
verb O
agreement O
, O
and O
the O
LR O
number O
units O
, O
as O
well O
as O
at O
the O
connections O
between O
the O
LR O
number O
units O
themselves O
. O

Figures O
4a O
and O
4b O
show O
the O
distribution O
of O
all O
the O
afferent O
recurrent O
weights O
to O
the O
input O
and O
forget O
gates O
of O
the O
LR O
units O
, O
scaled O
by O
the O
maximal O
activity O
of O
the O
pre O
- O
synaptic O
units O
during O
the O
nounPP B-TaskName
task O
. O
This O
scaling O
evaluates O
the O
effective O
input O
to O
the O
units O
and O
did O
not O
change O
the O
conclusions O
described O
below O
. O

We O
were O
the O
first O
to O
provide O
a O
detailed O
description O
of O
how O
an O
LSTM B-TaskName
language O
- O
model O
performs O
long O
- O
distance O
number O
agreement O
. O

In O
other O
words O
, O
training O
a O
model O
on O
raw O
text O
data O
using O
a O
language O
- O
model O
objective O
caused O
individual O
units O
within O
the O
model O
to O
learn O
very O
specific O
linguistic O
information O
. O

This O
research O
suggests O
that O
when O
training O
a O
language O
model O
, O
highly O
local O
encoding O
of O
linguistic O
features O
can O
emerge O
, O
as O
has O
been O
suggested O
by O
previous O
studies O
of O
artificial O
neural O
networks O
and O
neuroscience O
. O

The O
relationship O
we O
uncovered O
between O
syntax O
and O
number O
units O
suggests O
that O
agreement O
in O
an O
LSTM B-TaskName
language O
model O
can O
not O
be O
entirely O
explained O
away O
by O
superﬁcial O
heuristics O
, O
and O
the O
networks O
have O
, O
to O
some O
extent O
, O
learned O
to O
build O
and O
exploit O
structure O
- O
based O
syntactic O
representations O
, O
akin O
to O
those O
conjectured O
to O
support O
human O
- O
sentence O
processing O
. O

We O
hope O
that O
our O
study O
will O
inspire O
more O
analyses O
of O
the O
inner O
dynamics O
of O
LSTMs B-TaskName
and O
other O
sequence O
- O
processing O
networks O
, O
which O
would O
complement O
the O
currently O
popular O
" O
black O
- O
box O
probing O
" O
approach O
. O

RNN B-TaskName
simulations O
show O
that O
people O
can O
make O
grammaticality O
judgments O
on O
long O
- O
distance O
dependencies O
. O

The O
sentence O
is O
about O
a O
workshop O
that O
took O
place O
in O
Brussels O
, O
Belgium O
. O

At O
the O
2018 O
EMNLP B-TaskName
Workshop O
, O
it O
was O
noted O
that O
black- O
. O

This O
sentence O
is O
in O
all O
capital O
letters O
. O

The O
analysis O
and O
interpretation O
of O
neural O
networks O
for O
NLP B-TaskName
can O
be O
found O
on O
pages O
222 O
- O
231 O
. O

This O
sentence O
is O
from O
a O
paper O
discussing O
the O
analysis O
and O
interpretation O
of O
neural O
networks O
. O

LSTMs B-TaskName
performs O
better O
when O
modeling O
structure O
is O
taken O
into O
account O
when O
learning O
syntax O
- O
sensitive O
dependencies O
. O

Visualizing O
and O
understanding O
neural O
models O
in O
NLP B-TaskName
. O

To O
what O
extent O
does O
LSTMs B-TaskName
struggle O
with O
learning O
syntax O
- O
sensitive O
dependencies O
? O

What O
do O
language O
models O
that O
use O
the O
RNN B-TaskName
algorithm O
learn O
about O
dependencies O
between O
fillers O
and O
gaps O
? O

In O
the O
2018 O
EMNLP B-TaskName
Workshop O
, O
it O
was O
discussed O
that O
black- O
. O

The O
sentence O
can O
not O
be O
paraphrased O
because O
it O
is O
incomplete O
. O

Analyzing O
and O
interpreting O
neural O
networks O
for O
NLP B-TaskName
can O
be O
found O
on O
pages O
211 O
- O
221 O
. O
