Proceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
1030–1039 O
August O
1–6 O
, O
2021 O
. O

© O
2021 O
Association O
for O
Computational O
Linguistics1030A O
DQN O
- O
based O
Approach O
to O
Finding O
Precise O
Evidences O
for O
Fact O
Veriﬁcation O
Hai O
Wan1 O
, O
Haicheng O
Chen1 O
, O
Jianfeng O
Du2,3 O
, O
Weilin O
Luo1 O
, O
Rongzhen O
Ye1 O
1School O
of O
Computer O
Science O
and O
Engineering O
, O
Sun O
Yat O
- O
sen O
University O
, O
Guangzhou O
510006 O
, O
P.R.China O
2Guangzhou O
Key O
Laboratory O
of O
Multilingual O
Intelligent O
Processing O
, O
Guangdong O
University O
of O
Foreign O
Studies O
, O
Guangzhou O
510006 O
, O
P.R.China O
3Pazhou O
Lab O
, O
Guangzhou O
510330 O
, O
P.R.China O
wanhai@mail.sysu.edu.cn O
, O
jfdu@gdufs.edu.cn O
, O
fchenhch8 O
, O
luowlin3 O
, O
yerzh O
g@mail2.sysu.edu.cn O

Abstract O
Computing O
precise O
evidences O
, O
namely O
mini- O
mal O
sets O
of O
sentences O
that O
support O
or O
refute O
a O
given O
claim O
, O
rather O
than O
larger O
evidences O
is O
cru- O
cial O
in O
fact O
veriﬁcation O
( O
FV O
) O
, O
since O
larger O
ev- O
idences O
may O
contain O
conﬂicting O
pieces O
some O
of O
which O
support O
the O
claim O
while O
the O
other O
refute O
, O
thereby O
misleading O
FV O
. O

Despite O
being O
important O
, O
precise O
evidences O
are O
rarely O
stud- O
ied O
by O
existing O
methods O
for O
FV O
. O

It O
is O
challeng- O
ing O
to O
ﬁnd O
precise O
evidences O
due O
to O
a O
large O
search O
space O
with O
lots O
of O
local O
optimums O
. O

In- O
spired O
by O
the O
strong O
exploration O
ability O
of O
the O
deep O
Q O
- O
learning O
network O
( O
DQN O
) O
, O
we O
propose O
a O
DQN O
- O
based O
approach O
to O
retrieval O
of O
precise O
ev- O
idences O
. O

In O
addition O
, O
to O
tackle O
the O
label O
bias O
on O
Q O
- O
values O
computed O
by O
DQN O
, O
we O
design O
a O
post- O
processing O
strategy O
which O
seeks O
best O
thresh- O
olds O
for O
determining O
the O
true O
labels O
of O
com- O
puted O
evidences O
. O

Experimental O
results O
conﬁrm O
the O
effectiveness O
of O
DQN O
in O
computing O
pre- O
cise O
evidences O
and O
demonstrate O
improvements O
in O
achieving O
accurate O
claim O
veriﬁcation.1 O
1 O
Introduction O
With O
the O
growing O
false O
information O
, O
such O
as O
fake O
news O
, O
political O
deception O
and O
online O
rumors O
, O
auto- O
matic O
fact O
- O
checking O
systems O
have O
emerged O
to O
auto- O
matically O
identify O
and O
ﬁlter O
this O
information O
. O

Fact O
veriﬁcation O
( O
FV O
) O
is O
a O
special O
fact O
- O
checking O
task O
that O
aims O
to O
retrieve O
related O
evidences O
from O
a O
text O
corpus O
to O
verify O
a O
textual O
claim O
. O

Taking O
Figure O
1 O
as O
example O
, O
an O
existing O
method O
for O
FV O
ﬁrst O
retrieves O
related O
documents O
from O
the O
given O
corpus O
at O
stage O
1 O
( O
namely O
the O
document O
re- O
trieval O
stage O
) O
, O
then O
ﬁnds O
key O
sentences O
from O
the O
documents O
at O
stage O
2 O
( O
namely O
the O
sentence O
selec- O
tion O
stage O
) O
, O
and O
ﬁnally O
treats O
the O
set O
of O
key O
sen- O
tences O
as O
an O
evidence O
to O
verify O
the O
claim O
at O
stage O
Corresponding O
author O
1Source O
code O
and O
data O
are O
available O
at O
https:// O
github.com/sysulic/DQN-FV O
. O

Figure O
1 O
: O
The O
pipeline O
for O
FV O
on O
FEVER O
. O

Underlined O
words O
in O
blue O
italics O
given O
in O
evidence O
provide O
key O
in- O
formation O
to O
determine O
the O
truthfulness O
of O
the O
claim O
. O

“ O
SUPPORTS O
” O
/ O
“ O
REFUTES O
” O
/ O
“ O
NOT O
ENOUGH O
INFO O
” O
indicates O
that O
the O
evidence O
can O
support O
/ O
refute O
/ O
is O
in- O
sufﬁcient O
for O
supporting O
or O
refuting O
the O
claim O
. O

Both O
the O
evidence O
and O
label O
are O
output O
by O
FV O
. O

3 O
( O
namely O
the O
claim O
veriﬁcation O
stage O
) O
. O

As O
can O
be O
seen O
in O
this O
example O
, O
it O
is O
desirable O
to O
retrieve O
an O
evidence O
consisting O
of O
the O
ﬁrst O
two O
sentences O
only O
, O
since O
it O
does O
not O
contain O
unnecessary O
sentences O
to O
determine O
the O
truthfulness O
of O
the O
claim O
and O
can O
alle- O
viate O
human O
efforts O
to O
further O
validate O
the O
evidence O
. O

More O
importantly O
, O
an O
evidence O
containing O
unneces- O
sary O
sentences O
may O
involve O
conﬂicting O
pieces O
some O
of O
which O
support O
the O
claim O
while O
the O
other O
refute O
the O
claim O
. O

Thus O
, O
it O
is O
crucial O
to O
compute O
minimal O
sets O
of O
sentences O
that O
can O
determine O
the O
truthfulness O
of O
the O
claim O
. O

In O
this O
paper O
, O
we O
refer O
to O
a O
minimal O
set O
of O
sentences O
that O
supports O
or O
refutes O
a O
given O
claim O
as O
aprecise O
evidence O
. O

Existing O
methods O
for O
FV O
do O
not O
target O
the O
re- O
trieval O
of O
precise O
evidences O
. O

Most O
existing O
stud- O
ies O
( O
Thorne O
et O
al O
. O
, O
2018b O
; O
Nie O
et O
al O
. O
, O
2019 O
; O
Zhou O
et O

al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2020 O
; O
Zhong O
et O
al O
. O
, O
2020 O
; O
Ye O
et O
al O
. O
, O
2020 O
; O
Subramanian O
and O
Lee O
, O
2020 O
; O
Wang O
et O
al O
. O
, O
2020 O
) O
formulate O
FV O
as O
a O
three O
- O
stage O
pipeline O

1031task O
as O
illustrated O
in O
Figure O
1 O
. O

This O
way O
makes O
the O
retrieval O
of O
precise O
evidences O
extremely O
difﬁ- O
cult O
since O
the O
sentence O
selection O
stage O
is O
required O
to O
select O
a O
precise O
set O
of O
relevant O
sentences O
rather O
than O
a O
ﬁxed O
number O
of O
sentences O
as O
in O
existing O
methods O
. O

To O
the O
best O
of O
our O
knowledge O
, O
TwoWin- O
gOS O

( O
Yin O
and O
Roth O
, O
2018 O
) O
is O
the O
only O
method O
by O
now O
which O
does O
not O
follow O
the O
three O
- O
stage O
pipeline O
. O

Instead O
, O
it O
exploits O
a O
supervised O
training O
scheme O
to O
train O
the O
last O
two O
stages O
jointly O
and O
is O
able O
to O
compute O
precise O
evidences O
. O

However O
, O
it O
exhibits O
a O
signiﬁcantly O
worse O
performance O
than O
other O
state O
- O
of- O
the O
- O
art O
methods O
for O
FV O
, O
especially O
in O
terms O
of O
the O
recall O
of O
evidences O
. O

Therefore O
, O
there O
is O
still O
a O
need O
for O
designing O
new O
methods O
to O
compute O
precise O
ev- O
idences O
. O

These O
methods O
are O
expected O
to O
achieve O
better O
performance O
than O
TwoWingOS O
. O

It O
is O
challenging O
to O
compute O
precise O
evidences O
. O

On O
one O
hand O
, O
the O
search O
space O
for O
precise O
evi- O
dences O
is O
very O
large O
. O

For O
example O
, O
in O
the O
bench- O
mark O
Fact O
Extraction O
and O
VERiﬁcation O
( O
FEVER O
) O
dataset O
( O
Thorne O
et O
al O
. O
, O
2018b O
) O

the O
average O
num- O
ber O
of O
sentences O
for O
each O
claim O
input O
to O
the O
sen- O
tence O
selection O
stage O
is O
40 O
, O
and O
an O
output O
evidence O
has O
up O
to O
5sentences O
. O

Hence O
there O
are O
up O
toP5 O
i=1Ci O
40= O
760;098 O
candidates O
in O
the O
search O
space O
. O

On O
the O
other O
hand O
, O
greedy O
search O
of O
pre- O
cise O
evidences O
easily O
falls O
into O
a O
local O
optimum O
. O

As O
shown O
in O
our O
experiments O
( O
see O
Table O
6 O
) O
, O
a O
greedy O
search O
method O
does O
not O
perform O
well O
. O

Inspired O
by O
the O
strong O
exploration O
ability O
of O
the O
Deep O
Q O
- O
learning O
Network O
( O
DQN O
) O
( O
Mnih O
et O
al O
. O
, O
2015 O
) O
, O
we O
develop O
a O
DQN O
- O
based O
approach O
to O
re- O
trieval O
of O
precise O
evidences O
. O

In O
this O
approach O
, O
we O
ﬁrst O
employ O
DQN O
to O
compute O
candidate O
pairs O
of O
precise O
evidences O
and O
their O
labels O
, O
and O
then O
use O
a O
post O
- O
processing O
strategy O
to O
reﬁne O
candidate O
pairs O
. O

We O
notice O
that O
Q O
- O
values O
computed O
by O
DQN O
has O
label O
bias O
due O
to O
two O
reasons O
. O

On O
one O
hand O
, O
the O
label O
“ O
NOT O
ENOUGH O
INFO O
” O
does O
not O
locate O
at O
the O
same O
concept O
level O
as O
“ O
SUPPORTS O
” O
or O
“ O
RE- O
FUTES O
” O
. O

On O
the O
other O
hand O
, O
there O
is O
not O
a O
ﬁxed O
range O
for O
Q O
- O
values O
, O
making O
Q O
- O
values O
hard O
to O
accu- O
rately O
estimate O
. O

Thus O
, O
a O
post O
- O
processing O
strategy O
is O
needed O
to O
tackle O
the O
label O
bias O
on O
Q O
- O
values O
. O

We O
develop O
such O
a O
strategy O
to O
seek O
best O
thresholds O
in O
determining O
the O
true O
labels O
of O
computed O
evidences O
. O

Our O
experimental O
results O
on O
FEVER O
( O
Thorne O
et O
al O
. O
, O
2018b O
) O
conﬁrm O
that O
our O
DQN O
- O
based O
ap- O
proach O
is O
effective O
in O
ﬁnding O
precise O
evidences O
. O

More O
importantly O
, O
the O
approach O
is O
shown O
to O
outper O
- O
form O
state O
- O
of O
- O
the O
- O
art O
methods O
for O
FV O
. O

2 O
Related O
Work O
2.1 O
Fact O
Extraction O
and O
Claim O
Veriﬁcation O
The O
FEVER O
1.0 O
shared O
task O
( O
Thorne O
et O
al O
. O
, O
2018b O
) O
aims O
to O
develop O
an O
automatic O
fact O
veriﬁcation O
system O
to O
determine O
the O
truthfulness O
of O
a O
tex- O
tual O
claim O
by O
extracting O
related O
evidences O
from O
Wikipedia O
. O

Thorne O
et O

al O
. O

( O
2018a O
) O
has O
formalized O
this O
task O
, O
released O
a O
large O
- O
scale O
benchmark O
dataset O
FEVER O
( O
Thorne O
et O
al O
. O
, O
2018b O
) O
, O
and O
designed O
the O
three O
- O
stage O
pipeline O
framework O
for O
FV O
, O
which O
con- O
sists O
of O
the O
document O
retrieval O
stage O
, O
the O
sentence O
selection O
stage O
and O
the O
claim O
veriﬁcation O
stage O
. O

Most O
existing O
methods O
follow O
this O
framework O
and O
mainly O
focus O
on O
the O
last O
stage O
( O
Liu O
et O
al O
. O
, O
2020 O
) O
. O

For O
the O
document O
retrieval O
stage O
, O
most O
methods O
reuse O
the O
document O
retrieval O
component O
of O
top- O
performing O
systems O
( O
Hanselowski O
et O
al O
. O
, O
2018 O
; O
Yoneda O
et O
al O
. O
, O
2018 O
; O
Nie O
et O
al O
. O
, O
2019 O
) O
. O

For O
the O
sen- O
tence O
selection O
stage O
, O
there O
are O
three O
approaches O
commonly O
used O
, O
including O
keyword O
matching O
, O
su- O
pervised O
classiﬁcation O
, O
and O
sentence O
similarity O
scor- O
ing O
( O
Thorne O
et O
al O
. O
, O
2018b O
) O
. O

For O
the O
claim O
veriﬁca- O
tion O
stage O
, O
most O
recent O
studies O
formulate O
this O
task O
as O
a O
graph O
reasoning O
task O
( O
Zhou O
et O
al O
. O
, O
2019 O
; O

Liu O
et O
al O
. O
, O
2020 O
; O
Ye O
et O
al O
. O
, O
2020 O
; O
Zhong O
et O
al O
. O
, O
2020 O
; O
Subramanian O
and O
Lee O
, O
2020 O
; O
Wang O
et O
al O
. O
, O
2020 O
) O
. O

Different O
from O
most O
existing O
methods O
that O
focus O
on O
claim O
veriﬁcation O
, O
Yin O
and O
Roth O
( O
2018 O
) O
proposed O
a O
supervised O
training O
method O
named O
TwoWingOS O
to O
jointly O
conduct O
sentence O
selection O
and O
claim O
veriﬁ- O
cation O
. O

Nowadays O
pre O
- O
trained O
language O
models O
like O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
have O
been O
widely O
used O
in O
claim O
veriﬁcation O
( O
Li O
et O
al O
. O
, O
2019 O
; O
Zhou O
et O

al O
. O
, O
2019 O
; O
Soleimani O
et O
al O
. O
, O
2020 O
) O
. O

Following O
this O
way O
we O
employed O
RoBERTa O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
an O
en- O
hanced O
version O
of O
BERT O
, O
as O
the O
sentence O
encoder O
in O
our O
DQN O
- O
based O
approach O
in O
experiments O
. O

2.2 O
Deep O
Q O
- O
learning O
Network O
Reinforcement O
learning O
( O
RL O
) O
is O
about O
an O
agent O
in- O
teracting O
with O
the O
environment O
, O
objective O
to O
max- O
imize O
the O
cumulative O
rewards O
of O
a O
sequence O
of O
states O
and O
actions O
by O
adjusting O
its O
policies O
. O

Q- O
Learning O
( O
Mnih O
et O
al O
. O
, O
2015 O
) O
is O
a O
popular O
reinforce- O
ment O
learning O
technique O
. O

It O
aims O
to O
approximate O
the O
optimal O
value O
function O
Q(o;a)to O
measure O
the O
expected O
long O
- O
term O
rewards O
for O
a O
given O
pair O
of O
state O
oand O
actiona O
. O

Deep O
Q O
- O
learning O
Network O
( O
DQN O
) O

1032(Mnih O
et O

al O
. O
, O
2015 O
) O
is O
a O
combination O
of O
deep O
learn- O
ing O
and O
Q O
- O
Learning O
. O

It O
typically O
uses O
the O
following O
Equation O
( O
1 O
) O
derived O
from O
the O
Bellman O
equation O
( O
Cao O
and O
ZhiMin O
, O
2019 O
) O
to O
approximate O
the O
opti- O
mal O
Q O
- O
value O
function O
: O
Q(o(t);a(t O
) O
) O

= O
Eo(t+1)[r(t)+max O
a0Q(o(t+1);a0 O
) O
] O
; O
( O
1 O
) O
whereo(t);a(t);r(t)respectively O
denote O
the O
state O
, O
action O
and O
reward O
at O
step O
t O
, O
and2[0;1]is O
a O
discounted O
factor O
for O
future O
rewards O
. O

3 O
Approach O
3.1 O
Problem O
Setting O
Given O
a O
set O
of O
candidate O
sentences O
S O
= O
fs1,s2 O
, O
. O
. O

.g O
, O
a O
claimc O
, O
a O
set O
of O
precise O
evidences O
E2S O
, O
and O
a O
true O
label O
y2Y O
= O
fT O
, O
F O
, O
Ngthat O
deter- O
mines O
whether O
every O
precise O
evidence O
supports O
or O
refutes O
the O
claim O
, O
where O
T O
= O
F O
= O
Ndenotes O
“ O
SUP- O
PORTS”/“REFUTES”/“NOT O
ENOUGH O
INFO O
” O
, O
we O
aim O
to O
train O
a O
model O
to O
predict O
a O
precise O
evi- O
dence O
; O
more O
precisely O
, O
to O
train O
a O
model O
for O
retriev- O
ing O
an O
evidence O
^ESand O
predicting O
a O
label O
^y2Ysuch O
that O
^y O
= O
yand O
^E O
= O
Efor O
some O
E2E. O

This O
goal O
is O
different O
from O
the O
goal O
tar- O
geted O
by O
existing O
methods O
, O
which O
aim O
to O
retrieve O
an O
evidence O
^ESand O
predict O
a O
label O
^y2Ysuch O
that^y O
= O
yandE^Efor O
someE2E. O
We O
deﬁne O
the O
four O
ingredients O
of O
DQN O
namely O
states O
, O
actions O
, O
transitions O
and O
rewards O
as O
follows O
: O
State O
. O

A O
stateois O
a O
tuple O
( O
c;^E;^y)forca O
claim O
, O
^Ea O
set O
of O
sentences O
and O
^ya O
label O
. O

Action O
. O

An O
actionais O
a O
sentence O
in O
S. O
Transition O
. O

A O
transition O
at O
step O
tis O
a O
tuple O
( O
o(t);a(t);o(t+1 O
) O
) O
, O
whereo(t)= O
( O
c;^E(t);^y O
) O
, O
o(t+1)= O
( O
c;^E(t+1);^y)and^E(t+1)=^E(t O
) O
[ O
fa(t)g O
. O
Reward O
. O

The O
reward O
rfor O
a O
transition O
( O
o(t);a(t);o(t+1))is O
deﬁned O
as O
r(t)=8 O
> O
< O
> O
: O
1;^y O
= O
y^(y O
= O
N_9E2E O
: O
a(t)2E O
) O
 1;^y6 O
= O
y^j^E(t+1)j O
= O
K O
0;otherwise O
( O
2 O
) O
where O
the O
number O
Kis O
a O
hyper O
- O
parameter O
, O
and O
jSjdenotes O
the O
cardinality O
of O
a O
set O
S. O
3.2 O
The O
DQN O
- O
based O
Model O

The O
core O
of O
our O
proposed O
approach O
is O
the O
DQN- O
based O
model O
, O
illustrated O
in O
Figure O
2.3.2.1 O
Sentence O
Encoding O
Module O
We O
employ O
RoBERTa O
in O
this O
module O
to O
extract O
the O
ﬁnal O
hidden O
state O
of O
hsias O
the O
sentence O
representa- O
tion O
, O
wherehsiandh O
/ O
simentioned O
in O
the O
following O
are O
the O
special O
classiﬁcation O
tokens O
in O
RoBERTa O
. O

Speciﬁcally O
, O
following O
KGAT O
( O
Liu O
et O
al O
. O
, O
2020 O
) O
, O
we O
ﬁrst O
concatenate O
the O
claim O
c O
, O
the O
document O
titlel O
, O
and O
a O
sentence O
s(resp O
. O

an O
actiona O
) O
as O
“ O
hsich O
/ O
silh O
/ O
sish O
/ O
si O
” O
( O
resp O
. O

“ O
hsich O
/ O
silh O
/ O
siah O
/ O
si O
” O
) O
and O
then O
feed O
it O
into O
RoBERTa O
to O
obtain O
the O
sentence O
representation O
hs2Rd0(resp O
. O

the O
action O
represen- O
tation O
ha2Rd0 O
) O
, O
whered0is O
the O
dimension O
of O
the O
representation O
. O

We O
also O
feed O
the O
claim O
“ O
hsich O
/ O
si O
” O
alone O
to O
obtain O
the O
claim O
representation O
hc2Rd0 O
. O

3.2.2 O
Evidence O
Encoding O
Module O
This O
module O
is O
used O
to O
get O
an O
aggregated O
evidence O
representation O
. O

It O
consists O
of O
two O
sub O
- O
modules O
. O

Context O
sub O
- O
module O
. O

It O
is O
obvious O
that O
the O
sen- O
tences O
in O
an O
evidence O
are O
always O
contextual O
depen- O
dent O
, O
so O
we O
apply O
two O
different O
networks O
BiLSTM O
( O
Nguyen O
et O
al O
. O
, O
2016 O
) O
and O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
for O
comparison O
. O

These O
two O
different O
networks O
are O
widely O
used O
to O
encode O
contextual- O
aware O
information O
of O
sequential O
text O
in O
the O
NLP O
community O
. O

Formally O
, O
we O
either O
deﬁne O
[ O
h0 O
^E0 O
; O
: O
: O
: O
; O
h0 O
^Ej^Ej 1 O
] O
= O
BiLSTM O
( O
ha;H^E)(3 O
) O
if O
the O
BiLSTM O
network O
is O
used O
, O
or O
deﬁne O
[ O
h0 O
^E0 O
; O
: O
: O
: O
; O
h0 O
^Ej^Ej 1 O
] O

= O
Transformer O
( O
ha;H^E)(4 O
) O
if O
the O
Transformer O
is O
used O
, O
where O
H^E= O

[ O
h^E0 O
, O
. O
. O
. O

, O
h^Ej^Ej 1],h^Ei2Rd0is O
thei O
- O
th O
sentence O
repre- O
sentation O
in O
^E O
, O
h0 O
^Ei2Rd1is O

the O
corresponding O
context O
- O
aware O
sentence O
representation O
in O
^E O
, O
andd1 O
is O
the O
dimension O
of O
the O
representation O
. O

Aggregation O
sub O
- O
module O
. O

This O
sub O
- O
module O
is O
used O
to O
fuse O
the O
sentence O
representations O
in O
evi- O
dences O
to O
obtain O
an O
aggregated O
evidence O
represen- O
tation O
. O

We O
also O
apply O
two O
different O
networks O
in O
this O
sub O
- O
module O
: O
Transformer O
and O
attention O
. O

Unlike O
the O
Transformer O
with O
self O
- O
attention O
in O
the O
ﬁrst O
sub- O
module O
, O
the O
query O
in O
this O
sub O
- O
module O
is O
the O
claim O
and O
the O
key O
/ O
value O
is O
the O
context O
- O
aware O
sentence O
representation O
from O
the O
ﬁrst O
sub O
- O
module O
. O

For O
the O

1033 O
Figure O
2 O
: O
The O
architecture O
of O
the O
DQN O
- O
based O
model O
. O

The O
input O
is O
a O
state O
and O
an O
action O
, O
and O
the O
output O
is O
the O
Q- O
value O
of O
each O
label O
. O

The O
sentence O
encoding O
module O
is O
used O
to O
compute O
the O
sentence O
representation O
. O

The O
evidence O
encoding O
module O
is O
used O
to O
compute O
the O
evidence O
representation O
. O

The O
value O
module O
is O
used O
to O
predict O
the O
Q O
- O
value O
for O
each O
label O
. O

attention O
network O
, O
we O
deﬁne O
e O
= O
j^Ej 1X O
i=0 O

ih0 O

i O
( O
5 O
) O

i O
= O
exp O
( O
MLP O
( O
[ O
hc;h0 O
i O
] O
) O
) O

j^Ej 1X O
j=0exp O
( O
MLP O
( O
[ O
hc;h0 O
j]))(6 O
) O
where O
e2Rd1is O
the O
aggregated O
evidence O
repre- O
sentation O
, O
MLP O
( O
 O
) O

= O
Linear O
( O
ReLU O
( O
Linear O
( O
)))is O
a O
two O
- O
layer O
fully O
connected O
network O
using O
recti- O
ﬁed O
linear O
unit O
as O
the O
activation O
function O
, O
and O
[ O
; O
] O
denotes O
the O
concatenation O
of O
two O
vectors O
. O

3.2.3 O
Value O
Module O
This O
module O
is O
used O
to O
obtain O
the O
Q O
- O
value O
vector O
for O
all O
labels O
, O
simply O
written O
as O
Q(o;a;)for O
denoting O
the O
set O
of O
learnable O
parameters O
, O
which O
is O
formally O
deﬁned O
as O
Q(o;a; O
) O

= O
MLP O
( O
[ O
hcW;e O
] O
) O
( O
7 O
) O
where O
MLP O
( O
 O
) O

= O
Linear O
( O
ReLU O
( O
Linear O
( O
)))is O
sim- O
ilar O
to O
MLP O
( O
)used O
in O
Equation O
( O
6 O
) O
except O
that O
different O
parameters O
in O
linear O
layers O
are O
used O
, O
W2 O
Rd0d0is O
a O
learnable O
matrix O
, O
and O
Q(o;a;)2Rd2 O
ford2the O
number O
of O
different O
labels O
. O

3.3 O
Objective O
Function O
Given O
a O
transition O
( O
o(t);a(t);o(t+1))and O
its O
reward O
r(t O
) O
, O
we O
use O
the O
Double O
Deep O
Q O
- O
learning O
Network O
( O
DDQN O
) O
( O
Mnih O
et O
al O
. O
, O
2015 O
) O
technique O
to O
train O
ourmodel O
through O
the O
temporal O
difference O
error O
( O
Mnih O
et O
al O
. O
, O
2015 O
) O
. O

This O
error O
is O
formally O
deﬁned O
as O
=Q^y(o(t);a(t);) v(o(t+1);r(t))(8 O
) O
wherev()denotes O
the O
target O
value O
deﬁned O
as O
v(o;r O
) O
=( O

r O
; O
ifj^Ej O
= O
K O
r+^Q^y(o;a;^)otherwise(9 O
) O
fora= O
arg O
max O
a2Sn^EQ^y(o;a; O
) O
. O

In O
the O
above O
equation O
, O
^Q(;^)is O
the O
target O
net- O
work O
in O
DDQN O
, O
Q^ydenotes O
the O
Q O
- O
value O
of O
^yfor O
^ythe O
predicted O
label O
in O
o,^Eis O
the O
predicted O
ev- O
idence O
ino O
, O
and2[0;1]is O
a O
hyper O
- O
parameter O
representing O
the O
discount O
factor O
. O

We O
use O
the O
Huber O
loss O
to O
minimise O
 O
: O
L=1 O
jBjX O
( O
( O
o(t);a(t);o(t+1));r(t))2BL( O
) O
( O
10 O
) O
L( O
) O
= O
8 O
> O
< O
> O
:1 O
22ifjj1 O
jj 1 O
2otherwise(11 O
) O
whereBis O
a O
batch O
of O
transition O
- O
reward O
pairs O
. O

3.4 O
Algorithms O
3.4.1 O
Model O
Training O
Algorithm O
1 O
shows O
how O
to O
train O
the O
DQN O
- O
based O
model O
. O

First O
, O
we O
initialize O
three O
replay O
memories O
, O
the O
DQN O
- O
based O
model O
, O
and O
the O
target O
network O
in O
Line O
1 O
- O
3 O
. O

Then O
, O
in O
Line O
9 O
- O
17 O
, O
we O
obtain O
the O
training O

1034Algorithm O
1 O
: O
Model O
training O
for O
DQN O
, O
where O
the O
memory O
capacity O
M O
, O
the O
max- O
imum O
evidence O
size O
K O
, O
the O
maximum O
num- O
ber O
of O
epochs O
Tand O
the O
reset O
interval O
Care O
hyper O
- O
parameters O
. O

1initialize O
a O
replay O
memory O
with O
a O
capacity O
Mfor O
each O
label O
: O
R^y=;;8^y2fT;F;Ng O
. O

2initialize O
DQN O
Q(o;a;)with O
random O
weights O
. O

3initialize O
the O
target O
network O
^Q(o;a;^)with O
^=. O
4fore= O
1!Tdo O
5 O
shufﬂe O
the O
training O
set O
D. O
6 O
foreach O
( O
c;y;E;S)2D O
do O
7 O
initialize O
one O
state O
for O
each O
label O
: O
o(0 O
) O
^y= O
( O
c;^E(0);^y);8^y2fT;F;Ng O
, O
where O
^E(0)= O
; O
. O
8 O
fort= O
0!K 1do O
9 O
foreach O
^y2fT;F;Ngdo O
10 O
ifrandom O
( O
) O
< O
-greedy O
then O
11 O
a(t)= O
random O
select O
( O
Sn^E(t O
) O
) O
, O

where O
^E(t)comes O
fromo(t O
) O
^y O
. O

12 O
else O
13 O
a(t)= O
arg O
max O
a2Sn^E(t)Q^y(o(t O
) O
^y;a; O
) O
, O
whereQ()is O
deﬁned O
in O
Eq O
. O

( O
7 O
) O
andQ^ydenotes O
the O
Q O
- O
value O
of O
^y O
. O

14 O
end O
15 O
o(t+1 O
) O
^y O

= O
( O
c;^E(t+1);^y O
) O
, O
where O
^E(t+1)=^E(t)[fa(t)gand O
^E(t)comes O
fromo(t O
) O
^y O
. O

16 O
calculater(t)based O
on O
Eq O
. O

( O
2 O
) O
. O

17 O
store O
( O
( O
o(t O
) O
^y;a(t);o(t+1 O
) O
^y);r(t O
) O
) O
intoRy O
. O

18 O
end O
19 O
sample O
a O
mini O
- O
batch O
of O
transition O
- O
reward O
pairs O
from O
RT O
, O
RF O
, O
RNand O
updateQ(o;a;)based O
on O
Eq O
. O

( O
8)–(11 O
) O
. O

20 O
for O
everyCsteps O
reset O
the O
target O
network O
^Q(o;a;^)by^=. O
21 O
endfor O
22 O
end O
23endfor O
24returnQ(o;a; O
) O
transition O
- O
reward O
pairs O
by O
letting O
the O
DQN O
- O
based O
model O
interact O
with O
the O
environment O
in O
an O
-greedy O
exploration O
- O
exploitation O
way O
( O
Mnih O
et O
al O
. O
, O
2015 O
) O
. O

Finally O
, O
in O
Line O
19 O
, O
we O
sample O
a O
mini O
- O
batch O
of O
transition O
- O
reward O
pairs O
to O
update O
the O
DQN O
- O
based O
model O
, O
while O
in O
Line O
20 O
, O
for O
every O
Csteps O
we O
reset O
the O
target O
network O
to O
the O
DQN O
- O
based O
model O
. O

3.4.2 O
Candidate O
Retrieval O
Algorithm O
2 O
shows O
how O
to O
retrieve O
a O
pair O
( O
candi- O
date O
list O
, O
score O
list O
) O
for O
each O
label O
, O
where O
the O
can O
- O
Algorithm O
2 O
: O
Candidate O
retrieval O
for O
a O
claimcfrom O
a O
setSof O
sentences O
, O
where O
K O
is O
the O
maximum O
evidence O
size O
. O

1initialize O
^E^y= O

[ O
] O
; O
q^y= O
[ O
] O
; O
8^y2fT;F;Ng O
. O
2initialize O
one O
state O
for O
each O
label O
: O
o(0 O
) O
^y= O
( O
c;^E(0);^y);8^y2fT;F;Ng O
, O
where O
^E(0)= O
; O
. O

3fort= O
0!K 1do O

4 O
foreach O
^y2fT;F;Ngdo O
5a(t)= O
arg O
max O
a2Sn^E(t)Q^y(o(t O
) O
^y;a; O
) O
6q(t)=Q^y(o(t O
) O
^y;a(t O
) O
) O
7o(t+1 O
) O
^y O

= O
( O
c;^E(t+1);^y O
) O
, O
where O
^E(t+1)=^E(t)[fa(t)gand^E(t)comes O
fromo(t O
) O
^y O
. O

8 O
store O
^E(t+1)into^E^yandq(t)intoq^y O
. O

9 O
end O
10endfor O
11returnn O
( O
^E^y;q^y)o O
^y2fT;F;Ng O
Algorithm O
3 O
: O
Making O
ﬁnal O
prediction O
fromn O
( O
h^E(1 O
) O
^y O
; O
: O
: O
: O
; O
^E(K O
) O
^yi;hq(0 O
) O
^y;:::;q(K 1 O
) O
^yi)o O
^y2fT O
, O
F O
, O
Ng O
, O
using O
thresholds O

T O
; O

F O
; O

Nfor O
different O
labels O
. O

1letty= O
arg O
max O
0tK 1q(t O
) O
y;8y2fT;F;Ng O
. O

2let^E=^E(t^y+1 O
) O
^y O
, O
where O
^y= O
arg O
max O
y2fT;Fgq(ty O
) O
y. O
3ifq(tN O
) O
N O
> O
maxfq(tT O
) O
T;q(tF O
) O

Fgand O
min O
0tK 1q(t O
) O
N max O
^y2fT;Fgq(t^y O
) O

^y O
> O

  O
Nthen O
4 O
^y0 O
= O
N O
5else O
ifq(tT O
) O

T O
> O
q(tF O
) O

Fthen O
6 O
ifq(tT O
) O
T max O
^y2fF;Ngq(tT O
) O

^y O
> O

  O
Tthen O
^y0 O
= O
T O
; O
7 O
else O
^y0 O
= O
N O
; O
8else O
9 O
ifq(tF O
) O
F max O
^y2fT;Ngq(tF O
) O

^y O
> O

  O
Fthen O
^y0 O
= O
F O
; O
10 O
else O
^y0 O
= O
N O
; O
11end O
12return O
( O
^E;^y0 O
) O
didate O
list O
stores O
progressively O
enlarged O
sentence O
sets O
, O
where O
each O
sentence O
set O
is O
a O
candidate O
of O
the O
predicted O
evidence O
, O
and O
the O
score O
list O
stores O
the O
strengths O
that O
the O
corresponding O
candidates O
support O
the O
label O
. O

We O
enlarge O
the O
two O
- O
list O
pair O
for O
each O
label O
through O
a O
greedy O
- O
search O
way O
( O
Line O
3 O
- O
10 O
) O
. O

Speciﬁ- O
cally O
, O
for O
each O
label O
, O
we O
ﬁrst O
select O
the O
action O
with O
the O
largest O
Q O
- O
value O
( O
Line O
5 O
) O
, O
then O
update O
the O
state O
by O
adding O
the O
chosen O
action O
into O
its O
predicted O
ev- O
idence O
( O
Line O
7 O
) O
, O
and O
ﬁnally O
add O
the O
evidence O
and O
score O
into O
the O
corresponding O
list O
( O
Line O
8) O
. O

1035Algorithm O
4 O
: O
Searching O
for O
best O
thresholds O
, O
where O
minq^yis O
short O
for O
mintq(t O
) O
^yand O
maxq^yformax O
tq(t O
) O
^y O
, O
for O
all O
^y2fT;F;Ng O
. O

1constructV O
= O
f(qT;qF;qN;y)gfrom O
the O
development O
set O
by O
Algorithm O
2 O
. O
2initializeC^y O

= O
L^y O
= O
L0 O

^y= O

[ O
] O
; O
8^y2fT;F;Ng O
. O
3foreach O
( O
qT;qF;qN;y)2Vdo O
4 O
ifmaxqN O
> O
maxfmaxqT;maxqFgthen O
5v= O
minqN maxfmaxqT;maxqFg O
6 O
storevintoLNand(v;y)intoCN O
. O

7 O
end O
8end O
9sortLNin O
ascending O
order O
. O

10calculate O
the O
medians O
of O
adjacent O
values O
in O
LNand O
store O
them O
into O
L0 O
N. O
11 O

N= O
arg O
max O

2L0 O
NX O
( O
v;y)2CN1((v O
> O

^y O
= O
N)_(v O

^y6 O
= O
N O
) O
) O

12foreach O
( O
qT;qF;qN;y)2Vdo O
13 O
ifmaxqNmaxfmaxqT;maxqFgor O
minqN maxfmaxqT;maxqFg O

Nthen O
14t^y= O
arg O
max O
tq(t O
) O
^y;8^y2fT;Fg O
15 O
ifq(tT O
) O

T O
> O
q(tF O
) O

Fthen O
16 O
v O
= O
q(tT O
) O
T maxfq(tT O
) O
F;q(tT O
) O

Ng O
17 O
storevintoLTand(v;y)intoCT O
. O

18 O
else O
19 O
v O
= O
q(tF O
) O
F maxfq(tF O
) O
T;q(tF O
) O

Ng O
20 O
storevintoLFand(v;y)intoCF O
. O

21 O
end O
22 O
end O
23end O
24foreach O
^y2fT;Fgdo O
25 O
sortL^yin O
ascending O
order O
. O

26 O
calculate O
the O
medians O
of O
adjacent O
values O
in O
L^y O
and O
store O
them O
into O
L0 O
^y O
. O

27 O

^y= O
arg O
max O

2L0 O
^yX O

( O
v;y)2C^y1(v O
> O

^y= O
^y)  O
1(v O
> O

^y O
= O
N O
) O
28end O
29return O
( O

T O
; O

F O
; O

N O
) O
3.4.3 O
Final O
Prediction O
Algorithm O
3 O
shows O
how O
to O
compute O
the O
target O
evidence O
- O
label O
pair O
from O
the O
( O
candidate O
list O
, O
score O
list O
) O
pairs O
obtained O
by O
Algorithm O
2 O
, O
where O
the O
thresholds O
are O
determined O
by O
Algorithm O
4 O
. O

In O
this O
algorithm O
, O
we O
ﬁrst O
use O
the O
condition O
given O
by O
Al- O
gorithm O
4 O
to O
predict O
N(Line O
3 O
) O
, O
and O
then O
reﬁne O
the O
prediction O
of O
T(Line O
6 O
) O
and O
F(Line O
9 O
) O
in O
turn O
. O

In O
Line O
2 O
, O
we O
focus O
on O
the O
evidences O
with O
the O
highest O
score O
for O
TandF O
, O
while O
we O
ignore O
the O
evidence O
forN O
, O
due O
to O
the O
following O
reasons O
: O
( O
1 O
) O
there O
are O
no O
supporting O
sentences O
in O
the O
evidence O
for O
N O
; O
( O
2 O
) O
we O
follow O
a O
strategy O
commonly O
used O
in O
existing O
methods O
for O
FV O
, O
i.e. O
, O
focusing O
only O
on O
the O
evidence O
forTandF.Split O
SUPPORTS O
REFUTES O
NEI O
Train O
80,035 O
29,775 O
35,639 O
Dev O
6,666 O
6,666 O
6,666 O
Test O
6,666 O
6,666 O
6,666 O
Table O
1 O
: O
Dataset O
statistics O
for O
FEVER O
3.4.4 O
Threshold O
Searching O
Algorithm O
4 O
shows O
how O
to O
search O
for O
the O
best O
thresholds O
( O

T O
; O

F O
; O

N O
) O
to O
maximize O
the O
Label O
Accuracy O
( O
LA O
) O
over O
the O
development O
set O
. O

We O
ﬁrst O
call O
Algorithm O
2 O
to O
construct O
a O
set O
of O
tu- O

ples(qT;qF;qN;y)from O
the O
development O
set O
, O
each O
of O
which O
corresponds O
to O
a O
development O
instance O
, O
whereqT O
, O
qFandqNare O
respectively O
the O
output O
score O
lists O
for O
the O
three O
labels O
T O
, O
F O
andN O
, O
andyis O
the O
corresponding O
true O
label O
( O
Line O
1 O
) O
. O

We O
then O
go O
through O
the O
following O
two O
stages O
. O

The O
ﬁrst O
stage O
( O
Line O
3 O
- O
11 O
) O
ﬁnds O
a O
threshold O

Nthat O
can O
maximize O
LA O
for O
label O
N O
, O
where O
maximizing O
LA O
is O
amount O
to O
maximizing O
the O
difference O
between O
the O
number O
of O
correctly O
and O
incorrectly O
predicted O
instances O
. O

The O
second O
stage O
( O
Line O
12 O
- O
28 O
) O
ﬁnds O
the O
thresholds O

T O
and O

Fthat O
can O
maximize O
LA O
for O
label O
TandF O
, O
respectively O
, O
where O
those O
instances O
that O
satisfy O
the O
conditions O
for O
Nare O
neglected O
( O
Line O
13 O
) O
. O

4 O
Experiments O
4.1 O
Experimental O
setting O
4.1.1 O
Dataset O
Our O
experiments O
are O
conducted O
on O
the O
large O
- O
scale O
benchmark O
dataset O
FEVER O
( O
Thorne O
et O
al O
. O
, O
2018a O
) O
, O
which O
consists O
of O
185,455 O
annotated O
claims O
with O
a O
set O
of O
5,416,537 O
Wikipedia O
documents O
from O
the O
June O
2017 O
Wikipedia O
dump O
. O

All O
claims O
are O
la- O
beled O
as O
“ O
SUPPORTS O
” O
, O
“ O
REFUTES O
” O
, O
or O
“ O
NOT O
ENOUGH O
INFO O
” O
. O

What O
’s O
more O
, O
each O
claim O
for O
“ O
SUPPORTS O
” O
and O
“ O
REFUTES O
” O
is O
accompanied O
by O
some O
evidences O
extracted O
from O
Wikipedia O
docu- O
ments O
. O

The O
dataset O
partition O
is O
kept O
the O
same O
with O
Thorne O
et O

al O
. O
( O
2018b O
) O
as O
shown O
in O
Table O
1 O
. O
4.1.2 O
Evaluation O
Metrics O

The O
task O
has O
ﬁve O
evaluation O
metrics O
: O
1 O
) O
FEVER O
, O
the O
primary O
scoring O
metric O
that O
measures O
the O
accuracy O
of O
claim O
veriﬁcation O
with O
a O
requirement O
that O
the O
predicted O
evidences O
fully O
covers O
the O
ground O
- O
true O
evidences O
for O
SUPPORTS O
and O
REFUTES O
claims O
; O
2 O
) O
Label O
Accuracy O
( O
LA O
) O
, O
the O
accuracy O
of O
claim O
veriﬁcation O
without O
considering O
the O
validity O
of O
the O

1036Method O

T O

F O

N O
T O
- O
T O
-1.23361155390739 O
-1.26671668887138 O

0.0153777748346328 O
T O
- O
A O
-0.0631487071514129 O
0.0747150778770446 O
-1.48811344802379 O

BiLSTM O
- O
T O
0.184351719915866 O
-0.64785711467266 O
-0.465365642681717 O

BiLSTM O
- O
A O
-0.0904324240982532 O
-0.795884847640991 O
-0.403448916971683 O

Table O
2 O
: O
The O
thresholds O
determined O
by O
Algo- O
rithm O
4 O
. O
T O
- O
T O
, O
T O
- O
A O
, O
BiLSTM O
- O
T O
, O
and O
BiLSTM O
- O
A O
de- O
note O
the O
architectures O
of O
the O
evidence O
encoding O
mod- O
ule O
, O
which O
are O
respectively O
Transformer O
- O
Transformer O
, O
Transformer O
- O
Attention O
, O
BiLSTM O
- O
Transformer O
, O
and O
BiLSTM O
- O
Attention O
. O

predicted O
evidences O
; O
3 O
) O
Precision O
( O
Pre O
) O
, O
the O
macro- O
precision O
of O
the O
evidences O
for O
SUPPORTS O
and O
RE- O
FUTES O
claims O
; O
4 O
) O
Recall O
, O
the O
macro O
- O
recall O
of O
the O
evidences O
for O
SUPPORTS O
and O
REFUTES O
claims O
; O
5 O
) O
F1 O
, O
the O
F O
1 O
- O
score O
of O
the O
evidences O
for O
SUPPORTS O
and O
REFUTES O
claims O
. O

We O
choose O
F1 O
as O
our O
main O
metric O
because O
it O
can O
directly O
show O
the O
perfor- O
mance O
of O
methods O
on O
retrieval O
of O
precise O
evidences O
. O

4.1.3 O
Implementation O
Details O
Document O
retrieval O
. O

The O
document O
retrieval O
stage O
is O
kept O
the O
same O
as O
previous O
work O
( O
Hanselowski O
et O
al O
. O
, O
2018 O
; O
Zhou O
et O

al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2020 O
; O
Ye O
et O
al O
. O
, O
2020 O
) O
. O

Given O
a O
claim O
, O
the O
method O
ﬁrst O
utilizes O
the O
constituency O
parser O
from O
AllenNLP O
( O
Gardner O
et O
al O
. O
, O
2018 O
) O
to O
extract O
potential O
entities O
from O
the O
claim O
. O

Then O
it O
uses O
the O
entities O
as O
search O
queries O
to O
ﬁnd O
the O
relevant O
documents O
via O
the O
online O
Me- O
diaWiki O
API2 O
. O

The O
convinced O
articles O
are O
reserved O
( O
Hanselowski O
et O
al O
. O
, O
2018 O
) O
. O

Sentence O
selection O
and O
claim O
veriﬁcation O
. O

We O
implement O
our O
DQN O
- O
based O
model O
with O
PyTorch O
and O
train O
it O
with O
the O
AdamW O
( O
Loshchilov O
and O
Hut- O
ter O
, O
2019 O
) O
optimizer O
while O
keeping O
the O
sentence O
en- O
coding O
module O
frozen O
and O
inheriting O
the O
RoBERTa O
implementation O
from O
Wolf O
et O
al O
. O

( O
2020)3 O
. O
Specif- O

ically O
, O
the O
learning O
rate O
is O
5e-6 O
, O
the O
batch O
size O
is O
128 O
, O
the O
training O
epochs O
is O
30 O
, O
the O
iteration O
steps O
( O
or O
largest O
evidence O
size O
, O
i.e. O
,K O
) O
is O
5 O
, O
the O
discount O
factoris O
0.95 O
, O
and O
the O
layer O
number O
of O
the O
con- O
text O
sub O
- O
module O
is O
3 O
. O

Prioritized O
experience O
replay O
memory O
( O
Schaul O
et O
al O
. O
, O
2016 O
) O
with O
a O
capacity O
of O
10,000 O
is O
used O
to O
store O
transitions O
. O

The O
target O
net- O
work O
is O
reset O
when O
DQN O
is O
updated O
every O
10 O
times O
. O

The O
probability O
of O
-greedy O
policy O
starts O
at O
0.9 O
and O
decays O
exponentially O
towards O
0.05 O
, O
and O
the O
rate O
of O
the O
decay O
is1 O
2000 O
. O

Table O
2 O
shows O
the O
thresholds O
2https://www.mediawiki.org/wiki/API O
: O
Main_page O
3https://github.com/huggingface/ O
pytorch O
- O
transformers O

T O
; O

Fand O

Ncomputed O
by O
Algorithm O
4 O
. O

All O
ex- O
periments O
were O
conducted O
on O
an O
NVIDIA O
GTX O
2080ti O
10 O
GB O
GPU O
. O

4.1.4 O

Baselines O
We O
compare O
our O
method O
with O
the O
following O
base- O
lines O
, O
including O
six O
methods O
that O
focus O
on O
claim O
veriﬁcation O
and O
one O
joint O
method O
TwoWingOS O
( O
Yin O
and O
Roth O
, O
2018 O
) O
. O

The O
six O
methods O
include O
: O
( O
1)GEAR O
( O
Zhou O
et O
al O
. O
, O
2019 O
) O
uses O
two O
kinds O
of O
attentions O
to O
conduct O
reasoning O
and O
aggregation O
in O
a O
graph O
model O
; O
( O
2 O
) O
KGAT O
( O
Liu O
et O
al O
. O
, O
2020 O
) O

employes O
the O
Kernel O
Graph O
Attention O
Network O
to O
capture O
ﬁne O
- O
grained O
information O
over O
evidences O
for O
more O
accurate O
claim O
veriﬁcation O
; O
( O
3 O
) O
DREAM O
( O
Zhong O
et O
al O
. O
, O
2020 O
) O
introduces O
semantic O
structures O
for O
evidences O
obtained O
by O
semantic O
role O
labeling O
in O
claim O
veriﬁcation O
; O
( O
4 O
) O
CorefBERT O
( O
Ye O
et O
al O
. O
, O
2020 O
) O
extends O
KGAT O
and O
can O
explicitly O
model O
co O
- O
reference O
relationship O
in O
context O
; O
( O
5 O
) O
HESM O
( O
Subramanian O
and O
Lee O
, O
2020 O
) O
is O
a O
framework O
that O
can O
encode O
and O
attend O
the O
claim O
and O
evidence O
sets O
at O
different O
levels O
of O
hierarchy O
; O
( O
6 O
) O
DGAT O
( O
Wang O
et O
al O
. O
, O
2020 O
) O
is O
a O
double O
graph O
attention O
network O
that O
performs O
well O
in O
multi O
- O
domain O
datasets O
. O

The O
join O
method O
TwoWingOS O
( O
Yin O
and O
Roth O
, O
2018 O
) O
exploits O
a O
two O
- O
wing O
optimization O
strategy O
that O
opti- O
mizes O
sentence O
selection O
and O
claim O
veriﬁcation O
in O
a O
jointly O
supervised O
training O
scheme O
. O

4.2 O
Results O
and O
Analysis O
As O
shown O
in O
Table O
3 O
, O
we O
implement O
four O
versions O
of O
the O
evidence O
encoding O
module O
and O
evaluate O
them O
on O
the O
DEV O
set O
and O
the O
blind O
TEST O
set O
. O

The O
FEVER O
metric O
of O
the O
top O
six O
methods O
is O
calculated O
with O
the O
imprecise O
evidences O
, O
so O
we O
introduce O
the O
FEVER@5 O
metric O
for O
a O
fair O
comparison O
. O

We O
ana- O
lyze O
our O
method O
from O
the O
following O
four O
aspects O
. O

Comparison O
with O
the O
state O
- O
of O
- O
the O
- O
art O
methods O
. O

Results O
in O
Table O
3 O
show O
that O
all O
versions O
( O
except O
BiLSTM O
- O
A O
) O
with O
post O
- O
processing O
signiﬁcantly O
out- O
perform O
the O
state O
- O
of O
- O
the O
- O
art O
methods O
on O
FEVER O
, O
Pre O
, O
and O
F1 O
, O
especially O
for O
T O
- O
A O
on O
F1 O
, O
which O
shows O
the O
superiority O
of O
our O
method O
in O
retrival O
of O
pre- O
cise O
evidences O
. O

However O
, O
none O
of O
the O
four O
ver- O
sions O
of O
our O
method O
can O
achieve O
the O
best O
result O
on O
FEVER@5 O
, O
LA O
, O
and O
Recall O
. O

The O
reason O
for O
low O
recall O
is O
that O
the O
number O
of O
sentences O
in O
precise O
evidences O
is O
less O
than O
that O
in O
imprecise O
evidences O
, O
which O
means O
other O
methods O
have O
a O
higher O
proba- O
bility O
to O
recall O
the O
ground O
- O
true O
evidences O
than O
ours O
. O

Besides O
, O
the O
relatively O
low O
LA O
is O
caused O
by O
the O

1037MethodDEV O

TEST O
FEVER@5 O
FEVER O
LA O
Pre O
Recall O
F1 O
FEVER@5 O

FEVER O
LA O
Pre O
Recall O
F1 O
GEAR O
70.69 O
- O
74.84 O
24.08 O
86.72 O
37.69 O
67.10 O
- O
71.60 O
- O
- O
36.87 O
KGAT O
76.11 O
- O
78.29 O
27.79 O
94.37 O
42.34 O
70.38 O
- O
74.07 O
25.21 O
87.47 O
39.14 O
DREAM O
- O
- O
- O
26.60 O
87.33 O
40.79 O
70.60 O
- O
76.85 O
25.63 O
85.57 O
39.45 O
CorefBERT O
- O
- O
- O
- O
- O
- O
71.80 O
- O
- O
- O
- O
39.14 O
HESM O
73.44 O
- O
75.77 O
- O
- O
- O
71.48 O
- O
74.64 O
- O
- O
52.78 O
DGAT O
- O
- O
- O
- O
- O
- O
66.91 O
- O
71.79 O
- O
- O
- O
TwoWingOS O
- O
56.16 O
78.90 O
47.73 O
53.81 O
50.59 O
- O
54.33 O
75.99 O
44.68 O
49.91 O
47.15 O
OursT O
- O
T O
72.83 O
71.55 O
78.1850.42 O
81.82 O
62.3970.16 O
68.91 O
75.7448.76 O
79.91 O
60.56(w./o O
. O
) O
72.90 O
70.00 O
74.87 O
70.43 O
68.23 O
73.13 O
T O
- O
A O
73.32 O
72.79 O
78.3554.75 O
79.92 O
64.9870.81 O
70.28 O
76.1452.24 O
77.93 O
62.55(w./o O
. O
) O
73.29 O
72.60 O
78.12 O
70.82 O
70.18 O
76.00 O
BiLSTM O
- O
T O
73.15 O
63.77 O
73.9148.06 O
71.06 O
57.3470.54 O
61.51 O
70.2045.97 O
69.43 O
55.32(w./o O
. O
) O
73.19 O
55.39 O
63.55 O
70.81 O
53.21 O
61.68 O
BiLSTM O
- O
A O
72.99 O
70.88 O
77.7935.50 O
76.54 O
48.5070.11 O
68.21 O
75.5333.76 O
74.50 O
46.46(w./o O
. O
) O
73.20 O
65.65 O
71.21 O
70.55 O
63.38 O
69.32 O
Table O
3 O
: O
Performance O
on O
DEV O
set O
and O
blind O
TEST O
set O
of O
FEVER O
( O
% O
) O
. O

FEVER@5 O
and O
FEVER O
are O
computed O
based O
on O
imprecise O
and O
precise O
evidences O
, O
repectively O
. O

The O
result O
obtained O
with O
/ O
without O
post O
- O
processing O
( O
namely O
threshold O
searching O
and O
ﬁnal O
prediction O
) O

is O
displayed O
in O
each O
architecture O
’s O
ﬁrst O
/ O
second O
row O
( O
“ O
w. O
”/“o O
. O
” O
) O
. O

We O
directly O
output O
the O
evidence O
with O
the O
highest O
score O
in O
the O
candidate O
list O
and O
its O
corresponding O
label O
if O
post O
- O
processing O
is O
not O
performed O
. O

Pre O
, O
Recall O
, O
and O
F1 O
keep O
unchanged O
because O
they O
are O
not O
affected O
by O
the O
post O
- O
processing O
. O

‘ O
- O
’ O
denotes O
a O
missing O
value O
. O

# O
T O
- O
T O
T O
- O
A O
BiLSTM O
- O
T O
BiLSTM O
- O
A O
KGAT O
LA O
78.18 O
78.35 O
73.91 O
77.79 O
78.29 O
LA O
* O
82.82 O
82.48 O
84.93 O
83.95 O
79.08 O
Table O
4 O
: O
Comparison O
between O
our O
method O
and O
KGAT O
on O
LA O
( O
% O
) O
. O

LA O
and O
LA O
* O
are O
respectively O
evaluated O
on O
the O
DEV O
set O
and O
its O
subset O
constructed O
by O
selecting O
the O
samples O
where O
the O
ground O
- O
true O
evidences O
are O
success- O
fully O
recalled O
. O

Method O
Avg O
. O

Std O
. O

Three O
- O
stage O
pipeline O
4.00 O
0.07 O
Our O
method O
( O
T O
- O
A O
) O
1.07 O
0.89 O
Table O
5 O
: O
Comparison O
of O
the O
number O
of O
unnecessary O
sen- O
tences O
in O
predicted O
evidences O
. O

low O
Recall O
of O
precise O
evidences O
. O

To O
further O
clarify O
this O
point O
, O
we O
evaluate O
our O
method O
on O
a O
subset O
of O
the O
DEV O
set O
where O
the O
ground O
- O
true O
evidences O
are O
recalled O
successfully O
. O

Our O
method O
improves O
signif- O
icantly O
the O
performance O
on O
this O
subset O
, O
as O
shown O
in O
Table O
4 O
, O
which O
justiﬁes O
our O
point O
of O
view O
. O

FEVER O
is O
affected O
by O
the O
LA O
and O
Recall O
, O
thereby O
the O
low O
FEVER@5 O
is O
also O
due O
to O
the O
low O
recall O
of O
precise O
evidences O
. O

In O
addition O
, O
the O
results O
reported O
in O
Ta- O
ble O
5 O
show O
that O
our O
method O
can O
signiﬁcantly O
reduce O
the O
number O
of O
unnecessary O
sentences O
in O
a O
predicted O
evidence O
. O

Comparison O
between O
different O
versions O
. O

As O
shown O
in O
Table O
3 O
, O
T O
- O
T O
and O
T O
- O
A O
perform O
respec- O
tively O
better O
than O
BiLSTM O
- O
T O
and O
BiLSTM O
- O
A O
on O
almost O
all O
metrics O
except O
that O
T O
- O
T O
is O
slightly O
worsewidth O
FEVER@5 O
FEVER O
LA O
Pre O
Recall O
F1 O
1 O
60.73 O
54.91 O
72.6952.76 O
58.57 O
55.51(w./o O
. O
) O
50.09 O
46.55 O
53.00 O
2 O
60.74 O
54.94 O
72.6952.84 O
58.66 O
55.59(w./o O
. O
) O

50.09 O
46.53 O
53.00 O
3 O
60.70 O
54.96 O
72.6952.84 O
58.67 O
55.60(w./o O
. O
) O
50.10 O
46.54 O
53.00 O
4 O
60.67 O
54.95 O
72.6952.81 O
58.66 O
55.58(w./o O
. O
) O
50.09 O
46.54 O
53.00 O
5 O
60.68 O
54.95 O
72.6952.84 O
58.68 O
55.61(w./o O
. O
) O

50.09 O
46.54 O
53.00 O
Table O
6 O
: O
The O
beam O
- O
search O
result O
of O
KGAT O
on O
the O
DEV O
set O
( O
% O
) O
. O

The O
width O
( O
k O
) O
means O
to O
select O
the O
top- O
kresults O
at O
each O
search O
step O
. O

The O
result O
obtained O
with O
/ O
without O
post O
- O
processing O
( O
namely O
threshold O
searching O
and O
ﬁnal O
prediction O
) O

is O
displayed O
in O
each O
width O
’s O
ﬁrst O
/ O
second O
row O
( O
“ O
w. O
”/“o O
. O
” O
) O
. O

We O
employed O
the O
KGAT O
source O
code O
re- O
leased O
by O
Liu O
et O
al O
. O

( O
2020 O
) O
to O
implement O
beam O
- O
search O
for O
ﬁnding O
precise O
evidences O
and O
the O
evaluation O
data O
for O
KGAT O
was O
kept O
the O
same O
as O
ours O
. O

than O
BiLSTM O
- O
A O
on O
FEVER@5 O
, O
which O
suggests O
Transformer O
can O
encode O
better O
context O
- O
aware O
repre- O
sentations O
than O
BiLSTM O
in O
our O
context O
sub O
- O
module O
. O

Moreover O
, O
we O
ﬁnd O
that O
T O
- O
A O
performs O
better O
than O
T O
- O
T O
on O
almost O
all O
metrics O
except O
Recall O
and O
that O
BiLSTM O
- O
A O
is O
worse O
than O
BiLSTM O
- O
T O
on O
Pre O
and O
F1 O
. O

This O
contrary O
result O
shows O
that O
the O
performance O
of O
the O
aggregation O
sub O
- O
module O
is O
impacted O
by O
the O
context O
sub O
- O
module O
. O

Thus O
, O
the O
choice O
between O
Transformer O
and O
Attention O
should O
depend O
on O
the O
context O
sub O
- O
module O
. O

Overall O
, O
T O
- O
A O
achieves O
the O
best O
performance O
among O
all O
the O
four O
versions O
of O
our O

1038 O
# O
label O
claim O
ground O
- O
true O
evidencespredicted O
evidences O
GEAR O
KGAT O

Our O
method O
( O
T O
- O
A O
) O
1 O
FSavages O
was O
exclusively O
a O
German O
ﬁlm.(Savages(2012 O
ﬁlm O
) O
, O
3 O
) O
( O
Savages(2012 O
ﬁlm O
) O
, O
3 O
) O
( O
Savages(2012 O
ﬁlm O
) O
, O
3 O
) O
( O
Savages(2012 O
ﬁlm O
) O
, O
3 O
) O
( O
Savages(band O
) O
, O
0 O
) O
( O
Savages(2012 O
ﬁlm O
) O
, O
6 O
) O
( O
Savages(2012 O
ﬁlm O
) O
, O
6 O
) O
( O
Savages(2012 O
ﬁlm O
) O
, O
0 O
) O
( O
Savages(band O
) O
, O
2 O
) O
( O
Savages(band O
) O
, O
5 O
) O
( O
Savages(band O
) O
, O
4 O
) O
( O
Savages(band O
) O
, O
0 O
) O
2 O
TEd O
Gein O
murdered O
people O
around O
Plainﬁeld O
, O
Wisconsin.(Ed O
Gein O
, O
2 O
) O
( O
Ed O
Gein O
, O
1 O
) O
( O
Ed O
Gein O
, O
1 O
) O
( O
Ed O
Gein O
, O
2 O
) O
( O
Ed O
Gein O
, O
1 O
) O
( O
Ed O
Gein O
, O
0 O
) O
( O
Ed O
Gein O
, O
2 O
) O
( O
Ed O
Gein O
, O
1 O
) O
( O
Ed O
Gein O
, O
6 O
) O
( O
Ed O
Gein O
, O
0 O
) O
( O
Ed O
Gein O
, O
2 O
) O
( O
Ed O
Gein(band O
) O
, O
2 O
) O
( O
Ed O
Gein O
, O
5 O
) O
( O
Ed O
Gein O
, O
4 O
) O
3 O
TMarnie O
is O
a O
ﬁlm O
that O
was O
created O
in O
the O
United O
States.(Marnie(ﬁlm O
) O
, O
0 O
) O
( O
Marnie(ﬁlm O
) O
, O
0 O
) O
( O
Marnie(ﬁlm O
) O
, O
0 O
) O
( O
Marnie(ﬁlm O
) O
, O
0 O
) O
( O
Marnie O
, O
0 O
) O
( O
Marnie O
, O
0 O
) O
( O
Marnie(ﬁlm O
) O
, O
5 O
) O
( O
Marnie(ﬁlm O
) O
, O
2 O
) O
( O
Marnie(ﬁlm O
) O
, O
2 O
) O
( O
Marnie(ﬁlm O
) O
, O
6 O
) O
( O
Marnie(ﬁlm O
) O
, O
6 O
) O
( O
Marnie(dis O
... O
tion O
) O
, O
12 O
) O
( O
Marnie(ﬁlm O
) O
, O
5 O
) O
4 O
FFirst O
Motion O
Picture O
Unit O
produced O
zero O
ﬁlms.(First O
... O
Unit O
, O
1 O
) O
( O
First O
... O
Unit O
, O
0 O
) O
( O
First O
... O
Unit O
, O
1 O
) O
( O
First O
... O
Unit O
, O
1 O
) O
( O
First O
... O
Unit O
, O
4 O
) O
( O
First O
... O
Unit O
, O
4 O
) O
( O
First O
... O
Unit O
, O
4 O
) O
( O
First O
... O
Unit O
, O
4 O
) O
( O
First O
... O
Unit O
, O
0 O
) O
( O
First O
... O
Unit O
, O
1 O
) O
( O
First O
... O
Unit O
, O
0 O
) O
( O
First O
... O
Unit O
, O
0 O
) O
( O
Zero(2016 O
ﬁlm O
) O
, O
0 O
) O
( O
First O
... O
Unit O
, O
2 O
) O
( O
First O
... O
Unit O
, O
2 O
) O
( O
First O
... O
Unit O
, O
8) O
( O
Zero(2016 O
ﬁlm O
) O
, O
0 O
) O
Table O
7 O
: O
Cases O
in O
FEVER O
. O

We O
list O
the O
predicted O
evidences O
of O
GEAR O
, O
KGAT O
and O
our O
method O
. O

( O
title O
, O
i O
) O
de- O
notes O
thei O
- O
th O
sentence O
in O
the O
corresponding O
wiki O
document O
. O

In O
predicted O
evidences O
, O
the O
sentences O
highlighted O
inblue O
bold O
italics O
and O
underline O
are O
sentences O
in O
the O
target O
evidence O
while O
others O
in O
black O
are O
unnecessary O
ones O
. O

proposed O
method O
. O

Comparison O
on O
retrieval O
of O
precise O
evidences O
. O

TwoWingOS O
is O
a O
supervised O
- O
learning O
method O
that O
can O
also O
ﬁnd O
precise O
evidences O
. O

Although O
it O
achieves O
slightly O
better O
performance O
on O
LA O
than O
ours O
, O
its O
F1 O
and O
other O
metrics O
are O
much O
worse O
, O
in- O
dicating O
that O
it O
performs O
worse O
than O
our O
method O
except O
for O
BiLSTM O
- O
A O
in O
retrieval O
of O
precise- O
evidences O
. O

We O
also O
enhance O
KGAT O
to O
conduct O
beam O
- O
search O
for O
ﬁnding O
precise O
evidences O
and O
re- O
port O
the O
results O
in O
Table O
6 O
. O

The O
F1 O
score O
of O
KGAT O
is O
always O
higher O
than O
TwoWingOS O
but O
is O
still O
lower O
than O
our O
method O
except O
for O
BiLSTM O
- O
A. O
Comparison O
between O
the O
methods O
with O
and O
without O
post O
- O
processing O
. O

It O
can O
be O
seen O
from O
Ta- O
ble O
3 O
and O
Table O
6 O
that O
, O
post O
- O
processing O
( O
namely O
threshold O
searching O
and O
ﬁnal O
prediction O
from O
can- O
didates O
) O
consistently O
improves O
FEVER O
and O
LA O
. O

Al- O
though O
with O
post O
- O
processing O
, O
our O
method O
( O
except O
T O
- O
A O
) O
achieves O
slightly O
lower O
scores O
on O
FEVER@5 O
, O
KGAT O
still O
achieves O
signiﬁcantly O
higher O
scores O
on O
FEVER@5 O
as O
on O
other O
metrics O
. O

These O
results O
show O
that O
post O
processing O
is O
very O
important O
in O
retrieval O
of O
precise O
evidences O
. O

4.3 O
Case O
Study O
In O
Table O
7 O
we O
provide O
some O
cases O
to O
demonstrate O
the O
effectiveness O
of O
our O
method O
( O
T O
- O
A O
) O
in O
retriev- O
ing O
precise O
evidences O
. O

In O
case#1 O
and O
case#2 O
, O
our O
method O
exactly O
ﬁnds O
ground O
- O
true O
evidences O
with- O
out O
introducing O
any O
unnecessary O
sentence O
, O
whileGEAT O
and O
KGAT O
can O
not O
. O

In O
case#3 O
and O
case#4 O
, O
our O
method O
generates O
less O
unnessary O
sentences O
in O
prdicted O
evidents O
than O
GEAT O
and O
KGAT O
do O
. O

5 O
Conclusion O
and O
Future O
Work O

In O
this O
paper O
, O
we O
have O
proposed O
a O
novel O
DQN O
- O
based O
approach O
to O
ﬁnding O
precise O
evidences O
for O
fact O
veri- O
ﬁcation O
. O

It O
provides O
a O
method O
to O
solve O
the O
precise- O
evidence O
problem O
by O
ﬁrst O
employing O
a O
DQN O
to O
compute O
some O
candidates O
and O
then O
introducing O
a O
post O
- O
processing O
strategy O
to O
extract O
the O
target O
evi- O
dence O
and O
its O
label O
from O
the O
candidates O
. O

Exper- O

imental O
results O
show O
that O
the O
approach O
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
in O
terms O
of O
retrieval O
of O
precise O
evidences O
. O

Besides O
, O
to O
the O
best O
of O
our O
knowledge O
, O
it O
is O
the O
ﬁrst O
attempt O
to O
employ O
DQN O
in O
the O
fact O
veriﬁcation O
task O
. O

Future O
work O
will O
incorporate O
external O
knowledge O
into O
our O
approach O
to O
improve O
the O
retrieval O
recall O
. O

Acknowledgments O
This O
work O
is O
supported O
by O
the O
Guangdong O
Province O
Science O
and O
Technology O
Plan O
projects O
( O
2017B010110011 O
) O
, O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No O
. O
61876204 O
, O
61976232 O
, O
and O
51978675 O
) O
, O
the O
National O
Key O
R&D O
Pro- O
gram O
of O
China O
( O
No.2018YFC0830600 O
) O
, O
Guang- O

dong O
Province O
Natural O
Science O
Foundation O
( O
No O
. O
2018A030313086 O
) O
, O
All O
- O
China O
Federation O
of O
Re- O
turned O
Over O
- O
seas O
Chinese O
Research O
Project O
( O
No O
. O
17BZQK216 O
) O
. O

1039References O
LiChun O
Cao O
and O
ZhiMin O
. O

2019 O
. O

An O
overview O
of O
deep O
reinforcement O
learning O
. O

In O
CACRE O
, O
pages O
17:1 O
– O
17:9 O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT O
: O
pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
under- O
standing O
. O

In O
NAACL O
- O
HLT O
, O
pages O
4171–4186 O
. O

Matt O
Gardner O
, O
Joel O
Grus O
, O
Mark O
Neumann O
, O
Oyvind O
Tafjord O
, O
Pradeep O
Dasigi O
, O
Nelson O
F. O
Liu O
, O
Matthew O
E. O
Peters O
, O
Michael O
Schmitz O
, O
and O
Luke O
Zettlemoyer O
. O

2018 O
. O

Allennlp O
: O

A O
deep O
semantic O
natural O
language O
processing O
platform O
. O

In O
NLP O
- O
OSS O
Workshop O
, O
pages O
1–6 O
. O

Andreas O
Hanselowski O
, O
Hao O
Zhang O
, O
Zile O
Li O
, O
Daniil O
Sorokin O
, O
Benjamin O
Schiller O
, O
Claudia O
Schulz O
, O
and O
Iryna O
Gurevych O
. O

2018 O
. O

UKP O
- O
athene O
: O
Multi O
- O
sentence O
textual O
entailment O
for O
claim O
veriﬁcation O
. O

In O
FEVER O
, O
pages O
103–108 O
. O

Tianda O
Li O
, O
Xiaodan O
Zhu O
, O
Quan O
Liu O
, O
Qian O
Chen O
, O
Zhi- O
gang O
Chen O
, O
and O
Si O
Wei O
. O
2019 O
. O

Several O
experi- O
ments O
on O
investigating O
pretraining O
and O
knowledge- O
enhanced O
models O
for O
natural O
language O
inference O
. O

CoRR O
, O
abs/1904.12104 O
. O

Yinhan O
Liu O
, O
Myle O
Ott O
, O
Naman O
Goyal O
, O
Jingfei O
Du O
, O
Man- O
dar O
Joshi O
, O
Danqi O
Chen O
, O
Omer O
Levy O
, O
Mike O
Lewis O
, O
Luke O
Zettlemoyer O
, O
and O
Veselin O
Stoyanov O
. O

2019 O
. O

Roberta O
: O
A O
robustly O
optimized O
BERT O
pretraining O
ap- O
proach O
. O

CoRR O
, O
abs/1907.11692 O
. O

Zhenghao O
Liu O
, O
Chenyan O
Xiong O
, O
Maosong O
Sun O
, O
and O
Zhiyuan O
Liu O
. O
2020 O
. O

Fine O
- O
grained O
fact O
veriﬁcation O
with O
kernel O
graph O
attention O
network O
. O

In O
ACL O
, O
pages O
7342–7351 O
. O

Ilya O
Loshchilov O
and O
Frank O
Hutter O
. O

2019 O
. O

Decoupled O
weight O
decay O
regularization O
. O

In O
ICLR O
. O

V O
olodymyr O
Mnih O
, O
Koray O
Kavukcuoglu O
, O
David O
Silver O
, O
Andrei O
A. O
Rusu O
, O
Joel O
Veness O
, O
Marc O
G. O
Bellemare O
, O
Alex O
Graves O
, O
Martin O
A. O
Riedmiller O
, O
Andreas O
Fid- O
jeland O
, O
Georg O
Ostrovski O
, O
Stig O
Petersen O
, O
Charles O
Beattie O
, O
Amir O
Sadik O
, O
Ioannis O
Antonoglou O
, O
Helen O
King O
, O
Dharshan O
Kumaran O
, O
Daan O
Wierstra O
, O
Shane O
Legg O
, O
and O
Demis O
Hassabis O
. O

2015 O
. O

Human O
- O
level O
con- O
trol O
through O
deep O
reinforcement O
learning O
. O

Nature O
, O
518(7540):529–533 O
. O

Ngoc O
- O
Khuong O
Nguyen O
, O
Anh O
- O
Cuong O
Le O
, O
and O
Hong O
Thai O
Pham O
. O

2016 O
. O

Deep O
bi O
- O
directional O
long O
short O
- O
term O
memory O
neural O
networks O
for O
sentiment O
analysis O
of O
social O
data O
. O

In O
IUKM O
, O
volume O
9978 O
, O
pages O
255–268 O
. O

Yixin O
Nie O
, O
Haonan O
Chen O
, O
and O
Mohit O
Bansal O
. O

2019 O
. O

Combining O
fact O
extraction O
and O
veriﬁcation O
with O
neu- O
ral O
semantic O
matching O
networks O
. O

In O
AAAI O
, O
pages O
6859–6866 O
. O

Tom O
Schaul O
, O
John O
Quan O
, O
Ioannis O
Antonoglou O
, O
and O
David O
Silver O
. O
2016 O
. O

Prioritized O
experience O
replay O
. O

InICLR O
.Amir O

Soleimani O
, O
Christof O
Monz O
, O
and O
Marcel O
Worring O
. O

2020 O
. O

BERT O
for O
evidence O
retrieval O
and O
claim O
veriﬁ- O
cation O
. O

In O
ECIR O
, O
volume O
12036 O
, O
pages O
359–366 O
. O

Shyam O
Subramanian O
and O
Kyumin O
Lee O
. O

2020 O
. O

Hierar- O
chical O
evidence O
set O
modeling O
for O
automated O
fact O
ex- O
traction O
and O
veriﬁcation O
. O

In O
EMNLP O
, O
pages O
7798 O
– O
7809 O
. O

James O
Thorne O
, O
Andreas O
Vlachos O
, O
Christos O
Christodoulopoulos O
, O
and O
Arpit O
Mittal O
. O

2018a O
. O

FEVER O
: O
a O
large O
- O
scale O
dataset O
for O
fact O
extraction O
and O
veriﬁcation O
. O

In O
NAACL O
- O
HLT O
, O
pages O
809–819 O
. O

James O
Thorne O
, O
Andreas O
Vlachos O
, O
Oana O
Cocarascu O
, O
Christos O
Christodoulopoulos O
, O
and O
Arpit O
Mittal O
. O

2018b O
. O

The O
fact O
extraction O
and O
VERiﬁcation O
( O
FEVER O
) O
shared O
task O
. O

In O
FEVER O
, O
pages O
1–9 O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N. O
Gomez O
, O
Lukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
NeurIPS O
, O
pages O
5998–6008 O
. O

Yongyue O
Wang O
, O
Chunhe O
Xia O
, O
Chengxiang O
Si O
, O
Beitong O
Yao O
, O
and O
Tianbo O
Wang O
. O

2020 O
. O

Robust O
reasoning O
over O
heterogeneous O
textual O
information O
for O
fact O
veriﬁca- O
tion O
. O

IEEE O
Access O
, O
8:157140–157150 O
. O

Thomas O
Wolf O
, O
Lysandre O
Debut O
, O
Victor O
Sanh O
, O
Julien O
Chaumond O
, O
Clement O
Delangue O
, O
Anthony O
Moi O
, O
Pier- O
ric O
Cistac O
, O
Tim O
Rault O
, O
R O
´ O
emi O
Louf O
, O
Morgan O
Funtow- O
icz O
, O
Joe O
Davison O
, O
Sam O
Shleifer O
, O
Patrick O
von O
Platen O
, O
Clara O
Ma O
, O
Yacine O
Jernite O
, O
Julien O
Plu O
, O
Canwen O
Xu O
, O
Teven O
Le O
Scao O
, O
Sylvain O
Gugger O
, O
Mariama O
Drame O
, O
Quentin O
Lhoest O
, O
and O
Alexander O
M. O
Rush O
. O

2020 O
. O

Transformers O
: O
State O
- O
of O
- O
the O
- O
art O
natural O
language O
pro- O
cessing O
. O

In O
EMNLP O
, O
pages O
38–45 O
. O

Deming O
Ye O
, O
Yankai O
Lin O
, O
Jiaju O
Du O
, O
Zhenghao O
Liu O
, O
Peng O
Li O
, O
Maosong O
Sun O
, O
and O
Zhiyuan O
Liu O
. O
2020 O
. O

Coref- O
erential O
reasoning O
learning O
for O
language O
representa- O
tion O
. O

In O
EMNLP O
, O
pages O
7170–7186 O
. O

Wenpeng O
Yin O
and O
Dan O
Roth O
. O

2018 O
. O

Twowingos O
: O
A O
two- O
wing O
optimization O
strategy O
for O
evidential O
claim O
veri- O
ﬁcation O
. O

In O
EMNLP O
, O
pages O
105–114 O
. O

Takuma O
Yoneda O
, O
Jeff O
Mitchell O
, O
Johannes O
Welbl O
, O
Pon- O
tus O
Stenetorp O
, O
and O
Sebastian O
Riedel O
. O

2018 O
. O

UCL O
ma- O
chine O
reading O
group O
: O
Four O
factor O
framework O
for O
fact O
ﬁnding O
( O
HexaF O
) O
. O

In O
FEVER O
, O
pages O
97–102 O
. O

Wanjun O
Zhong O
, O
Jingjing O
Xu O
, O
Duyu O
Tang O
, O
Zenan O
Xu O
, O
Nan O
Duan O
, O
Ming O
Zhou O
, O
Jiahai O
Wang O
, O
and O
Jian O
Yin O
. O

2020 O
. O

Reasoning O
over O
semantic O
- O
level O
graph O
for O
fact O
checking O
. O

In O
ACL O
, O
pages O
6170–6180 O
. O

Jie O
Zhou O
, O
Xu O
Han O
, O
Cheng O
Yang O
, O
Zhiyuan O
Liu O
, O
Lifeng O
Wang O
, O
Changcheng O
Li O
, O
and O
Maosong O
Sun O
. O
2019 O
. O

GEAR O
: O
graph O
- O
based O
evidence O
aggregating O
and O
rea- O
soning O
for O
fact O
veriﬁcation O
. O

In O
ACL O
, O
pages O
892–901 O
. O

