Proceedings O
of O
NAACL O
- O
HLT O
2019 O
, O
pages O
3036–3046 O

Minneapolis O
, O
Minnesota O
, O
June O
2 O
- O
June O
7 O
, O
2019 O
. O

c O

2019 O
Association O
for O
Computational O
Linguistics3036A O
General O
Framework O
for O
Information O
Extraction O
using O
Dynamic O
Span O
Graphs O
Yi O
LuanyDave O
WaddenyLuheng O

HezAmy O
Shahy O
Mari O
OstendorfyHannaneh O
Hajishirziy O

yUniversity O
of O
Washington O
Allen O
Institute O
for O
Artiﬁcial O
Intelligence O
zGoogle O
AI O
Language O
fluanyi O
, O
dwadden O
, O
amyshah O
, O
ostendor O
, O
hannaneh O
g@uw.edu O

luheng@google.com O

Abstract O
We O
introduce O
a O
general O
framework O
for O
sev- O
eral O
information O
extraction O
tasks O
that O
share O
span O
representations O
using O
dynamically O
con- O
structed O
span O
graphs O
. O

The O
graphs O
are O
con- O
structed O
by O
selecting O
the O
most O
conﬁdent O
entity O
spans O
and O
linking O
these O
nodes O
with O
conﬁdence- O
weighted O
relation O
types O
and O
coreferences O
. O

The O
dynamic O
span O
graph O
allows O
coreference O
and O
re- O
lation O
type O
conﬁdences O
to O
propagate O
through O
the O
graph O
to O
iteratively O
reﬁne O
the O
span O
rep- O
resentations O
. O

This O
is O
unlike O
previous O
multi- O
task O
frameworks O
for O
information O
extraction O
in O
which O
the O
only O
interaction O
between O
tasks O
is O
in O
the O
shared O
ﬁrst O
- O
layer O
LSTM O
. O

Our O
framework O
signiﬁcantly O
outperforms O
the O
state O
- O
of O
- O
the O
- O
art O
on O
multiple O
information O
extraction O
tasks O
across O
multiple O
datasets O
reﬂecting O
different O
domains O
. O

We O
further O
observe O
that O
the O
span O
enumeration O
approach O
is O
good O
at O
detecting O
nested O
span O
enti- O
ties O
, O
with O
signiﬁcant O
F1 O
score O
improvement O
on O
the O
ACE O
dataset.1 O
1 O
Introduction O
Most O
Information O
Extraction O
( O
IE O
) O
tasks O
require O
identifying O
and O
categorizing O
phrase O
spans O
, O
some O
of O
which O
might O
be O
nested O
. O

For O
example O
, O
entity O
recognition O
involves O
assigning O
an O
entity O
label O
to O
a O
phrase O
span O
. O

Relation O
Extraction O
( O
RE O
) O
involves O
assigning O
a O
relation O
type O
between O
pairs O
of O
spans O
. O

Coreference O
resolution O
groups O
spans O
referring O
to O
the O
same O
entity O
into O
one O
cluster O
. O

Thus O
, O
we O
might O
expect O
that O
knowledge O
learned O
from O
one O
task O
might O
beneﬁt O
another O
. O

Most O
previous O
work O
in O
IE O
( O
e.g. O
, O
( O
Nadeau O
and O
Sekine O
, O
2007 O
; O
Chan O
and O
Roth O
, O
2011 O
) O
) O
employs O
a O
pipeline O
approach O
, O
ﬁrst O
detecting O
entities O
and O
then O
using O
the O
detected O
entity O
spans O
for O
relation O
extrac- O
tion O
and O
coreference O
resolution O
. O

To O
avoid O
cascading O
1Code O
and O
pre O
- O
trained O
models O
are O
publicly O
available O
at O
https://github.com/luanyi/DyGIE O
. O

COREFTom O
’s O
car O
broke O
down O
as O
he O
arrived O
at O
Starbucks O
to O
meet O
Mike O
. O

“ O
This O
thing O
’s O
useless O
! O
” O

Tom O
exclaimed O
as O
it O
gave O
off O
smoke O
. O

PER O
- O
SOCPHYSVEHCOREFPERLOCPERPERVEHPERVEHPHYSFigure O
1 O
: O
A O
text O
passage O
illustrating O
interactions O
be- O
tween O
entities O
, O
relations O
and O
coreference O
links O
. O

Some O
relation O
and O
coreference O
links O
are O
omitted O
. O

errors O
introduced O
by O
pipeline O
- O
style O
systems O
, O
recent O
work O
has O
focused O
on O
coupling O
different O
IE O
tasks O
as O
in O
joint O
modeling O
of O
entities O
and O
relations O
( O
Miwa O
and O
Bansal O
, O
2016 O
; O
Zhang O
et O
al O
. O
, O
2017 O
) O
, O
entities O
and O
coreferences O
( O
Hajishirzi O
et O
al O
. O
, O
2013 O
; O
Durrett O
and O
Klein O
, O
2014 O
) O
, O
joint O
inference O
( O
Singh O
et O
al O
. O
, O
2013 O
) O
or O
multi O
- O
task O
( O
entity O
/ O
relation O
/ O
coreference O
) O
learn- O
ing O
( O
Luan O
et O
al O
. O
, O
2018a O
) O
. O

These O
models O
mostly O
rely O
on O
the O
ﬁrst O
layer O
LSTM O
to O
share O
span O
repre- O

sentations O
between O
different O
tasks O
and O
are O
usually O
designed O
for O
speciﬁc O
domains O
. O

In O
this O
paper O
, O
we O
introduce O
a O
general O
framework O
Dynamic O
Graph O
IE O
( O
DYGIE O
) O
for O
coupling O
multiple O
information O
extraction O
tasks O
through O
shared O
span O
representations O
which O
are O
reﬁned O
leveraging O
con- O
textualized O
information O
from O
relations O
and O
coref- O
erences O
. O

Our O
framework O
is O
effective O
in O
several O
do- O
mains O
, O
demonstrating O
a O
beneﬁt O
from O
incorporating O
broader O
context O
learned O
from O
relation O
and O
corefer- O
ence O
annotations O
. O

Figure O
1 O
shows O
an O
example O
illustrating O
the O
po- O
tential O
beneﬁts O
of O
entity O
, O
relation O
, O
and O
coreference O
contexts O
. O

It O
is O
impossible O
to O
predict O
the O
entity O
la- O
bels O
for O
This O
thing O
anditfrom O
within O
- O
sentence O
con- O
text O
alone O
. O

However O
, O
the O
antecedent O
carstrongly O
suggests O
that O
these O
two O
entities O
have O
a O
VEH O
type O
. O

Similarly O
, O
the O
fact O
that O
Tom O
is O
located O
at O
Starbucks O
andMike O
has O
a O
relation O
to O
Tom O
provides O
support O
for O

3037the O
fact O
that O
Mike O
is O
located O
at O
Starbucks O
. O

DYGIE O
uses O
multi O
- O
task O
learning O
to O
identify O
en- O
tities O
, O
relations O
, O
and O
coreferences O
through O
shared O
span O
representations O
using O
dynamically O
constructed O
span O
graphs O
. O

The O
nodes O
in O
the O
graph O
are O
dynam- O
ically O
selected O
from O
a O
beam O
of O
highly O
- O
conﬁdent O
mentions O
, O
and O
the O
edges O
are O
weighted O
according O
to O
the O
conﬁdence O
scores O
of O
relation O
types O
or O
coref- O
erences O
. O

Unlike O
the O
multi O
- O
task O
method O
that O
only O
shares O
span O
representations O
from O
the O
local O
con- O
text O
( O
Luan O
et O
al O
. O
, O
2018a O
) O
, O
our O
framework O
leverages O
rich O
contextual O
span O
representations O
by O
propagat- O
ing O
information O
through O
coreference O
and O
relation O
links O
. O

Unlike O
previous O
BIO O
- O
based O
entity O
recogni- O
tion O
systems O
( O
Collobert O
and O
Weston O
, O
2008 O
; O
Lample O
et O
al O
. O
, O
2016 O
; O
Ma O
and O
Hovy O
, O
2016 O
) O
that O
assign O
a O
text O
span O
to O
at O
most O
one O
entity O
, O
our O
framework O
enumer- O
ates O
and O
represents O
all O
possible O
spans O
to O
recognize O
arbitrarily O
overlapping O
entities O
. O

We O
evaluate O
DYGIE O
on O
several O
datasets O
span- O
ning O
many O
domains O
( O
including O
news O
, O
scientiﬁc O
arti- O
cles O
, O
and O
wet O
lab O
experimental O
protocols O
) O
, O
achiev- O

ing O
state O
- O
of O
- O
the O
- O
art O
performance O
across O
all O
tasks O
and O
domains O
and O
demonstrating O
the O
value O
of O
coupling O
related O
tasks O
to O
learn O
richer O
span O
representations O
. O

For O
example O
, O
DYGIE O
achieves O
relative O
improve- O

ments O
of O
5.7 O
% O
and O
9.9 O
% O
over O
state O
of O
the O
art O
on O
the O
ACE05 O
entity O
and O
relation O
extraction O
tasks O
, O
and O
an O
11.3 O
% O
relative O
improvement O
on O
the O
ACE05 O
over- O
lapping O
entity O
extraction O
task O
. O

The O
contributions O
of O
this O
paper O
are O
threefold O
. O

1 O
) O
We O
introduce O
the O
dynamic O
span O
graph O
frame- O
work O
as O
a O
method O
to O
propagate O
global O
contextual O
information O
, O
making O
the O
code O
publicly O
available O
. O

2 O
) O
We O
demonstrate O
that O
our O
framework O
signiﬁcantly O
outperforms O
the O
state O
- O
of O
- O
the O
- O
art O
on O
joint O
entity O
and O
relation O
detection O
tasks O
across O
four O
datasets O
: O
ACE O
2004 O
, O
ACE O
2005 O
, O
SciERC O
and O
the O
Wet O
Lab O
Proto- O
col O
Corpus O
. O

3 O
) O
We O
further O
show O
that O
our O
approach O
excels O
at O
detecting O
entities O
with O
overlapping O
spans O
, O
achieving O
an O
improvement O
of O
up O
to O
8 O
F1 O
points O
on O
three O
benchmarks O
annotated O
with O
overlapped O
spans O
: O
ACE O
2004 O
, O
ACE O
2005 O
and O
GENIA O
. O

2 O
Related O
Work O
Previous O
studies O
have O
explored O
joint O
model- O
ing O
( O
Miwa O
and O
Bansal O
, O
2016 O
; O
Zhang O
et O
al O
. O
, O
2017 O
; O
Singh O
et O

al O
. O
, O
2013 O
; O
Yang O
and O
Mitchell O
, O
2016 O
) O
) O
and O
multi O
- O
task O
learning O
( O
Peng O
and O
Dredze O
, O
2015 O
; O
Peng O
et O
al O
. O
, O
2017 O
; O
Luan O
et O
al O
. O
, O
2018a O
, O
2017a O
) O
as O
methods O
to O
share O
representational O
strength O
across O
related O
in O
- O
formation O
extraction O
tasks O
. O

The O
most O
similar O
to O
ours O
is O
the O
work O
in O
Luan O
et O

al O
. O
( O
2018a O
) O
that O
takes O
a O
multi O
- O
task O
learning O
approach O
to O
entity O
, O
relation O
, O
and O
coreference O
extraction O
. O

In O
this O
model O
, O
the O
dif- O
ferent O
tasks O
share O
span O
representations O
that O
only O
incorporate O
broader O
context O
indirectly O
via O
the O
gra- O
dients O
passed O
back O
to O
the O
LSTM O
layer O
. O

In O
contrast O
, O
DYGIE O
uses O
dynamic O
graph O
propagation O
to O
explic- O
itly O
incorporate O
rich O
contextual O
information O
into O
the O
span O
representations O
. O

Entity O
recognition O
has O
commonly O
been O
cast O
as O
a O
sequence O
labeling O
problem O
, O
and O
has O
beneﬁted O
substantially O
from O
the O
use O
of O
neural O
architectures O
( O
Collobert O
et O
al O
. O
, O
2011 O
; O
Lample O
et O
al O
. O
, O
2016 O
; O
Ma O
and O
Hovy O
, O
2016 O
; O
Luan O
et O

al O
. O
, O
2017b O
, O
2018b O
) O
. O

However O
, O
most O
systems O
based O
on O
sequence O
labeling O
suffer O
from O
an O
inability O
to O
extract O
entities O
with O
overlap- O
ping O
spans O
. O

Recently O
Katiyar O
and O
Cardie O
( O
2018 O
) O
and O
Wang O
and O
Lu O
( O
2018 O
) O
have O
presented O
methods O
enabling O
neural O
models O
to O
extract O
overlapping O
enti- O
ties O
, O
applying O
hypergraph O
- O
based O
representations O
on O
top O
of O
sequence O
labeling O
systems O
. O

Our O
framework O
offers O
an O
alternative O
approach O
, O
forgoing O
sequence O
labeling O
entirely O
and O
simply O
considering O
all O
possi- O
ble O
spans O
as O
candidate O
entities O
. O

Neural O
graph O
- O
based O
models O
have O
achieved O
sig- O

niﬁcant O
improvements O
over O
traditional O
feature- O
based O
approaches O
on O
several O
graph O
modeling O
tasks O
. O

Knowledge O
graph O
completion O
( O
Yang O
et O
al O
. O
, O
2015 O
; O
Bordes O
et O
al O
. O
, O
2013 O
) O
is O
one O
prominent O
example O
. O

For O
relation O
extraction O
tasks O
, O
graphs O
have O
been O
used O
primarily O
as O
a O
means O
to O
incorporate O
pipelined O
features O
such O
as O
syntactic O
or O
discourse O
relations O
( O
Peng O
et O
al O
. O
, O
2017 O
; O
Song O
et O
al O
. O
, O
2018 O
; O
Zhang O
et O
al O
. O
, O
2018 O
) O
. O

Christopoulou O
et O

al O
. O

( O
2018 O
) O
models O
all O
pos- O
sible O
paths O
between O
entities O
as O
a O
graph O
, O
and O
reﬁnes O
pair O
- O
wise O
embeddings O
by O
performing O
a O
walk O
on O
the O
graph O
structure O
. O

All O
these O
previous O
works O
assume O
that O
the O
nodes O
of O
the O
graph O
( O
i.e. O
the O
entity O
candi- O
dates O
to O
be O
considered O
during O
relation O
extraction O
) O
are O
predeﬁned O
and O
ﬁxed O
throughout O
the O
learning O
process O
. O

On O
the O
other O
hand O
, O
our O
framework O
does O
not O
require O
a O
ﬁxed O
set O
of O
entity O
boundaries O
as O
an O
input O
for O
graph O
construction O
. O

Motivated O
by O
state O
- O
of- O
the O
- O
art O
span O
- O
based O
approaches O
to O
coreference O
res- O

olution O
( O
Lee O
et O
al O
. O
, O
2017 O
, O
2018 O
) O
and O
semantic O
role O
labeling O
( O
He O
et O
al O
. O
, O
2018 O
) O
, O
the O
model O
uses O
a O
beam O
pruning O
strategy O
to O
dynamically O
select O
high O
- O
quality O
spans O
, O
and O
constructs O
a O
graph O
using O
the O
selected O
spans O
as O
nodes O
. O

Many O
state O
- O
of O
- O
the O
- O
art O
RE O
models O
rely O
upon O

3038domain O
- O
speciﬁc O
external O
syntactic O
tools O
to O
con- O
struct O
dependency O
paths O
between O
the O
entities O
in O
a O
sentence O
( O
Li O
and O
Ji O
, O
2014 O
; O
Xu O
et O

al O
. O
, O
2015 O
; O
Miwa O
and O
Bansal O
, O
2016 O
; O
Zhang O
et O
al O
. O
, O
2017 O
) O
. O

These O
sys- O
tems O
suffer O
from O
cascading O
errors O
from O
these O
tools O
and O
are O
hard O
to O
generalize O
to O
different O
domains O
. O

To O
make O
the O
model O
more O
general O
, O
we O
combine O
the O
multitask O
learning O
framework O
with O
ELMo O
em- O
beddings O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
without O
relying O
on O
external O
syntactic O
tools O
and O
risking O
the O
cascading O
errors O
that O
accompany O
them O
, O
and O
improve O
the O
inter- O
action O
between O
tasks O
through O
dynamic O
graph O
prop- O
agation O
. O

While O
the O
performance O
of O
DyGIE O
beneﬁts O
from O
ELMo O
, O
it O
advances O
over O
some O
systems O
( O
Luan O
et O
al O
. O
, O
2018a O
; O
Sanh O
et O
al O
. O
, O
2019 O
) O
that O
also O
incorporate O
ELMo O
. O

The O
analyses O
presented O
here O
give O
insights O
into O
the O
beneﬁts O
of O
joint O
modeling O
. O

3 O
Model O
Problem O
Deﬁnition O

The O
input O
is O
a O
document O
rep- O
resented O
as O
a O
sequence O
of O
words O

D O
, O
from O
which O
we O
deriveS O
= O
fs1;:::;s O
Tg O
, O
the O
set O
of O
all O
possible O
within O
- O
sentence O
word O
sequence O
spans O
( O
up O
to O
length O
L O
) O
in O
the O
document O
. O

The O
output O
contains O
three O
structures O
: O
the O
entity O
types O
Efor O
all O
spans O
S O
, O
the O
relationsRfor O
all O
span O
pairs O
SSwithin O
the O
same O
sentence O
, O
and O
the O
coreference O
links O
Cfor O
all O
spans O
inSacross O
sentences O
. O

We O
consider O
two O
primary O
tasks O
. O

First O
, O
Entity O
Recognition O
is O
the O
task O
of O
pre- O
dicting O
the O
best O
entity O
type O
labels O
eifor O
each O
span O
si O
. O

Second O
, O
Relation O
Extraction O
involves O
predicting O
the O
best O
relation O
type O
rijfor O
all O
span O
pairs O
( O
si;sj O
) O
. O

We O
provide O
additional O
supervision O
by O
also O
training O
our O
model O
to O
perform O
a O
third O
, O
auxiliary O
task O
: O
Coref- O
erence O
resolution O
. O

For O
this O
task O
we O
predict O
the O
best O
antecedentcifor O
each O
span O
si O
. O

Our O
Model O
We O
develop O
a O
general O
information O
extraction O
framework O
( O
DYGIE O
) O
to O
identify O
and O
classify O
entities O
, O
relations O
, O
and O
coreference O
in O
a O
multi O
- O
task O
setup O
. O

DYGIE O
ﬁrst O
enumerates O
all O
text O
spans O
in O
each O
sentence O
, O
and O
computes O
a O
locally- O
contextualized O
vector O
space O
representation O
of O
each O
span O
. O

The O
model O
then O
employs O
a O
dynamic O
span O
graph O
to O
incorporate O
global O
information O
into O
its O
span O
representations O
, O
as O
follows O
. O

At O
each O
training O
step O
, O
the O
model O
identiﬁes O
the O
text O
spans O
that O
are O
most O
likely O
to O
represent O
entities O
, O
and O
treats O
these O
spans O
as O
nodes O
in O
a O
graph O
structure O
. O

It O
constructs O
conﬁdence O
- O
weighted O
arcs O
for O
each O
node O
according O
to O
its O
predicted O
coreference O
and O
relation O
links O
with O
the O
other O
nodes O
in O
the O
graph O
. O

Then O
, O
the O
span O
repre O
- O
sentations O
are O
reﬁned O
using O
broader O
context O
from O
gated O
updates O
propagated O
from O
neighboring O
rela- O
tion O
types O
and O
co O
- O
referred O
entities O
. O

These O
reﬁned O
span O
representations O
are O
used O
in O
a O
multi O
- O
task O
frame- O
work O
to O
predict O
entity O
types O
, O
relation O
types O
, O
and O
coreference O
links O
. O

3.1 O
Model O
Architecture O
In O
this O
section O
, O
we O
give O
an O
overview O
of O
the O
main O
components O
and O
layers O
of O
the O
DYGIE O
framework O
, O
as O
illustrated O
in O
Figure O
2 O
. O

Details O
of O
the O
graph O
con- O
struction O
and O
reﬁnement O
process O
will O
be O
presented O
in O
the O
next O
section O
. O

Token O
Representation O
Layer O
We O
apply O
a O
bidi- O
rectional O
LSTM O
over O
the O
input O
tokens O
. O

The O
input O
for O
each O
token O
is O
a O
concatenation O
of O
the O
character O
reprensetation O
, O
GLoVe O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
word O
embeddings O
, O
and O
ELMo O
embeddings O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
. O

The O
output O
token O
representations O
are O
obtained O
by O
stacking O
the O
forward O
and O
backward O
LSTM O
hidden O
states O
. O

Span O
Representation O
Layer O
For O
each O
span O
si O
, O
its O
initial O
vector O
representation O
g0 O
iis O
obtained O
by O
concatenating O
BiLSTM O
outputs O
at O
the O
left O
and O
right O
end O
points O
of O
si O
, O
an O
attention O
- O
based O
soft O
“ O
head- O
word O
, O
” O
and O
an O
embedded O
span O
width O
feature O
, O
fol- O
lowing O
Lee O
et O
al O
. O

( O
2017 O
) O
. O

Coreference O
Propagation O
Layer O
The O
propaga- O
tion O
process O
starts O
from O
the O
span O
representations O
g0 O
i. O

At O
each O
iteration O
t O
, O
we O
ﬁrst O
compute O
an O
update O
vector O
ut O
Cfor O
each O
span O
si O
. O

Then O
we O
use O
ut O
Cto O
update O
the O
current O
representation O
gt O
i O
, O
producing O
the O
next O
span O
representation O
gt+1 O
i. O

By O
repeating O
this O
processNtimes O
, O
the O
ﬁnal O
span O
representations O
gN O

i O
share O
contextual O
information O
across O
spans O
that O
are O
likely O
to O
be O
antecedents O
in O
the O
coreference O
graph O
, O
similar O
to O
the O
process O
in O
( O
Lee O
et O
al O
. O
, O
2018 O
) O
. O

Relation O
Propagation O
Layer O
The O
outputs O
gN O
i O
from O
the O
coreference O
propagation O
layer O
are O
passed O
as O
inputs O
to O
the O
relation O
propagation O
layer O
. O

Similar O
to O
the O
coreference O
propagation O
process O
, O
at O
each O
it- O
erationt O
, O
we O
ﬁrst O
compute O
the O
update O
vectors O
ut O
R O
for O
each O
span O
si O
, O
then O
use O
it O
to O
compute O
gt+1 O
i. O
In- O
formation O
can O
be O
integrated O
from O
multiple O
relation O
paths O
by O
repeating O
this O
process O
Mtimes O
. O

Final O
Prediction O
Layer O
We O
use O
the O
outputs O
of O

the O
relation O
graph O
layer O
gN+M O
ito O
predict O
the O
entity O
labelsEand O
relation O
labels O
R. O

For O
entities O
, O
we O
passgN+M O
i O
to O
a O
feed O
- O
forward O
network O
( O
FFNN O
) O
to O

3039 O
arrive O
atStarbuckscarTom O
Input O
documentSpan O
enumerationFinal O
prediction O
  O
of O
entities O
and O
relations O
Coref O
. O

Tom O
’s O
car O
broke O
down O
as O
he O
arrived O
at O
Starbucks O
to O
meet O
Mike O
. O

“ O
This O
thing O
’s O
useless O
! O
” O

Tom O
exclaimed O
as O
it O
gave O
off O
smoke O
. O

Sentence O
- O
level O
BiLSTMSentence O
- O
level O
BiLSTMTomcararrive O
atStarbucksMikeTomthis O
thingitToken O
  O
representations…Coref.carthis O
thingitTomMikeStarbucksPER O
- O
SOCPHYSMikeTomthis O

thingit…PERVEHNULLPHYSVEHCoref.carthis O
thingitFinal O
prediction O
of O
coreferenceIterative O
inference O
and O
propagation O
for O
relationsIterative O
inference O
and O
propagation O
for O
coreferencePERPERPHYSVEHPER O
- O
SOCPHYSCoref O
. O

LOCM O
times O
… O
… O
N O
times O
… O
Figure O
2 O
: O
Overview O
of O
our O
D O
YGIE O
model O
. O

Dotted O
arcs O
indicate O
conﬁdence O
weighted O
graph O
edges O
. O

Solid O
lines O
indicate O
the O
ﬁnal O
predictions O
. O

produce O
per O
- O
class O
scores O
PE(i)for O
spansi O
. O

For O
relations O
, O
we O
pass O
the O
concatenation O
of O
gN+M O
i O
and O
gN+M O
j O
to O
a O
FFNN O
to O
produce O
per O
- O
class O
relation O
scores O
PR(i;j)between O
spans O
siandsj O
. O

Entity O
and O
relation O
scores O
are O
normalized O
across O
the O
label O
space O
, O
similar O
to O
Luan O
et O
al O
. O
( O
2018a O
) O
. O

For O
coref- O
erence O
, O
the O
scores O
between O
span O
pairs O
( O
si;sj O
) O
are O
computed O
from O
the O
coreference O
graph O
layer O
outputs O
( O
gN O
i;gN O
j O
) O
, O
and O
then O
normalized O
across O
all O
possible O
antecedents O
, O
similar O
to O
Lee O
et O
al O
. O

( O
2018 O
) O
. O

3.2 O
Dynamic O
Graph O
Construction O
and O
Span O
Reﬁnement O
The O
dynamic O
span O
graph O
facilitates O
propagating O
broader O
contexts O
through O
soft O
coreference O
and O
rela- O
tion O
links O
to O
reﬁne O
span O
representations O
. O

The O
nodes O
in O
the O
graph O
are O
spans O
siwith O
vector O
representa- O
tionsgt O
i2Rdfor O
thet O
- O
th O
iteration O
. O

The O
edges O
are O
weighted O
by O
the O
coreference O
and O
relation O
scores O
, O
which O
are O
trained O
according O
to O
the O
neural O
archi- O
tecture O
explained O
in O
Section O
3.1 O
. O

In O
this O
section O
, O
we O
explain O
how O
coreference O
and O
relation O
links O
can O
update O
span O
representations O
. O

Coreference O
Propagation O
Similar O
to O
( O
Luan O
et O
al O
. O
, O
2018a O
) O
, O
we O
deﬁne O
a O
beam O
BCconsisting O
ofbcspans O
that O
are O
most O
likely O
to O
be O
in O
a O
corefer- O
ence O
chain O
. O

We O
consider O
Pt O
Cto O
be O
a O
matrix O
of O
real O
values O
that O
indicate O
coreference O
conﬁdence O
scores O
between O
these O
spans O
at O
the O
t O
- O
th O
iteration O
. O

Pt O
Cis O
of O
sizebcK O
, O
whereKis O
the O
maximum O
num- O
ber O
of O
antecedents O
considered O
. O

For O
the O
coreferencegraph O
, O
an O
edge O
in O
the O
graph O
is O
single O
directional O
, O
connecting O
the O
current O
span O
siwith O
all O
its O
poten- O
tial O
antecedents O
sjin O
the O
coreference O
beam O
, O
where O
j O
< O
i O
. O

The O
edge O
between O
siandsjis O
weighted O
by O
coreference O
conﬁdence O
score O
at O
the O
current O
itera- O
tionPt O
C(i;j O
) O
. O

The O
span O
update O
vector O
ut O
C(i)2Rd O
is O
computed O
by O
aggregating O
the O
neighboring O
span O
representations O
gt O
j O
, O
weighted O
by O
their O
coreference O
scoresPt O
C(i;j O
): O
ut O
C(i O
) O
= O
X O
j2BC(i)Pt O
C(i;j)gt O

j O
( O
1 O
) O
whereBC(i)is O
the O
set O
of O
Kspans O
that O
are O
an- O
tecedents O
of O
si O
, O
Pt O
C(i;j O
) O

= O
exp(Vt O
C(i;j))P O
j02BC(i)exp(Vt O
C(i;j))(2 O
) O

Vt O
C(i;j)is O
a O
scalar O
score O
computed O
by O
concate- O
nating O
the O
span O
representations O

[ O

gt O
i;gt O
j;gt O

i O

gt O
j O
] O
, O
where O

is O
element O
- O
wise O
multiplication O
. O

The O
con- O
catenated O
vector O
is O
then O
fed O
as O
input O
to O
a O
FFNN O
, O
similar O
to O
( O
Lee O
et O
al O
. O
, O
2018 O
) O
. O

Relation O
Propagation O
For O
each O
sentence O
, O
we O
deﬁne O
a O
beam O
BRconsisting O
of O
brentity O
spans O
that O
are O
mostly O
likely O
to O
be O
involved O
in O
a O
rela- O
tion O
. O

Unlike O
the O
coreference O
graph O
, O
the O
weights O
of O
relation O
edges O
capture O
different O
relation O
types O
. O

Therefore O
, O
for O
the O
t O
- O
th O
iteration O
, O
we O
use O
a O
tensor O
Vt O
R2RbRbRLRto O
capture O
scores O
of O
each O
of O
the O
LRrelation O
types O
. O

In O
other O
words O
, O
each O
edge O
in O
the O

3040relation O
graph O
connects O
two O
entity O
spans O
siandsj O
in O
the O
relation O
beam O
BR.Vt O
R(i;j)is O
aLR O
- O
length O
vector O
of O
relation O
scores O
, O
computed O
with O
a O
FFNN O
with[gt O
i;gt O
j]as O
the O
input O
. O

The O
relation O
update O
vec- O
torut O
R(i)2Rdis O
computed O
by O
aggregating O
neigh- O
boring O
span O
representations O
on O
the O
relation O
graph O
: O
ut O
R(i O
) O
= O
X O
j2BRf(Vt O
R(i;j))AR O

gt O
j;(3 O
) O
where O
AR2RLRdis O
a O
trainable O
linear O
projection O
matrix O
, O
fis O
a O
non O
- O
linear O
function O
to O
select O
the O
most O
important O
relations O
. O

Because O
only O
a O
small O
number O
of O
entities O
in O
the O
relation O
beam O
are O
actually O
linked O
to O
the O
target O
span O
, O
propagation O
among O
all O
possi- O
ble O
span O
pairs O
would O
introduce O
too O
much O
noise O
to O
the O
new O
representation O
. O

Therefore O
, O
we O
choose O
f O
to O
be O
the O
ReLU O
function O
to O
remove O
the O
effect O
of O
unlikely O
relations O
by O
setting O
the O
all O
negative O
rela- O
tion O
scores O
to O
0 O
. O

Unlike O
coreference O
connections O
, O
two O
spans O
linked O
via O
a O
relation O
are O
not O
expected O
to O
have O
similar O
representations O
, O
so O
the O
matrix O
AR O
helps O
to O
transform O
the O
embedding O
gt O
jaccording O
to O
each O
relation O
type O
. O

Updating O
Span O
Representations O
with O
Gating O
To O
compute O
the O
span O
representations O
for O
the O
next O
iterationt2f1;:::;N O

+ O
Mg O
, O
we O
deﬁne O
a O
gating O
vector O
ft O
x(i)2Rd O
, O
wherex2fC;Rg O
, O
to O
deter- O
mine O
whether O
to O
keep O
the O
previous O
span O
represen- O
tation O
gt O
ior O
to O
integrate O
new O
information O
from O
the O
coreference O
or O
relation O
update O
vectors O
ut O
x(i O
) O
. O

For- O
mally O
, O
ft O
x(i O
) O
= O
g(Wf O
x[gt O
i;ut O
x(i O
) O
] O
) O
( O
4 O
) O
gt+1 O

i O
= O
ft O
x(i O
) O

gt O
i+ O
( O
1 ft O
x(i O
) O
) O

ut O
x(i O
) O
; O
where O
Wf O
x2Rd2dare O
trainable O
parameters O
, O
and O
gis O
an O
element O
- O
wise O
sigmoid O
function O
. O

3.3 O
Training O
The O
loss O
function O
is O
deﬁned O
as O
a O
weighted O
sum O
of O
the O
log O
- O
likelihood O
of O
all O
three O
tasks O
: O
X O
( O
D;R;E;C)2Dn O
ElogP(EjC;R;D O
) O
( O
5 O
) O
+ O
RlogP(RjC;D O
) O
+ O
ClogP(CjD)o O
whereE,RandCare O
gold O
structures O
of O
the O
entity O
types O
, O
relations O
and O
coreference O
, O
respec- O
tively O
. O

Dis O
the O
collection O
of O
all O
training O
documents O
D. O

The O
task O
weights O
E,R O
, O
andCare O
hyper- O
parameters O
to O
control O
the O
importance O
of O
each O
task O
. O

Domain O
Docs O
Ent O
Rel O
Coref O
ACE04 O
News O
348 O
7 O
7 O
3 O
ACE05 O
News O
511 O
7 O
6 O
7 O
SciERC O
AI O
500 O
6 O
7 O
3 O
WLP O
Bio O
lab O
622 O
18 O
13 O
7 O
Table O
1 O
: O
Datasets O
for O
joint O
entity O
and O
relation O
extraction O
and O
their O
statistics O
. O

Ent O
: O
Number O
of O
entity O
categories O
. O

Rel O
: O
Number O
of O
relation O
categories O
. O

We O
use O
a O
1 O
layer O
BiLSTM O
with O
200 O
- O
dimensional O
hidden O
layers O
. O

All O
the O
feed O
- O
forward O
functions O
have O
2 O
hidden O
layers O
of O
150 O
dimensions O
each O
. O

We O
use O
0.4 O
variational O
dropout O
( O
Gal O
and O
Ghahramani O
, O
2016 O
) O
for O
the O
LSTMs O
, O
0.4 O
dropout O
for O
the O
FFNNs O
, O
and O
0.5 O
dropout O
for O
the O
input O
embeddings O
. O

The O
hidden O
layer O
dimensions O
and O
dropout O
rates O
are O
chosen O
based O
on O
the O
development O
set O
performance O
in O
multiple O
do- O
mains O
. O

The O
task O
weights O
, O
learning O
rate O
, O
maximum O
span O
length O
, O
number O
of O
propagation O
iterations O
and O
beam O
size O
are O
tuned O
speciﬁcally O
for O
each O
dataset O
using O
development O
data O
. O

4 O
Experiments O
DYGIE O
is O
a O
general O
IE O
framework O
that O
can O
be O
ap- O
plied O
to O
multiple O
tasks O
. O

We O
evaluate O
the O
perfor- O
mance O
of O
DYGIE O
against O
models O
from O
two O
lines O
of O
work O
: O
combined O
entity O
and O
relation O
extraction O
, O
and O
overlapping O
entity O
extraction O
. O

4.1 O
Entity O
and O
relation O
extraction O
For O
the O
entity O
and O
relation O
extraction O
task O
, O
we O
test O
the O
performance O
of O
DYGIE O
on O
four O
different O
datasets O
: O
ACE2004 O
, O
ACE2005 O
, O
SciERC O
and O
the O
Wet O
Lab O
Protocol O
Corpus O
. O

We O
include O
the O
rela- O
tion O
graph O
propagation O
layer O
in O
our O
models O
for O
all O
datasets O
. O

We O
include O
the O
coreference O
graph O
propa- O
gation O
layer O
on O
the O
data O
sets O
that O
have O
coreference O
annotations O
available O
. O

Data O
All O
four O
data O
sets O
are O
annotated O
with O
entity O
and O
relation O
labels O
. O

Only O
a O
small O
fraction O
of O
entities O
( O
< O
3%of O
total O
) O
in O
these O
data O
sets O
have O
a O
text O
span O
that O
overlaps O
the O
span O
of O
another O
entity O
. O

Statistics O
on O
all O
four O
data O
sets O
are O
displayed O
in O
Table O
1 O
. O

The O
ACE2004 O
andACE2005 O
corpora O
provide O
entity O
and O
relation O
labels O
for O
a O
collection O
of O
docu- O
ments O
from O
a O
variety O
of O
domains O
, O
such O
as O
newswire O
and O
online O
forums O
. O

We O
use O
the O
same O
entity O
and O
relation O
types O
, O
data O
splits O
, O
and O
preprocessing O
as O
Miwa O
and O
Bansal O
( O
2016 O
) O
and O
Li O
and O
Ji O
( O
2014 O
) O
. O

Fol- O
lowing O
the O
convention O
established O
in O
this O
line O
of O
work O
, O
an O
entity O
prediction O
is O
considered O
correct O

3041Dataset O
System O
Entity O
Relation O
ACE04Bekoulis O
et O
al O
. O

( O
2018 O
) O
81.6 O
47.5 O
Miwa O
and O
Bansal O
( O
2016 O
) O
81.8 O
48.4 O
DYGIE O
87.4 O
59.7 O
ACE05Miwa O
and O
Bansal O
( O
2016 O
) O
83.4 O
55.6 O
Zhang O
et O
al O
. O

( O
2017 O
) O
83.6 O
57.5 O
Sanh O
et O
al O
. O

( O
2019 O
) O
87.5 O
62.7 O
DYGIE O
88.4 O
63.2 O
SciERCLuan O
et O
al O
. O

( O
2018a O
) O
64.2 O
39.3 O
DYGIE O
65.2 O
41.6 O
WLPCKulkarni O
et O
al O
. O

( O
2018 O
) O

78.0 O
* O
54.9 O
DYGIE O
79.5 O
64.1 O
Table O
2 O
: O
F1 O
scores O
on O
the O
joint O
entity O
and O
relation O

ex- O
traction O
task O
on O
each O
test O
set O
, O
compared O
against O
the O
pre- O
vious O
best O
systems O
. O

* O
indicates O
relation O
extraction O
sys- O
tem O
that O
takes O
gold O
entity O
boundary O
as O
input O
. O

if O
its O
type O
label O
and O
head O
region O
match O
those O
of O
a O
gold O
entity O
. O

We O
will O
refer O
to O
this O
version O
of O
the O
ACE2004 O
and O
ACE2005 O
data O
as O
ACE04 O
and O
ACE05 O
. O

Since O
the O
domain O
and O
mention O
span O
an- O
notations O
in O
the O
ACE O
datasets O
are O
very O
similar O
to O
those O
of O
OntoNotes O
( O
Pradhan O
et O
al O
. O
, O
2012 O
) O
, O
and O
OntoNotes O
contains O
signiﬁcantly O
more O
documents O
with O
coreference O
annotations O
, O
we O
use O
OntoNotes O
to O
train O
the O
parameters O
for O
the O
auxiliary O
corefer- O
ence O
task O
. O

The O
OntoNotes O
corpus O
contains O
3493 O
documents O
, O
averaging O
roughly O
450 O
words O
in O
length O
. O

The O
SciERC O
corpus O
( O
Luan O
et O
al O
. O
, O
2018a O
) O
pro- O
vides O
entity O
, O
coreference O
and O
relation O
annotations O
for O
a O
collection O
of O
documents O
from O
500 O
AI O
paper O
abstracts O
. O

The O
dataset O
deﬁnes O
scientiﬁc O
term O
types O
and O
relation O
types O
specially O
designed O
for O
AI O
domain O
knowledge O
graph O
construction O
. O

An O
entity O
predic- O
tion O
is O
considered O
correct O
if O
its O
label O
and O
span O
match O
with O
a O
gold O
entity O
. O

The O
Wet O
Lab O
Protocol O
Corpus O
( O
WLPC O
) O
pro- O
vides O
entity O
, O
relation O
, O
and O
event O
annotations O
for O
622 O
wet O
lab O
protocols O
( O
Kulkarni O
et O
al O
. O
, O
2018 O
) O
. O

A O
wet O
lab O
protocol O
is O
a O
series O
of O
instructions O
specifying O
how O
to O
perform O
a O
biological O
experiment O
. O

Following O
the O
procedure O
in O
Kulkarni O
et O
al O
. O

( O
2018 O
) O
, O
we O
perform O
entity O
recognition O
on O
the O
union O
of O
entity O
tags O
and O
event O
trigger O
tags O
, O
and O
relation O
extraction O
on O
the O
union O
of O
entity O
- O
entity O
relations O
and O
entity O
- O
trigger O
event O
roles O
. O

Coreference O
annotations O
are O
not O
avail- O
able O
for O
this O
dataset O
. O

Baselines O
We O
compare O
DYGIE O
with O
current O
state O
of O
the O
art O
methods O
in O
different O
datasets O
. O

Miwa O
and O
Bansal O
( O
2016 O
) O
provide O
the O
current O
state O
of O
the O
art O
on O
ACE04 O
. O

They O
construct O
a O
Tree O
LSTM O
using O
dependency O
parse O
information O
, O
and O
use O
the O
repre O
- O
sentations O
learned O
by O
the O
tree O
structure O
as O
features O
for O
relation O
classiﬁcation O
. O

Bekoulis O
et O

al O
. O

( O
2018 O
) O
use O
adversarial O
training O
as O
regularization O
for O
a O
neu- O
ral O
model O
. O

Zhang O
et O
al O
. O

( O
2017 O
) O
cast O
joint O
entity O
and O
relation O
extraction O
as O
a O
table O
ﬁlling O
problem O
and O
build O
a O
globally O
optimized O
neural O
model O
incorpo- O
rating O
syntactic O
representations O
from O
a O
dependency O
parser O
. O

Similar O
to O
DYGIE O
, O
Sanh O
et O
al O
. O

( O
2019 O
) O
and O
Luan O
et O

al O
. O

( O
2018a O
) O
use O
a O
multi O
- O
task O
learning O
frame- O
work O
for O
extracting O
entity O
, O
relation O
and O
coreference O
labels O
. O

Sanh O
et O
al O
. O

( O
2019 O
) O
improved O
the O
state O
of O
the O
art O
on O
ACE05 O
using O
multi O
- O
task O
, O
hierarchical O
supervised O
training O
with O
a O
set O
of O
low O
level O
tasks O
at O
the O
bottom O
layers O
of O
the O
model O
and O
more O
com- O
plex O
tasks O
at O
the O
top O
layers O
of O
the O
model O
. O

Luan O
et O

al O
. O

( O
2018a O
) O
previously O
achieved O
the O
state O
of O
the O
art O
on O
SciERC O
and O
use O
a O
span O
- O
based O
neural O
model O
like O
our O
DYGIE O
. O

Kulkarni O
et O
al O
. O

( O
2018 O
) O
provide O
a O
baseline O
for O
the O
WLPC O
data O
set O
. O

They O
employ O
an O
LSTM O
- O
CRF O
for O
entity O
recognition O
, O
following O
Lample O

et O
al O
. O

( O
2016 O
) O

. O

For O
relation O
extraction O
, O
they O
assume O
the O
presence O
of O
gold O
entities O
and O
train O
a O
maximum O
- O
entropy O
classiﬁer O
using O
features O
from O
the O
labeled O
entities O
. O

Results O
Table O
2 O
shows O
test O
set O
F1 O
on O
the O
joint O
entity O
and O
relation O
extraction O
task O
. O

We O
observe O
that O
DYGIE O
achieves O
substantial O
improvements O
on O
both O
entity O
recognition O
and O
relation O
extraction O
across O
the O
four O
data O
sets O
and O
three O
domains O
, O
all O
in O
the O
realistic O
setting O
where O
no O
“ O
gold O
” O
entity O
labels O
are O
supplied O
at O
test O
time O
. O

DYGIE O
achieves O
7.1 O
% O
and O
7.0 O
% O
rela- O
tive O
improvements O
over O
the O
state O
of O
the O
art O
on O
NER O
for O
ACE04 O
and O
ACE05 O
, O
respectively O
. O

For O
the O
rela- O
tion O
extraction O
task O
, O
DYGIE O
attains O
25.8 O
% O
relative O
improvement O
over O
SOTA O
on O
ACE04 O
and O
13.7 O
% O
rel- O

ative O
improvement O
on O
ACE05 O
. O

For O
ACE05 O
, O
the O
best O
entity O
extraction O
performance O
is O
obtained O
by O
switch- O
ing O
the O
order O
between O
CorefProp O
andRelProp O
( O
RelProp O
ﬁrst O
then O
CorefProp O
) O
. O

On O
SciERC O
, O
DYGIE O
advances O
the O
state O
of O
the O
art O
by O
5.9 O
% O
and O
1.9 O
% O
for O
relation O
extraction O
and O
NER O
, O
respectively O
. O

The O
improvement O
of O
DYGIE O
over O
the O
previous O
SciERC O
model O
underscores O
the O
ability O
of O
coreference O
and O
relation O
propagation O
to O
construct O
rich O
contextualized O
representations O
. O

The O
results O
from O
Kulkarni O
et O
al O
. O

( O
2018 O
) O
estab- O
lish O
a O
baseline O
for O
IE O
on O
the O
WLPC O
. O

In O
that O
work O
, O
relation O
extraction O
is O
performed O
using O
gold O
entity O
boundaries O
as O
input O
. O

Without O
using O
any O
gold O
entity O
information O
, O
DYGIE O
improves O
on O
the O
baselines O
by O
16.8 O
% O
for O
relation O
extraction O
and O
2.2 O
% O
for O
NER O
. O

3042Domain O
Docs O
Ent O
Overlap O
Coref O
ACE04 O
- O
O O
News O
443 O
7 O
42 O
% O
3 O
ACE05 O
- O
O O
News O
437 O
7 O
32 O
% O
7 O
GENIA O
Biomed O
1999 O
5 O
24 O
% O
3 O
Table O
3 O
: O
Datasets O
for O
overlapping O
entity O
extraction O
and O
their O
statistics O
. O

Ent O
: O
Number O
of O
entity O
categories O
. O

Over- O
lap O
: O
Percentage O
of O
sentences O
that O
contain O
overlapping O
entities O
. O

On O
the O
OntoNotes O
data O
set O
used O
for O
the O
auxiliary O
coreference O
task O
with O
ACE05 O
, O
our O
model O
achieves O
coreference O
test O
set O
performance O
of O
70.4 O
F1 O
, O
which O
is O
competitive O
with O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
reported O
in O
Lee O
et O
al O
. O

( O
2017 O
) O
. O

4.2 O
Overlapping O
Entity O
Extraction O
There O
are O
many O
applications O
where O
the O
correct O
iden- O
tiﬁcation O
of O
overlapping O
entities O
is O
crucial O
for O
cor- O
rect O
document O
understanding O
. O

For O
instance O
, O
in O
the O
biomedical O
domain O
, O
a O
BRCA1 O
mutation O
carrier O
could O
refer O
to O
a O
patient O
taking O
part O
in O
a O
clinical O
trial O
, O
while O
BRCA1 O
is O
the O
name O
of O
a O
gene O
. O

We O
evaluate O
the O
performance O
of O
DYGIE O
on O
overlapping O
entity O
extraction O
in O
three O
datasets O
: O
ACE2004 O
, O
ACE2005 O
and O
GENIA O
. O

Since O
relation O
annotations O
are O
not O
available O
for O
these O
datasets O
, O
we O
include O
the O
coreference O
propagation O
layer O
in O
our O
models O
but O
not O
the O
relation O
layer.2 O
Data O
Statistics O
on O
our O
three O
datasets O
are O
listed O
in O
Table O
3 O
. O

All O
three O
have O
a O
substantial O
number O
( O
> O
20 O
% O
of O
total O
) O
of O
overlapping O
entities O
, O
making O
them O
appropriate O
for O
this O
task O
. O

As O
in O
the O
joint O
case O
, O
we O
evaluate O
our O
model O
on O
ACE2004 O
andACE2005 O
, O
but O
here O
we O
follow O
the O
same O
data O
preprocessing O
and O
evaluation O
scheme O
as O
Wang O
and O
Lu O
( O
2018 O
) O
. O

We O
refer O
to O
these O
data O
sets O
as O
ACE04 O
- O
O O
and O
ACE05 O
- O
O. O
Unlike O
the O
joint O
en- O
tity O
and O
relation O
task O
in O
Sec O
. O

4.1 O
, O
where O
only O
the O
entity O
head O
span O
need O
be O
predicted O
, O
an O
entity O
pre- O
diction O
is O
considered O
correct O
in O
these O
experiments O
if O
both O
its O
entity O
label O
and O
its O
full O
text O
span O
match O
a O
gold O
prediction O
. O

This O
is O
a O
more O
stringent O
evalua- O
tion O
criterion O
than O
the O
one O
used O
in O
Section O
4.1 O
. O

As O
before O
, O
we O
use O
the O
OntoNotes O
annotations O
to O
train O
the O
parameters O
of O
the O
coreference O
layer O
. O

TheGENIA O
corpus O
( O
Kim O
et O
al O
. O
, O
2003 O
) O
provides O
entity O
tags O
and O
coreferences O
for O
1999 O
abstracts O
from O
the O
biomedical O
research O
literature O
. O

We O
only O
use O
the O
IDENT O
label O
to O
extract O
coreference O
clusters O
. O

2We O
use O
the O
pre O
- O
processed O
ACE O
dataset O
from O
previous O
work O
and O
relation O
annotation O
is O
not O
available O
. O

Dataset O
System O
Entity O
F1 O
ACE04 O
- O
OKatiyar O
and O
Cardie O
( O
2018 O
) O
72.7 O
Wang O
and O
Lu O
( O
2018 O
) O

75.1 O
DYGIE O
84.7 O

ACE05 O
- O
OKatiyar O
and O
Cardie O
( O
2018 O
) O
70.5 O
Wang O
and O
Lu O
( O
2018 O
) O
74.5 O
DYGIE O
82.9 O
GENIAKatiyar O
and O
Cardie O
( O
2018 O
) O
73.8 O
Wang O
and O
Lu O
( O
2018 O
) O
75.1 O
DYGIE O
76.2 O
Table O
4 O
: O
Performance O
on O
the O
overlapping O
entity O
extrac- O
tion O
task O
, O
compared O
to O
previous O
best O
systems O
. O

We O
re- O
port O
F1 O
of O
extracted O
entities O
on O
the O
test O
sets O
. O

Entity O
Relation O
Model O
P O
R O
F1 O
P O
R O
F1 O
DYGIE O
87.4 O
86.7 O
87.1 O
56.2 O
60.9 O
58.4 O
 CorefProp O
86.2 O
85.2 O
85.7 O
64.3 O
56.7 O
60.2 O
 RelProp O
87.0 O
86.7 O
86.9 O
60.4 O
55.8 O
58.0 O
Base O
86.1 O
85.7 O
85.9 O
59.5 O
55.7 O
57.6 O
Table O
5 O
: O
Ablations O
on O
the O
ACE05 O
development O
set O
with O
different O
graph O
propagation O
setups O
. O

 CorefProp O
ablates O
the O
coreference O
propagation O
layers O
, O
while O
 RelProp O
ablates O
the O
relation O
propagation O
layers O
. O

Base O
is O
the O
system O
without O
any O
propagation O
. O

We O
use O
the O
same O
data O
set O
split O
and O
preprocessing O
procedure O
as O
Wang O
and O
Lu O
( O
2018 O
) O
for O
overlapping O
entity O
recognition O
. O

Baselines O
The O
current O
state O
- O
of O
- O
the O
- O
art O
approach O
on O
all O
three O
data O
sets O
is O
Wang O
and O
Lu O
( O
2018 O
) O
, O
which O
uses O
a O
segmental O
hypergraph O
coupled O
with O
neural O
networks O
for O
feature O
learning O
. O

Katiyar O
and O
Cardie O
( O
2018 O
) O
also O
propose O
a O
hypergraph O
approach O
using O
a O
recurrent O
neural O
network O
as O
a O
feature O
extractor O
. O

Results O
Table O
4 O
presents O
the O
results O
of O
our O
over- O
lapping O
entity O
extraction O
experiments O
on O
the O
differ- O
ent O
datsets O
. O

DYGIE O
improves O
11.6 O
% O
on O
the O
state O
of O
the O
art O
for O
ACE04 O
- O
O O
and O
11.3 O
% O
for O
ACE05 O
- O
O. O
DY- O
GIE O
also O
advances O
the O
state O
of O
the O
art O
on O
GENIA O
, O
albeit O
by O
a O
more O
modest O
1.5 O
% O
. O

Together O
these O
re- O
sults O
suggest O
that O
DYGIE O
can O
be O
utilized O
fruitfully O
for O
information O
extraction O
across O
different O
domains O
with O
overlapped O
entities O
, O
such O
as O
bio O
- O
medicine O
. O

5 O
Analysis O
of O
Graph O
Propagation O
We O
use O
the O
dev O
sets O
of O
ACE2005 O
and O
SciERC O
to O
analyze O
the O
effect O
of O
different O
model O
components O
. O

5.1 O
Coreference O
and O
Relation O
Graph O
Layers O
Tables O
5 O
and O
6 O
show O
the O
effects O
of O
graph O
propa- O
gation O
on O
entity O
and O
relation O
prediction O
accuracy O
, O

3043Entity O
Relation O
Model O
P O
R O
F1 O
P O
R O
F1 O
DYGIE O
68.6 O
67.8 O
68.2 O
46.2 O

38.5 O
42.0 O
 CorefProp O
69.2 O
66.9 O
68.0 O
42.0 O
40.5 O
41.2 O
 RelProp O
69.1 O
66.0 O
67.5 O
43.6 O
37.6 O
40.4 O
Base O
70.0 O
66.3 O
68.1 O
45.4 O
34.9 O
39.5 O
Table O
6 O
: O
Ablations O
on O
the O
SciERC O
development O
set O
on O
different O
graph O
progation O
setups O

. O

CorefProp O
has O
a O
much O
smaller O
effect O
on O
entity O
F1 O
compared O
to O
ACE05 O
. O
0 O
1 O
2 O
3808284868890 O
Num O
. O
iterations O
NEntity O
F1 O
( O
a O
) O
Entity O
F1 O
with O
different O
number O
of O
CorefProp O
it- O
erations O
N.0 O
1 O
2 O
3545658606264 O
Num O
. O
iterations O
MRelation O
F1 O
( O
b O
) O
Relation O
F1 O
with O
differ- O
ent O
number O
of O
RelProp O
it- O
erations O
M. O
Figure O
3 O
: O
F1 O
score O
of O
each O
layer O
on O
ACE O
development O
set O
for O
different O
number O
of O
iterations O
. O

N= O
0orM= O
0 O
indicates O
no O
propagation O
is O
made O
for O
the O
layer O
. O

where CorefProp O
and RelProp O
denote O
ab- O
lating O
the O
propagation O
process O
by O
setting O
N= O
0 O

orM= O
0 O
, O
respectively O
. O

Base O
is O
the O
base O
model O
without O
any O
propagation O
. O

For O
ACE05 O
, O
we O
observe O
that O
coreference O
propagation O
is O
mainly O
helpful O
for O
entities O
; O
it O
appears O
to O
hurt O
relation O
extraction O
. O

On O
SciIE O
, O
coreference O
propagation O
gives O
a O
small O
ben- O
eﬁt O
on O
both O
tasks O
. O

Relation O
propagation O
signiﬁ- O
cantly O
beneﬁts O
both O
entity O
and O
relation O
extraction O
in O
both O
domains O
. O

In O
particular O
, O
there O
are O
a O
large O
por- O
tion O
of O
sentences O
with O
multiple O
relation O
instances O
across O
different O
entities O
in O
both O
ACE05 O
and O
Sci- O
ERC O
, O
which O
is O
the O
scenario O
in O
which O
we O
expect O
relation O
propagation O
to O
help O
. O

Since O
coreference O
propagation O
has O
more O
effect O
on O
entity O
extraction O
and O
relation O
propagation O
has O
more O
effect O
on O
relation O
extraction O
, O
we O
mainly O
focus O
on O
ablating O
the O
effect O
of O
coreference O
propagation O
on O
entity O
extraction O
and O
relation O
propagation O
on O
relation O
extraction O
in O
the O
following O
subsections O
. O

5.2 O
Coreference O
Propagation O
and O
Entities O
A O
major O
challenge O
of O
ACE05 O
is O
to O
disambiguate O
the O
entity O
class O
for O
pronominal O
mentions O
, O
which O
requires O
reasoning O
with O
cross O
- O
sentence O
contexts O
. O

For O
example O
, O
in O
a O
sentence O
from O
ACE05 O
dataset O
, O
“ O
One O
of O
[ O
them O
] O
PER O
, O
from O
a O
very O
close O
friend O
of O
[ O
ours O
] O
ORG O
. O
” O

It O
is O
impossible O
to O
identity O
whether O
them O
andours O
is O
a O
person O
( O
PER O
) O
or O
organization O
( O
ORG O
) O
unless O
we O
have O
read O
previous O
sentences O
. O

WeEntity O
Perf O
. O

on O
Pronouns O
P O
R O
F1 O
DYGIE O
79.0 O
77.1 O
78.0 O
DYGIE CorefProp O
73.8 O
72.6 O
73.2 O
Table O
7 O
: O
Entity O
extraction O
performance O
on O
pronouns O
in O
ACE05 O
. O

CorefProp O
signiﬁcantly O
increases O
entity O
ex- O
traction O
F1 O
on O
hard O
- O
to O
- O
disambiguate O
pronouns O
by O
allow- O
ing O
the O
model O
to O
leverage O
cross O
- O
sentence O
contexts O
. O

hypothesize O
that O
this O
is O
a O
context O
where O
coreference O
propagation O
can O
help O
. O

Table O
7 O
shows O
the O
effect O
of O
the O
coreference O
layer O
for O
entity O
categorization O
of O
pronouns.3DYGIE O
has O
6.6 O
% O
improvement O
on O
pronoun O
performance O
, O
conﬁrming O
our O
hypothesis O
. O

Looking O
further O
, O
Table O
8 O
shows O
the O
impact O
on O
all O
entity O
categories O
, O
giving O
the O
difference O
between O
the O
confusion O
matrix O
entries O
with O
and O
without O
CorefProp O
. O

The O
frequent O
confusions O
associated O
with O
pronouns O
( O
GPE O
/ O
PER O
andPER O
/ O
ORG O
, O
where O
GPE O
is O
a O
geopolitical O
entity O
) O
greatly O
improve O
, O
but O
the O
beneﬁt O
of O
CorefProp O
extends O
to O
most O
cate- O
gories O
. O

Of O
course O
, O
there O
are O
a O
few O
instances O
where O
CorefProp O
causes O
errors O
in O
entity O
extraction O
. O

For O
example O
, O
in O
the O
sentence O
“ O
[ O
They]ORG O
PER O
might O
have O
been O
using O
Northshore O
... O
” O
, O
DYGIE O
predicted O
They O
to O
be O
of O
ORG O
type O
because O
the O
most O
conﬁdent O
an- O
tecedent O
is O
those O
companies O
in O
the O
previous O
sen- O
tence O
: O
“ O
The O
money O
was O
invested O
in O
those O
compa- O
nies O
. O
” O

However O
, O
They O
is O
actually O
referring O
to O
these O
fund O
managers O
earlier O
in O
the O
document O
, O
which O
be- O
longs O
to O
PER O
category O
. O

In O
the O
SciERC O
dataset O
, O
the O
pronouns O
are O
uni- O
formly O
assigned O
with O
a O
Generic O
label O
, O
which O
ex- O
plains O
why O
CorefProp O
does O
not O
have O
much O
ef- O
fect O
on O
entity O
extraction O
performance O
. O

The O
Figure O
3a O
shows O
the O
effect O
of O
number O
of O
iterations O
for O
coreference O
propagation O
in O
the O
entity O
extraction O
task O
. O

The O
ﬁgure O
shows O
that O
coreference O
layer O
obtains O
the O
best O
performance O
on O
the O
second O
iteration O
( O
N= O
2 O
) O
. O

5.3 O
Relation O
Propagation O
Impact O
Figure O
4 O
shows O
relation O
scores O
as O
a O
function O
of O
num- O
ber O
of O
entities O
in O
sentence O
for O
DYGIE O
andDYGIE O
without O
relation O
propagation O
on O
ACE05 O
. O

The O
ﬁgure O
indicates O
that O
relation O
propagation O
achieves O
signiﬁ- O
ca O
nt O
improvement O
in O
sentences O
with O
more O
entities O
, O
where O
one O
might O
expect O
that O
using O
broader O
context O
3Pronouns O
included O
: O
anyone O
, O
everyone O
, O
it O
, O
itself O
, O
one O
, O
our O
, O
ours O
, O
their O
, O
theirs O
, O
them O
, O
themselves O
, O
they O
, O
us O
, O
we O
, O

who O

3044LOC O
WEA O
GPE O
PER O
FAC O
ORG O
VEH O
LOC O
5 O
0 O
-2 O
-1 O
2 O
-1 O
0 O
WEA O
0 O
3 O
0 O
0 O
1 O
-3 O
-1 O
GPE O
-3 O
0 O
31 O
-26 O
3 O
-7 O
0 O
PER O
0 O
-2 O
-3 O
18 O
-1 O
-26 O
4 O
FAC O
4 O
-1 O
2 O
-3 O
2 O
-5 O
1 O
ORG O
0 O
0 O
0 O
-8 O
-1 O
6 O
0 O
VEH O
0 O
-2 O
-1 O
2 O
5 O
-1 O
1 O
Table O
8 O
: O
Difference O
in O
the O
confusion O
matrix O
counts O
for O
ACE05 O
entity O
extraction O
associated O
with O
adding O
CorefProp O
. O

2 O
3 O
4 O
- O
5 O
6 O
- O
11 O
12 O
- O
max506070 O
Num O
. O
entities O
in O
sentenceRelation O
F1DYGIE O
DYGIE RelProp O
Figure O
4 O
: O
Relation O
F1 O
broken O
down O
by O
number O
of O
enti- O
ties O
in O
each O
sentence O
. O

The O
performance O
of O
relation O
ex- O

traction O
degrades O
on O
sentences O
containing O
more O
entities O
. O

Adding O
relation O
propagation O
alleviates O
this O
problem O
. O

could O
have O
more O
impact O
. O

Figure O
3b O
shows O
the O
effect O
of O
number O
of O
itera- O
tions O
for O
relation O
propagation O
in O
the O
relation O
extrac- O
tion O
task O
. O

Our O
model O
achieves O
the O
best O
performance O
on O
the O
second O
iteration O
( O
M= O
2 O
) O
. O

6 O
Conclusion O
We O
have O
introduced O
DYGIE O
as O
a O
general O
informa- O
tion O
extraction O
framework O
, O
and O
have O
demonstrated O
that O
our O
system O
achieves O
state O
- O
of O
- O
the O
art O
results O
on O
entity O
recognition O
and O
relation O
extraction O
tasks O
across O
a O
diverse O
range O
of O
domains O
. O

The O
key O
con- O
tribution O
of O
our O
model O
is O
the O
dynamic O
span O
graph O
approach O
, O
which O
enhance O
interaction O
across O
tasks O
that O
allows O
the O
model O
to O
learn O
useful O
information O
from O
broader O
context O
. O

Unlike O
many O
IE O
frameworks O
, O
our O
model O
does O
not O
require O
any O
preprocessing O
using O
syntactic O
tools O
, O
and O
has O
signiﬁcant O
improvement O
across O
different O
IE O
tasks O
including O
entity O
, O
relation O
extraction O
and O
overlapping O
entity O
extraction O
. O

The O
addition O
of O
co O
- O
reference O
and O
relation O
propagation O
across O
sentences O
adds O
only O
a O
small O
computation O
cost O
to O
inference O
; O
the O
memory O
cost O
is O
controlled O
by O
beam O
search O
. O

These O
added O
costs O
are O
small O
relative O
to O
those O
of O
the O
baseline O
span O
- O
based O
model O
. O

We O
wel- O
come O
the O
community O
to O
test O
our O
model O
on O
different O
information O
extraction O
tasks O
. O

Future O
directions O
in- O
clude O
extending O
the O
framework O
to O
encompass O
more O
structural O
IE O
tasks O
such O
as O
event O
extraction O
. O

Acknowledgments O
This O
research O
was O
supported O
by O
the O
Ofﬁce O
of O
Naval O
Research O
under O
the O
MURI O
grant O
N00014 O
- O
18 O
- O
1- O
2670 O
, O
NSF O
( O
IIS O
1616112 O
, O
III O
1703166 O
) O
, O
Allen O
Dis- O
tinguished O
Investigator O
Award O
, O
Samsung O
GRO O
and O
gifts O
from O
Allen O
Institute O
for O
AI O
, O
Google O
, O
Amazon O
, O
and O
Bloomberg O
. O

We O
also O
thank O
the O
anonymous O
re- O
viewers O
and O
the O
UW O
- O
NLP O
group O
for O
their O
helpful O
comments O
. O

References O
Giannis O
Bekoulis O
, O
Johannes O
Deleu O
, O
Thomas O
Demeester O
, O
and O
Chris O
Develder O
. O

2018 O
. O

Adversarial O
training O
for O
multi O
- O
context O
joint O
entity O
and O
relation O
extraction O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Methods O
Natural O
Language O
Process O
. O

( O
EMNLP O
) O
, O
pages O
2830–2836 O
. O

Antoine O
Bordes O
, O
Nicolas O
Usunier O
, O
Alberto O
Garcia- O
Duran O
, O
Jason O
Weston O
, O
and O
Oksana O
Yakhnenko O
. O

2013 O
. O

Translating O
embeddings O
for O
modeling O
multi- O
relational O
data O
. O

In O
Advances O
in O
neural O
information O
processing O
systems O
. O

Yee O
Seng O
Chan O
and O
Dan O
Roth O
. O

2011 O
. O

Exploiting O
syntactico O
- O
semantic O
structures O
for O
relation O
extrac- O
tion O
. O

In O
Proc O
. O

Annu O
. O

Meeting O
Assoc O
. O

for O
Computa- O
tional O
Linguistics O
( O
ACL O
) O

. O

Fenia O
Christopoulou O
, O
Makoto O
Miwa O
, O
and O
Sophia O
Ana- O
niadou O
. O

2018 O
. O

A O
walk O
- O
based O
model O
on O
entity O
graphs O
for O
relation O
extraction O
. O

In O
Proc O
. O

Annu O
. O

Meeting O
As- O
soc O
. O

for O
Computational O
Linguistics O
( O
ACL O
) O
, O
volume O
2 O
, O
pages O
81–88 O
. O

Ronan O
Collobert O
and O
Jason O
Weston O
. O

2008 O
. O

A O
uniﬁed O
architecture O
for O
natural O
language O
processing O
: O

Deep O
neural O
networks O
with O
multitask O
learning O
. O

In O
Proc O
. O

Int O
. O

Conf O
. O

Machine O
Learning O
( O
ICML O
) O
, O
pages O
160 O
– O
167 O
. O

Ronan O
Collobert O
, O
Jason O
Weston O
, O
L O
´ O
eon O
Bottou O
, O
Michael O
Karlen O
, O
Koray O
Kavukcuoglu O
, O
and O
Pavel O
Kuksa O
. O
2011 O
. O

Natural O
language O
processing O
( O
almost O
) O
from O
scratch O
. O

J. O
Machine O
Learning O
Research O
, O
12(Aug):2493 O
– O
2537 O
. O

Greg O
Durrett O
and O
Dan O
Klein O
. O

2014 O
. O

A O
joint O
model O
for O
entity O
analysis O
: O
Coreference O
, O
typing O
, O
and O
linking O
. O

Trans O
. O

Assoc O
. O

for O
Computational O
Linguistics O
( O
TACL O
) O
, O
2:477–490 O
. O

Yarin O
Gal O
and O
Zoubin O
Ghahramani O
. O
2016 O
. O

A O
theoret- O
ically O
grounded O
application O
of O
dropout O
in O
recurrent O
neural O
networks O
. O

In O
Proc O
. O

Annu O
. O

Conf O
. O

Neural O
In- O
form O
. O

Process O
. O

Syst O
. O

( O
NIPS O
) O
. O

Hannaneh O
Hajishirzi O
, O
Leila O
Zilles O
, O
Daniel O
S O
Weld O
, O
and O
Luke O
Zettlemoyer O
. O

2013 O
. O

Joint O
coreference O
res- O
olution O
and O
named O
- O
entity O
linking O
with O
multi O
- O
pass O
sieves O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Methods O
Natural O
Language O
Process O
. O

( O
EMNLP O
) O
, O
pages O
289–299 O
. O

3045Luheng O
He O
, O
Kenton O
Lee O
, O
Omer O
Levy O
, O
and O
Luke O
Zettle- O
moyer O
. O

2018 O
. O

Jointly O
predicting O
predicates O
and O
argu- O
ments O
in O
neural O
semantic O
role O
labeling O
. O

In O
ACL O
. O

Arzoo O
Katiyar O
and O
Claire O
Cardie O
. O

2018 O
. O

Nested O
named O
entity O
recognition O
revisited O
. O

In O
Proc O
. O

Conf O
. O

North O
American O
Assoc O
. O

for O
Computational O
Linguis- O
tics O
( O
NAACL O
) O
. O

Jin O
- O
Dong O
Kim O
, O
Tomoko O
Ohta O
, O
Yuka O
Tateisi O
, O
and O
Jun’ichi O
Tsujii O
. O

2003 O
. O

Genia O
corpus O
- O
a O
semantically O
annotated O
corpus O
for O
bio O
- O
textmining O
. O
Bioinformat- O

ics O
, O
19 O
Suppl O
1 O
: O
i180–2 O
. O

Chaitanya O
Kulkarni O
, O
Wei O
Xu O
, O
Alan O
Ritter O
, O
and O
Raghu O
Machiraju O
. O
2018 O
. O

An O
annotated O
corpus O
for O
machine O
reading O
of O
instructions O
in O
wet O
lab O
protocols O
. O

In O
NAACL O
- O
HLT O
. O

Guillaume O
Lample O
, O
Miguel O
Ballesteros O
, O
Sandeep O
Sub- O
ramanian O
, O
Kazuya O
Kawakami O
, O
and O
Chris O
Dyer O
. O
2016 O
. O

Neural O
architectures O
for O
named O
entity O
recognition O
. O

InProc O
. O

Conf O
. O

North O
American O
Assoc O
. O

for O
Compu- O
tational O
Linguistics O
( O
NAACL O
) O
. O

Kenton O
Lee O
, O
Luheng O
He O
, O
Mike O
Lewis O
, O
and O
Luke O
S. O
Zettlemoyer O
. O

2017 O
. O

End O
- O
to O
- O
end O
neural O
coreference O
resolution O
. O

In O
EMNLP O
. O

Kenton O
Lee O
, O
Luheng O
He O
, O
and O
Luke O
Zettlemoyer O
. O

2018 O
. O

Higher O
- O
order O
coreference O
resolution O
with O
coarse O
- O
to- O
ﬁne O
inference O
. O

In O
NAACL O
. O

Qi O
Li O
and O
Heng O
Ji O
. O
2014 O
. O

Incremental O
joint O
extrac- O
tion O
of O
entity O
mentions O
and O
relations O
. O

In O
Proc O
. O

Annu O
. O

Meeting O
Assoc O
. O

for O
Computational O
Linguistics O
( O
ACL O
) O
, O
volume O
1 O
, O
pages O
402–412 O
. O

Yi O
Luan O
, O
Chris O
Brockett O
, O
Bill O
Dolan O
, O
Jianfeng O
Gao O
, O
and O
Michel O
Galley O
. O

2017a O
. O

Multi O
- O
task O
learning O
for O
speaker O
- O
role O
adaptation O
in O
neural O
conversation O
mod- O
els O
. O

In O
Proc O
. O

IJCNLP O
. O

Yi O
Luan O
, O
Luheng O
He O
, O
Mari O
Ostendorf O
, O
and O
Hannaneh O
Hajishirzi O
. O
2018a O
. O

Multi O
- O
task O
identiﬁcation O
of O
enti- O
ties O
, O
relations O
, O
and O
coreference O
for O
scientiﬁc O
knowl- O
edge O
graph O
construction O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Methods O
Natural O
Language O
Process O
. O

( O
EMNLP O
) O
. O

Yi O
Luan O
, O
Mari O
Ostendorf O
, O
and O
Hannaneh O
Hajishirzi O
. O

2017b O
. O

Scientiﬁc O
information O
extraction O
with O
semi- O
supervised O
neural O
tagging O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Methods O
Natural O
Language O
Process O
. O

( O
EMNLP O
) O
. O

Yi O
Luan O
, O
Mari O
Ostendorf O
, O
and O
Hannaneh O
Hajishirzi O
. O
2018b O
. O

The O
uwnlp O
system O
at O
semeval-2018 O
task O
7 O
: O
Neural O
relation O
extraction O
model O
with O
selectively O
in- O
corporated O
concept O
embeddings O
. O

In O
Proc O
. O

Int O
. O

Work- O
shop O
on O
Semantic O
Evaluation O
( O
SemEval O
) O
, O
pages O
788 O
– O
792 O
. O

Xuezhe O
Ma O
and O
Eduard O
Hovy O
. O
2016 O
. O

End O
- O
to O
- O
end O
sequence O
labeling O
via O
bi O
- O
directional O
LSTM O
- O
CNNs- O
CRF O
. O

In O
Proc O
. O

Annu O
. O

Meeting O
Assoc O
. O

for O
Computa- O
tional O
Linguistics O
( O
ACL O
) O
.Makoto O

Miwa O
and O
Mohit O
Bansal O
. O
2016 O
. O

End O
- O
to O
- O
end O
re- O
lation O
extraction O
using O
lstms O
on O
sequences O
and O
tree O
structures O
. O

In O
Proc O
. O

Annu O
. O

Meeting O
Assoc O
. O

for O
Com- O
putational O
Linguistics O
( O
ACL O
) O
, O
pages O
1105–1116 O
. O

David O
Nadeau O
and O
Satoshi O
Sekine O
. O
2007 O
. O

A O
survey O
of O
named O
entity O
recognition O
and O
classiﬁcation O
. O

Lingvis- O
ticae O
Investigationes O
, O
30(1):3–26 O
. O

Nanyun O
Peng O
and O
Mark O
Dredze O
. O
2015 O
. O

Named O
en- O
tity O
recognition O
for O
chinese O
social O
media O
with O
jointly O
trained O
embeddings O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Meth- O
ods O
Natural O
Language O
Process O
. O

( O
EMNLP O
) O
, O
pages O
548–554 O
. O

Nanyun O
Peng O
, O
Hoifung O
Poon O
, O
Chris O
Quirk O
, O
Kristina O
Toutanova O
, O
and O
Wen O
- O
tau O
Yih O
. O
2017 O
. O

Cross O
- O
sentence O
n O
- O
ary O
relation O
extraction O
with O
graph O
lstms O
. O

Trans O
. O

As- O
soc O
. O

for O
Computational O
Linguistics O
( O
TACL O
) O
, O
5:101 O
– O
115 O
. O

Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christopher O
D O
Manning O
. O

2014 O
. O

Glove O
: O
Global O
vectors O
for O
word O
rep- O

resentation O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Methods O
Natu- O
ral O
Language O
Process O
. O

( O
EMNLP O
) O
, O
volume O
14 O
, O
pages O
1532–1543 O
. O

Matthew O
E. O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O

2018 O
. O

Deep O
contextualized O
word O
repre- O
sentations O
. O

In O
NAACL O
. O

Sameer O
Pradhan O
, O
Alessandro O
Moschitti O
, O
Nianwen O
Xue O
, O
Olga O
Uryupina O
, O
and O
Yuchen O
Zhang O
. O

2012 O
. O

Conll- O
2012 O
shared O
task O
: O
Modeling O
multilingual O
unre- O
stricted O
coreference O
in O
ontonotes O
. O

In O
Joint O
Confer- O
ence O
on O
EMNLP O
and O
CoNLL O
- O
Shared O
Task O
, O
pages O
1 O
– O
40 O
. O

Association O
for O
Computational O
Linguistics O
. O

Victor O
Sanh O
, O
Thomas O
Wolf O
, O
and O
Sebastian O
Ruder O
. O

2019 O
. O

A O
hierarchical O
multi O
- O
task O
approach O
for O
learning O
em- O
beddings O
from O
semantic O
tasks O
. O

AAAI O
. O

Sameer O
Singh O
, O
Sebastian O
Riedel O
, O
Brian O
Martin O
, O
Jiaping O
Zheng O
, O
and O
Andrew O
McCallum O
. O

2013 O
. O

Joint O
infer- O
ence O
of O
entities O
, O
relations O
, O
and O
coreference O
. O

In O
Proc O
. O

of O
the O
2013 O
workshop O
on O
Automated O
knowledge O
base O
construction O
, O
pages O
1–6 O
. O

ACM O
. O

Linfeng O
Song O
, O
Yue O
Zhang O
, O
Zhiguo O
Wang O
, O
and O
Daniel O
Gildea O
. O

2018 O
. O

N O
- O
ary O
relation O
extraction O
using O
graph- O
state O
lstm O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Methods O
Natu- O
ral O
Language O
Process O
. O

( O
EMNLP O
) O
, O
pages O
2226–2235 O
. O

Bailin O
Wang O
and O
Wei O
Lu O
. O

2018 O
. O

Neural O
segmental O
hy- O
pergraphs O
for O
overlapping O
mention O
recognition O
. O

In O
EMNLP O
. O

Kun O
Xu O
, O
Yansong O
Feng O
, O
Songfang O
Huang O
, O
and O
Dongyan O
Zhao O
. O
2015 O
. O

Semantic O
relation O
classiﬁca- O
tion O
via O
convolutional O
neural O
networks O
with O
simple O
negative O
sampling O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Meth- O
ods O
Natural O
Language O
Process O
. O

( O
EMNLP O
) O
, O
pages O
536–540 O
. O

3046Bishan O
Yang O
and O
Tom O
M O
Mitchell O
. O

2016 O
. O

Joint O
extrac- O
tion O
of O
events O
and O
entities O
within O
a O
document O
context O
. O

InProceedings O
of O
the O
2016 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computa- O
tional O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
289–299 O
. O

Bishan O
Yang O
, O
Wen O
- O
tau O
Yih O
, O
Xiaodong O
He O
, O
Jianfeng O
Gao O
, O
and O
Li O
Deng O
. O
2015 O
. O

Embedding O
entities O
and O
relations O
for O
learning O
and O
inference O
in O
knowledge O
bases O
. O

In O
Proc O
. O

Int O
. O

Conf O
. O

Learning O
Representations O
( O
ICLR O
) O
. O

Meishan O
Zhang O
, O
Yue O
Zhang O
, O
and O
Guohong O
Fu O
. O
2017 O
. O

End O
- O
to O
- O
end O
neural O
relation O
extraction O
with O
global O
op- O
timization O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Methods O
Natu- O
ral O
Language O
Process O
. O

( O
EMNLP O
) O
, O
pages O
1730–1740 O
. O

Yuhao O
Zhang O
, O
Peng O
Qi O
, O
and O
Christopher O
D O
Man- O
ning O
. O

2018 O
. O

Graph O
convolution O
over O
pruned O
depen- O
dency O
trees O
improves O
relation O
extraction O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Methods O
Natural O
Language O
Pro- O
cess O
. O

( O
EMNLP O
) O
. O

