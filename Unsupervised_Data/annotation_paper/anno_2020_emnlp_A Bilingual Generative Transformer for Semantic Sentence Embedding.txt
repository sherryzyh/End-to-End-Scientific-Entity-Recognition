Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1581‚Äì1594 O
, O
November O
16‚Äì20 O
, O
2020 O
. O

c O

2020 O
Association O
for O
Computational O
Linguistics1581A O
Bilingual O
Generative O
Transformer O
for O
Semantic O
Sentence O
Embedding O
John O
Wieting1 O
, O
Graham O
Neubig1 O
, O
and O
Taylor O
Berg O
- O
Kirkpatrick2 O
1Carnegie O
Mellon O
University O
, O
Pittsburgh O
, O
PA O
, O
15213 O
, O
USA O
2University O
of O
California O
San O
Diego O
, O
San O
Diego O
, O
CA O
, O
92093 O
, O
USA O
fjwieting O
, O
gneubig O
g@cs.cmu.edu O
, O
tberg@eng.ucsd.edu O
Abstract O
Semantic O
sentence O
embedding O
models O
encode O
natural O
language O
sentences O
into O
vectors O
, O
such O
that O
closeness O
in O
embedding O
space O
indicates O
closeness O
in O
the O
semantics O
between O
the O
sen- O
tences O
. O

Bilingual O
data O
offers O
a O
useful O
signal O
for O
learning O
such O
embeddings O
: O
properties O
shared O
by O
both O
sentences O
in O
a O
translation O
pair O
are O
likely O
semantic O
, O
while O
divergent O
properties O
are O
likely O
stylistic O
or O
language O
- O
speciÔ¨Åc O
. O

We O
pro- O
pose O
a O
deep O
latent O
variable O
model O
that O
attempts O
to O
perform O
source O
separation O
on O
parallel O
sen- O
tences O
, O
isolating O
what O
they O
have O
in O
common O
in O
a O
latent O
semantic O
vector O
, O
and O
explaining O
what O
is O
left O
over O
with O
language O
- O
speciÔ¨Åc O
latent O
vec- O
tors O
. O

Our O
proposed O
approach O
differs O
from O
past O
work O
on O
semantic O
sentence O
encoding O
in O
two O
ways O
. O

First O
, O
by O
using O
a O
variational O
probabilis- O
tic O
framework O
, O
we O
introduce O
priors O
that O
encour- O
age O
source O
separation O
, O
and O
can O
use O
our O
model O
‚Äôs O
posterior O
to O
predict O
sentence O
embeddings O
for O
monolingual O
data O
at O
test O
time O
. O

Second O
, O
we O
use O
high O
- O
capacity O
transformers O
as O
both O
data O
gen- O
erating O
distributions O
and O
inference O
networks O
‚Äì O
contrasting O
with O
most O
past O
work O
on O
sentence O
embeddings O
. O

In O
experiments O
, O
our O
approach O
substantially O
outperforms O
the O
state O
- O
of O
- O
the O
- O
art O
on O
a O
standard O
suite O
of O
unsupervised O
seman- O
tic O
similarity O
evaluations O
. O

Further O
, O
we O
demon- O
strate O
that O
our O
approach O
yields O
the O
largest O
gains O
on O
more O
difÔ¨Åcult O
subsets O
of O
these O
evaluations O
where O
simple O
word O
overlap O
is O
not O
a O
good O
indi- O
cator O
of O
similarity.1 O

1 O
Introduction O
Learning O
useful O
representations O
of O
language O
has O
been O
a O
source O
of O
recent O
success O
in O
natural O
language O
processing O
( O
NLP O
) O
. O

Much O
work O
has O
been O
done O
on O
learning O
representations O
for O
words O
( O
Mikolov O
et O
al O
. O
, O
2013 O
; O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
sentences O
( O
Kiros O
et O
al O
. O
, O
2015 O
; O
Conneau O
et O
al O
. O
, O
2017 O
) O
. O

More O
recently O
, O
1Code O
and O
data O
to O
replicate O
results O
available O
at O
https O
: O
//www.cs.cmu.edu/ O

Àújwieting O
.deep O
neural O
architectures O
have O
been O
used O
to O
learn O
contextualized O
word O
embeddings O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2018 O
) O
enabling O
state O
- O
of O
- O
the- O
art O
results O
on O
many O
tasks O
. O

We O
focus O
on O
learning O
semantic O
sentence O
embeddings O
in O
this O
paper O
, O
which O
play O
an O
important O
role O
in O
many O
downstream O
ap- O
plications O
. O

Since O
they O
do O
not O
require O
any O
labelled O
data O
for O
Ô¨Åne O
- O
tuning O
, O
sentence O
embeddings O
are O
use- O
ful O
out O
- O
of O
- O
the O
- O
box O
for O
problems O
such O
as O
measure- O
ment O
of O
Semantic O
Textual O
Similarity O
( O
STS O
; O
Agirre O
et O
al O
. O

( O
2012 O
) O
) O
, O
mining O
bitext O
( O
Zweigenbaum O
et O
al O
. O
, O
2018 O
) O
, O
and O
paraphrase O
identiÔ¨Åcation O
( O
Dolan O
et O
al O
. O
, O
2004 O
) O
. O

Semantic O
similarity O
measures O
also O
have O
downstream O
uses O
such O
as O
Ô¨Åne O
- O
tuning O
machine O
trans- O
lation O
systems O
( O
Wieting O
et O
al O
. O
, O
2019a O
) O
. O

There O
are O
three O
main O
ingredients O
when O
design- O
ing O
a O
sentence O
embedding O
model O
: O
the O
architecture O
, O
the O
training O
data O
, O
and O
the O
objective O
function O
. O

Many O
architectures O
including O
LSTMs O
( O
Hill O
et O
al O
. O
, O
2016 O
; O
Conneau O
et O
al O
. O
, O
2017 O
; O
Schwenk O
and O
Douze O
, O
2017 O
; O
Subramanian O
et O
al O
. O
, O
2018 O
) O
, O
Transformers O
( O
Cer O
et O
al O
. O
, O
2018 O
; O
Reimers O
and O
Gurevych O
, O
2019 O
) O
, O
and O
averag- O
ing O
models O
( O
Wieting O
et O

al O
. O
, O
2016b O
; O
Arora O
et O
al O
. O
, O
2017 O
) O
are O
capable O
of O
learning O
sentence O
embeddings O
. O

The O
choice O
of O
training O
data O
and O
objective O
are O
inti- O
mately O
intertwined O
, O
and O
there O
are O
a O
wide O
variety O
of O
options O
including O
next O
- O
sentence O
prediction O
( O
Kiros O
et O
al O
. O
, O
2015 O
) O
, O
machine O
translation O
( O
Espana O
- O
Bonet O
et O

al O
. O
, O
2017 O
; O
Schwenk O
and O
Douze O
, O
2017 O
; O
Schwenk O
, O
2018 O
; O
Artetxe O
and O
Schwenk O
, O
2018 O
) O
, O
natural O
lan- O
guage O
inference O
( O
NLI O
) O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
, O
and O
multi O
- O
task O
objectives O
which O
include O
some O
of O
the O
previously O
mentioned O
objectives O
( O
Cer O
et O
al O
. O
, O
2018 O
) O
potentially O
combined O
with O
additional O
tasks O
like O
con- O
stituency O
parsing O
( O
Subramanian O
et O
al O
. O
, O
2018 O
) O

. O

Surprisingly O
, O
despite O
ample O
testing O
of O
more O
pow- O
erful O
architectures O
, O
the O
best O
performing O
models O
for O
many O
sentence O
embedding O
tasks O
related O
to O
seman- O
tic O
similarity O
often O
use O
simple O
architectures O
that O
are O
mostly O
agnostic O
to O
the O
interactions O
between O

1582words O
. O

For O
instance O
, O
some O
of O
the O
top O
performing O
techniques O
use O
word O
embedding O
averaging O
( O
Wiet- O
ing O
et O

al O
. O
, O
2016b O
) O
, O
character O
n O
- O
grams O
( O
Wieting O
et O
al O
. O
, O
2016a O
) O
, O
and O
subword O
embedding O
averaging O
( O
Wiet- O
ing O
et O

al O
. O
, O
2019b O
) O
to O
create O
representations O
. O

These O
simple O
approaches O
are O
competitive O
with O
much O
more O
complicated O
architectures O
on O
in O
- O
domain O
data O
and O
generalize O
well O
to O
unseen O
domains O
, O
but O
are O
funda- O
mentally O
limited O
by O
their O
inability O
to O
capture O
word O
order O
. O

Training O
these O
approaches O
generally O
relies O
on O
discriminative O
objectives O
deÔ¨Åned O
on O
paraphrase O
data O
( O
Ganitkevitch O
et O
al O
. O
, O
2013 O
; O
Wieting O
and O
Gim- O
pel O
, O
2018 O
) O
or O
bilingual O
data O
( O
Wieting O
et O
al O
. O
, O
2019b O
; O
Chidambaram O
et O
al O
. O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2020 O
) O
. O

The O
inclusion O
of O
latent O
variables O
in O
these O
models O
has O
also O
been O
explored O
( O
Chen O
et O
al O
. O
, O
2019 O
) O
. O

Intuitively O
, O
bilingual O
data O
in O
particular O
is O
promis- O
ing O
because O
it O
potentially O
offers O
a O
useful O
signal O
for O
learning O
the O
underlying O
semantics O
of O
sentences O
. O

Within O
a O
translation O
pair O
, O
properties O
shared O
by O
both O
sentences O
are O
more O
likely O
semantic O
, O
while O
those O
that O
are O
divergent O
are O
more O
likely O
stylistic O
or O
language- O
speciÔ¨Åc O
. O

While O
previous O
work O
learning O
from O
bilin- O
gual O
data O
perhaps O
takes O
advantage O
of O
this O
fact O
im- O

plicitly O
, O
the O
focus O
of O
this O
paper O
is O
modelling O
this O
in- O
tuition O
explicitly O
, O
and O
to O
the O
best O
of O
our O
knowledge O
, O
this O
has O
not O
been O
explored O
in O
prior O
work O
. O

SpeciÔ¨Å- O

cally O
, O
we O
propose O
a O
deep O
generative O
model O
that O
is O
encouraged O
to O
perform O
source O
separation O
on O
paral- O
lel O
sentences O
, O
isolating O
what O
they O
have O
in O
common O
in O
a O
latent O
semantic O
embedding O
and O
explaining O
what O
is O
left O
over O
with O
language O
- O
speciÔ¨Åc O
latent O
vectors O
. O

At O
test O
time O
, O
we O
use O
inference O
networks O
( O
Kingma O
and O
Welling O
, O
2013 O
) O
for O
approximating O
the O
model O
‚Äôs O
posterior O
on O
the O
semantic O
and O
source O
- O
separated O
la- O
tent O
variables O
to O
encode O
monolingual O
sentences O
. O

Finally O
, O
since O
our O
model O
and O
training O
objective O
are O
generative O
, O
our O
approach O
does O
not O
require O
knowl- O
edge O
of O
the O
distance O
metrics O
to O
be O
used O
during O
eval- O
uation O
, O
and O
it O
has O
the O
additional O
property O
of O
being O
able O
to O
generate O
text O
. O

In O
experiments O
, O
we O
evaluate O
our O
probabilistic O
source O
- O
separation O
approach O
on O
a O
standard O
suite O
of O
STS O
evaluations O
. O

We O
demonstrate O
that O
the O
proposed O
approach O
is O
effective O
, O
most O
notably O
allowing O
the O
learning O
of O
high O
- O
capacity O
deep O
Transformer O
archi- O
tectures O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
while O
still O
general- O
izing O
to O
new O
domains O
, O
signiÔ¨Åcantly O
outperforming O
a O
variety O
of O
state O
- O
of O
- O
the O
- O
art O
baselines O
. O

Further O
, O
we O
conduct O
a O
thorough O
analysis O
by O
identifying O
subsets O
of O
the O
STS O
evaluation O
where O
simple O
word O
overlap O
zenzsemGaussian O
Priorszfrùí©(0,ùêº)ùí©(0,ùêº O
) O
xfrxenObserved O
  O
French O
Sentenceùí©(0,ùêº O
) O

Observed O
  O
English O
SentenceFrench O
- O
specific O
Latent O
VectorEnglish O
- O
specific O
Latent O
VectorCommon O
Semantic O
Latent O
Vector O
French O
Decoder O
( O
Section O
2)English O
Decoder O
( O
Section O
2)Semantic O
Inference O
Net O
( O
Section O
3)French O
Inference O
Net O
( O
Section O
3)English O
Inference O
Net O
( O
Section O
3 O
) O
Translation O
Pair O
Linguistic O
variation O
specific O
to O
English O
Linguistic O
variation O
specific O
to O
French O
Shared O
  O
semantic O
variationFigure O
1 O
: O
The O
generative O
process O
of O
our O
model O
. O

Latent O
variables O
modeling O
the O
linguistic O
variation O
in O
French O
and O
English O
, O
zfrandzen O
, O
as O
well O
as O
a O
latent O
vari- O
able O
modeling O
the O
common O
semantics O
, O
zsem O
, O
are O
drawn O
from O
a O
multivariate O
Gaussian O
prior O
. O

The O
observed O
text O
in O
each O
language O
is O
then O
conditioned O
on O
its O
language- O
speciÔ¨Åc O
variable O
and O
zsem O
. O

is O
not O
able O
to O
accurately O
assess O
semantic O
similarity O
. O

On O
these O
most O
difÔ¨Åcult O
instances O
, O
we O
Ô¨Ånd O
that O
our O
approach O
yields O
the O
largest O
gains O
, O
indicating O
that O
our O
system O
is O
modeling O
interactions O
between O
words O
to O
good O
effect O
. O

We O
also O
Ô¨Ånd O
that O
our O
model O
better O
handles O
cross O
- O
lingual O
semantic O
similarity O
than O
mul- O
tilingual O
translation O
baseline O
approaches O
, O
indicating O
that O
stripping O
away O
language O
- O
speciÔ¨Åc O
information O
allows O
for O
better O
comparisons O
between O
sentences O
from O
different O
languages O
. O

Finally O
, O
we O
analyze O
our O
model O
to O
uncover O
what O
information O
was O
captured O
by O
the O
source O
separation O
into O
the O
semantic O
and O
language O
- O
speciÔ¨Åc O
variables O
and O
the O
relationship O
between O
this O
encoded O
infor- O
mation O
and O
language O
distance O
to O
English O
. O

We O
Ô¨Ånd O
that O
the O
language O
- O
speciÔ¨Åc O
variables O
tend O
to O
explain O
more O
superÔ¨Åcial O
or O
language O
- O
speciÔ¨Åc O
properties O
such O
as O
overall O
sentence O
length O
, O
amount O
and O
loca- O
tion O
of O
punctuation O
, O
and O
the O
gender O
of O
articles O
( O
if O
gender O
is O
present O
in O
the O
language O
) O
, O
but O
semantic O
and O
syntactic O
information O
is O
more O
concentrated O
in O
the O
shared O
semantic O
variables O
, O
matching O
our O
intuition O
. O

Language O
distance O
has O
an O
effect O
as O
well O
, O
where O
lan- O
guages O
that O
share O
common O
structures O
with O
English O
put O
more O
information O
into O
the O
semantic O
variables O
, O
while O
more O
distant O
languages O
put O
more O
informa- O
tion O
into O
the O
language O
- O
speciÔ¨Åc O
variables O
. O

Lastly O
, O
we O
show O
outputs O
generated O
from O
our O
model O
that O
exhibit O
its O
ability O
to O
do O
a O
type O
of O
style O
transfer O
. O

1583 O
XfrXenSemantic O
Inference O
NetEnglish O
Inference O
Net O
French O
Inference O
NetLatent O
Space O
XfrXenconcat O
concatGenerative O
ModelKL O
between O
prior O
and O
approximate O
posteriorsReconstruction O
of O
input O
translation O
pairConcatenate O
sampled O
latent O
vectors O
and O
decodeInference O
Networks O
mergeEnglish O
  O
DecoderFrench O
  O
DecoderInput O
  O
English O
and O
French O
  O
translation O
pairFrench O
  O
EncoderEnglish O
  O
EncoderSemantic O
  O
EncoderSemantic O
Encoderq(zsem|xen O
, O
xfr;œï)q(zen|xen;œï O
) O
q(zfr|xfr;œï)p(xen|zsem O
, O
zen;Œ∏)p(xfr|zsem O
, O
zfr;Œ∏)ùí©(Œºsem O
, O
Œ£sem)ùí©(Œºen O
, O
Œ£en O
) O
ùí©(Œºfr O
, O
Œ£fr)Œºsem O
, O
Œ£sem O
Œºfr O
, O
Œ£frŒºen O
, O
Œ£enSemantic O
  O
EncodingzsemMerging O
  O
operation O
( O
Section O
3)Figure O
2 O
: O
The O
computation O
graph O
for O
the O
variational O
lower O
bound O
used O
during O
training O
. O

The O
English O
and O
French O
text O
are O
fed O
into O
their O
respective O
inference O
networks O
and O
the O
semantic O
inference O
network O
to O
ultimately O
produce O
the O
language O
variables O
zfrandzenand O
semantic O
variable O
zsem O
. O

Each O
language O
- O
speciÔ¨Åc O
variable O
is O
then O
concatenated O
tozsemand O
used O
by O
the O
decoder O
to O
reconstruct O
the O
input O
sentence O
pair O
. O

2 O
Model O
Our O
proposed O
training O
objective O
leverages O
a O
gen- O
erative O
model O
of O
parallel O
text O
in O
two O
languages O
. O

Our O
model O
is O
language O
agnostic O
, O
and O
applies O
to O
a O
wide O
variety O
of O
languages O
( O
see O
Section O
5 O
) O
but O
we O
will O
use O
the O
running O
example O
of O
English O
( O
en O
) O
and O
French O
( O
fr O
) O
pairs O
consisting O
of O
an O
English O
sentence O
xenand O

a O
French O
sentence O
xfr O
. O

Importantly O
, O
the O
generative O
process O
utilizes O
three O
underlying O
latent O
vectors O
: O
language O
- O
speciÔ¨Åc O
variation O
variables O
( O
lan- O
guage O
variables O
) O
zfrandzenfor O
each O
side O
of O
the O
translation O
, O
as O
well O
as O
a O
shared O
semantic O
variation O
variable O
( O
semantic O
variable O
) O
zsem O
. O

In O
this O
section O
we O
will O
Ô¨Årst O
describe O
the O
generative O
model O
for O
the O
text O
and O
latent O
variables O
. O

In O
the O
following O
section O
we O
will O
describe O
the O
inference O
procedure O
of O
zsem O
given O
an O
input O
sentence O
, O
which O
corresponds O
to O
our O
core O
task O
of O
obtaining O
sentence O
embeddings O
useful O
for O
downstream O
tasks O
such O
as O
semantic O
similarity O
. O

The O
generative O
process O
of O
our O
model O
, O
the O
Bilin- O
gual O
Generative O
Transformer O
( O
BGT O
) O
, O
is O
depicted O
in O
Figure O
1 O
and O
the O
training O
computation O
graph O
is O
shown O
in O
Figure O
2 O
. O

First O
, O
we O
sample O
latent O
variables O
hzfr;zen;zsemi O
, O
wherezi2Rk O
, O
from O
a O
multivari- O
ate O
Gaussian O
prior O
N(0;Ik O
) O
. O

These O
variables O
are O
then O
fed O
into O
a O
decoder O
that O
samples O
sentences O
; O
xen O
is O
sampled O
conditioned O
on O
zsemandzen O
, O
whilexfr O
is O
sampled O
conditioned O
on O
zsemandzfr O
. O

Because O
sentences O
in O
both O
languages O
will O
use O
zsemin O
gener- O
ation O
, O
we O
expect O
that O
in O
a O
well O
- O
trained O
model O
this O
variable O
will O
encode O
semantic O
, O
syntactic O
, O
or O
stylis- O
tic O
information O
shared O
across O
both O
sentences O
, O
while O
zfrandzenwill O
handle O
any O
language O
- O
speciÔ¨Åc O
pecu- O
liarities O
or O
speciÔ¨Åc O
stylistic O
decisions O
that O
are O
less O
central O
to O
the O
sentence O
meaning O
and O
thus O
do O
not O
translate O
across O
sentences O
. O

In O
the O
following O
section O
, O
we O
further O
discuss O
how O
this O
is O
explicitly O
encouraged O
by O
the O
learning O
process O
. O

Decoder O
Architecture O
. O

Many O
latent O
variable O
mod- O
els O
for O
text O
use O
LSTMs O
( O
Hochreiter O
and O
Schmid- O
huber O
, O
1997 O
) O
as O
their O
decoders O
( O
Yang O
et O
al O
. O
, O
2017 O
; O
Ziegler O
and O
Rush O
, O
2019 O
; O
Ma O
et O
al O
. O
, O
2019 O
) O
. O

However O
, O
state O
- O
of O
- O
the O
- O
art O
models O
in O
neural O
machine O
transla- O
tion O
have O
seen O
increased O
performance O
and O
speed O
us- O
ing O
deep O
Transformer O
architectures O
. O

We O
also O
found O
in O
our O
experiments O
( O
see O
Appendix O
C O
for O
details O
) O
that O
Transformers O
led O
to O
increased O
performance O
in O
our O
setting O
, O
so O
they O
are O
used O
in O
our O
main O
model O
. O

We O
use O
two O
decoders O
in O
our O
model O
, O
one O
for O
modellingp(xfrjzsem;zfr;)and O
one O
for O
model- O
ingp(xenjzsem;zen;)(see O
right O
side O
of O
Figure O
2 O
) O
. O

Each O
decoder O
takes O
in O
a O
language O
variable O
and O
a O
se- O
mantic O
variable O
, O
which O
are O
concatenated O
and O
used O
by O
the O
decoder O
for O
reconstruction O
. O

We O
explore O
four O
ways O
of O
using O
this O
latent O
vector O
: O
( O
1 O
) O
Concatenate O
it O
to O
the O
word O
embeddings O
( O
Word O
) O
( O
2 O
) O
Use O
it O
as O
the O
ini- O
tial O
hidden O
state O
( O
Hidden O
, O
LSTM O
only O
) O
( O
3 O
) O
Use O
it O
as O
you O
would O
the O
attention O
context O
vector O
in O
the O
tradi- O
tional O
sequence O
- O
to O
- O
sequence O
framework O
( O
Attention O
) O
and O
( O
4 O
) O
Concatenate O
it O
to O
the O
hidden O
state O
immedi- O
ately O
prior O
to O
computing O
the O
logits O
( O
Logit O
) O
. O

Unlike O
Attention O
, O
there O
is O
no O
additional O
feedforward O
layer O
in O
this O
setting O
. O

We O
experimented O
with O
these O
four O
approaches O
, O
as O
well O
as O
combinations O
thereof O
, O
and O
report O
this O
analysis O
in O
Appendix O
A. O
From O
these O
experiments O
, O
we O
see O
that O
the O
closer O
the O
sentence O
embedding O
is O
to O
the O
Ô¨Ånal O
word O
predictor O
, O
the O
better O
the O
performance O
on O
downstream O
tasks O
evaluating O
its O
semantic O
content O
. O

We O
hypothesise O
that O
this O
is O
due O
to O
better O
gradient O
propagation O
because O
the O
sen- O
tence O
embedding O
is O
now O
closer O
to O
the O
error O
signal O
. O

Since O
Attention O
and O
Logit O
performed O
best O
, O
we O
use O

1584these O
in O
our O
main O
experiments O
. O

3 O

Learning O
and O
Inference O
Our O
model O
is O
trained O
on O
a O
set O
of O
paral- O
lel O
sentences O
Xconsisting O
of O
Nexamples O
, O
X O
= O
fhx1 O
en;x1 O
fri;:::;hxN O
en;xN O
frig O
, O
andZ O
is O
our O
collection O
of O
latent O
variables O
Z= O

( O
hz1 O
en;z1 O
fr;z1 O
semi;:::;hzN O
en;zN O
fr;zN O
semi O
) O
. O

We O
wish O
to O
maximize O
the O
likelihood O
of O
the O
parameters O
of O
the O
two O
decoders O
with O
respect O
to O
the O
observed O
X O
, O
marginalizing O
over O
the O
latent O
variables O
Z. O
p(X; O
) O

= O
Z O
Zp(X;Z;)dZ O

Unfortunately O
, O
this O
integral O
is O
intractable O
due O
to O
the O
complex O
relationship O
between O
XandZ. O
How- O
ever O
, O
related O
latent O
variable O
models O
like O
variational O
autoencoders O
( O
V O
AEs O
; O
Kingma O
and O
Welling O
( O
2013 O
) O
) O
learn O
by O
optimizing O
a O
variational O
lower O
bound O
on O
the O
log O
marginal O
likelihood O
. O

This O
surrogate O
ob- O
jective O
is O
called O
the O
evidence O
lower O
bound O
( O
ELBO O
) O
and O
introduces O
a O
variational O
approximation O
, O
qto O
the O
true O
posterior O
of O
the O
model O

p. O
Theqdistribution O
is O
parameterized O
by O
a O
neural O
network O
with O
parameters O

. O

ELBO O
for O
our O
model O
is O
written O
as O
: O
ELBO O
= O
Eq(ZjX O
; O

) O

[ O
logp(XjZ;)]  O
KL(q(ZjX O
; O

) O
jjp(Z; O
) O
) O

This O
lower O
bound O
on O
the O
marginal O
can O
be O
opti- O
mized O
by O
gradient O
ascent O
by O
using O
the O
reparameteri- O
zation O
trick O
( O
Kingma O
and O
Welling O
, O
2013 O
) O
. O

This O
trick O
allows O
for O
the O
expectation O
under O
qto O
be O
approxi- O
mated O
through O
sampling O
in O
a O
way O
that O
preserves O
backpropagation O
. O

We O
make O
several O
independence O
assumptions O
for O
q(zsem;zen;zfrjxen;xfr O
; O

) O
to O
match O
our O
goal O
of O
source O
separation O
: O
we O
factor O
q O
asq(zsemjxen;xfr O
; O

) O
q(zenjxen O
; O

) O
q(zfrjxfr O
; O

) O
. O

The O
parameters O
of O
the O
encoders O
that O
make O
up O
the O
inference O
networks O
, O
deÔ¨Åned O
in O
the O
next O
paragraph O
, O
are O
denoted O
as O

. O

Lastly O
, O
we O
note O
that O
the O
KL O
term O
in O
our O
ELBO O
equation O
explicitly O
encourages O
explaining O
variation O
that O
is O
shared O
by O
translations O
with O
the O
shared O
se- O
mantic O
variable O
, O
and O
explaining O
language O
- O
speciÔ¨Åc O
variation O
with O
the O
corresponding O
language O
- O
speciÔ¨Åc O
variables O
. O

Encoding O
information O
shared O
by O
the O
two O
sentences O
in O
the O
shared O
variable O
results O
in O
only O
a O
single O
penalty O
from O
the O
KL O
loss O
, O
while O
encoding O
the O
information O
separately O
in O
both O
language O
spe- O
ciÔ¨Åc O
variables O
will O
cause O
unnecessary O
replication O
, O
doubling O
the O
overall O
cost O
incurred O
by O
the O
KL O
term O
. O

Encoder O
Architecture O
. O

We O
use O
three O
inference O
networks O
as O
shown O
on O
the O
left O
side O
of O
Figure O
2 O
: O
an O
English O
inference O
network O
to O
produce O
the O
English O
language O
variable O
, O
a O
French O
inference O
network O
to O
produce O
the O
French O
language O
variable O
, O
and O
a O
se- O
mantic O
inference O
network O
to O
produce O
the O
semantic O
variable O
. O

Just O
as O
in O
the O
decoder O
architecture O
, O
we O
use O
a O
Transformer O
for O
the O
encoders O
. O

The O
semantic O
inference O
network O
is O
a O
bilingual O
encoder O
that O
encodes O
each O
language O
. O

For O
each O
trans- O
lation O
pair O
, O
we O
alternate O
which O
of O
the O
two O
parallel O
sentences O
is O
fed O
into O
the O
semantic O
encoder O
within O
a O
batch O
. O

Since O
the O
semantic O
encoder O
is O
meant O
to O
capture O
language O
agnostic O
semantic O
information O
, O
its O
outputs O
for O
a O
translation O
pair O
should O
be O
similar O
regardless O
of O
the O
language O
of O
the O
input O
sentence O
. O

We O
note O
that O
other O
operations O
are O
possible O
for O
com- O
bining O
the O
views O
each O
parallel O
sentence O
offers O
. O

For O
instance O
, O
we O
could O
feed O
both O
sentences O
into O
the O
semantic O
encoder O
and O
pool O
their O
representations O
. O

However O
, O
in O
practice O
we O
Ô¨Ånd O
that O
alternating O
works O
well O
and O
also O
can O
be O
used O
to O
obtain O
sentence O
em- O

beddings O
for O
text O
that O
is O
not O
part O
of O
a O
translation O
pair O
. O

We O
leave O
further O
study O
of O
combining O
views O
to O
future O
work O
. O

4 O
Experiments O
4.1 O
Baseline O
Models O
We O
experiment O
with O
fourteen O
baseline O
models O
, O
cov- O
ering O
both O
the O
most O
effective O
approaches O
for O
learn- O
ing O
sentence O
embeddings O
from O
the O
literature O
and O
ablations O
of O
our O
own O
BGT O
model O
. O

These O
baselines O
can O
be O
split O
into O
three O
groups O
as O
detailed O
below O
. O

Models O
from O
the O
Literature O
( O
Trained O
on O
Differ- O
ent O
Data O
) O
We O
compare O
to O
well O
known O
sentence O
embedding O
models O
Infersent O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
, O
GenSen O
( O
Subramanian O
et O
al O
. O
, O
2018 O
) O
, O
the O
Univer- O
sal O
Sentence O
Encoder O
( O
USE O
) O
( O
Cer O
et O
al O
. O
, O
2018 O
) O
, O
LASER O
( O
Artetxe O
and O
Schwenk O
, O
2018 O
) O
, O
as O
well O
as O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018).2We O
used O
the O
pre- O
trained O
BERT O
model O
in O
two O
ways O
to O
create O
a O
sen- O
tence O
embedding O
. O

The O
Ô¨Årst O
way O
is O
to O
concatenate O
the O
hidden O
states O
for O
the O
CLS O
token O
in O
the O
last O
four O
layers O
. O

The O
second O
way O
is O
to O
concatenate O
the O
hid- O
den O
states O
of O
all O
word O
tokens O
in O
the O
last O
four O
layers O
and O
mean O
pool O
these O
representations O
. O

Both O
methods O
result O
in O
a O
4096 O
dimension O
embedding O
. O

We O
also O
compare O
to O
the O
newly O
released O
model O
, O
Sentence- O
2Note O
that O
in O
all O
experiments O
using O
BERT O
, O
including O
Sentence O
- O
BERT O
, O
the O
large O
, O
uncased O
version O
is O
used O
. O

1585Bert O
( O
Reimers O
and O
Gurevych O
, O
2019 O
) O
. O

This O
model O
is O
similar O
to O
Infersent O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
in O
that O
it O
is O
trained O
on O
natural O
language O
inference O
data O
, O
SNLI O
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
. O

However O
, O
instead O
of O
using O
pretrained O
word O
embeddings O
, O
they O
Ô¨Åne O
- O
tune O
BERT O
in O
a O
way O
to O
induce O
sentence O
embeddings.3 O
Models O
from O
the O
Literature O
( O
Trained O
on O
Our O
Data O
) O

These O
models O
are O
amenable O
to O
being O
trained O
in O
the O
exact O
same O
setting O
as O
our O
own O
models O
as O
they O
only O
require O
parallel O
text O
. O

These O
include O
the O
sentence O
piece O
averaging O
model O
, O
SP O
, O
from O
Wi- O
eting O
et O

al O
. O
( O
2019b O
) O
, O
which O
is O
among O
the O
best O
of O
the O
averaging O
models O
( O
i.e. O
compared O
to O
averag- O
ing O
only O
words O
or O
character O
n O
- O
grams O
) O
as O
well O
the O
LSTM O
model O
, O
BILSTM O
, O
from O
Wieting O
and O
Gimpel O
( O
2017 O
) O
. O

These O
models O
use O
a O
contrastive O
loss O
with O
a O
margin O
. O

Following O
their O
settings O
, O
we O
Ô¨Åx O
the O
margin O
to O
0.4 O
and O
tune O
the O
number O
of O
batches O
to O
pool O
for O
se- O
lecting O
negative O
examples O
from O
f40;60;80;100 O
g. O

For O
both O
models O
, O
we O
set O
the O
dimension O
of O
the O
em- O
beddings O
to O
1024 O
. O

For O
BILSTM O
, O
we O
train O
a O
single O
layer O
bidirectional O
LSTM O
with O
hidden O
states O
of O
512 O
dimensions O
. O

To O
create O
the O
sentence O
embedding O
, O
the O
forward O
and O
backward O
hidden O
states O
are O
concate- O
nated O
and O
mean O
- O
pooled O
. O

Following O
Wieting O
and O
Gimpel O
( O
2017 O
) O
, O
we O
shufÔ¨Çe O
the O
inputs O
with O
probabil- O
ityp O
, O
tuningpfromf0:3;0:5 O
g. O
We O
also O
implicitly O
compare O
to O
previous O
machine O
translation O
approaches O
like O
Espana O
- O
Bonet O

et O
al O
. O

( O
2017 O
) O
; O

Schwenk O
and O
Douze O
( O
2017 O
) O
; O
Artetxe O
and O
Schwenk O
( O
2018 O
) O
in O
Appendix O
A O
where O
we O
explore O
different O
variations O
of O
training O
LSTM O
sequence- O

to O
- O
sequence O
models O
. O

We O
Ô¨Ånd O
that O
our O
transla- O
tion O
baselines O
reported O
in O
the O
tables O
below O
( O
both O
LSTM O
and O
Transformer O
) O
outperform O
the O
architec- O
tures O
from O
these O
works O
due O
to O
using O
the O
Attention O
and O
Logit O
methods O
mentioned O
in O
Section O
2 O
, O
demon- O
strating O
that O
our O
baselines O
represent O
, O
or O
even O
over- O
represent O
, O
the O
state O
- O
of O
- O
the O
- O
art O
for O
machine O
transla- O
tion O
approaches O
. O

BGT O
Ablations O
Lastly O
, O
we O
compare O
to O
ablations O
of O
our O
model O
to O
better O
understand O
the O
beneÔ¨Åts O
of O
parallel O
data O
, O
language O
- O
speciÔ¨Åc O
variables O
, O
the O
KL O
loss O
term O
, O
and O
how O
much O
we O
gain O
from O
the O
more O
3Most O
work O
evaluating O
accuracy O
on O
STS O
tasks O
has O
averaged O
the O
Pearson O
‚Äôs O
rover O
each O
individual O
dataset O
for O
each O
year O
of O
the O
STS O
competition O
. O

However O
, O
Reimers O
and O
Gurevych O
( O
2019 O
) O
computed O
Spearman O
‚Äôs O
over O
concatenated O
datasets O
for O
each O
year O
of O
the O
STS O
competition O
. O

To O
be O
consistent O
with O
previous O
work O
, O
we O
re O
- O
ran O
their O
model O
and O
calculated O
results O
using O
the O
standard O
method O
, O
and O
thus O
our O
results O
are O
not O
the O
same O
as O
those O
reported O
Reimers O
and O
Gurevych O
( O
2019).conventional O
translation O
baselines O
. O

ENGLISH O
AE O
: O
English O
autoencoder O
on O
the O
En- O
glish O
side O
of O
our O
en O
- O
fr O
data O
. O

ENGLISH O
VAE O
: O
English O
variational O
autoencoder O
on O
the O
English O
side O
of O
our O
en O
- O
fr O
data O
. O

ENGLISH O
TRANS O
: O
Translation O
from O
entofr O
. O

BILINGUAL O
TRANS O
: O

Translation O
from O
both O
en O
tofrandfrtoenwhere O
the O
encoding O
param- O
eters O
are O
shared O
but O
each O
language O
has O
its O
own O
decoder O
. O

BGT O
W O
/ O
OLANG O
VARS O
: O
A O
model O
similar O
to O
BILINGUAL O
TRANS O
, O
but O
it O
includes O
a O
prior O
over O
the O
embedding O
space O
and O
therefore O
a O
KL O
loss O
term O
. O

This O
model O
differs O
from O
BGT O
since O
it O
does O
not O
have O
any O
language O
- O
speciÔ¨Åc O
variables O
. O

BGT O
W O
/ O
OPRIOR O
: O

Follows O
the O
same O
architecture O
asBGT O
, O
but O
without O
the O
priors O
and O
KL O
loss O
term O
. O

4.2 O
Experimental O
Settings O
The O
training O
data O
for O
our O
models O
is O
a O
mixture O
of O
OpenSubtitles O
20184en O
- O
frdata O
and O
en O
- O
frGiga- O
word5data O
. O

To O
create O
our O
dataset O
, O
we O
combined O
the O
complete O
corpora O
of O
each O
dataset O
and O
then O
ran- O
domly O
selected O
1,000,000 O
sentence O
pairs O
to O
be O
used O
for O
training O
with O
10,000 O
used O
for O
validation O
. O

We O
use O
sentencepiece O
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
with O
a O
vocabulary O
size O
of O
20,000 O
to O
segment O
the O
sentences O
, O
and O
we O
chose O
sentence O
pairs O
whose O
sen- O
tences O
are O
between O
5 O
and O
100 O
tokens O
each O
. O

In O
designing O
the O
model O
architectures O
for O
the O
en- O
coders O
and O
decoders O
, O
we O
experimented O
with O
Trans- O
formers O
and O
LSTMs O
. O

Due O
to O
better O
performance O
, O
we O
use O
a O
5 O
layer O
Transformer O
for O
each O
of O
the O
en- O
coders O
and O
a O
single O
layer O
decoder O
for O
each O
of O
the O
decoders O
. O

This O
design O
decision O
was O
empirically O
motivated O
as O
we O
found O
using O
a O
larger O
decoder O
was O
slower O
and O
worsened O
performance O
, O
but O
conversely O
, O
adding O
more O
encoder O
layers O
improved O
performance O
. O

More O
discussion O
of O
these O
trade O
- O
offs O
along O
with O
ab- O
lations O
and O
comparisons O
to O
LSTMs O
are O
included O
in O
Appendix O
C. O

For O
all O
of O
our O
models O
, O
we O
set O
the O
dimension O
of O
the O
embeddings O
and O
hidden O
states O
for O
the O
encoders O
and O
decoders O
to O
1024 O
. O

Since O
we O
experiment O
with O
two O
different O
architectures,6we O
follow O
two O
differ- O
ent O
optimization O
strategies O
. O

For O
training O
models O
4http://opus.nlpl.eu/OpenSubtitles.php O
5https://www.statmt.org/wmt10/ O
training-giga-fren.tar O
6We O
use O
LSTMs O
in O
our O
ablations O
. O

1586Data O
Sentence O
1 O
Sentence O
2 O
Gold O
Score O
Hard+ O
Other O
ways O
are O
needed O
. O

It O
is O
necessary O
to O
Ô¨Ånd O
other O
means O
. O

4.5 O
Hard- O
How O
long O
can O
you O
keep O
chocolate O
in O
the O
freezer?How O
long O
can O
I O
keep O
bread O
dough O
in O
the O
refrigerator?1.0 O
Negation O
It O
‚Äôs O
not O
a O
good O
idea O
. O

It O
‚Äôs O
a O
good O
idea O
to O
do O
both O
. O

1.0 O
Table O
1 O
: O
Examples O
from O
our O
Hard O
STS O
dataset O
and O
our O
negation O
split O
. O

The O
sentence O
pair O
in O
the O
Ô¨Årst O
row O
has O
dissimilar O
structure O
and O
vocabulary O
yet O
a O
high O
gold O
score O
. O

The O
second O
sentence O
pair O
has O
similar O
structure O
and O
vocabulary O
and O
a O
low O
gold O
score O
. O

The O
last O
sentence O
pair O
contains O
negation O
, O
where O
there O
is O
a O
notin O
Sentence O
1 O
that O
causes O
otherwise O
similar O
sentences O
to O
have O
low O
semantic O
similarity O
. O

with O
Transformers O
, O
we O
use O
Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
with O

1= O
0:9 O
, O

2= O
0:98 O
, O
and= O

10 8 O
. O

We O
use O
the O
same O
learning O
rate O
schedule O
as O
Vaswani O
et O

al O
. O
( O
2017 O
) O
, O
i.e. O
, O
the O
learning O
rate O
increases O
lin- O
early O
for O
4,000 O
steps O
to O
510 4 O
, O
after O
which O
it O
is O
decayed O
proportionally O
to O
the O
inverse O
square O
root O
of O
the O
number O
of O
steps O
. O

For O
training O
the O
LSTM O
models O
, O
we O
use O
Adam O
with O
a O
Ô¨Åxed O
learning O
rate O
of O
0.001 O
. O

We O
train O
our O
models O
for O
20 O
epochs O
. O

For O
models O
incorporating O
a O
translation O
loss O
, O
we O
used O
label O
smoothed O
cross O
entropy O
( O
Szegedy O
et O
al O
. O
, O
2016 O
; O
Pereyra O
et O
al O
. O
, O
2017 O
) O
with O
= O
0:1 O
. O

For O
EN- O
GLISH O
VAE O
, O
BGT O
andBILINGUAL O
TRANS O
, O
we O
an- O
neal O
the O
KL O
term O
so O
that O
it O
increased O
linearly O
for O
216 O
updates O
, O
which O
robustly O
gave O
good O
results O
in O
pre- O
liminary O
experiments O
. O

We O
also O
found O
that O
in O
train- O
ing O
BGT O
, O
combining O
its O
loss O
with O
the O
BILINGUAL O
- O
TRANS O
objective O
during O
training O
of O
both O
models O
in- O
creased O
performance O
, O
and O
so O
this O
loss O
was O
summed O
with O
the O
BGT O
loss O
in O
all O
of O
our O
experiments O
. O

We O
note O
that O
this O
does O
not O
affect O
our O
claim O
of O
BGT O
be- O
ing O
a O
generative O
model O
, O
as O
this O
loss O
is O
only O
used O
in O
a O
multi O
- O
task O
objective O
at O
training O
time O
, O
and O
we O
calculate O
the O
generation O
probabilities O
according O
to O
standard O
BGT O
at O
test O
time O
. O

Lastly O
, O
in O
Appendix O
B O
, O
we O
illustrate O
that O
it O
is O
cru- O
cial O
to O
train O
the O
Transformers O
with O
large O
batch O
sizes O
. O

Without O
this O
, O
the O
model O
can O
learn O
the O
goal O
task O
( O
such O
as O
translation O
) O
with O
reasonable O
accuracy O
, O
but O
the O
learned O
semantic O
embeddings O
are O
of O
poor O
qual- O
ity O
until O
batch O
sizes O
approximately O
reach O
25,000 O
tokens O
. O

Therefore O
, O
we O
use O
a O
maximum O
batch O
size O
of O
50,000 O
tokens O
in O
our O
ENGLISH O
TRANS O
, O
BILIN- O
GUAL O
TRANS O
, O
and O
BGT O
W O
/ O
OPRIOR O
, O
experiments O
and O
25,000 O
tokens O
in O
our O
BGT O
W O
/ O
OLANG O
VARS O
and O
BGT O
experiments O
. O

4.3 O
Evaluation O
Our O
primary O
evaluation O
are O
the O
2012 O
- O
2016 O
Se- O
mEval O
Semantic O
Textual O
Similarity O
( O
STS O
) O
shared O
tasks O
( O
Agirre O
et O
al O
. O
, O
2012 O
, O
2013 O
, O
2014 O
, O
2015 O
, O
2016 O
) O
, O
where O
the O
goal O
is O
to O
accurately O
predict O
the O
degree O
to O
which O
two O
sentences O
have O
the O
same O
meaning O
as O
measured O
by O
human O
judges O
. O

The O
evaluation O
metricis O
Pearson O
‚Äôs O
rwith O
the O
gold O
labels O
. O

Secondly O
, O
we O
evaluate O
on O
Hard O
STS O
, O
where O
we O
combine O
and O
Ô¨Ålter O
the O
STS O
datasets O
in O
order O
to O
make O
a O
more O
difÔ¨Åcult O
evaluation O
. O

We O
hypothesize O
that O
these O
datasets O
contain O
many O
examples O
where O
their O
gold O
scores O
are O
easy O
to O
predict O
by O
either O
hav- O
ing O
similar O
structure O
and O
word O
choice O
and O
a O
high O
score O
or O
dissimilar O
structure O
and O
word O
choice O
and O
a O
low O
score O
. O

Therefore O
, O
we O
split O
the O
data O
using O
sym- O
metric O
word O
error O
rate O
( O
SWER),7Ô¨Ånding O
sentence O
pairs O
with O
low O
SWER O
and O
low O
gold O
scores O
as O
well O
as O
sentence O
pairs O
with O
high O
SWER O
and O
high O
gold O
scores O
. O

This O
results O
in O
two O
datasets O
, O
Hard+ O
which O
have O
SWERs O
in O
the O
bottom O
20 O
% O
of O
all O
STS O
pairs O
and O
whose O
gold O
label O
is O
between O
0 O
and O
1,8and O
Hard- O
where O
the O
SWERs O
are O
in O
the O
top O
20 O
% O
of O
the O
gold O
scores O
are O
between O
4 O
and O
5 O
. O

We O
also O
evaluate O
on O
a O
split O
where O
negation O
was O
likely O
present O
in O
the O
example.9Examples O
are O
shown O
in O
Table O
1 O
. O

Lastly O
, O
we O
evaluate O
on O
STS O
in O
esandaras O
well O
as O
cross O
- O
lingual O
evaluations O
for O
en O
- O
es O
, O
en O
- O
ar O
, O
anden O
- O
tr O
. O

We O
use O
the O
datasets O
from O
SemEval O
2017 O
( O
Cer O
et O
al O
. O
, O
2017 O
) O
. O

For O
this O
setting O
, O
we O
train O
BILINGUAL O
TRANS O
andBGT O
on O
1 O
million O
exam- O
ples O
from O
en O
- O
es O
, O
en O
- O
ar O
, O
anden O
- O
tr O
OpenSub- O
titles O
2018 O
data O
. O

4.4 O
Results O
The O
results O
on O
the O
STS O
and O
Hard O
STS O
are O
shown O
in O
Table O
3.10From O
the O
results O
, O
we O
see O
that O
BGT O
has O
the O
highest O
overall O
performance O
. O

It O
does O
especially O
well O
compared O
to O
prior O
work O
on O
the O
two O
Hard O
STS O
datasets O
. O

We O
used O
paired O
bootstrap O
resampling O
to O
check O
whether O
BGT O
signiÔ¨Åcantly O
outperforms O
Sen- O
tenceBert O
, O
SP O
, O
BILINGUAL O
TRANS O
, O
and O
BGT O
W O
/ O
O O
PRIOR O
on O
the O
STS O
task O
. O

We O
found O
all O
gains O
to O
be O
7We O
deÔ¨Åne O
symmetric O
word O
error O
rate O
for O
sentences O
s1and O
s2as1 O
2WER O
( O
s1;s2)+1 O
2WER O
( O
s2;s1 O
) O
, O
since O
word O
error O
rate O
( O
WER O
) O
is O
an O
asymmetric O
measure O
. O

8STS O
scores O
are O
between O
0 O
and O
5 O
. O
9We O
selected O
examples O
for O
the O
negation O
split O
where O
one O
sentence O
contained O

notor‚Äôtand O
the O
other O
did O
not O
. O

10We O
obtained O
values O
for O
STS O
2012 O
- O
2016 O
from O
prior O
works O
using O
SentEval O
( O
Conneau O
and O
Kiela O
, O
2018 O
) O
. O

Note O
that O
we O
include O
all O
datasets O
for O
the O
2013 O
competition O
, O
including O
SMT O
, O
which O
is O
not O
included O
in O
SentEval O
. O

1587ModelSemantic O
Textual O
Similarity O
( O
STS O
) O
2012 O
2013 O
2014 O
2015 O
2016 O
Avg O
. O
Hard+ O

Hard- O

Avg O
. O
BERT O
( O
CLS O
) O
33.2 O
29.6 O
34.3 O
45.1 O
48.4 O
38.1 O
7.8 O
12.5 O
10.2 O
BERT O
( O
Mean O
) O

48.8 O
46.5 O
54.0 O
59.2 O
63.4 O
54.4 O
3.1 O
24.1 O
13.6 O
Infersent O
61.1 O
51.4 O
68.1 O
70.9 O
70.7 O
64.4 O
4.2 O
29.6 O
16.9 O
GenSen O
60.7 O
50.8 O
64.1 O
73.3 O
66.0 O
63.0 O
24.2 O
6.3 O
15.3 O
USE O
61.4 O
59.0 O
70.6 O
74.3 O
73.9 O
67.8 O
16.4 O
28.1 O
22.3 O
LASER O
63.1 O
47.0 O
67.7 O
74.9 O
71.9 O
64.9 O
18.1 O
23.8 O
20.9 O
Sentence O
- O
BERT O
66.9 O
63.2 O
74.2 O
77.3 O
72.8 O
70.9 O
23.9 O
3.6 O
13.8 O
SP O
68.4 O
60.3 O
75.1 O
78.7 O
76.8 O
71.9 O
19.1 O
29.8 O
24.5 O
BILSTM O
67.9 O
56.4 O
74.5 O
78.2 O
75.9 O
70.6 O
18.5 O
23.2 O
20.9 O
ENGLISH O
AE O
60.2 O
52.7 O
68.6 O
74.0 O
73.2 O
65.7 O
15.7 O
36.0 O
25.9 O
ENGLISH O
VAE O
59.5 O
54.0 O
67.3 O
74.6 O
74.1 O
65.9 O
16.8 O
42.7 O
29.8 O
ENGLISH O
TRANS O
66.5 O
60.7 O
72.9 O
78.1 O
78.3 O
71.3 O
18.0 O
47.2 O
32.6 O
BILINGUAL O
TRANS O
67.1 O
61.0 O
73.3 O
78.0 O
77.8 O
71.4 O
20.0 O
48.2 O
34.1 O
BGT O
W O
/ O
OLANG O
VARS O
68.3 O
61.3 O
74.5 O
79.0 O
78.5 O
72.3 O
24.1 O
46.8 O
35.5 O
BGT O
W O
/ O
OPRIOR O
67.6 O
59.8 O
74.1 O
78.4 O
77.9 O
71.6 O
17.9 O
45.5 O
31.7 O
BGT O
68.9 O
62.2 O
75.9 O
79.4 O
79.3 O
73.1 O
22.5 O
46.6 O
34.6Figure O
3 O
: O
Results O
of O
our O
mod- O
els O
and O
models O
from O
prior O
work O
. O

The O
Ô¨Årst O
six O
rows O
are O
pretrained O
models O
from O
the O
literature O
, O
the O
next O
two O
rows O
are O
strong O
base- O
lines O
trained O
on O
the O
same O
data O
as O
our O
models O
, O
and O
the O
last O
seven O
rows O
include O
model O
ablations O
and O
BGT O
, O
our O
Ô¨Ånal O
model O
. O

We O
show O
results O
, O
measured O
in O
Pear- O
son‚Äôsr100 O
, O
for O
each O
year O
of O
the O
STS O
tasks O
2012 O
- O
2016 O
and O
our O
two O
Hard O
STS O
datasets O
. O

Model O
es O
- O
es O
ar O
- O
ar O
en O
- O
es O
en O
- O
ar O
en O
- O
tr O
LASER O
79.7 O
69.3 O
59.7 O
65.5 O
72.0 O
BILINGUAL O
TRANS O

83.4 O
72.6 O
64.1 O
37.6 O
59.1 O
BGT O
W O
/ O
OLANG O
VARS O
81.7 O
72.8 O
72.6 O
73.4 O
74.8 O
BGT O
W O
/ O
OPRIOR O
84.5 O
73.2 O
68.0 O
66.5 O
70.9 O
BGT O
85.7 O
74.9 O
75.6 O
73.5 O
74.9Figure O
4 O
: O
Performance O
measured O
in O
Pearson‚Äôsr100 O
, O
on O
the O
Se- O
mEval O
2017 O
STS O
task O
on O
the O
es O
- O
es O
, O
ar O
- O
ar O
, O
en O
- O
es O
, O
en O
- O
ar O
, O
anden O
- O
tr O
datasets O
. O

signiÔ¨Åcant O
with O
p<0:01.11 O
From O
these O
results O
, O
we O
see O
that O
both O
positive O
ex- O
amples O
that O
have O
little O
shared O
vocabulary O
and O
struc- O
ture O
and O
negative O
examples O
with O
signiÔ¨Åcant O
shared O
vocabulary O
and O
structure O
beneÔ¨Åt O
signiÔ¨Åcantly O
from O
using O
a O
deeper O
architecture O
. O

Similarly O
, O
examples O
where O
negation O
occurs O
also O
beneÔ¨Åt O
from O
our O
deeper O
model O
. O

These O
examples O
are O
difÔ¨Åcult O
because O
more O
than O
just O
the O
identity O
of O
the O
words O
is O
needed O
to O
de- O
termine O
the O
relationship O
of O
the O
two O
sentences O
, O
and O
this O
is O
something O
that O
SPis O
not O
equipped O
for O
since O
it O
is O
unable O
to O
model O
word O
order O
. O

The O
bottom O
two O
rows O
show O
easier O
examples O
where O
positive O
exam- O
ples O
have O
high O
overlap O
and O
low O
SWER O
and O
vice O
versa O
for O
negative O
examples O
. O

Both O
models O
perform O
similarly O
on O
this O
data O
, O
with O
the O
BGT O
model O
hav- O
ing O
a O
small O
edge O
consistent O
with O
the O
overall O
gap O
between O
these O
two O
models O
. O

Lastly O
, O
in O
Table O
4 O
, O
we O
show O
the O
results O
of O
STS O
evaluations O
in O
esandarand O
cross O
- O
lingual O
evalua- O
tions O
for O
en O
- O
es O
, O
en O
- O
ar O
, O
anden O
- O
tr O
. O

We O
also O
in- O
clude O
a O
comparison O
to O
LASER O
, O
which O
is O
a O
multilin- O
gual O
model.12From O
these O
results O
, O
we O
see O
that O
BGT O
11We O
show O
further O
difÔ¨Åcult O
splits O
in O
Appendix O
D O
, O
including O
a O
negation O
split O
, O
beyond O
those O
used O
in O
Hard O
STS O
and O
compare O
the O
top O
two O
performing O
models O
in O
the O
STS O
task O
from O
Table O
3 O
. O

We O
also O
show O
easier O
splits O
of O
the O
data O
to O
illustrate O
that O
the O
difference O
between O
these O
models O
is O
smaller O
on O
these O
splits O
. O

12We O
note O
that O
this O
is O
not O
a O
fair O
comparison O
for O
a O
variety O
of O
reasons O
. O

For O
instance O
, O
our O
models O
are O
just O
trained O
on O
two O
languages O
at O
a O
time O
, O
but O
are O
only O
trained O
on O
1 O
M O
translation O
pairs O
from O
OpenSubtitles O
. O

LASER O
, O
in O
turn O
, O
is O
trained O
on O
223 O
M O
translation O
pairs O
covering O
more O
domains O
, O
but O
is O
also O
trained O
onhas O
the O
best O
performance O
across O
all O
datasets O
, O
how- O
ever O
the O
performance O
is O
signiÔ¨Åcantly O
stronger O
than O
theBILINGUAL O
TRANS O
andBGT O
W O
/ O
OPRIOR O
base- O
lines O
in O
the O
cross O
- O
lingual O
setting O
. O

Since O
BGT O
W O
/ O
O O
LANG O
VARS O
also O
has O
signiÔ¨Åcantly O
better O
perfor- O
mance O
on O
these O
tasks O
, O
most O
of O
this O
gain O
seems O
to O
be O
due O
to O
the O
prior O
having O
a O
regularizing O
effect O
. O

How- O
ever O
, O
BGT O
outperforms O
BGT O
W O
/ O
OLANG O
VARS O
overall O
, O
and O
we O
hypothesize O
that O
the O
gap O
in O
perfor- O
mance O
between O
these O
two O
models O
is O
due O
to O
BGT O
being O
able O
to O
strip O
away O
the O
language O
- O
speciÔ¨Åc O
in- O
formation O
in O
the O
representations O
with O
its O
language- O
speciÔ¨Åc O
variables O
, O
allowing O
for O
the O
semantics O
of O
the O
sentences O
to O
be O
more O
directly O
compared O
. O

5 O
Analysis O
We O
next O
analyze O
our O
BGT O
model O
by O
examining O
what O
elements O
of O
syntax O
and O
semantics O
the O
lan- O
guage O
and O
semantic O
variables O
capture O
relative O
both O
to O
each O
- O
other O
and O
to O
the O
sentence O
embeddings O
from O
theBILINGUAL O
TRANS O
models O
. O

We O
also O
analyze O
how O
the O
choice O
of O
language O
and O
its O
lexical O
and O
syn- O
tactic O
distance O
from O
English O
affects O
the O
semantic O
and O
syntactic O
information O
captured O
by O
the O
semantic O
and O
language O
- O
speciÔ¨Åc O
encoders O
. O

Finally O
, O
we O
also O
show O
that O
our O
model O
is O
capable O
of O
sentence O
gener- O
ation O
in O
a O
type O
of O
style O
transfer O
, O
demonstrating O
its O
capabilities O
as O
a O
generative O
model O
. O

93 O
languages O
simultaneously O
. O

1588ModelSemantic O
Textual O
Similarity O
( O
STS O
) O
2012 O
2013 O
2014 O
2015 O
2016 O
Hard+ O
Hard- O
Random O
Encoder O
51.4 O
34.6 O
52.7 O
52.3 O
49.7 O
4.8 O
17.9 O
English O
Language O
Encoder O
44.4 O
41.7 O
53.8 O
62.4 O
61.7 O
15.3 O
26.5 O

Semantic O
Encoder O
68.9 O
62.2 O
75.9 O
79.4 O
79.3 O
22.5 O
46.6 O
Table O
2 O
: O
STS O
performance O
on O
the O
2012 O
- O
2016 O
datasets O
and O
our O
STS O
Hard O
datasets O
for O
a O
randomly O
initialized O
Trans- O
former O
, O
the O
trained O
English O
language O
- O
speciÔ¨Åc O
encoder O
from O
BGT O
, O
and O
the O
trained O
semantic O
encoder O
from O
BGT O
. O

Performance O
is O
measured O
in O
Pearson O
‚Äôs O
r100 O
. O

Lang O
. O

Model O
STS O
S. O
Num O
. O

O. O
Num O
. O

Depth O
Top O
Con O
. O

Word O
Len O
. O

P. O
Num O
. O

P. O
First O
Gend O
. O

frBILINGUAL O
TRANS O
71.2 O
78.0 O
76.5 O
28.2 O
65.9 O
80.2 O
74.0 O
56.9 O
88.3 O
53.0 O
Semantic O
Encoder O
72.4 O
84.6 O
80.9 O
29.7 O
70.5 O
77.4 O
73.0 O
60.7 O
87.9 O
52.6 O
enLanguage O
Encoder O
56.8 O
75.2 O
72.0 O
28.0 O
63.6 O
65.4 O
80.2 O
65.3 O

92.2 O
- O
frLanguage O
Encoder O
- O
- O
- O
- O
- O
- O
- O
- O
- O
56.5 O
esBILINGUAL O
TRANS O
70.5 O
84.5 O
82.1 O
29.7 O
68.5 O
79.2 O
77.7 O
63.4 O
90.1 O
54.3 O
Semantic O
Encoder O
72.1 O
85.7 O
83.6 O
32.5 O
71.0 O
77.3 O
76.7 O
63.1 O
89.9 O
52.6 O
enLanguage O
Encoder O
55.8 O
75.7 O
73.7 O
29.1 O
63.9 O
63.3 O
80.2 O
64.2 O
92.7 O
- O
esLanguage O
Encoder O
- O
- O
- O
- O
- O
- O
- O
- O
- O
54.7 O
arBILINGUAL O
TRANS O
70.2 O
77.6 O
74.5 O
28.1 O
67.0 O
77.5 O
72.3 O
57.5 O
89.0 O
- O
Semantic O
Encoder O
70.8 O
81.9 O
80.8 O
32.1 O
71.7 O
71.9 O
73.3 O
61.8 O
88.5 O
- O
enLanguage O
Encoder O
58.9 O

76.2 O
73.1 O
28.4 O
60.7 O
71.2 O
79.8 O
63.4 O
92.4 O
- O
trBILINGUAL O
TRANS O
70.7 O
78.5 O
74.9 O
28.1 O
60.2 O
78.4 O
72.1 O
54.8 O
87.3 O
- O
Semantic O
Encoder O
72.3 O
81.7 O

80.2 O
30.6 O
66.0 O
75.2 O
72.4 O
59.3 O
86.7 O
- O
enLanguage O
Encoder O
57.8 O
77.3 O
74.4 O
28.3 O
63.1 O
67.1 O
79.7 O
67.0 O
92.5 O
- O
jaBILINGUAL O
TRANS O
71.0 O
66.4 O
64.6 O
25.4 O
54.1 O
76.0 O
67.6 O
53.8 O
87.8 O
- O
Semantic O
Encoder O
71.9 O
68.0 O
66.8 O
27.5 O
58.9 O
70.1 O
68.7 O
52.9 O
86.6 O
- O
enLanguage O
Encoder O
60.6 O
77.6 O
76.4 O
28.0 O
64.6 O
70.0 O
80.4 O
62.8 O
92.0 O
- O
Table O
3 O
: O
Average O
STS O
performance O
for O
the O
2012 O
- O
2016 O
datasets O
, O
measured O
in O
Pearson O
‚Äôs O
r100 O
, O
followed O
by O
probing O
results O
on O
predicting O
number O
of O
subjects O
, O
number O
of O
objects O
, O
constituent O
tree O
depth O
, O
top O
constituent O
, O
word O
content O
, O
length O
, O
number O
of O
punctuation O
marks O
, O
the O
Ô¨Årst O
punctuation O
mark O
, O
and O
whether O
the O
articles O
in O
the O
sentence O
are O
the O
correct O
gender O
. O

All O
probing O
results O
are O
measured O
in O
accuracy O
100 O
. O

5.1 O
STS O

We O
Ô¨Årst O
show O
that O
the O
language O
variables O
are O
cap- O
turing O
little O
semantic O
information O
by O
evaluating O
the O
learned O
English O
language O
- O
speciÔ¨Åc O
variable O
from O
ourBGT O
model O
on O
our O
suite O
of O
semantic O
tasks O
. O

The O
results O
in O
Table O
2 O
show O
that O
these O
encoders O
perform O
closer O
to O
a O
random O
encoder O
than O
the O
se- O
mantic O
encoder O
from O
BGT O
. O

This O
is O
consistent O
with O
what O
we O
would O
expect O
to O
see O
if O
they O
are O
capturing O
extraneous O
language O
- O
speciÔ¨Åc O
information O
. O

5.2 O

Probing O
We O
probe O
our O
BGT O
semantic O
and O
language O
- O
speciÔ¨Åc O
encoders O
, O
along O
with O
our O
BILINGUAL O
TRANS O
en- O

coders O
as O
a O
baseline O
, O
to O
compare O
and O
contrast O
what O
aspects O
of O
syntax O
and O
semantics O
they O
are O
learning O
relative O
to O
each O
other O
across O
Ô¨Åve O
languages O
with O
var- O
ious O
degrees O
of O
similarity O
with O
English O
. O

All O
models O
are O
trained O
on O
the O
OpenSubtitles O
2018 O
corpus O
. O

We O
use O
the O
datasets O
from O
Conneau O
et O
al O
. O

( O
2018 O
) O
for O
se- O
mantic O
tasks O
like O
number O
of O
subjects O
and O
number O
of O
objects O
, O
and O
syntactic O
tasks O
like O
tree O
depth O
, O
and O
top O
constituent O
. O

Additionally O
, O
we O
include O
predicting O
the O
word O
content O
and O
sentence O
length O
. O

We O
also O
add O
our O
own O
tasks O
to O
validate O
our O
intuitions O
about O
punc- O
tuation O
and O
language O
- O
speciÔ¨Åc O
information O
. O

In O
theÔ¨Årst O
of O
these O
, O
punctuation O
number O
, O
we O
train O
a O
clas- O
siÔ¨Åer O
to O
predict O
the O
number O
of O
punctuation O
marks13 O
in O
a O
sentence O
. O

To O
make O
the O
task O
more O
challenging O
, O
we O
limit O
each O
label O
to O
have O
at O
most O
20,000 O
exam- O
ples O
split O
among O
training O
, O
validation O
, O
and O
testing O
data.14In O
the O
second O
task O
, O
punctuation O
Ô¨Årst O
, O
we O
train O
a O
classiÔ¨Åer O
to O
predict O
the O
identity O
of O
the O
Ô¨Årst O
punctuation O
mark O
in O
the O
sentence O
. O

In O
our O
last O
task O
, O
gender O
, O
we O
detect O
examples O
where O
the O
gender O
of O
the O
articles O
in O
the O
sentence O
is O
incorrect O
in O
French O
or O
Spanish O
. O

To O
create O
an O
incorrect O
example O
, O
we O
switch O
articles O
fromfle O
, O
la O
, O
un O
, O
unegfor O
French O
andfel O
, O
la O
, O
los O
, O
lasgfor O
Spanish O
, O
with O
their O
( O
indeÔ¨Ånite O
or O
deÔ¨Å- O
nite O
for O
French O
and O
singular O
or O
plural O
for O
Spanish O
) O
counterpart O
with O
the O
opposite O
gender O
. O

This O
dataset O
was O
balanced O
so O
random O
chance O
gives O
50 O
% O
on O
the O
testing O
data O
. O

All O
tasks O
use O
100,000 O
examples O
for O
training O
and O
10,000 O
examples O
for O
validation O
and O
testing O
. O

The O
results O
of O
these O
experiments O
are O
shown O
in O
Table O
3 O
. O

These O
results O
show O
that O
the O
source O
separation O
is O
effective O
- O
stylistic O
and O
language O
- O
speciÔ¨Åc O
informa- O
tion O
like O
length O
, O
punctuation O
and O
language O
- O
speciÔ¨Åc O
13Punctuation O
were O
taken O
from O
the O
set O
f O
‚Äô O
! O
‚Äù O

# O
$ O
% O
& O
n O
‚Äô O
( O
) O
+, .= O
: O
; O
< O
= O
> O
? O

@ O
[ O
] O
ÀÜ O
‚Äò O
f O
‚Äî O
gÀú O
‚Äô O
.g O
. O

14The O
labels O
are O
from O
1 O
punctuation O
mark O
up O
to O
10 O
marks O
with O
an O
additional O
label O
consolidating O
11 O
or O
more O
marks O
. O

1589gender O
information O
are O
more O
concentrated O
in O
the O
language O
variables O
, O
while O
word O
content O
, O
semantic O
and O
syntactic O
information O
are O
more O
concentrated O
in O
the O
semantic O
encoder O
. O

The O
choice O
of O
language O
is O
also O
seen O
to O
be O
inÔ¨Çuential O
on O
what O
these O
encoders O
are O
capturing O
. O

When O
the O
languages O
are O
closely O
re- O
lated O
to O
English O
, O
like O
in O
French O
and O
Spanish O
, O
the O
performance O
difference O
between O
the O
semantic O
and O
English O
language O
encoder O
is O
larger O
for O
word O
con- O
tent O
, O
subject O
number O
, O
object O
number O
than O
for O
more O
distantly O
related O
languages O
like O
Arabic O
and O
Turk- O
ish O
. O

In O
fact O
, O
word O
content O
performance O
is O
directly O
tied O
to O
how O
well O
the O
alphabets O
of O
the O
two O
languages O
overlap O
. O

This O
relationship O
matches O
our O
intuition O
, O
because O
lexical O
information O
will O
be O
cheaper O
to O
en- O
code O
in O
the O
semantic O
variable O
when O
it O
is O
shared O
between O
the O
languages O
. O

Similarly O
for O
the O
tasks O
of O
length O
, O
punctuation O
Ô¨Årst O
, O
and O
punctuation O
num- O
ber O
, O
the O
gap O
in O
performance O
between O
the O
two O
en- O
coders O
also O
grows O
as O
the O
languages O
become O
more O
distant O
from O
English O
. O

Lastly O
, O
the O
gap O
on O
STS O
per- O
formance O
between O
the O
two O
encoders O
shrinks O
as O
the O
languages O
become O
more O
distant O
, O
which O
again O
is O
what O
we O
would O
expect O
, O
as O
the O
language O
- O
speciÔ¨Åc O
encoders O
are O
forced O
to O
capture O
more O
information O
. O

Japanese O
is O
an O
interesting O
case O
in O
these O
exper- O
iments O
, O
where O
the O
English O
language O
- O
speciÔ¨Åc O
en- O
coder O
outperforms O
the O
semantic O
encoder O
on O
the O
semantic O
and O
syntactic O
probing O
tasks O
. O

Japanese O
is O
a O
very O
distant O
language O
to O
English O
both O
in O
its O
writing O
system O
and O
in O
its O
sentence O
structure O
( O
it O
is O
an O
SOV O
language O
, O
where O
English O
is O
an O
SVO O
lan- O
guage O
) O
. O

However O
, O
despite O
these O
difference O
, O
the O
se- O
mantic O
encoder O
strongly O
outperforms O
the O
English O
language O
- O
speciÔ¨Åc O
encoder O
, O
suggesting O
that O
the O
un- O
derlying O
meaning O
of O
the O
sentence O
is O
much O
better O
captured O
by O
the O
semantic O
encoder O
. O

5.3 O
Generation O
and O
Style O
Transfer O
In O
this O
section O
, O
we O
qualitatively O
demonstrate O
the O
ability O
of O
our O
model O
to O
generate O
sentences O
. O

We O
focus O
on O
a O
style O
- O
transfer O
task O
where O
we O
have O
orig- O
inal O
seed O
sentences O
from O
which O
we O
calculate O
our O
semantic O
vector O
zsemand O
language O
speciÔ¨Åc O
vector O
zen O
. O

SpeciÔ¨Åcally O
, O
we O
feed O
in O
a O
Source O
sentence O
into O
the O
semantic O
encoder O
to O
obtain O
zsem O
, O
and O
another O
Style O
sentence O
into O
the O
English O
language O
- O
speciÔ¨Åc O
encoder O
to O
obtain O
zen O
. O

We O
then O
generate O
a O
new O
sentence O
using O
these O
two O
latent O
variables O
. O

This O
can O
be O
seen O
as O
a O
type O
of O
style O
transfer O
where O
we O
expect O
the O
model O
to O
generate O
a O
sentence O
that O
has O
the O
se O
- O
Source O
you O
know O
what O
i O
‚Äôve O
seen O
? O

Style O
he O
said O
, O
‚Äú O
since O
when O
is O
going O
Ô¨Åshing O
‚Äù O
had O
any- O
thing O
to O
do O
with O
Ô¨Åsh O
? O
‚Äù O

Output O
he O
said O
, O
‚Äú O
what O
is O
going O
to O
do O
with O
me O
since O
i O
saw O
you O
? O
‚Äù O

Source O
guys O
, O
that O
was O
the O
tech O
unit O
. O

Style O
is O
well O
, O
‚Äú O
capicci O
‚Äù O
... O

Output O
is O
that O
what O
, O
‚Äú O
technician O
‚Äù O
? O

Source O
the O
pay O
is O
no O
good O
, O
but O
it O
‚Äôs O
money O
. O

Style O
do O
we O
know O
cause O
of O
death O
? O

Output O
do O
we O
have O
any O
money O
? O

Source O
we O
‚Äôre O
always O
doing O
stupid O
things O
. O

Style O
all O
right O
listen O
, O
i O
like O
being O
exactly O
where O
i O
am O
, O
Output O
all O
right O
, O
i O
like O
being O
stupid O
, O
but O
i O
am O
always O
here O
. O

Table O
4 O
: O
Style O
transfer O
generations O
from O
our O
learned O
BGT O
model O
. O

Source O
refers O
to O
the O
sentence O
fed O
into O
the O
semantic O
encoder O
, O
Style O
refers O
to O
the O
sentence O
fed O
into O
the O
English O
language O
- O
speciÔ¨Åc O
encoder O
, O
and O
Output O
refers O
to O
the O
text O
generated O
by O
our O
model O
. O

mantics O
of O
the O
Source O
sentence O
and O
the O
style O
of O
the O
Style O
sentence O
. O

We O
use O
our O
en O
- O
fr O
BGT O
model O
from O
Table O
3 O
and O
show O
some O
examples O
in O
Table O
4 O
. O

All O
input O
sentences O
are O
from O
held O
- O
out O
en O
- O
fr O
Open- O
Subtitles O
data O
. O

From O
these O
examples O
, O
we O
see O
further O
evidence O
of O
the O
role O
of O
the O
semantic O
and O
language- O
speciÔ¨Åc O
encoders O
, O
where O
most O
of O
the O
semantics O
( O
e.g. O
topical O
word O
such O
as O
seen O
andtech O
in O
the O
Source O
sentence O
) O
are O
reÔ¨Çected O
in O
the O
output O
, O
but O
length O
and O
structure O
are O
more O
strongly O
inÔ¨Çuenced O
by O
the O
language O
- O
speciÔ¨Åc O
encoder O
. O

6 O
Conclusion O
We O
propose O
Bilingual O
Generative O
Transformers O
, O
a O
model O
that O
uses O
parallel O
data O
to O
learn O
to O
perform O
source O
separation O
of O
common O
semantic O
informa- O
tion O
between O
two O
languages O
from O
language O
- O
speciÔ¨Åc O
information O
. O

We O
show O
that O
the O
model O
is O
able O
to O
ac- O
complish O
this O
source O
separation O
through O
probing O
tasks O
and O
text O
generation O
in O
a O
style O
- O
transfer O
setting O
. O

We O
Ô¨Ånd O
that O
our O
model O
bests O
all O
baselines O
on O
unsu- O
pervised O
semantic O
similarity O
tasks O
, O
with O
the O
largest O
gains O
coming O
from O
a O
new O
challenge O
we O
propose O
asHard O
STS O
, O
designed O
to O
foil O
methods O
approxi- O
mating O
semantic O
similarity O
as O
word O
overlap O
. O

We O
also O
Ô¨Ånd O
our O
model O
to O
be O
especially O
effective O
on O
unsupervised O
cross O
- O
lingual O
semantic O
similarity O
, O
due O
to O
its O
stripping O
away O
of O
language O
- O
speciÔ¨Åc O
informa- O
tion O
allowing O
for O
the O
underlying O
semantics O
to O
be O
more O
directly O
compared O
. O

In O
future O
work O
, O
we O
will O
explore O
generalizing O
this O
approach O
to O
the O
multi- O
lingual O
setting O
, O
or O
applying O
it O
to O
the O
pre O
- O
train O
and O
Ô¨Åne O
- O
tune O
paradigm O
used O
widely O
in O
other O
models O
such O
as O
BERT O
. O

1590Acknowledgments O
The O
authors O
thank O
Amazon O
AWS O
for O
the O
gen- O
erous O
donation O
of O
computation O
credits O
. O

This O
project O
is O
funded O
in O
part O
by O
the O
NSF O
under O
grants O
1618044 O
and O
1936155 O
, O
and O
by O
the O
NEH O
under O
grant O
HAA256044 O
- O
17 O
. O

References O
Eneko O
Agirre O
, O
Carmen O
Banea O
, O
Claire O
Cardie O
, O
Daniel O
Cer O
, O
Mona O
Diab O
, O
Aitor O
Gonzalez O
- O
Agirre O
, O
Weiwei O
Guo O
, O
Inigo O
Lopez O
- O
Gazpio O
, O
Montse O
Maritxalar O
, O
Rada O
Mihalcea O
, O
German O
Rigau O
, O
Larraitz O
Uria O
, O
and O
Janyce O
Wiebe O
. O

2015 O
. O

SemEval-2015 O
task O
2 O
: O
Semantic O
tex- O
tual O
similarity O
, O
English O
, O
Spanish O
and O
pilot O
on O
inter- O
pretability O
. O

In O
Proceedings O
of O
the O
9th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval O
2015 O
) O
. O

Eneko O
Agirre O
, O
Carmen O
Banea O
, O
Claire O
Cardie O
, O
Daniel O
Cer O
, O
Mona O
Diab O
, O
Aitor O
Gonzalez O
- O
Agirre O
, O
Weiwei O
Guo O
, O
Rada O
Mihalcea O
, O
German O
Rigau O
, O
and O
Janyce O
Wiebe O
. O

2014 O
. O

SemEval-2014 O
task O
10 O
: O
Multilingual O
semantic O
textual O
similarity O
. O

In O
Proceedings O
of O
the O
8th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval O
2014 O
) O
. O

Eneko O
Agirre O
, O
Carmen O
Banea O
, O
Daniel O
Cer O
, O
Mona O
Diab O
, O
Aitor O
Gonzalez O
- O
Agirre O
, O
Rada O
Mihalcea O
, O
German O
Rigau O
, O
and O
Janyce O
Wiebe O
. O

2016 O
. O

SemEval-2016 O
task O
1 O
: O
Semantic O
textual O
similarity O
, O
monolingual O
and O
cross O
- O
lingual O
evaluation O
. O

Proceedings O
of O
SemEval O
, O
pages O
497‚Äì511 O
. O

Eneko O
Agirre O
, O
Daniel O
Cer O
, O
Mona O
Diab O
, O
Aitor O
Gonzalez- O
Agirre O
, O
and O
Weiwei O
Guo O
. O

2013 O
. O

* O
sem O
2013 O
shared O
task O
: O
Semantic O
textual O
similarity O
. O

In O
Second O
Joint O
Conference O
on O
Lexical O
and O
Computational O
Seman- O
tics O
( O
* O
SEM O
) O
, O
Volume O
1 O
: O
Proceedings O
of O
the O
Main O
Conference O
and O
the O
Shared O
Task O
: O

Semantic O
Textual O
Similarity O
, O
volume O
1 O
, O
pages O
32‚Äì43 O
. O

Eneko O
Agirre O
, O
Mona O
Diab O
, O
Daniel O
Cer O
, O
and O
Aitor O
Gonzalez O
- O
Agirre O
. O

2012 O
. O

SemEval-2012 O
task O
6 O
: O
A O
pilot O
on O
semantic O
textual O
similarity O
. O

In O
Proceedings O
of O
the O
First O
Joint O
Conference O
on O
Lexical O
and O
Com- O
putational O
Semantics O
- O
Volume O
1 O
: O
Proceedings O
of O
the O
main O
conference O
and O
the O
shared O
task O
, O
and O
Volume O
2 O
: O
Proceedings O
of O
the O
Sixth O
International O
Workshop O
on O
Semantic O
Evaluation O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Sanjeev O
Arora O
, O
Yingyu O
Liang O
, O
and O
Tengyu O
Ma O
. O
2017 O
. O

A O
simple O
but O
tough O
- O
to O
- O
beat O
baseline O
for O
sentence O
em- O
beddings O
. O

In O
Proceedings O
of O
the O
International O
Con- O
ference O
on O
Learning O
Representations O
. O

Mikel O
Artetxe O
and O
Holger O
Schwenk O
. O

2018 O
. O

Mas- O
sively O
multilingual O
sentence O
embeddings O
for O
zero- O
shot O
cross O
- O
lingual O
transfer O
and O
beyond O
. O

arXiv O
preprint O
arXiv:1812.10464 O
.Samuel O

R. O
Bowman O
, O
Gabor O
Angeli O
, O
Christopher O
Potts O
, O
and O
Christopher O
D. O
Manning O
. O

2015 O
. O

A O
large O
anno- O
tated O
corpus O
for O
learning O
natural O
language O
inference O
. O

InProceedings O
of O
the O
2015 O
Conference O
on O
Empiri- O
cal O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
632‚Äì642 O
, O
Lisbon O
, O
Portugal O
. O

Daniel O
Cer O
, O
Mona O
Diab O
, O
Eneko O
Agirre O
, O
Inigo O
Lopez- O
Gazpio O
, O
and O
Lucia O
Specia O
. O

2017 O
. O

SemEval-2017 O
Task O
1 O
: O
Semantic O
textual O
similarity O
multilingual O
and O
crosslingual O
focused O
evaluation O
. O

In O
Proceed- O
ings O
of O
the O
11th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval-2017 O
) O
, O
pages O
1‚Äì14 O
, O
Vancouver O
, O
Canada O
. O

Daniel O
Cer O
, O
Yinfei O
Yang O
, O
Sheng O
- O
yi O
Kong O
, O
Nan O
Hua O
, O
Nicole O
Limtiaco O
, O
Rhomni O
St O
John O
, O
Noah O
Constant O
, O
Mario O
Guajardo O
- O
Cespedes O
, O
Steve O
Yuan O
, O
Chris O
Tar O
, O
et O
al O
. O
2018 O
. O

Universal O
sentence O
encoder O
. O

arXiv O
preprint O
arXiv:1803.11175 O
. O

Mingda O
Chen O
, O
Qingming O
Tang O
, O
Sam O
Wiseman O
, O
and O
Kevin O
Gimpel O
. O

2019 O
. O

A O
multi O
- O
task O
approach O
for O
dis- O
entangling O
syntax O
and O
semantics O
in O
sentence O
repre- O
sentations O
. O

arXiv O
preprint O
arXiv:1904.01173 O
. O

Muthu O
Chidambaram O
, O
Yinfei O
Yang O
, O
Daniel O
Cer O
, O
Steve O
Yuan O
, O
Yunhsuan O
Sung O
, O
Brian O
Strope O
, O
and O
Ray O
Kurzweil O
. O
2019 O
. O

Learning O
cross O
- O
lingual O
sentence O
representations O
via O
a O
multi O
- O
task O
dual O
- O
encoder O
model O
. O

InProceedings O
of O
the O
4th O
Workshop O
on O
Represen- O
tation O
Learning O
for O
NLP O
( O
RepL4NLP-2019 O
) O
, O
pages O
250‚Äì259 O
, O
Florence O
, O
Italy O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Alexis O
Conneau O
and O
Douwe O
Kiela O
. O

2018 O
. O

Senteval O
: O
An O
evaluation O
toolkit O
for O
universal O
sentence O
representa- O
tions O
. O

arXiv O
preprint O
arXiv:1803.05449 O
. O

Alexis O
Conneau O
, O
Douwe O
Kiela O
, O
Holger O
Schwenk O
, O
Lo O
¬®ƒ±c O
Barrault O
, O
and O
Antoine O
Bordes O
. O

2017 O
. O

Supervised O
learning O
of O
universal O
sentence O
representations O
from O
natural O
language O
inference O
data O
. O

In O
Proceedings O
of O
the O
2017 O
Conference O
on O
Empirical O
Methods O
in O
Nat- O
ural O
Language O
Processing O
, O
pages O
670‚Äì680 O
, O
Copen- O
hagen O
, O
Denmark O
. O

Alexis O
Conneau O
, O
German O
Kruszewski O
, O
Guillaume O
Lam- O
ple O
, O
Lo O
¬®ƒ±c O
Barrault O
, O
and O
Marco O
Baroni O
. O

2018 O
. O

What O
you O
can O
cram O
into O
a O
single O
vector O
: O
Probing O
sentence O
embeddings O
for O
linguistic O
properties O
. O

arXiv O
preprint O
arXiv:1805.01070 O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2018 O
. O

Bert O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understand- O
ing O
. O

arXiv O
preprint O
arXiv:1810.04805 O
. O

Bill O
Dolan O
, O
Chris O
Quirk O
, O
and O
Chris O
Brockett O
. O

2004 O
. O

Un- O
supervised O
construction O
of O
large O
paraphrase O
corpora O
: O
Exploiting O
massively O
parallel O
news O
sources O
. O

In O
Pro- O
ceedings O
of O
COLING O
. O

Cristina O
Espana O
- O
Bonet O
, O
Ad O
¬¥ O
am O
Csaba O
Varga O
, O
Alberto O
Barr¬¥on O
- O
Cede O
Àúno O
, O
and O
Josef O
van O
Genabith O
. O
2017 O
. O

An O

1591empirical O
analysis O
of O
nmt O
- O
derived O
interlingual O
em- O
beddings O
and O
their O
use O
in O
parallel O
sentence O
identiÔ¨Å- O
cation O
. O

IEEE O
Journal O
of O
Selected O
Topics O
in O
Signal O
Processing O
, O
11(8):1340‚Äì1350 O
. O

Juri O
Ganitkevitch O
, O
Benjamin O
Van O
Durme O
, O
and O
Chris O
Callison O
- O
Burch O
. O

2013 O
. O

PPDB O
: O
The O
Paraphrase O
Database O
. O

In O
Proceedings O
of O
HLT O
- O
NAACL O
. O

Junxian O
He O
, O
Daniel O
Spokoyny O
, O
Graham O
Neubig O
, O
and O
Taylor O
Berg O
- O
Kirkpatrick O
. O

2019 O
. O

Lagging O
inference O
networks O
and O
posterior O
collapse O
in O
variational O
au- O
toencoders O
. O

arXiv O
preprint O
arXiv:1901.05534 O
. O

Felix O
Hill O
, O
Kyunghyun O
Cho O
, O
and O
Anna O
Korhonen O
. O

2016 O
. O

Learning O
distributed O
representations O
of O
sentences O
from O
unlabelled O
data O
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
. O

Sepp O
Hochreiter O
and O
J O
¬®urgen O
Schmidhuber O
. O

1997 O
. O

Long O
short O
- O
term O
memory O
. O

Neural O
computation O
, O
9(8 O
) O
. O

Diederik O
Kingma O
and O
Jimmy O
Ba O
. O
2014 O
. O

Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O

arXiv O
preprint O
arXiv:1412.6980 O
. O

Diederik O
P O
Kingma O
and O
Max O
Welling O
. O

2013 O
. O

Auto- O
encoding O
variational O
bayes O
. O

arXiv O
preprint O
arXiv:1312.6114 O
. O

Ryan O
Kiros O
, O
Yukun O
Zhu O
, O
Ruslan O
R O
Salakhutdinov O
, O
Richard O
Zemel O
, O
Raquel O
Urtasun O
, O
Antonio O
Torralba O
, O
and O
Sanja O
Fidler O
. O

2015 O
. O

Skip O
- O
thought O
vectors O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
28 O
, O
pages O
3294‚Äì3302 O
. O

Taku O
Kudo O
and O
John O
Richardson O
. O

2018 O
. O

Sentencepiece O
: O

A O
simple O
and O
language O
independent O
subword O
tok- O
enizer O
and O
detokenizer O
for O
neural O
text O
processing O
. O

arXiv O
preprint O
arXiv:1808.06226 O
. O

Xuezhe O
Ma O
, O
Chunting O
Zhou O
, O
Xian O
Li O
, O
Graham O
Neu- O
big O
, O
and O
Eduard O
Hovy O
. O

2019 O
. O

FlowSeq O
: O
Non- O
autoregressive O
conditional O
sequence O
generation O
with O
generative O
Ô¨Çow O
. O

In O
Proceedings O
of O
the O
2019 O
Con- O
ference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Confer- O
ence O
on O
Natural O
Language O
Processing O
( O
EMNLP- O
IJCNLP O
) O
, O
pages O
4273‚Äì4283 O
, O
Hong O
Kong O
, O
China O
. O

As- O
sociation O
for O
Computational O
Linguistics O
. O

Tomas O
Mikolov O
, O
Ilya O
Sutskever O
, O
Kai O
Chen O
, O
Greg O
S. O
Cor- O
rado O
, O
and O
Jeff O
Dean O
. O

2013 O
. O

Distributed O
representa- O
tions O
of O
words O
and O
phrases O
and O
their O
compositional- O
ity O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
. O

Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christopher O
D. O
Manning O
. O

2014 O
. O

Glove O
: O
Global O
vectors O
for O
word O
rep- O

resentation O
. O

Proceedings O
of O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
2014 O
) O
.Gabriel O

Pereyra O
, O
George O
Tucker O
, O
Jan O
Chorowski O
, O
≈Åukasz O
Kaiser O
, O
and O
Geoffrey O
Hinton O
. O
2017 O
. O

Regular- O
izing O
neural O
networks O
by O
penalizing O
conÔ¨Ådent O
output O
distributions O
. O

arXiv O
preprint O
arXiv:1701.06548 O
. O

Matthew O
E O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O

2018 O
. O

Deep O
contextualized O
word O
rep- O
resentations O
. O

In O
Proceedings O
of O
NAACL O
- O
HLT O
, O
pages O
2227‚Äì2237 O
. O

Martin O
Popel O
and O
Ond O
Àárej O
Bojar O
. O

2018 O
. O

Training O
tips O
for O
the O
transformer O
model O
. O

The O
Prague O
Bulletin O
of O
Mathematical O
Linguistics O
, O
110(1):43‚Äì70 O
. O

Nils O
Reimers O
and O
Iryna O
Gurevych O
. O

2019 O
. O

Sentence- O
bert O
: O
Sentence O
embeddings O
using O
siamese O
bert- O
networks O
. O

arXiv O
preprint O
arXiv:1908.10084 O
. O

Holger O
Schwenk O
. O

2018 O
. O

Filtering O
and O
mining O
parallel O
data O
in O
a O
joint O
multilingual O
space O
. O

arXiv O
preprint O
arXiv:1805.09822 O
. O

Holger O
Schwenk O
and O
Matthijs O
Douze O
. O
2017 O
. O

Learn- O
ing O
joint O
multilingual O
sentence O
representations O
with O
neural O
machine O
translation O
. O

arXiv O
preprint O
arXiv:1704.04154 O
. O

Sandeep O
Subramanian O
, O
Adam O
Trischler O
, O
Yoshua O
Ben- O
gio O
, O
and O
Christopher O
J O
Pal O
. O

2018 O
. O

Learning O
gen- O
eral O
purpose O
distributed O
sentence O
representations O
via O
large O
scale O
multi O
- O
task O
learning O
. O

arXiv O
preprint O
arXiv:1804.00079 O
. O

Christian O
Szegedy O
, O
Vincent O
Vanhoucke O
, O
Sergey O
Ioffe O
, O
Jon O
Shlens O
, O
and O
Zbigniew O
Wojna O
. O

2016 O
. O

Rethinking O
the O
inception O
architecture O
for O
computer O
vision O
. O

In O
Proceedings O
of O
the O
IEEE O
conference O
on O
computer O
vi- O
sion O
and O
pattern O
recognition O
, O
pages O
2818‚Äì2826 O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
≈Åukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
Advances O
in O
Neural O
Information O
Pro- O
cessing O
Systems O
, O
pages O
5998‚Äì6008 O
. O

John O
Wieting O
, O
Mohit O
Bansal O
, O
Kevin O
Gimpel O
, O
and O
Karen O
Livescu O
. O

2016a O
. O

Charagram O
: O
Embedding O
words O
and O
sentences O
via O
character O
n O
- O
grams O
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natu- O
ral O
Language O
Processing O
, O
pages O
1504‚Äì1515 O
. O

John O
Wieting O
, O
Mohit O
Bansal O
, O
Kevin O
Gimpel O
, O
and O
Karen O
Livescu O
. O

2016b O
. O

Towards O
universal O
paraphrastic O
sen- O
tence O
embeddings O
. O

In O
Proceedings O
of O
the O
Interna- O
tional O
Conference O
on O
Learning O
Representations O
. O

John O
Wieting O
, O
Taylor O
Berg O
- O
Kirkpatrick O
, O
Kevin O
Gimpel O
, O
and O
Graham O
Neubig O
. O
2019a O
. O

Beyond O
bleu O
: O
Train- O
ing O
neural O
machine O
translation O
with O
semantic O
sim- O
ilarity O
. O

In O
Proceedings O
of O
the O
57th O
Annual O
Meet- O
ing O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
4344‚Äì4355 O
. O

1592John O
Wieting O
and O
Kevin O
Gimpel O
. O
2017 O
. O

Revisiting O
re- O
current O
networks O
for O
paraphrastic O
sentence O

embed- O
dings O
. O

In O
Proceedings O
of O
the O
55th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Vol- O
ume O
1 O
: O
Long O
Papers O
) O
, O
pages O
2078‚Äì2088 O
, O
Vancouver O
, O
Canada O
. O

John O
Wieting O
and O
Kevin O
Gimpel O
. O

2018 O
. O

ParaNMT- O
50 O
M O
: O
Pushing O
the O
limits O
of O
paraphrastic O
sentence O
em- O
beddings O
with O
millions O
of O
machine O
translations O
. O

In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
As- O
sociation O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
451‚Äì462 O
. O

Association O
for O
Com- O
putational O
Linguistics O
. O

John O
Wieting O
, O
Kevin O
Gimpel O
, O
Graham O
Neubig O
, O
and O
Taylor O
Berg O
- O
Kirkpatrick O
. O

2019b O
. O

Simple O
and O
effec- O
tive O
paraphrastic O
similarity O
from O
parallel O
translations O
. O

Proceedings O
of O
the O
ACL O
. O

Yinfei O
Yang O
, O
Daniel O
Cer O
, O
Amin O
Ahmad O
, O
Mandy O
Guo O
, O
Jax O
Law O
, O
Noah O
Constant O
, O
Gustavo O
Hernandez O
Abrego O
, O
Steve O
Yuan O
, O
Chris O
Tar O
, O
Yun O
- O
hsuan O
Sung O
, O
Brian O
Strope O
, O
and O
Ray O
Kurzweil O
. O

2020 O
. O

Multilingual O
universal O
sentence O
encoder O
for O
semantic O
retrieval O
. O

InProceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
System O
Demonstrations O
, O
pages O
87‚Äì94 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Zichao O
Yang O
, O
Zhiting O
Hu O
, O
Ruslan O
Salakhutdinov O
, O
and O
Taylor O
Berg O
- O
Kirkpatrick O
. O
2017 O
. O

Improved O
varia- O
tional O
autoencoders O
for O
text O
modeling O
using O
dilated O
convolutions O
. O

In O
Proceedings O
of O
the O
34th O
Interna- O
tional O
Conference O
on O
Machine O
Learning O
- O
Volume O
70 O
, O
pages O
3881‚Äì3890 O
. O

JMLR O
. O

org O
. O

Zachary O
Ziegler O
and O
Alexander O
Rush O
. O

2019 O
. O

Latent O
normalizing O
Ô¨Çows O
for O
discrete O
sequences O
. O

In O
Inter- O
national O
Conference O
on O
Machine O
Learning O
, O
pages O
7673‚Äì7682 O
. O

Pierre O
Zweigenbaum O
, O
Serge O
Sharoff O
, O
and O
Reinhard O
Rapp O
. O

2018 O
. O

Overview O
of O
the O
third O
bucc O
shared O
task O
: O

Spotting O
parallel O
sentences O
in O
comparable O
corpora O
. O

InProceedings O
of O
11th O
Workshop O
on O
Building O
and O
Using O
Comparable O
Corpora O
, O
pages O
39‚Äì42.- O

1593A O
Location O
of O
Sentence O
Embedding O
in O
Decoder O
for O
Learning O
Representations O
As O
mentioned O
in O
Section O
2 O
, O
we O
experimented O
with O
4 O
ways O
to O
incorporate O
the O
sentence O
embedding O
into O
the O
decoder O
: O
Word O
, O
Hidden O
, O
Attention O
, O
and O
Logit O
. O

We O
also O
experimented O
with O
combinations O
of O
these O
4 O
approaches O
. O

We O
evaluate O
these O
embeddings O
on O
the O
STS O
tasks O
and O
show O
the O
results O
, O
along O
with O
the O
time O
to O
train O
the O
models O
1 O
epoch O
in O
Table O
5 O
. O

For O
these O
experiments O
, O
we O
train O
a O
single O
layer O
bidirectional O
LSTM O
( O
BiLSTM O
) O
ENGLISH O
TRANS O
model O
with O
embedding O
size O
set O
to O
1024 O
and O
hidden O
states O
set O
to O
512 O
dimensions O
( O
in O
order O
to O
be O
roughly O
equivalent O
to O
our O
Transformer O
models O
) O
. O

To O
form O
the O
sentence O
embedding O
in O
this O
variant O
, O
we O
mean O
pool O
the O
hidden O
states O
for O
each O
time O
step O
. O

The O
cell O
states O
of O
the O
decoder O
are O
initialized O
to O
the O
zero O
vector O
. O

Architecture O
STS O
Time O
( O
s O
) O
BiLSTM O
( O
Hidden O
) O
54.3 O
1226 O
BiLSTM O
( O
Word O
) O
67.2 O
1341 O
BiLSTM O
( O
Attention O
) O
68.8 O
1481 O
BiLSTM O
( O
Logit O
) O
69.4 O
1603 O
BiLSTM O
( O
Wd O
. O

+ O

Hd O
. O
) O
67.3 O
1377 O
BiLSTM O
( O
Wd O
. O

+ O

Hd O
. O

+ O

Att O
. O
) O

68.3 O
1669 O
BiLSTM O
( O
Wd O
. O

+ O

Hd O
. O

+ O
Log O
. O
) O

69.1 O
1655 O
BiLSTM O
( O
Wd O
. O

+ O

Hd O
. O

+ O
Att O
. O

+ O
Log O
. O
) O

68.9 O
1856 O
Table O
5 O
: O
Results O
for O
different O
ways O
of O
incorporating O
the O
sentence O
embedding O
in O
the O
decoder O
for O
a O
BiLSTM O
on O
the O
Semantic O
Textual O
Similarity O
( O
STS O
) O
datasets O
, O
along O
with O
the O
time O
taken O
to O
train O
the O
model O
for O
1 O
epoch O
. O

Per- O
formance O
is O
measured O
in O
Pearson O
‚Äôs O
r100 O
. O

From O
this O
analysis O
, O
we O
see O
that O
the O
best O
perfor- O
mance O
is O
achieved O
with O
Logit O
, O
when O
the O
sentence O
embedding O
is O
place O
just O
prior O
to O
the O
softmax O
. O

The O
performance O
is O
much O
better O
than O
Hidden O
or O
Hid- O
den+Word O
used O
in O
prior O
work O
. O

For O
instance O
, O
re- O
cently O
( O
Artetxe O
and O
Schwenk O
, O
2018 O
) O
used O
the O
Hid- O
den+Word O
strategy O
in O
learning O
multilingual O
sen- O
tence O
embeddings O
. O

A.1 O
VAE O
Training O
We O
also O
found O
that O
incorporating O
the O
latent O
code O
of O
a O
V O
AE O
into O
the O
decoder O
using O
the O
Logit O
strategy O
increases O
the O
mutual O
information O
while O
having O
little O
effect O
on O
the O
log O
likelihood O
. O

We O
trained O
two O
LSTM O
V O
AE O
models O
following O
the O
settings O
and O
aggressive O
training O
strategy O
in O
( O
He O
et O
al O
. O
, O
2019 O
) O
, O
where O
one O
LSTM O
model O
used O
the O
Hidden O
strategy O
and O
the O
other O
used O
the O
Hidden O
+ O
Logit O
strategy O
. O

We O
trained O
the O
models O
on O
the O
enside O
of O
our O
en O
- O
fr O
data O
. O

We O
found O
that O
the O
mutual O
information O
increased O
form O
0.89 O
to O
2.46 O
, O
while O
the O
approximate O
negativelog O
likelihood O
, O
estimated O
by O
importance O
weighting O
, O
increased O
slightly O
from O
53.3 O
to O
54.0 O
when O
using O
Logit O
. O

B O
Relationship O
Between O
Batch O
Size O
and O
Performance O
for O
Transformer O
and O
LSTM O
0 O
50000 O
100000 O
150000 O
200000 O
250000 O
300000 O
Batch O
size O
( O
Max O
Tokens O
Per O
Batch)0.500.550.600.650.70Avg O
. O

STS O
Performance O
Performace O
Vs O
. O

Batch O
Size O
Transformer O
LSTM O
Figure O
5 O
: O
The O
relationship O
between O
average O
perfor- O
mance O
for O
each O
year O
of O
the O
STS O
tasks O
2012 O
- O
2016 O
( O
Pear- O
son‚Äôsr100 O
) O
and O
batch O
size O
( O
maximum O
number O
of O
words O
per O
batch O
) O
. O

It O
has O
been O
observed O
previously O
that O
the O
per- O
formance O
of O
Transformer O
models O
is O
sensitive O
to O
batch O
size O
( O
Popel O
and O
Bojar O
, O
2018 O
) O
. O

We O
found O
this O
to O
be O
especially O
true O
when O
training O
sequence- O
to O
- O
sequence O
models O
to O
learn O
sentence O
embeddings O
. O

Figure O
5 O
shows O
plots O
of O
the O
average O
2012 O
- O
2016 O
STS O
performance O
of O
the O
learned O
sentence O
embedding O
as O
batch O
size O
increases O
for O
both O
the O
BiLSTM O
and O
Transformer O
. O

Initially O
, O
at O
a O
batch O
size O
of O
2500 O
to- O
kens O
, O
sentence O
embeddings O
learned O
are O
worse O
than O
random O
, O
even O
though O
validation O
perplexity O
does O
decrease O
during O
this O
time O
. O

Performance O
rises O
as O
batch O
size O
increases O
up O
to O
around O
100,000 O
tokens O
. O

In O
contrast O
, O
the O
BiLSTM O
is O
more O
robust O
to O
batch O
size O
, O
peaking O
much O
earlier O
around O
25,000 O
tokens O
, O
and O
even O
degrading O
at O
higher O
batch O
sizes O
. O

C O
Model O
Ablations O

In O
this O
section O
, O
we O
vary O
the O
number O
of O
layers O
in O
the O
encoder O
and O
decoder O
in O
BGT O
W O
/ O
OPRIOR O
. O

We O
see O
that O
performance O
increases O
as O
the O
number O
of O
encoder O
layers O
increases O
, O
and O
also O
that O
a O
large O
de- O
coder O
hurts O
performance O
, O
allowing O
us O
to O
save O
train- O
ing O
time O
by O
using O
a O
single O
layer O
. O

These O
results O
can O
be O
compared O
to O
those O
in O
Table O
7 O
showing O
that O

1594Data O
Split O
n O
BGT O
SP O
All O
13,023 O
75.3 O
74.1 O
Negation O
705 O
73.1 O
68.7 O
Bottom O
20 O
% O
SWER O
, O
label O
2[0;2 O
] O
404 O
63.6 O
54.9 O
Bottom O
10 O
% O
SWER O
, O
label O
2[0;1 O
] O
72 O
47.1 O
22.5 O
Top O
20 O
% O
SWER O
, O
label O
2[3;5 O
] O
937 O
20.0 O
14.4 O
Top O
10 O
% O
SWER O
, O
label O
2[4;5 O
] O
159 O
18.1 O
10.8 O
Top O
20 O
% O
SWER O
, O
label O
2[0;2 O
] O
1380 O
51.5 O
49.9 O
Bottom O
20 O
% O
SWER O
, O
label O
2[3;5 O
] O
2079 O
43.0 O
42.2 O
Table O
6 O
: O
Performance O
, O
measured O
in O
Pearson O
‚Äôs O
r100 O
, O
for O
different O
data O
splits O
of O
the O
STS O
data O
. O

The O
Ô¨Årst O
row O
shows O
performance O
across O
all O
unique O
examples O
, O
the O
next O
row O
shows O
the O
negation O
split O
, O
and O
the O
last O
four O
rows O
show O
difÔ¨Åcult O
examples O
Ô¨Åltered O
symmetric O
word O
error O
rate O
( O
SWER O
) O
. O

The O
last O
two O
rows O
show O
relatively O
easy O
examples O
according O
to O
SWER O
. O

Architecture O
STS O
Time O
( O
s O
) O
Transformer O
( O
5L/1L O
) O
70.3 O
1767 O
Transformer O
( O
3L/1L O
) O
70.1 O
1548 O
Transformer O
( O
1L/1L O
) O
70.0 O
1244 O
Transformer O
( O
5L/5L O
) O
69.8 O
2799 O
Table O
7 O
: O
Results O
on O
the O
Semantic O
Textual O
Similarity O
( O
STS O
) O
datasets O
for O
different O
conÔ¨Ågurations O
of O
E O
NGLISH O
- O
TRANS O
, O
along O
with O
the O
time O
taken O
to O
train O
the O
model O
for O
1 O
epoch O
. O

( O
XL O
/ O
YL O
) O
means O
X O
layers O
were O
used O
in O
the O
encoder O
and O
Y O
layers O
in O
the O
decoder O
. O

Performance O
is O
measured O
in O
Pearson O
‚Äôs O
r100 O
. O

Transformers O
outperform O
BiLSTMS O
in O
these O
exper- O
iments O
. O

D O
Hard O
STS O

We O
show O
further O
difÔ¨Åcult O
splits O
in O
Table O
6 O
, O
including O
a O
negation O
split O
, O
beyond O
those O
used O
in O
Hard O
STS O
and O
compare O
the O
top O
two O
performing O
models O
in O
the O
STS O
task O
from O
Table O
3 O
. O

We O
also O
show O
easier O
splits O
in O
the O
bottom O
of O
the O
table O
. O

