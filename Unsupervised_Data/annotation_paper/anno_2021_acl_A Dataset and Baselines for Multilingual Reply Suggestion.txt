Proceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
1207–1220 O
August O
1–6 O
, O
2021 O
. O

© O
2021 O
Association O
for O
Computational O
Linguistics1207A O
Dataset O
and O
Baselines O
for O
Multilingual O
Reply O
Suggestion O
Mozhi O
Zhang O

University O
of O
Maryland O
mozhi@cs.umd.eduWei O

Wangy O
Qualtrics O
wwang@qualtrics.comBudhaditya O

Deb O
Microsoft O
AI O
budeb@microsoft.com O

Guoqing O
Zheng O
Microsoft O
Research O
zheng@microsoft.comMilad O
Shokouhi O
Microsoft O
AI O
milads@microsoft.comAhmed O

Hassan O
Awadallah O
Microsoft O
Research O
hassanam@microsoft.com O

Abstract O
Reply O
suggestion O
models O
help O
users O
process O
emails O
and O
chats O
faster O
. O

Previous O
work O
only O
studies O
English O
reply O
suggestion O
. O

Instead O
, O
we O
present O
MRS O
, O
a O
multilingual O
reply O
suggestion O
dataset O
with O
ten O
languages O
. O

MRS O
can O
be O
used O
to O
compare O
two O
families O
of O
models O
: O
1 O
) O
re- O

trieval O
models O
that O
select O
the O
reply O
from O
a O
ﬁxed O
set O
and O
2 O
) O
generation O
models O
that O
produce O
the O
reply O
from O
scratch O
. O

Therefore O
, O
MRS O
com- O
plements O
existing O
cross O
- O
lingual O
generalization O
benchmarks O
that O
focus O
on O
classiﬁcation O
and O
sequence O
labeling O
tasks O
. O

We O
build O
a O
gener- O
ation O
model O
and O
a O
retrieval O
model O
as O
base- O
lines O
for O
MRS O
. O

The O
two O
models O
have O
differ- O
ent O
strengths O
in O
the O
monolingual O
setting O
, O
and O
they O
require O
different O
strategies O
to O
generalize O
across O
languages O
. O

MRS O
is O
publicly O
available O
at O
https://github.com/zhangmozhi/mrs O
. O

1 O
Multilingual O
Reply O
Suggestion O
Automated O
reply O
suggestion O
is O
a O
useful O
feature O
for O
email O
and O
chat O
applications O
. O

Given O
an O
input O
mes- O
sage O
, O
the O
system O
suggests O
several O
replies O
, O
and O
users O
may O
click O
on O
them O
to O
save O
typing O
time O
( O
Figure O
1 O
) O
. O

This O
feature O
is O
available O
in O
many O
applications O
in- O
cluding O
Gmail O
, O
Outlook O
, O
LinkedIn O
, O
Facebook O
Mes- O
senger O
, O
Microsoft O
Teams O
, O
and O
Uber O
. O

Reply O
suggestion O
is O
related O
to O
but O
different O
from O
open O
- O
domain O
dialog O
systems O
or O
chatbots O
( O
Adiwar- O
dana O
et O
al O
. O
, O
2020 O
; O
Huang O
et O
al O
. O
, O
2020 O
) O
. O

While O
both O
are O
conversational O
AItasks O
( O
Gao O
et O
al O
. O
, O
2019 O
) O
, O
the O
goals O
are O
different O
: O
reply O
suggestion O
systems O
help O
the O
user O
quickly O
reply O
to O
a O
message O
, O
while O
chatbots O
aim O
to O
continue O
the O
conversation O
and O
focus O
more O
on O
multi O
- O
turn O
dialogues O
. O

Ideally O
, O
we O
want O
our O
model O
to O
generate O
replies O
in O
any O
language O
. O

However O
, O
reply O
suggestion O
models O
require O
large O
training O
sets O
, O
so O
previous O
work O
mostly O
Work O
mostly O
done O
as O
an O
intern O
at O
Microsoft O
Research O
. O

yWork O
done O
at O
Microsoft O
Research O
. O

Figure O
1 O
: O
An O
example O
of O
reply O
suggestion O
system O
. O

User O
can O
click O
on O
the O
suggestions O
for O
a O
quick O
reply O
. O

focuses O
on O
English O
( O
Kannan O
et O
al O
. O
, O
2016 O
; O
Henderson O
et O
al O
. O
, O
2017 O
; O
Deb O
et O
al O
. O
, O
2019 O
) O
. O

To O
investigate O
reply O
suggestion O
for O
other O
languages O
with O
possibly O
lim- O
ited O
data O
, O
we O
build O
a O
multilingual O
dataset O
, O
dubbed O
MRS O
( O
Multilingual O
ReplySuggestion O
) O
. O

From O
pub- O
licly O
available O
Reddit O
threads O
, O
we O
extract O
message- O
reply O
pairs O
, O
response O
sets O
, O
and O
machine O
- O
translated O
examples O
in O
ten O
languages O
( O
Table O
1 O
) O
. O

One O
interesting O
aspect O
of O
the O
reply O
suggestion O
problem O
is O
that O
there O
are O
two O
modeling O
approaches O
. O

Some O
models O
follow O
the O
retrieval O
framework O
and O
select O
the O
reply O
from O
a O
predetermined O
response O
set O
( O
Henderson O
et O
al O
. O
, O
2017 O
) O
. O

Others O
follow O
the O
generation O
framework O
and O
generate O
the O
reply O
from O
scratch O
( O
Kannan O
et O
al O
. O
, O
2016 O
) O
. O

The O
two O
approaches O
have O
different O
advantages O
. O

Generation O
models O
are O
more O
powerful O
because O
they O
are O
not O
constrained O
by O
the O
response O
set O
. O

In O
comparison O
, O
retrieval O
models O
are O
easier O
to O
train O
and O
runs O
faster O
, O
and O
a O
curated O
re- O
sponse O
set O
guarantees O
the O
coherence O
and O
the O
safety O
of O
the O
model O
output O
. O

The O
two O
frameworks O
make O
reply O
suggestion O
an O
interesting O
task O
for O
studying O
cross O
- O
lingual O
general- O
ization O
. O

Most O
cross O
- O
lingual O
generalization O
bench- O
marks O
use O
classiﬁcation O
and O
sequence O
labeling O
tasks O
( O
Tjong O
Kim O
Sang O
, O
2002 O
; O
Nivre O
et O
al O
. O
, O
2016 O
; O
Strassel O
and O
Tracey O
, O
2016 O
; O
Conneau O
et O
al O
. O
, O
2018 O
; O
Schwenk O
and O
Li O
, O
2018 O
; O

Clark O
et O
al O
. O
, O
2020 O
; O
Hu O
et O
al O
. O
, O
2020 O
; O
Lewis O
et O
al O
. O
, O
2020b O
) O
. O

In O
contrast O
, O
reply O
sug- O
gestion O
has O
two O
formulations O
that O
require O
differ- O
ent O
cross O
- O
lingual O
generalization O
strategies O
. O

While O
some O
recent O
work O
explores O
cross O
- O
lingual O
transfer O

1208Language O
Code O
Family O
Examples O
Tokens O
Response O
Set O
English O
EN O
West O
Germanic O
48,750,948 O
1,700,066,696 O
36,997 O
Spanish O
ES O
Romance O
2,325,877 O
195,424,517 O
45,152 O
German O
DE O
West O
Germanic O

1,864,688 O
118,711,662 O
34,747 O
Portuguese O
PT O
Romance O
1,822,594 O
114,642,809 O
45,225 O
French O
FR O
Romance O
1,396,806 O
133,068,740 O
32,350 O
Japanese O
JA O
Japonic O
727,668 O
46,124,966 O
38,817 O
Swedish O
SV O
North O
Germanic O
738,254 O
47,845,497 O
32,165 O
Italian O
IT O
Romance O
736,296 O
58,715,043 O
31,855 O
Dutch O
NL O
West O
Germanic O
638,634 O
43,847,547 O
32,293 O
Russian O
RU O
East O
Slavic O
516,739 O
23,109,295 O
31,475 O
Table O
1 O
: O
Dataset O
statistics O
for O
MRS O
. O

We O
collect O
Reddit O
message O
- O
reply O
pairs O
for O
ten O
language O
. O

For O
each O
language O
, O
we O
use O
80 O
% O
examples O
for O
training O
, O
10 O
% O
for O
validation O
, O
and O
10 O
% O
for O
testing O
. O

We O
then O
create O
response O
sets O
for O
retrieval O
models O
. O

We O
also O
use O
MTto O
translate O
nineteen O
million O
English O
training O
examples O
to O
other O
languages O
. O

learning O
in O
generation O
tasks O
, O
the O
tasks O
are O
extrac- O
tive O
; O
i.e. O
, O
the O
output O
often O
has O
signiﬁcant O
overlap O
with O
the O
input O
. O

These O
tasks O
include O
news O
title O
gen- O
eration O
, O
text O
summarization O
, O
and O
question O
genera- O
tion O
( O
Chi O
et O
al O
. O
, O
2020 O
; O
Liang O
et O
al O
. O
, O
2020 O
; O
Scialom O
et O
al O
. O
, O
2020 O
) O
. O

Reply O
suggestion O
is O
more O
challenging O
because O
the O
reply O
often O
does O
not O
overlap O
with O
the O
message O
( O
Figure O
1 O
) O
, O
so O
the O
model O
needs O
to O
address O
different O
cross O
- O
lingual O
generalization O
challenges O
( O
Section O
5.2 O
) O
. O

We O
build O
two O
baselines O
for O
MRS O
: O
a O
retrieval O
model O
and O
a O
generation O
model O
. O

We O
ﬁrst O
compare O
the O
models O
in O
English O
, O
where O
we O
have O
abundant O
training O
data O
and O
human O
referees O
. O

We O
evaluate O
the O
models O
with O
both O
automatic O
metrics O
and O
hu- O
man O
judgments O
. O

The O
two O
models O
have O
different O
strengths O
. O

The O
generation O
model O
has O
higher O
word O
overlap O
scores O
and O
is O
favored O
by O
humans O
on O
av- O
erage O
, O
but O
inference O
is O
slower O
, O
and O
the O
output O
is O
sometimes O
contradictory O
or O
repetitive O
( O
Holtzman O
et O
al O
. O
, O
2020 O
) O
. O

In O
contrast O
, O
the O
retrieval O
model O
is O
faster O
and O
always O
produces O
coherent O
replies O
, O
but O
the O
replies O
are O
sometimes O
too O
generic O
or O
irrelevant O
due O
to O
the O
ﬁxed O
response O
set O
. O

Next O
, O
we O
test O
models O
in O
other O
languages O
. O

We O
compare O
different O
training O
settings O
and O
investigate O
two O
cross O
- O
lingual O
generalization O
methods O
: O
initial- O
izing O
with O
pre O
- O
trained O
multilingual O
models O
( O
Wu O
and O
Dredze O
, O
2019 O
; O
Conneau O
et O
al O
. O
, O
2020 O
; O
Liang O
et O
al O
. O
, O
2020 O
) O
and O
training O
on O
machine O
- O
translated O
data O
( O
Banea O
et O
al O
. O
, O
2008 O
) O
. O

Interestingly O
, O
the O
two O
models O
prefer O
different O
methods O
: O
multilingual O
pre- O
training O
works O
better O
for O
the O
retrieval O
model O
, O
while O
the O
generation O
model O
prefers O
machine O
translation O
. O

In O
summary O
, O
we O
present O
MRS O
, O
a O
multilingualreply O
suggestion O
dataset O
. O

We O
use O
MRS O
to O
provide O
the O
ﬁrst O
systematic O
comparison O
between O
generation O
and O
retrieval O
models O
for O
reply O
suggestion O
in O
both O
monolingual O
and O
multilingual O
settings O
. O

MRS O
is O
also O
a O
useful O
benchmark O
for O
future O
research O
in O
reply O
suggestion O
and O
cross O
- O
lingual O
generalization O
. O

The O
rest O
of O
the O
paper O
is O
organized O
as O
follows O
. O

Section O
2 O
describes O
the O
data O
collection O
process O
for O
MRS O
. O

Section O
3 O
introduces O
task O
formulations O
, O
exper- O
iment O
settings O
, O
and O
evaluation O
metrics O
. O

Section O
4 O
describes O
the O
baseline O
generation O
and O
retrieval O
mod- O
els O
. O

Section O
5 O
presents O
our O
experiment O
results O
. O

Sec- O
tion O
6 O
discusses O
how O
MRS O
can O
help O
future O
research O
. O

2 O
Dataset O
Construction O
To O
study O
reply O
suggestion O
in O
multiple O
languages O
, O
we O
build O
MRS O
, O
a O
dataset O
with O
message O
- O
reply O
pairs O
based O
on O
Reddit O
comments O
. O

The O
dataset O
is O
available O
athttps://github.com/zhangmozhi/mrs O
. O

We O
download O
Reddit O
comments O
between O
January O
2010 O
and O
December O
2019 O
from O
the O
Pushshift O
Red- O
dit O
dataset O
( O
Baumgartner O
et O
al O
. O
, O
2020).1We O
extract O
message O
- O
reply O
pairs O
from O
each O
thread O
by O
consider- O

ing O
the O
parent O
comment O
as O
an O
input O
message O
and O
the O
response O
to O
the O
comment O
as O
the O
reference O
reply O
. O

We O
remove O
comments O
starting O
with O
[ O
removed O
] O
or O
[ O
deleted O
] O
, O
which O
are O
deleted O
messages O
. O

We O
also O
skip O
comments O
with O
a O
rating O
of O
less O
than O
one O
, O
since O
they O
are O
likely O
to O
contain O
inappropriate O
content O
. O

After O
extracting O
examples O
, O
we O
identify O
their O
lan- O
guages O
with O
fastText O
language O
detector O
( O
Joulin O
et O
al O
. O
, O
2016 O
) O
. O

For O
each O
example O
, O
we O
run O
the O
model O
1https://files.pushshift.io/reddit/ O
comments O

1209on O
the O
concatenation O
of O
the O
message O
and O
the O
reply O
. O

We O
discard O
low O
- O
conﬁdence O
examples O
where O
none O
of O
the O
languages O
has O
a O
score O
higher O
than O
0.7 O
. O

For O
the O
remaining O
examples O
, O
we O
use O
the O
highest O
- O
scoring O
label O
as O
the O
language O
. O

We O
only O
use O
English O
data O
from O
2018 O
because O
English O
data O
is O
abundant O
on O
Reddit O
. O

Non O
- O
English O
examples O
are O
much O
more O
scarce O
, O
so O
we O
use O
data O
from O
the O
last O
ten O
years O
. O

We O
select O
the O
top O
ten O
lan- O
guages O
with O
at O
least O
100 O
K O
examples O
. O

We O
create O
three O
splits O
for O
each O
language O
: O
80 O
% O
examples O
for O
training O
, O
10 O
% O
for O
validation O
, O
and O
10 O
% O
for O
testing O
. O

Table O
1 O
shows O
some O
dataset O
statistics O
. O

MRS O
is O
heavily O
biased O
towards O
English O
. O

We O
have O
more O
than O
48 O
million O
English O
examples O
, O
but O
fewer O
than O
one O
million O
examples O
for O
half O
of O
the O
languages O
. O

This O
gap O
reﬂects O
a O
practical O
challenge O
for O
reply O
suggestion O
— O
we O
do O
not O
have O
enough O
data O
for O
most O
languages O
in O
the O
world O
. O

Nevertheless O
, O
we O
can O
use O
MRS O
to O
test O
models O
in O
different O
multilingual O
settings O
, O
including O
cross O
- O
lingual O
transfer O
learning O
, O
where O
we O
build O
non O
- O
English O
reply O
suggestion O
mod- O
els O
from O
English O
data O
( O
Section O
3.2 O
) O
. O

We O
also O
build O
response O
sets O
and O
ﬁlter O
out O
toxic O
examples O
. O

We O
describe O
these O
steps O
next O
. O

2.1 O
Response O
Set O
We O
build O
a O
response O
set O
of O
30 O
K O
to O
50 O
K O
most O
fre- O
quent O
replies O
for O
each O
language O
, O
which O
are O
used O
in O
the O
retrieval O
model O
. O

We O
want O
the O
response O
set O
to O
cover O
generic O
responses O
, O
so O
we O
select O
replies O
that O
appear O
at O
least O
twenty O
times O
in O
the O
dataset O
. O

This O
simple O
criterion O
works O
well O
for O
English O
, O
but O
the O
set O
is O
too O
small O
for O
other O
languages O
. O

For O
non O
- O
English O
languages O
, O
we O
augment O
the O
response O
set O
by O
trans- O
lating O
the O
English O
response O
set O
to O
other O
languages O
with O
Microsoft O
Translator O
. O

The O
non O
- O
English O
re- O
sponse O
set O
is O
sometimes O
smaller O
than O
the O
English O
set O
, O
because O
different O
English O
responses O
may O
have O
the O
same O
translation O
. O

2.2 O
Filtering O
Toxic O
Examples O
Exchanges O
on O
Reddit O
are O
sometimes O
uncivil O
, O
inap- O
propriate O
, O
or O
even O
abusive O
( O
Massanari O
, O
2017 O
; O
Mo- O
han O
et O
al O
. O
, O
2017 O
) O
. O

We O
try O
to O
ﬁlter O
out O
toxic O
contents O
, O
as O
they O
are O
not O
desirable O
for O
reply O
suggestion O
sys- O
tems O
. O

We O
use O
two O
toxicity O
detection O
models O
. O

First O
, O
we O
use O
an O
in O
- O
house O
multilingual O
model O
. O

The O
model O
is O
initialized O
with O
multilingual O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
, O
MBERT O
) O
and O
ﬁne O
- O
tuned O
on O
a O
mixture O
of O
pro- O
prietary O
and O
public O
datasets O
with O
toxic O
and O
offen O
- O
sive O
language O
labels O
. O

The O
model O
outputs O
a O
score O
from O
zero O
to O
one O
, O
with O
a O
higher O
score O
correspond- O
ing O
to O
a O
higher O
level O
of O
toxicity O
. O

Second O
, O
we O
use O
Perspective O
API2 O
, O
a O
publicly O
available O
model O
. O

Per- O
spective O
APIhas O
limited O
free O
access O
( O
one O
query O
per O
second O
) O
, O
so O
we O
only O
use O
the O
APIon O
the O
English O
validation O
, O
test O
, O
and O
response O
set O
. O

For O
other O
lan- O
guages O
, O
we O
rely O
on O
our O
in O
- O
house O
model O
. O

We O
ﬁlter O
message O
- O
reply O
pairs O
if O
it O
has O
greater O
than O
0.9 O
score O
according O
to O
the O
in O
- O
house O
model O
, O
or O
greater O
than O
0.5 O
score O
according O
to O
Perspective O
API(Gehman O
et O

al O
. O
, O
2020 O
) O
. O

About O
one O
percent O
of O
examples O
are O
ﬁltered O
. O

After O
ﬁltering O
the O
data O
, O
we O
manually O
val- O
idate O
three O
hundred O
random O
examples O
and O
do O
not O
ﬁnd O
any O
toxic O
examples O
, O
which O
conﬁrms O
that O
our O
ﬁlter O
method O
have O
a O
high O
recall O
. O

While O
we O
hope O
the O
ﬁltered O
dataset O
leads O
to O
better O
reply O
suggestion O
models O
, O
existing O
ﬁltering O
meth- O
ods O
are O
not O
perfect O
and O
can O
introduce O
other O
bi- O
ases O
( O
Dixon O
et O
al O
. O
, O
2018 O
; O
Sap O
et O
al O
. O
, O
2019 O
; O
Hutchin- O
son O
et O
al O
. O
, O
2020 O
) O
. O

Therefore O
, O
models O
trained O
on O
allMRS O
data O
may O
still O
have O
undesirable O
behavior O
. O

MRS O
is O
intended O
to O
be O
used O
as O
a O
benchmark O
for O
testing O
cross O
- O
lingual O
generalization O
of O
generation O
and O
retrieval O
models O
. O

The O
dataset O
should O
not O
be O
directly O
used O
in O
production O
systems O
. O

To O
use O
the O
dataset O
in O
practice O
, O
additional O
work O
is O
required O
to O
address O
other O
possible O
biases O
and O
toxic O
or O
inappro- O
priate O
content O
that O
may O
exist O
in O
the O
data O
. O

3 O
Experiment O
Settings O
After O
presenting O
the O
dataset O
, O
we O
explain O
how O
we O
use O
MRS O
to O
compare O
reply O
suggestion O
models O
. O

We O
describe O
the O
two O
frameworks O
for O
reply O
suggestion O
, O
our O
experiment O
settings O
, O
and O
evaluation O
metrics O
. O

3.1 O
Task O
Formulation O
In O
reply O
suggestion O
, O
the O
input O
is O
a O
message O
x O
, O
and O
the O
output O
is O
one O
or O
more O
suggested O
replies O
y. O

In O
practice O
, O
reply O
suggestion O
systems O
can O
choose O
to O
not O
suggest O
any O
replies O
. O

This O
decision O
is O
usually O
made O
by O
a O
separate O
trigger O
model O
( O
Kannan O
et O
al O
. O
, O
2016 O
) O
. O

In O
this O
paper O
, O
we O
focus O
on O
reply O
generation O
, O
so O
we O
assume O
that O
the O
models O
always O
need O
to O
sug- O
gest O
a O
ﬁxed O
number O
of O
replies O
. O

Reply O
suggestion O
can O
be O
formulated O
as O
either O
a O
retrieval O
problem O
or O
ageneration O
problem O
. O

Retrieval O
Model O
. O

A O
retrieval O
model O
selects O
the O
replyyfrom O
a O
ﬁxed O
response O
set O
Y(Section O
2.1 O
) O
. O

2https://www.perspectiveapi.com O

1210Given O
an O
input O
message O
x O
, O
the O
model O
computes O
a O
relevance O
score O
xyfor O
each O
candidate O
reply O
y2 O
Y O
. O

The O
model O
then O
selects O
the O
highest- O
scoring O
replies O
as O
suggestions O
; O
e.g. O
, O
the O
top-1 O
reply O
isarg O
maxy2Yxy O
. O

Generation O
Model O
. O

A O
generation O
model O
gener- O
ates O
the O
reply O
yfrom O
scratch O
. O

Generation O
mod- O

els O
usually O
follow O
the O
sequence O
- O
to O
- O
sequence O
frame- O
work O
( O
Sutskever O
et O
al O
. O
, O
2014 O
, O
SEQ2SEQ O
) O
, O
which O
generates O
ytoken O
by O
token O
. O

Given O
an O
input O
mes- O
sagex= O
( O
x1;x2;;xn)ofntokens O
, O
a O
SEQ2SEQ O
model O
estimates O
the O
probability O
of O
a O
reply O
y= O
( O
y1;y2;;ym)ofmtokens O
as O
following O
: O
p(yjx O
) O

= O
mY O
i=1p(yijx;y O
< O
i O
): O
( O
1 O
) O
The O
model O
computes O
probability O
for O
the O
next O
token O
p(yijx;y O
< O
i)based O
on O
the O
input O
xand O
the O
ﬁrst O
( O
i  O
1)tokens O
of O
the O
output O
y. O

The O
model O
is O
trained O
to O
maximize O
the O
probability O
of O
reference O
replies O
in O
the O
training O
set O
. O

At O
test O
time O
, O
we O
ﬁnd O
the O
top O
replies O
that O
approximately O
maximize O
( O
1)with O
beam O
search O
. O

The O
two O
models O
have O
different O
strengths O
. O

The O
generation O
model O
is O
more O
ﬂexible O
, O
but O
the O
retrieval O
model O
is O
faster O
( O
Henderson O
et O
al O
. O
, O
2017 O
) O
, O
and O
the O
output O
can O
be O
controlled O
by O
curating O
the O
response O
set O
( O
Kannan O
et O
al O
. O
, O
2016 O
) O
. O

We O
compare O
a O
retrieval O
model O
and O
a O
generation O
model O
as O
baselines O
for O
MRS O
. O

To O
our O
knowledge O
, O
we O
are O
the O
ﬁrst O
to O
systematically O
compare O
the O
two O
mod- O
els O
in O
both O
monolingual O
and O
multilingual O
settings O
. O

We O
explain O
our O
training O
settings O
and O
metrics O
next O
. O

3.2 O
Training O
Settings O
For O
each O
language O
in O
MRS O
, O
we O
train O
and O
compare O
models O
in O
four O
settings O
. O

Future O
work O
can O
experi- O
ment O
with O
other O
settings O
( O
discussed O
in O
Section O
6 O
) O
. O

Monolingual O
. O

Here O
, O
we O
simply O
train O
and O
test O
models O
in O
a O
single O
language O
. O

This O
setting O
simu- O
lates O
the O
scenario O
where O
we O
have O
adequate O
training O
data O
for O
the O
target O
language O
. O

Previous O
reply O
sug- O
gestion O
models O
were O
only O
studied O
in O
the O
English O
monolingual O
setting O
. O

Zero O
- O
Shot O
. O

Next O
, O
we O
train O
models O
in O
a O
zero O
- O
shot O
cross O
- O
lingual O
setting O
. O

We O
train O
the O
model O
on O
the O
English O
training O
set O
and O
use O
the O
model O
on O
the O
test O
set O
for O
another O
language O
. O

This O
setting O
simulates O
the O
scenario O
where O
we O
want O
to O
build O
models O
for O
a O
low O
- O
resource O
language O
using O
our O
large O
English O
set O
. O

To O
generalize O
across O
languages O
, O
we O
initialize O
the O
models O
with O
pre O
- O
trained O
multilingual O
models O
( O
de- O
tails O
in O
Section O
4 O
) O
. O

These O
models O
work O
well O
in O
other O
tasks O
( O
Wu O
and O
Dredze O
, O
2019 O
; O
Liang O
et O
al O
. O
, O
2020 O
) O
. O

We O
test O
if O
they O
also O
work O
for O
reply O
suggestion O
, O
as O
different O
tasks O
often O
prefer O
different O
multilingual O
representations O
( O
Zhang O
et O
al O
. O
, O
2020b O
) O
. O

Machine O
Translation O
( O
MT).Another O
strategy O
for O
cross O
- O
lingual O
generalization O
is O
to O
train O
on O
machine O
- O
translated O
data O
( O
Banea O
et O
al O
. O
, O
2008 O
) O
. O

We O
train O
models O
on O
nineteen O
million O
English O
training O
examples O
machine O
- O
translated O
to O
the O
target O
language O
with O
Microsoft O
Translator O
. O

We O
compare O
against O
the O
zero O
- O
shot O
setting O
to O
compare O
the O
two O
cross O
- O
lingual O
generalization O
strategies O
. O

Multilingual O
. O

Finally O
, O
we O
build O
a O
multilingual O
model O
by O
jointly O
training O
on O
the O
ﬁve O
languages O
with O
the O
most O
training O
data O
: O
English O
, O
Spanish O
, O
Ger- O
man O
, O
Portuguese O
, O
and O
French O
. O

We O
oversample O
non- O

English O
training O
data O
to O
have O
the O
same O
number O
of O
training O
examples O
data O
across O
all O
languages O
( O
John- O
son O
et O
al O
. O
, O
2017 O
) O
. O

We O
make O
two O
comparisons O
: O
1 O
) O
for O
the O
ﬁve O
training O
languages O
, O
we O
compare O
against O
the O
monolingual O
setting O
to O
test O
whether O
ﬁtting O
multi- O
ple O
languages O
in O
a O
single O
model O
hurts O
performance O
; O
and O
2 O
) O
for O
other O
languages O
, O
we O
compare O
against O
the O
zero O
- O
shot O
setting O
to O
check O
if O
adding O
more O
training O
languages O
helps O
cross O
- O
lingual O
generalization O
. O

3.3 O
Evaluation O
Metrics O
The O
goal O
of O
reply O
suggestion O
is O
to O
save O
user O
typing O
time O
, O
so O
the O
ideal O
metrics O
are O
click O
- O
through O
rate O
( O
CTR O
) O
, O
how O
often O
the O
user O
chooses O
a O
suggested O
re- O
ply O
, O
and O
time O
reduction O
, O
how O
much O
time O
is O
saved O
by O
clicking O
the O
suggestion O
instead O
of O
typing O
. O

However O
, O
these O
metrics O
require O
deploying O
the O
model O
to O
test O
on O
real O
users O
, O
which O
is O
not O
feasible O
at O
full O
- O
scale O
while O
writing O
this O
paper O
. O

Instead O
, O
we O
focus O
on O
automated O
ofﬂine O
metrics O
that O
can O
guide O
research O
and O
model O
development O
before O
deploying O
production O
systems O
. O

Speciﬁcally O
, O
we O
evaluate O
models O
using O
a O
test O
set O
of O
message O
- O
reply O
pairs O
. O

To O
identify O
a O
good O
metric O
, O
we O
compare O
several O
metrics O
in O
a O
pilot O
study O
by O
deploying O
an O
English O
system O
. O

We O
collect O
millions O
of O
user O
interactions O
and O
measure O
Pearson O
’s O
correlation O
between O
CTR O
and O
automated O
ofﬂine O
metrics O
. O

The O
next O
paragraph O
lists O
the O
metrics O
. O

Based O
on O
the O
study O
, O
we O
recommend O
weighted O
ROUGE O
F1 O
ensemble O
( O
ROUGE O
in O
tables O
) O
, O
which O
has O
the O
highest O
correlation O
with O
CTR O
. O

1211For O
the O
retrieval O
model O
, O
we O
follow O
previous O
work O
and O
consider O
mean O
reciprocal O
rank O
( O
Kannan O
et O
al O
. O
, O
2016 O
, O
MRR O
) O
and O
precision O
at O
one O
( O
Henderson O
et O
al O
. O
, O
2017 O
) O
. O

These O
metrics O
test O
if O
the O
model O
can O
retrieve O
the O
reference O
response O
from O
a O
random O
set O
of O
re- O
sponses O
. O

Alternatively O
, O
we O
compute O
MRR O
and O
pre- O
cision O
on O
a O
subset O
of O
examples O
where O
the O
reference O
reply O
is O
in O
the O
response O
set O
so O
that O
we O
can O
directly O
measure O
the O
rank O
of O
the O
reference O
response O
in O
the O
response O
set O
. O

This O
set O
also O
allows O
us O
to O
compute O
MRR O
for O
individual O
responses O
, O
so O
we O
can O
compute O
macro- O

MRR O
, O
the O
average O
MRR O
over O
each O
response O
in O
the O
set O
. O

Higher O
macro- O

MRR O
can O
indicate O
di- O
versity O
but O
has O
a O
worse O
correlation O
than O
comput- O
ing O
MRR O
over O
the O
entire O
test O
set O
. O

For O
the O
genera- O
tion O
model O
, O
we O
consider O
model O
perplexity O
( O
Adiwar- O
dana O
et O
al O
. O
, O
2020 O
) O
. O

Finally O
, O
we O
consider O
two O
word O
overlap O
scores O
, O
BLEU O
( O
Papineni O
et O

al O
. O
, O
2002 O
) O
and O
ROUGE O
( O
Lin O
, O
2004 O
) O
, O
which O
can O
be O
used O
for O
both O
retrieval O
and O
generation O
models O

. O

Our O
pilot O
study O
shows O
that O
ROUGE O
has O
the O
best O
correlation O
. O

However O
, O
individual O
ROUGE O
F1 O
scores O
( O
ROUGE O
-1/2/3 O
) O
are O
sensitive O
to O
small O
changes O
in O
sequence O
lengths O
( O
more O
so O
because O
our O
responses O
are O
generally O
short O
) O
. O

Therefore O
, O
we O
use O
a O
weighted O
average O
of O
the O
three O
scores O
: O
ROUGE O
-1 O
6+ROUGE O
-2 O
3+ROUGE O
-3 O
2 O
: O
( O
2 O
) O
This O
weighted O
score O
leads O
to O
the O
highest O
correlation O
with O
CTR O
. O

Intuitively O
, O
the O
weights O
balance O
the O
dif- O
ferences O
in O
the O
average O
magnitude O
of O
each O
metric O
and O
thus O
reduce O
variance O
on O
short O
responses O
. O

Popular O
reply O
suggestion O
systems O
( O
such O
as O
Gmail O
and O
Outlook O
) O
suggest O
three O
replies O
for O
each O
mes- O
sage O
, O
while O
the O
user O
only O
selects O
one O
. O

To O
simulate O
this O
setting O
, O
we O
predict O
three O
replies O
for O
each O
mes- O
sage O
. O

For O
the O
retrieval O
model O
, O
we O
use O
the O
three O
highest O
- O
scoring O
replies O
from O
the O
response O
set O
. O

For O
the O
generation O
model O
, O
we O
use O
top O
- O
three O
results O
from O
beam O
search O
. O

Out O
of O
the O
three O
replies O
, O
we O
only O
use O
the O
reply O
with O
the O
highest O
ROUGE O
compared O
to O
the O
reference O
reply O
when O
computing O
the O
ﬁnal O
metrics O
; O
i.e. O
, O
the O
model O
only O
has O
to O
provide O
one O
“ O
correct O
” O
reply O
to O
have O
a O
full O
score O
. O

We O
compare O
models O
primarily O
with O
ROUGE O
, O
since O
the O
metric O
has O
the O
best O
correlation O
in O
the O
pi- O
lot O
study O
. O

Nevertheless O
, O
word O
overlap O
scores O
have O
known O
limitations O
( O
Liu O
et O
al O
. O
, O
2016 O
) O
, O
as O
there O
are O
different O
ways O
to O
reply O
to O
a O
message O
. O

We O
encour- O
age O
future O
research O
to O
investigate O
other O
metrics O
to O
understand O
different O
aspects O
of O
the O
model O
. O

As O
examples O
, O
we O
also O
report O
two O
diversity O
scores O
: O
the O
proportion O
of O
distinct O
unigrams O
( O
Dist-1 O
) O
and O
bigrams O
( O
Dist-2 O
) O
in O
the O
generated O
replies O
( O
Li O
et O
al O
. O
, O
2016 O
) O
. O

While O
ROUGE O
measures O
the O
relevance O
of O
the O
replies O
, O
higher O
diversity O
can O
also O
increase O
CTR(Deb O
et O
al O
. O
, O
2019 O
) O
. O

We O
can O
improve O
the O
diversity O
of O
the O
three O
replies O
with O
diversity O
- O
promoting O
decoding O
( O
Li O
et O
al O
. O
, O
2016 O
; O
Vijayakumar O
et O
al O
. O
, O
2018 O
; O
Zhang O
et O
al O
. O
, O
2018 O
) O
or O
latent O
variable O
models O
( O
Deb O
et O
al O
. O
, O
2019 O
) O
, O
but O
we O
leave O
this O
direction O
to O
future O
work O
. O

For O
our O
English O
monolingual O
experiments O
, O
we O
also O
complement O
automatic O
metrics O
with O
human O
judgments O
( O
Human O
in O
Figure O
2 O
) O
. O

For O
each O
ex- O
ample O
, O
we O
display O
the O
input O
message O
and O
sets O
of O
three O
suggested O
replies O
from O
both O
generation O
and O
retrieval O
models O
to O
three O
human O
annotators O
( O
crowd O
workers O
) O
. O

We O
then O
ask O
the O
annotators O
to O
select O
the O
set O
with O
more O
responses O
that O
they O
prefer O
to O
send O
as O
a O
reply O
. O

We O
leave O
evaluations O
for O
other O
languages O
to O
future O
work O
due O
to O
resource O
limitations O
. O

4 O
Baseline O
Models O
This O
section O
introduces O
the O
two O
baseline O
models O
: O
a O
retrieval O
model O
and O
a O
generation O
model O
. O

4.1 O
Retrieval O
Model O
For O
the O
retrieval O
model O
, O
we O
use O
the O
architecture O
from O
Henderson O
et O
al O
. O

( O
2017 O
) O
, O
except O
we O
replace O
the O
feedforward O
network O
encoders O
with O
Transform- O
ers O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O

Given O
an O
input O
message O
xand O
candidate O
reply O
y O
, O
two O
Transformer O
encoders O
xandymap O
the O
message O
and O
the O
reply O
to O
two O
vectors O
x(x)andy(y O
) O
. O

The O
relevance O
score O
xybetween O
the O
message O
xand O
the O
reply O
yis O
the O
dot O
product O
of O
the O
two O
vectors O
: O
xy= O

 O
x(x)>y(y O
): O
( O
3 O
) O
Henderson O
et O

al O
. O

( O
2017 O
) O
also O
adds O
a O
language O
model O
score O
to O
encourage O
more O
frequent O
replies O
. O

We O
do O
not O
use O
language O
model O
score O
for O
simplicity O
. O

We O
train O
the O
model O
with O
the O
symmetric O
loss O
from O
Deb O
et O
al O
. O

( O
2019 O
) O
. O

Suppose O
the O
batch O
size O
isn O
. O

For O
a O
batch O
of O
training O
messages O
fxign O
i=1and O
corresponding O
replies O
fyign O
j=1 O
, O
we O
maximize O
: O
nX O
i=1exiyi O
Pn O
j=1 O
exiyj+exjyi O
 exiyi:(4 O
) O

In O
a O
regular O
softmax O
loss O
, O
the O
denominator O
only O
sums O
over O
one O
variable O
. O

The O
denominator O
in O
the O

1212 O
ROUGE O
Dist-1 O
Dist-2 O
Human.0543 O
.0341 O
.1608 O

.484 O
.0331 O
.0194 O
.0480.320Generation O

RetrievalFigure O
2 O
: O
Generation O
vs. O
retrieval O
model O
on O
English O
. O

Both O
automatic O
metrics O
and O
human O
referees O
prefer O
the O
generation O
model O
. O

The O
human O
score O
measures O
how O
often O
the O
human O
referee O
prefers O
the O
replies O
from O
one O
model O
over O
another O
. O

The O
human O
score O
does O
not O
add O
up O
to O
one O
, O
because O
the O
referee O
can O
choose O
tie O
. O

Message O
: O

I O
think O
I O
want O
to O
play O
it O
so O
it O
better O
be O
available O
for O
PS4 O
. O

Generation O
: O
I O
’m O
sure O
it O
’ll O
be O
available O
for O
PS4 O
as O
well O
. O

Retrieval O
: O
I O
’d O
play O
it O
. O

Message O
: O
Very O
nice O
! O

What O
model O
is O
this O
? O

Obviously O
new O
Softail O
/ O
Dyna O
, O
but O
which O
? O

Generation O
: O

It O
’s O
a O
Softail O
, O
but O
I O
’m O
not O
sure O
what O
model O
it O
is O
. O

Retrieval O
: O
Delta O
. O

Message O
: O
Interesting O
proposal O
. O

Especially O
since O
they O
just O
announced O
Mixon O
is O
going O
to O
have O
a O
scope O
. O

Generation O
: O
I O
’m O
pretty O
sure O
it O
’s O
going O
to O
have O
a O
scope O
, O
but O
I O
do O
n’t O
think O
it O
’s O
going O
to O
have O
a O
scope O
. O

Retrieval O
: O
Where O
did O
they O
say O
that O
? O

Message O
: O

Oh O
the O
stickers O
! O

I O
envy O
you O
, O
Lush O
did O
n’t O
sent O
me O
any O
with O
my O
body O
lotion O
order O
. O

Generation O
: O
I O
’m O
so O
sorry O
. O

I O
’m O
so O
sorry O
. O

I O
’m O
so O
sorry O
. O

Retrieval O
: O
What O
did O
you O
order O
? O

Table O
2 O
: O
Example O
replies O
suggested O
by O
English O
models O
. O

In O
the O
top O
two O
examples O
, O
the O
generation O
model O
produces O
more O
relevant O
replies O
than O
the O
retrieval O
model O
. O

In O
the O
bottom O
two O
examples O
, O
the O
generation O
model O
produces O
contra- O
dictory O
and O
repetitive O
replies O
, O
while O
the O
retrieval O
model O
selects O
appropriate O
replies O
from O
the O
response O
set O
. O

symmetric O
loss O
sum O
over O
both O
variables O
to O
encour- O
age O
bidirectional O
compatibility O
: O
the O
message O
should O
be O
predictive O
of O
the O
reply O
, O
and O
the O
reply O
should O
be O
predictive O
of O
the O
message O
. O

This O
encourages O
the O
model O
to O
select O
responses O
speciﬁc O
to O
the O
message O
, O
similar O
to O
the O
Maximum O
Mutual O
Information O
objec- O
tive O
from O
Li O
et O

al O
. O
( O
2016 O
) O
. O

The O
two O
encoders O
xandyare O
initialized O
with O
MBERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
a O
Transformer O
with O
110 O
million O
parameters O
pre O
- O
trained O
on O
multilingual O
corpora O
. O

Initializing O
with O
MBERT O
allows O
the O
model O
to O
generalize O
across O
languages O
( O
Wu O
and O
Dredze O
, O
2019 O
) O
. O

In O
Appendix O
A O
, O
we O
experiment O
with O
another O
pre O
- O
trained O
multilingual O
Transformer O
, O
XLM O
-R(Con- O
neau O
et O
al O
. O
, O
2020 O
) O
. O

We O
use O
the O
“ O
base O
” O
version O
with O
270 O
million O
parameters O
. O

4.2 O
Generation O
Model O
For O
the O
generation O
model O
, O
we O
follow O
the O
SEQ2SEQ O
architecture O
( O
Section O
3.1 O
) O
. O

We O
use O
a O
Transformer O
encoder O
to O
read O
the O
input O
x O
, O
and O
another O
Trans- O
former O
decoder O
to O
estimate O
p(yijx;y O
< O
i)in O
( O
1).We O
can O
not O
initialize O
the O
generation O
model O
with O
MBERT O
orXLM O
-R O
, O
because O
the O
model O
also O
has O
a O
decoder O
. O

Instead O
, O
we O
use O
Unicoder- O
XDAE O
( O
Liang O
et O
al O
. O
, O
2020 O
) O
, O
a O
pre O
- O
trained O
multilingual O
SEQ2SEQ O
model O
, O
which O
can O
generalize O
across O
languages O
in O
extractive O
generation O
tasks O
such O
as O
news O
title O
gener- O
ation O
and O
question O
generation O
. O

We O
test O
if O
Unicoder- O
XDAE O
also O
generalizes O
in O
the O
more O
challenging O
re- O
ply O
suggestion O
task O
. O

There O
are O
other O
generation O
models O
we O
can O
use O
, O
which O
we O
discuss O
as O
future O
work O
in O
Section O
6 O
. O

4.3 O
Training O
Details O
We O
train O
the O
retrieval O
model O
using O
Adam O
opti- O
mizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
1e-6 O
learning O
rate O
, O
default O

, O
and O
256 O
batch O
size O
. O

For O
monolin- O
gual O
and O
zero O
- O
shot O
settings O
, O
we O
use O
twenty O
epochs O
for O
English O
and O
ﬁfty O
epochs O
for O
other O
languages O
. O

We O
use O
ten O
epochs O
for O
MTand O
multilingual O
set- O
tings O
. O

The O
ﬁrst O
1 O
% O
training O
steps O
are O
warmup O
steps O
. O

During O
training O
, O
we O
freeze O
the O
embedding O
layers O
and O
the O
bottom O
two O
Transformer O
layers O
of O
both O
en- O

1213Monolingual O
Zero O
- O
Shot O
MT O
Multilingual O
ROUGE O
Dist-1 O
Dist-2 O
ROUGE O
Dist-1 O
Dist-2 O
ROUGE O
Dist-1 O
Dist-2 O
ROUGE O
Dist-1 O
Dist-2 O
EN O
.0331 O
.0194 O
.0480 O
.0331 O
.0194 O
.0480 O
- O
- O
- O
.0265 O
.0158 O
.0376 O
ES O
.0187 O

.0157 O
.0353 O

.0156 O
.0113 O

.0271 O
.0139 O

.0164 O
.0350 O
.0181 O
.0151 O
.0333 O

DE O
.0215 O
.0134 O
.0298 O

.0178 O
.0098 O

.0240 O
.0141 O

.0152 O
.0333 O
.0190 O

.0140 O

.0314 O
PT O
.0509 O
.0158 O
.0393 O
.0115 O
.0121 O

.0323 O
.0110 O
.0184 O

.0449 O
.0460 O
.0161 O
.0401 O

FR O
.0216 O
.0191 O
.0468 O
.0168 O
.0133 O
.0343 O
.0166 O
.0196 O

.0461 O
.0212 O
.0169 O
.0411 O

JA O
.0311 O
.0220 O
.0540 O
.0213 O

.0236 O
.0250 O
.0153 O
.1031 O
.0444 O
.0144 O

.0677 O
.0286 O

IT O
.0200 O
.0357 O
.0768 O
.0172 O
.0246 O
.0576 O

.0150 O
.0378 O
.0811 O
.0171 O
.0278 O
.0614 O

SV O
.0188 O
.0287 O
.0658 O

.0168 O
.0203 O

.0506 O
.0176 O
.0302 O

.0677 O
.0169 O

.0224 O
.0518 O

NL O
.0184 O
.0316 O

.0766 O
.0167 O
.0199 O
.0533 O
.0169 O

.0297 O
.0710 O
.0170 O

.0221 O
.0551 O

RU O
.0142 O
.0486 O

.0946 O
.0138 O
.0298 O
.0604 O
.0130 O

.0431 O
.0804 O
.0246 O
.0405 O
.0761 O

Table O
3 O
: O
Results O
for O
retrieval O
model O
initialized O
with O
MBERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

The O
settings O
are O
in O
Section O
3.2 O
. O

Gray O
cells O
indicate O
when O
the O
model O
is O
trained O
on O
the O
target O
language O
training O
set O
. O

White O
cells O
indicate O
cross- O
lingual O
settings O
where O
the O
target O
language O
training O
set O
is O
not O
used O
for O
training O
. O

For O
each O
language O
, O
we O
boldface O
the O
best O
ROUGE O
scores O
in O
cross O
- O
lingual O
settings O
( O
white O
cells O
) O
. O

The O
zero O
- O
shot O
setting O
has O
better O
ROUGE O
scores O
than O
using O
MTdata O
for O
most O
languages O
, O
and O
the O
results O
are O
sometimes O
close O
to O
monolingual O
training O
, O
conﬁrming O
the O
effectiveness O
of O
MBERT O
. O

Multilingual O
training O
hurts O
training O
languages O
( O
gray O
cells O
compared O
to O
monolingual O
) O
but O
sometimes O
improves O
cross O
- O
lingual O
generalization O
( O
white O
cells O
compared O
to O
zero O
- O
shot O
) O
. O

coders O
, O
which O
preserves O
multilingual O
knowledge O
from O
the O
pre O
- O
trained O
model O
and O
improves O
cross- O
lingual O
transfer O
learning O
( O
Wu O
and O
Dredze O
, O
2019 O
) O
. O

All O
hyperparameters O
are O
manually O
tuned O
on O
the O
En- O
glish O
validation O
set O
. O

We O
use O
almost O
the O
same O
hyperparameters O
as O
Liang O
et O
al O
. O

( O
2020 O
) O
to O
train O
generation O
models O
. O

Speciﬁcally O
, O
we O
use O
Adam O
optimizer O
with O
1e-5 O
initial O
learning O
rate O
, O
default O

, O
and O
1024 O
batch O
size O
. O

For O
the O
monolingual O
and O
zero O
- O
shot O
setting O
, O
we O
use O
four O
epochs O
for O
English O
and O
5000 O
steps O
for O
other O
languages O
( O
equivalent O
to O
two O
to O
nine O
epochs O
de- O
pending O
on O
the O
language O
) O
. O

We O
use O
one O
epoch O
for O
theMTsetting O
and O
40,000 O
steps O
for O
the O
multilingual O
setting O
. O

The O
ﬁrst O
20 O
% O
training O
steps O
are O
warmup O
steps O
. O

We O
freeze O
the O
embedding O
layer O
during O
train- O
ing O
for O
faster O
training O
. O

All O
models O
are O
trained O
with O
eight O
Tesla O
V100 O
GPU O
. O

It O
takes O
about O
an O
hour O
to O
train O
the O
generation O
model O
for O
1000 O
steps O
( O
covering O
about O
one O
million O
examples O
) O
. O

For O
the O
retrieval O
model O
, O
an O
epoch O
on O
the O
English O
training O
set O
( O
about O
48 O
million O
examples O
) O
takes O
about O
seven O
hours O
. O

5 O
Results O
and O
Discussion O
We O
experiment O
with O
the O
two O
baselines O
from O
Sec- O
tion O
4 O
on O
MRS O
. O

We O
ﬁrst O
compare O
the O
models O
in O
English O
, O
where O
we O
have O
enough O
training O
data O
and O
human O
referees O
. O

We O
then O
build O
models O
for O
otherlanguages O
and O
compare O
training O
settings O
listed O
in O
Section O
3.2 O
. O

5.1 O
Results O
on O
English O
Figure O
2 O
compares O
the O
generation O
and O
retrieval O
mod- O
els O
in O
the O
English O
monolingual O
setting O
. O

Generation O
model O
not O
only O
has O
higher O
relevance O
( O
ROUGE O
) O
score O
but O
also O
can O
generate O
more O
diverse O
replies O
( O
higher O
DIST O
scores O
) O
. O

For O
English O
, O
we O
also O
ask O
three O
human O
referees O
to O
compare O
the O
model O
outputs O
on O
a O
subset O
of O
500 O
test O
examples O
. O

Again O
, O
the O
referees O
prefer O
the O
generation O
model O
more O
often O
than O
the O
retrieval O
model O
( O
Figure O
2 O
) O
. O

We O
look O
at O
some O
generated O
responses O
to O
under- O
stand O
the O
models O
qualitatively O
. O

In O
the O
top O
two O
ex- O
amples O
in O
Table O
2 O
, O
the O
generation O
model O
produces O
replies O
highly O
speciﬁc O
to O
the O
input O
message O
. O

In O
contrast O
, O
the O
retrieval O
model O
fails O
to O
ﬁnd O
a O
relevant O
reply O
, O
because O
the O
response O
set O
does O
not O
cover O
these O
topics O
. O

This O
explains O
why O
the O
generation O
model O
has O
much O
higher O
ROUGE O
and O
distinct O
n O
- O
gram O
scores O
than O
the O
retrieval O
model O
. O

However O
, O
the O
expressiveness O
comes O
at O
the O
cost O
of O
a O
lack O
of O
control O
over O
the O
generated O
replies O
. O

The O
generation O
model O
sometimes O
produces O
incoherent O
replies O
that O
are O
repetitive O
and/or O
contradictory O
, O
as O
shown O
in O
the O
bottom O
two O
examples O
of O
Table O
2 O
. O

For O
the O
retrieval O
model O
, O
we O
can O
easily O
avoid O
these O
prob- O
lems O
by O
curating O
the O
ﬁxed O
response O
set O
. O

These O
degenerative O
behaviors O
are O
observed O
in O
other O
text O

1214Monolingual O
MT O
Multilingual O
ROUGE O
DIST O
1DIST O
2ROUGE O
DIST O
1DIST O
2ROUGE O
DIST O
1DIST O
2 O
EN O
.0543 O
.0341 O

.161 O
- O
- O
- O
.0412 O
.0352 O
.175 O
ES O
.0397 O
.0214 O

.182 O
.0270 O
.0261 O

.190 O
.0366 O
.0209 O

.175 O
DE O
.0469 O
.0332 O

.228 O
.0288 O

.0244 O
.142 O
.0454 O

.0321 O
.220 O
PT O
.0566 O
.0209 O

.194 O
.0276 O
.0221 O

.161 O
.0564 O
.0207 O

.190 O
FR O
.0446 O
.0207 O

.174 O
.0271 O
.0165 O

.109 O
.0428 O
.0211 O
.175 O
JA O
.0139 O
.1931 O

.245 O
.0042 O
.2812 O
.216 O
.0114 O
.0954 O

.179 O
IT O
.0493 O
.0322 O

.243 O
.0316 O
.0393 O

.240 O
.0295 O
.0312 O
.222 O
SV O
.0387 O
.0376 O

.236 O
.0369 O

.0359 O

.203 O
.0241 O
.0380 O

.227 O
NL O
.0377 O
.0337 O

.230 O
.0320 O
.0284 O

.162 O
.0233 O
.0334 O
.219 O
RU O
.0286 O
.0825 O

.349 O
.0238 O
.0310 O

.094 O
.0165 O
.0607 O
.224 O
Table O
4 O
: O
Results O
for O
generation O
model O
. O

The O
settings O
are O
in O
Section O
3.2 O
. O

Gray O
cells O
indicate O
when O
the O
model O
is O
trained O
on O
the O
target O
language O
training O
set O
. O

White O
cells O
indicate O
cross O
- O
lingual O
settings O
where O
the O
target O
language O
training O
set O
is O
not O
used O
for O
training O
. O

For O
each O
language O
, O
we O
boldface O
the O
best O
ROUGE O
scores O
in O
cross O
- O
lingual O
settings O
( O
white O
cells O
) O
. O

Despite O
initializing O
with O
Unicoder- O
XDAE O
( O
Liang O
et O
al O
. O
, O
2020 O
) O
, O
the O
model O
fails O
to O
generalize O
across O
languages O
in O
zero O
- O
shot O
settings O
. O

The O
table O
does O
not O
include O
zero O
- O
shot O
results O
because O
the O
model O
only O
produces O
English O
replies O
and O
thus O
has O
near O
- O
zero O
ROUGE O
. O

Multilingual O
training O
hurts O
training O
languages O
( O
gray O
cells O
compared O
to O
monolingual O
) O
, O
but O
the O
model O
can O
now O
generalize O
to O
unseen O
languages O
. O

Training O
on O
MTdata O
is O
the O
best O
cross- O
lingual O
generalization O
method O
for O
the O
generation O
model O
. O

generation O
tasks O

and O
can O
be O
mitigated O
by O
chang- O
ing O
training O
and O
decoding O
objectives O
( O
Holtzman O
et O
al O
. O
, O
2020 O
; O

Welleck O
et O
al O
. O
, O
2020 O
) O
. O

We O
leave O
these O
directions O
for O
future O
research O
. O

5.2 O
Results O
on O
Other O
Languages O
After O
comparing O
English O
models O
, O
we O
experiment O
on O
other O
languages O
using O
the O
settings O
from O
Section O
3.2 O
. O

Retrieval O
Model O
. O

Table O
3 O
shows O
results O
for O
the O
retrieval O
model O
when O
initialized O
with O
MBERT O
. O

The O
retrieval O
model O
can O
generalize O
fairly O
well O
across O
languages O
, O
as O
the O
ROUGE O
in O
the O
zero O
- O
shot O
setting O
is O
often O
close O
to O
the O
monolingual O
setting O
. O

This O
re- O
sult O
conﬁrms O
that O
initializing O
with O
MBERT O
is O
an O
effective O
strategy O
for O
cross O
- O
lingual O
generalization O
. O

Training O
on O
MTdata O
is O
usually O
worse O
than O
training O
in O
the O
zero O
- O
shot O
setting O
. O

This O
is O
possible O
because O
the O
MTsystem O
may O
create O
artifacts O
that O
do O
not O
ap- O
pear O
in O
organic O
data O
( O
Artetxe O
et O
al O
. O
, O
2020 O
) O
. O

For O
the O
multilingual O
model O
, O
the O
training O
language O
ROUGE O
scores O
are O
lower O
than O
monolingual O
training O
( O
gray O
cells O
in O
Table O
3 O
) O
. O

However O
, O
multilingual O
training O
sometimes O
leads O
to O
better O
ROUGE O
on O
unseen O
lan- O
guages O
compared O
to O
transferring O
from O
only O
English O
( O
zero O
- O
shot O
) O
. O

Previous O
work O
observes O
similar O
re- O
sults O
on O
other O
tasks O
, O
where O
multilingual O
training O
hurts O
training O
languages O
but O
helps O
generalization O
to O
unseen O
languages O
( O
Johnson O
et O
al O
. O
, O
2017 O
; O
Con O
- O
neau O
et O
al O
. O
, O
2020 O
; O
Wang O
et O
al O
. O
, O
2020 O
) O
. O

Finally O
, O
Ap- O
pendix O
A O
shows O
similar O
results O
when O
initializing O
with O
XLM O
-R(Conneau O
et O
al O
. O
, O
2020 O
) O
. O

Generation O
Model O
. O

Table O
4 O
shows O
results O
for O
the O
generation O
model O
. O

In O
the O
monolingual O
setting O
, O
the O
generation O
model O
has O
higher O
scores O
than O
the O
re- O
trieval O
model O
on O
most O
languages O
, O
consistent O
with O
the O
English O
result O
( O
Figure O
2 O
) O
. O

However O
, O
unlike O
the O
retrieval O
model O
, O
the O
generation O
model O
fails O
to O
generalize O
across O
languages O
in O
the O
zero O
- O
shot O
set- O
ting O
, O
despite O
using O
Unicoder- O
XDAE O
for O
initializa- O
tion O
. O

We O
do O
not O
show O
zero O
- O
shot O
results O
in O
Table O
4 O
, O
because O
ROUGE O
are O
close O
to O
zero O
for O
non O
- O
English O
languages O
. O

After O
training O
on O
English O
data O
, O
the O
model O
always O
produces O
English O
replies O
, O
regardless O
of O
the O
input O
language O
; O
i.e. O
, O
the O
generation O
model O
“ O
forgets O
” O
multilingual O
knowledge O
acquired O
during O
pre O
- O
training O
( O
Kirkpatrick O
et O
al O
. O
, O
2017 O
) O
. O

This O
result O
is O
surprising O
because O
Unicoder- O
XDAE O
works O
in O
the O
zero O
- O
shot O
setting O
for O
other O
generation O
tasks O
( O
Liang O
et O
al O
. O
, O
2020 O
) O
, O
which O
suggests O
that O
reply O
suggestion O
poses O
unique O
challenges O
for O
cross O
- O
lingual O
transfer O
learning O
. O

Interestingly O
, O
the O
multilingual O
model O
can O
generalize O
to O
unseen O
languages O
; O
perhaps O
training O
on O
multiple O
languages O
regularizes O
the O
model O
to O
pro- O
duce O
replies O
in O
the O
input O
language O
. O

Overall O
, O
the O
best O
method O
to O
generalize O
the O
generation O
model O
across O
languages O
is O
to O
use O
machine O
- O
translated O
data O
. O

12156 O
Future O
Work O
MRS O
opens O
up O
opportunities O
for O
future O
research O
. O

Our O
experiments O
use O
four O
training O
settings O
( O
Sec- O
tion O
3.2 O
) O
, O
but O
there O
are O
many O
other O
settings O
to O
ex- O
plore O
. O

For O
example O
, O
we O
can O
use O
other O
combinations O
of O
training O
languages O
, O
which O
may O
work O
better O
for O
some O
target O
languages O
( O
Ammar O
et O
al O
. O
, O
2016 O
; O
Cot- O
terell O
and O
Heigold O
, O
2017 O
; O
Ahmad O
et O

al O
. O
, O
2019 O
; O
Lin O
et O
al O
. O
, O
2019 O
; O
Zhang O
et O
al O
. O
, O
2020a O
) O
. O

We O
are O
also O
inter- O
ested O
in O
training O
on O
both O
organic O
data O
and O
MTdata O
; O
i.e. O
, O
mixing O
the O
zero O
- O
shot O
and O
MTsetting O
. O

We O
can O
also O
compare O
other O
models O
on O
MRS O
. O

For O
the O
English O
monolingual O
setting O
, O
we O
can O
initialize O
the O
generation O
model O
with O
state O
- O
of O
- O
the O
- O
art O
language O
models O
( O
Radford O
et O
al O
. O
, O
2019 O
; O
Brown O
et O
al O
. O
, O
2020 O
; O
Zhang O
et O
al O
. O
, O
2020c O
) O
. O

For O
cross O
- O
lingual O
settings O
, O
we O
can O
initialize O
the O
generation O
model O
with O
sev- O
eral O
recent O
pre O
- O
trained O
multilingual O
SEQ2SEQmod- O
els O
( O
Chi O
et O
al O
. O
, O
2020 O
, O
2021 O
; O
Liu O
et O
al O
. O
, O
2020 O
; O
Tran O
et O
al O
. O
, O
2020 O
; O
Lewis O
et O
al O
. O
, O
2020a O
; O
Xue O
et O
al O
. O
, O
2020 O
) O
. O

For O
retrieval O
models O
, O
we O
can O
experiment O
with O
other O
multilingual O
encoders O
that O
use O
different O
pre O
- O
training O
tasks O
( O
Artetxe O
and O
Schwenk O
, O
2019 O
; O
Chidambaram O
et O
al O
. O
, O
2019 O
; O
Reimers O
and O
Gurevych O
, O
2020 O
; O
Feng O
et O
al O
. O
, O
2020 O
) O
. O

Another O
idea O
is O
to O
combine O
the O
two O
models O
. O

Given O
an O
input O
message O
, O
we O
ﬁrst O
use O
a O
generation O
model O
to O
create O
a O
set O
of O
candidate O
replies O
. O

We O
then O
use O
a O
retrieval O
model O
to O
compute O
relevance O
scores O
and O
rerank O
these O
candidates O
. O

Reranking O
the O
output O
of O
a O
generation O
model O
helps O
other O
natural O
language O
processing O
tasks O
( O
Shen O
et O
al O
. O
, O
2004 O
; O
Collins O
and O
Koo O
, O
2005 O
; O
Ge O
and O
Mooney O
, O
2006 O
) O
, O
and O
previous O
work O
uses O
a O
similar O
idea O
for O
chatbots O
( O
Qiu O
et O
al O
. O
, O
2017 O
) O
. O

Our O
experiment O
shows O
that O
reply O
suggestion O
poses O
unique O
challenges O
for O
cross O
- O
lingual O
general- O
ization O
, O
especially O
for O
the O
generation O
model O
. O

Future O
work O
can O
study O
methods O
to O
improve O
cross O
- O
lingual O
generalization O
methods O
. O

Some O
examples O
include O
applying O
adversarial O
learning O
( O
Chen O
et O
al O
. O
, O
2018 O
, O
2019 O
; O
Huang O
et O
al O
. O
, O
2019 O
) O
, O
using O
adapters O
( O
Pfeiffer O
et O
al O
. O
, O
2020 O
) O
, O
adaptive O
transfer O
( O
Xia O
et O
al O
. O
, O
2021 O
) O
, O
mixing O
pre O
- O
training O
and O
ﬁne O
- O
tuning O
( O
Phang O
et O
al O
. O
, O
2020 O
) O
, O
and O
bringing O
a O
human O
in O
the O
loop O
( O
Yuan O
et O
al O
. O
, O
2020 O
) O
. O

7 O
Conclusion O
We O
present O
MRS O
, O
a O
multilingual O
dataset O
for O
reply O
suggestion O
. O

We O
compare O
a O
generation O
and O
a O
re- O
trieval O
baseline O
on O
MRS O
. O

The O
two O
models O
have O
dif O
- O
ferent O
strengths O
in O
the O
English O
monolingual O
setting O
and O
require O
different O
strategies O
to O
transfer O
across O
languages O
. O

MRS O
provides O
a O
benchmark O
for O
future O
research O
in O
both O
reply O
suggestion O
and O
cross O
- O
lingual O
transfer O
learning O
. O

Ethical O
Considerations O
Data O
Collection O
. O

No O
human O
annotators O
are O
in- O
volved O
while O
creating O
MRS O
. O

The O
examples O
and O
response O
sets O
of O
MRS O
come O
from O
publicly O
avail- O
able O
Reddit O
dumps O
from O
Pushshift O
, O
which O
are O
used O
in O
more O
than O
a O
hundred O
peer O
- O
reviewed O
publica- O
tions O
( O
Baumgartner O
et O
al O
. O
, O
2020 O
) O
. O

Privacy O
. O

Examples O
in O
MRS O
do O
not O
have O
the O
user- O
name O
and O
are O
from O
publicly O
available O
data O
. O

There- O

fore O
, O
we O
do O
not O
anticipate O
any O
privacy O
issues O
. O

In O
the O
pilot O
study O
( O
Section O
3.3 O
) O
, O
we O
measure O
the O
correla- O
tion O
of O
user O
CTR O
with O
different O
evaluation O
metrics O
. O

To O
protect O
user O
privacy O
, O
we O
only O
collect O
aggregated O
statistics O
( O
CTR O
) O
and O
use O
no O
other O
information O
. O

Potential O
Biased O
and O
Toxic O
Content O
. O

Despite O
our O
best O
effort O
to O
ﬁlter O
toxic O
contents O
( O
Section O
2.2 O
) O
, O
the O
dataset O
may O
not O
be O
perfectly O
cleansed O
and O
may O
have O
other O
biases O
that O
are O
typical O
in O
open O
fo- O
rums O
( O
Massanari O
, O
2017 O
; O
Mohan O
et O
al O
. O
, O
2017 O
) O
. O

Users O
should O
be O
aware O
of O
these O
issues O
. O

We O
will O
continue O
to O
improve O
the O
quality O
of O
the O
dataset O
. O

Intended O
Use O
of O
MRS.Because O
of O
the O
possi- O
ble O
biases O
and O
inappropriateness O
in O
the O
data O
, O
MRS O
should O
notbe O
directly O
used O
to O
build O
production O
systems O
( O
as O
mentioned O
in O
Section O
2.2 O
) O
. O

The O
main O
use O
of O
MRS O
is O
to O
test O
cross O
- O
lingual O
generalization O
for O
text O
retrieval O
and O
generation O
models O
, O
and O
re- O
searchers O
should O
be O
aware O
of O
possible O
ethical O
issues O
of O
Reddit O
data O
before O
using O
MRS O
. O

Acknowledgement O
We O
appreciate O
the O
feedback O
from O
anonymous O
re- O
viewers O
. O

MZ O
is O
supported O
in O
part O
by O
the O
Ofﬁce O
of O
the O
Director O
of O
National O
Intelligence O
( O
ODNI O
) O
, O
Intelligence O
Advanced O
Research O
Projects O
Activity O
( O
IARPA O
) O
, O
via O
the O
BETTER O
Program O
contract O
# O
2019- O
19051600005 O
. O

The O
views O
and O
conclusions O
con- O
tained O
herein O
are O
those O
of O
the O
authors O
and O
should O
not O
be O
interpreted O
as O
necessarily O
representing O
the O
of- O
ﬁcial O
policies O
, O
either O
expressed O
or O
implied O
, O
of O
ODNI O
, O
IARPA O
, O
or O
the O
U.S. O
Government O
. O

The O
U.S. O
Gov- O
ernment O
is O
authorized O
to O
reproduce O
and O
distribute O
reprints O
for O
governmental O
purposes O
notwithstand- O
ing O
any O
copyright O
annotation O
therein O
. O

1216References O
Daniel O
Adiwardana O
, O
Minh O
- O
Thang O
Luong O
, O
David O
R. O
So O
, O
Jamie O
Hall O
, O
Noah O
Fiedel O
, O
Romal O
Thoppilan O
, O
Zi O
Yang O
, O
Apoorv O
Kulshreshtha O
, O
Gaurav O
Nemade O
, O
Yifeng O
Lu O
, O
and O
Quoc O
V O
. O

Le O
. O
2020 O
. O

Towards O
a O
human O
- O
like O
open- O
domain O
chatbot O
. O

arXiv O
preprint O
arXiv:2001.09977 O
. O

Wasi O
Ahmad O
, O
Zhisong O
Zhang O
, O
Xuezhe O
Ma O
, O
Eduard O
Hovy O
, O
Kai O
- O
Wei O
Chang O
, O
and O
Nanyun O
Peng O
. O

2019 O
. O

On O
difﬁculties O
of O
cross O
- O
lingual O
transfer O
with O
order O
dif- O
ferences O
: O
A O
case O
study O
on O
dependency O
parsing O
. O

In O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
. O

Waleed O
Ammar O
, O
George O
Mulcaire O
, O
Miguel O
Ballesteros O
, O
Chris O
Dyer O
, O
and O
Noah O
A. O
Smith O
. O

2016 O
. O

Many O
lan- O
guages O
, O
one O
parser O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
4:431–444 O
. O

Mikel O
Artetxe O
, O
Gorka O
Labaka O
, O
and O
Eneko O
Agirre O
. O

2020 O
. O

Translation O
artifacts O
in O
cross O
- O
lingual O
transfer O
learn- O
ing O
. O

In O
Proceedings O
of O
Empirical O
Methods O
in O
Natu- O
ral O
Language O
Processing O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Mikel O
Artetxe O
and O
Holger O
Schwenk O
. O

2019 O
. O

Mas- O
sively O
multilingual O
sentence O
embeddings O
for O
zero- O
shot O
cross O
- O
lingual O
transfer O
and O
beyond O
. O

Transac- O
tions O
of O
the O
Association O
for O
Computational O
Linguis- O
tics O
, O
7:597–610 O
. O

Carmen O
Banea O
, O
Rada O
Mihalcea O
, O
Janyce O
Wiebe O
, O
and O
Samer O
Hassan O
. O
2008 O
. O

Multilingual O
subjectivity O
anal- O
ysis O
using O
machine O
translation O
. O

In O
Proceedings O
of O
Empirical O
Methods O
in O
Natural O
Language O
Process- O
ing O
. O

Jason O
Baumgartner O
, O
Savvas O
Zannettou O
, O
Brian O
Keegan O
, O
Megan O
Squire O
, O
and O
Jeremy O
Blackburn O
. O

2020 O
. O

The O
Pushshift O
Reddit O
dataset O
. O

In O
International O
Confer- O
ence O
on O
Weblogs O
and O
Social O
Media O
. O

Tom O
B O
Brown O
, O
Benjamin O
Mann O
, O
Nick O
Ryder O
, O
Melanie O
Subbiah O
, O
Jared O
Kaplan O
, O
Prafulla O
Dhariwal O
, O
Arvind O
Neelakantan O
, O
Pranav O
Shyam O
, O
Girish O
Sastry O
, O
Amanda O
Askell O
, O
et O
al O
. O
2020 O
. O

Language O
models O
are O
few O
- O
shot O
learners O
. O

arXiv O
preprint O
arXiv:2005.14165 O
. O

Xilun O
Chen O
, O
Ahmed O
Hassan O
Awadallah O
, O
Hany O
Hassan O
, O
Wei O
Wang O
, O
and O
Claire O
Cardie O
. O

2019 O
. O

Multi O
- O
source O
cross O
- O
lingual O
model O
transfer O
: O
Learning O
what O
to O
share O
. O

InProceedings O
of O
the O
Association O
for O
Computational O
Linguistics O
. O

Xilun O
Chen O
, O
Yu O
Sun O
, O
Ben O
Athiwaratkun O
, O
Claire O
Cardie O
, O
and O
Kilian O
Weinberger O
. O

2018 O
. O

Adversarial O
deep O
av- O
eraging O
networks O
for O
cross O
- O
lingual O
sentiment O
classi- O
ﬁcation O
. O

Transactions O
of O
the O
Association O
for O
Compu- O
tational O
Linguistics O
, O
6:557–570 O
. O

Zewen O
Chi O
, O
Li O
Dong O
, O
Furu O
Wei O
, O
Wenhui O
Wang O
, O
Xian- O
Ling O
Mao O
, O
and O
Heyan O
Huang O
. O

2020 O
. O

Cross O
- O
lingual O
natural O
language O
generation O
via O
pre O
- O
training O
. O

In O
As- O
sociation O
for O
the O
Advancement O
of O
Artiﬁcial O
Intelli- O
gence O
.Zewen O

Chi O
, O
Li O
Dong O
, O
Furu O
Wei O
, O
Nan O
Yang O
, O
Saksham O
Singhal O
, O
Wenhui O
Wang O
, O
Xia O
Song O
, O
Xian O
- O
Ling O
Mao O
, O
Heyan O
Huang O
, O
and O
Ming O
Zhou O
. O

2021 O
. O

InfoXLM O
: O
An O
information O
- O
theoretic O
framework O
for O
cross O
- O
lingual O
language O
model O
pre O
- O
training O
. O

In O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Com- O
putational O
Linguistics O
. O

Muthu O
Chidambaram O
, O
Yinfei O
Yang O
, O
Daniel O
Cer O
, O
Steve O
Yuan O
, O
Yunhsuan O
Sung O
, O
Brian O
Strope O
, O
and O
Ray O
Kurzweil O
. O

2019 O
. O

Learning O
cross O
- O
lingual O
sentence O
representations O
via O
a O
multi O
- O
task O
dual O
- O
encoder O
model O
. O

InProceedings O
of O
ACL O
Workshop O
on O
Representation O
Learning O
for O
NLP O
. O

Jonathan O
H. O
Clark O
, O
Eunsol O
Choi O
, O
Michael O
Collins O
, O
Dan O
Garrette O
, O
Tom O
Kwiatkowski O
, O
Vitaly O
Nikolaev O
, O
and O
Jennimaria O
Palomaki O
. O

2020 O
. O

TyDi O
QA O
: O
A O
bench- O
mark O
for O
information O
- O
seeking O
question O
answering O
in O
typologically O
diverse O
languages O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
8:454 O
– O
470 O
. O

Michael O
Collins O
and O
Terry O
Koo O
. O
2005 O
. O

Discriminative O
reranking O
for O
natural O
language O
parsing O
. O

Computa- O

tional O
Linguistics O
, O
31(1):25–70 O
. O

Alexis O
Conneau O
, O
Kartikay O
Khandelwal O
, O
Naman O
Goyal O
, O
Vishrav O
Chaudhary O
, O
Guillaume O
Wenzek O
, O
Francisco O
Guzm O
´ O
an O
, O
Edouard O
Grave O
, O
Myle O
Ott O
, O
Luke O
Zettle- O
moyer O
, O
and O
Veselin O
Stoyanov O
. O

2020 O
. O

Unsupervised O
cross O
- O
lingual O
representation O
learning O
at O
scale O
. O

In O
Proceedings O
of O
the O
Association O
for O
Computational O
Linguistics O
. O

Alexis O
Conneau O
, O
Guillaume O
Lample O
, O
Ruty O
Rinott O
, O
Ad- O
ina O
Williams O
, O
Samuel O
R O
Bowman O
, O
Holger O
Schwenk O
, O
and O
Veselin O
Stoyanov O
. O

2018 O
. O

XNLI O
: O
Evaluating O
cross O
- O
lingual O
sentence O
representations O
. O

In O
Proceed- O
ings O
of O
Empirical O
Methods O
in O
Natural O
Language O
Pro- O
cessing O
. O

Ryan O
Cotterell O
and O
Georg O
Heigold O
. O

2017 O
. O
Cross- O

lingual O
character O
- O
level O
neural O
morphological O
tag- O
ging O
. O

In O
Proceedings O
of O
Empirical O
Methods O
in O
Natu- O
ral O
Language O
Processing O
. O

Budhaditya O
Deb O
, O
Peter O
Bailey O
, O
and O
Milad O
Shokouhi O
. O

2019 O
. O

Diversifying O
reply O
suggestions O
using O
a O
matching O
- O
conditional O
variational O
autoencoder O
. O

In O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Industry O
Papers O
) O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
under- O
standing O
. O

In O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
. O

Association O
for O
Computational O
Linguistics O
. O

Lucas O
Dixon O
, O
John O
Li O
, O
Jeffrey O
Sorensen O
, O
Nithum O
Thain O
, O
and O
Lucy O
Vasserman O
. O

2018 O
. O

Measuring O
and O
mitigat- O
ing O
unintended O
bias O
in O
text O
classiﬁcation O
. O

In O
Pro- O
ceedings O
of O
AAAI O
/ O
ACM O
Conference O
on O
AI O
, O
Ethics O
, O
and O
Society O
. O

1217Fangxiaoyu O
Feng O
, O
Yinfei O
Yang O
, O
Daniel O
Cer O
, O
Naveen O
Arivazhagan O
, O
and O
Wei O
Wang O
. O

2020 O
. O

Language- O
agnostic O
BERT O
sentence O
embedding O
. O
arXiv O
preprint O
arXiv:2007.01852 O
. O

Jianfeng O
Gao O
, O
Michel O
Galley O
, O
Lihong O
Li O
, O
et O
al O
. O
2019 O
. O

Neural O
approaches O
to O
conversational O
AI O
. O

Founda- O
tions O
and O
Trends O
in O
Information O
Retrieval O
, O
13(2- O
3):127–298 O
. O

Ruifang O
Ge O
and O
Raymond O
J. O
Mooney O
. O

2006 O
. O

Discrim- O
inative O
reranking O
for O
semantic O
parsing O
. O

In O
Proceed- O
ings O
of O
the O
Association O
for O
Computational O
Linguis- O
tics O
. O

Samuel O
Gehman O
, O
Suchin O
Gururangan O
, O
Maarten O
Sap O
, O
Yejin O
Choi O
, O
and O
Noah O
A. O
Smith O
. O

2020 O
. O

RealToxi- O
cityPrompts O
: O
Evaluating O
neural O
toxic O
degeneration O
in O
language O
models O
. O

In O
Findings O
of O
the O
Associa- O
tion O
for O
Computational O
Linguistics O
: O
EMNLP O
. O

Asso- O
ciation O
for O
Computational O
Linguistics O
. O

Matthew O
Henderson O
, O
Rami O
Al O
- O
Rfou O
, O
Brian O
Strope O
, O
Yun- O
Hsuan O
Sung O
, O
L O
´ O
aszl´o O
Luk O
´ O
acs O
, O
Ruiqi O
Guo O
, O
Sanjiv O
Ku- O
mar O
, O
Balint O
Miklos O
, O
and O
Ray O
Kurzweil O
. O
2017 O
. O

Efﬁ- O
cient O
natural O
language O
response O
suggestion O
for O
Smart O
Reply O
. O

arXiv O
preprint O
arXiv:1705.00652 O
. O

Ari O
Holtzman O
, O
Jan O
Buys O
, O
Li O
Du O
, O
Maxwell O
Forbes O
, O
and O
Yejin O
Choi O
. O

2020 O
. O

The O
curious O
case O
of O
neural O
text O
degeneration O
. O

In O
Proceedings O
of O
the O
International O
Conference O
on O
Learning O
Representations O
. O

Junjie O
Hu O
, O
Sebastian O
Ruder O
, O
Aditya O
Siddhant O
, O
Gra- O
ham O
Neubig O
, O
Orhan O
Firat O
, O
and O
Melvin O
Johnson O
. O

2020 O
. O

XTREME O
: O

A O
massively O
multilingual O
multi- O
task O
benchmark O
for O
evaluating O
cross O
- O
lingual O
general- O
ization O
. O

In O
Proceedings O
of O
the O
International O
Confer- O
ence O
of O
Machine O
Learning O
. O

Lifu O
Huang O
, O
Heng O
Ji O
, O
and O
Jonathan O
May O
. O

2019 O
. O
Cross- O

lingual O
multi O
- O
level O
adversarial O
transfer O
to O
enhance O
low O
- O
resource O
name O
tagging O
. O

In O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Com- O
putational O
Linguistics O
. O

Minlie O
Huang O
, O
Xiaoyan O
Zhu O
, O
and O
Jianfeng O
Gao O
. O

2020 O
. O

Challenges O
in O
building O
intelligent O
open O
- O
domain O
dia- O
log O
systems O
. O

ACM O
Transactions O
on O
Information O
Sys- O
tems O
, O
38(3):1–32 O
. O

Ben O
Hutchinson O
, O
Vinodkumar O
Prabhakaran O
, O
Emily O
Denton O
, O
Kellie O
Webster O
, O
Yu O
Zhong O
, O
and O
Stephen O
De- O
nuyl O
. O

2020 O
. O

Social O
biases O
in O
NLP O
models O
as O
barriers O
for O
persons O
with O
disabilities O
. O

In O
Proceedings O
of O
the O
Association O
for O
Computational O
Linguistics O
. O

Melvin O
Johnson O
, O
Mike O
Schuster O
, O
Quoc O
V O
. O

Le O
, O
Maxim O
Krikun O
, O
Yonghui O
Wu O
, O
Zhifeng O
Chen O
, O
Nikhil O
Thorat O
, O
Fernanda O
Vi O
´ O
egas O
, O
Martin O
Wattenberg O
, O
Greg O
Corrado O
, O
Macduff O
Hughes O
, O
and O
Jeffrey O
Dean O
. O
2017 O
. O

Google O
’s O
multilingual O
neural O
machine O
translation O
system O
: O
En- O
abling O
zero O
- O
shot O
translation O
. O

Transactions O
of O
the O
As- O
sociation O
for O
Computational O
Linguistics O
, O
5:339–351.Armand O
Joulin O
, O
Edouard O
Grave O
, O
Piotr O
Bojanowski O
, O
Matthijs O
Douze O
, O
H O
´ O
erve O
J O
´ O
egou O
, O
and O
Tomas O
Mikolov O
. O
2016 O
. O

FastText.zip O
: O

Compressing O
text O
classiﬁcation O
models O
. O

arXiv O
preprint O
arXiv:1612.03651 O
. O

Anjuli O
Kannan O
, O
Karol O
Kurach O
, O
Sujith O
Ravi O
, O
Tobias O
Kaufmann O
, O
Andrew O
Tomkins O
, O
Balint O
Miklos O
, O
Greg O
Corrado O
, O
Laszlo O
Lukacs O
, O
Marina O
Ganea O
, O
Peter O
Young O
, O
et O
al O
. O
2016 O
. O

Smart O
Reply O
: O
Automated O
re- O
sponse O
suggestion O
for O
email O
. O

In O
Knowledge O
Discov- O
ery O
and O
Data O
Mining O
. O

Diederik O
P O
Kingma O
and O
Jimmy O
Ba O
. O
2015 O
. O

Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O

In O
Proceedings O
of O
the O
International O
Conference O
on O
Learning O
Repre- O
sentations O
. O

James O
Kirkpatrick O
, O
Razvan O
Pascanu O
, O
Neil O
Rabinowitz O
, O
Joel O
Veness O
, O
Guillaume O
Desjardins O
, O
Andrei O
A O
Rusu O
, O
Kieran O
Milan O
, O
John O
Quan O
, O
Tiago O
Ramalho O
, O
Ag- O
nieszka O
Grabska O
- O
Barwinska O
, O
et O
al O
. O
2017 O
. O

Over- O
coming O
catastrophic O
forgetting O
in O
neural O
networks O
. O

Proceedings O
of O
the O
National O
Academy O
of O
Sciences O
, O
114(13):3521–3526 O
. O

Mike O
Lewis O
, O
Marjan O
Ghazvininejad O
, O
Gargi O
Ghosh O
, O
Ar- O
men O
Aghajanyan O
, O
Sida O
Wang O
, O
and O
Luke O
Zettlemoyer O
. O
2020a O
. O

Pre O
- O
training O
via O
paraphrasing O
. O

In O
Proceed- O
ings O
of O
Advances O
in O
Neural O
Information O
Processing O
Systems O
. O

Patrick O
Lewis O
, O
Barlas O
Oguz O
, O
Ruty O
Rinott O
, O
Sebastian O
Riedel O
, O
and O
Holger O
Schwenk O
. O
2020b O
. O

MLQA O
: O
Evalu- O
ating O
cross O
- O
lingual O
extractive O
question O
answering O
. O

In O
Proceedings O
of O
the O
Association O
for O
Computational O
Linguistics O
. O

Jiwei O
Li O
, O
Michel O
Galley O
, O
Chris O
Brockett O
, O
Jianfeng O
Gao O
, O
and O
Bill O
Dolan O
. O

2016 O
. O

A O
diversity O
- O
promoting O
ob- O
jective O
function O
for O
neural O
conversation O
models O
. O

In O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
. O

Yaobo O
Liang O
, O
Nan O
Duan O
, O
Yeyun O
Gong O
, O
Ning O
Wu O
, O
Fen- O
fei O

Guo O
, O
Weizhen O
Qi O
, O
Ming O
Gong O
, O
Linjun O
Shou O
, O
Daxin O
Jiang O
, O
Guihong O
Cao O
, O
Xiaodong O
Fan O
, O
Ruofei O
Zhang O
, O
Rahul O
Agrawal O
, O
Edward O
Cui O
, O
Sining O
Wei O
, O
Taroon O
Bharti O
, O
Ying O
Qiao O
, O
Jiun O
- O
Hung O
Chen O
, O
Winnie O
Wu O
, O
Shuguang O
Liu O
, O
Fan O
Yang O
, O
Daniel O
Campos O
, O
Ran- O
gan O
Majumder O
, O
and O
Ming O
Zhou O
. O

2020 O
. O

XGLUE O
: O

A O
new O
benchmark O
datasetfor O
cross O
- O
lingual O
pre O
- O
training O
, O
understanding O
and O
generation O
. O

In O
Proceedings O
of O
Empirical O
Methods O
in O
Natural O
Language O
Process- O
ing O
. O

Chin O
- O
Yew O
Lin O
. O

2004 O
. O

ROUGE O
: O

A O
package O
for O
auto- O
matic O
evaluation O
of O
summaries O
. O

In O
Text O
Summariza- O
tion O
Branches O
Out O
. O

Yu O
- O
Hsiang O
Lin O
, O
Chian O
- O
Yu O
Chen O
, O
Jean O
Lee O
, O
Zirui O
Li O
, O
Yuyan O
Zhang O
, O
Mengzhou O
Xia O
, O
Shruti O
Rijhwani O
, O
Junxian O
He O
, O
Zhisong O
Zhang O
, O
Xuezhe O
Ma O
, O
Antonios O
Anastasopoulos O
, O
Patrick O
Littell O
, O
and O
Graham O
Neubig O
. O

2019 O
. O

Choosing O
transfer O
languages O
for O
cross O
- O
lingual O
learning O
. O

In O
Proceedings O
of O
the O
Association O
for O
Com- O
putational O
Linguistics O
. O

1218Chia O
- O
Wei O
Liu O
, O
Ryan O
Lowe O
, O
Iulian O
Serban O
, O
Mike O
Nose- O
worthy O
, O
Laurent O
Charlin O
, O
and O
Joelle O
Pineau O
. O

2016 O
. O

How O
NOT O
to O
evaluate O
your O
dialogue O
system O
: O
An O
em- O
pirical O
study O
of O
unsupervised O
evaluation O
metrics O
for O
dialogue O
response O
generation O
. O

In O
Proceedings O
of O
Em- O
pirical O
Methods O
in O
Natural O
Language O
Processing O
. O

Yinhan O
Liu O
, O
Jiatao O
Gu O
, O
Naman O
Goyal O
, O
Xian O
Li O
, O
Sergey O
Edunov O
, O
Marjan O
Ghazvininejad O
, O
Mike O
Lewis O
, O
and O
Luke O
Zettlemoyer O
. O

2020 O
. O

Multilingual O
denoising O
pre O
- O
training O
for O
neural O
machine O
translation O

. O

Transac- O
tions O
of O
the O
Association O
for O
Computational O
Linguis- O
tics O
, O
8:726–742 O
. O

Adrienne O
Massanari O
. O
2017 O
. O

# O
Gamergate O
and O
The O
Fap- O
pening O
: O
How O
Reddit O
’s O
algorithm O
, O
governance O
, O
and O
culture O
support O
toxic O
technocultures O
. O

New O
Media O
& O
Society O
, O
19(3):329–346 O
. O

Shruthi O
Mohan O
, O
Apala O
Guha O
, O
Michael O
Harris O
, O
Fred O
Popowich O
, O
Ashley O
Schuster O
, O
and O
Chris O
Priebe O
. O
2017 O
. O

The O
impact O
of O
toxic O
language O
on O
the O
health O
of O
Reddit O
communities O
. O

In O
Canadian O
Conference O
on O
Artiﬁcial O
Intelligence O
. O

Joakim O
Nivre O
, O
Marie O
- O
Catherine O
de O
Marneffe O
, O
Filip O
Gin- O
ter O
, O
Yoav O
Goldberg O
, O
Jan O
Haji O
ˇc O
, O
Christopher O
D. O
Man- O
ning O
, O
Ryan O
McDonald O
, O
Slav O
Petrov O
, O
Sampo O
Pyysalo O
, O
Natalia O
Silveira O
, O
Reut O
Tsarfaty O
, O
and O
Daniel O
Zeman O
. O

2016 O
. O

Universal O
Dependencies O
v1 O
: O
A O
multilingual O
treebank O
collection O
. O

In O
Proceedings O
of O
the O
Language O
Resources O
and O
Evaluation O
Conference O
. O

Kishore O
Papineni O
, O
Salim O
Roukos O
, O
Todd O
Ward O
, O
and O
Wei- O
Jing O
Zhu O
. O
2002 O
. O

BLEU O
: O
a O
method O
for O
automatic O
eval- O
uation O
of O
machine O
translation O
. O

In O
Proceedings O
of O
the O
Association O
for O
Computational O
Linguistics O
. O

Jonas O
Pfeiffer O
, O
Ivan O
Vuli O
´ O
c O
, O
Iryna O
Gurevych O
, O
and O
Se- O
bastian O
Ruder O
. O

2020 O
. O

MAD O
- O
X O
: O
An O
Adapter O
- O
Based O
Framework O
for O
Multi O
- O
Task O
Cross O
- O
Lingual O
Transfer O
. O

InProceedings O
of O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
. O

Jason O
Phang O
, O
Iacer O
Calixto O
, O
Phu O
Mon O
Htut O
, O
Yada O
Pruksachatkun O
, O
Haokun O
Liu O
, O
Clara O
Vania O
, O
Katha- O
rina O
Kann O
, O
and O
Samuel O
R. O
Bowman O
. O
2020 O
. O

En- O
glish O
intermediate O
- O
task O
training O
improves O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
too O
. O

In O
Conference O
of O
the O
Asia O
- O
Paciﬁc O
Chapter O
of O
the O
Association O
for O
Compu- O
tational O
Linguistics O
. O

Minghui O
Qiu O
, O
Feng O
- O
Lin O
Li O
, O
Siyu O
Wang O
, O
Xing O
Gao O
, O
Yan O
Chen O
, O
Weipeng O
Zhao O
, O
Haiqing O
Chen O
, O
Jun O
Huang O
, O
and O
Wei O
Chu O
. O
2017 O
. O

AliMe O
chat O
: O
A O
sequence O
to O
sequence O
and O
rerank O
based O
chatbot O
engine O
. O

In O
Pro- O
ceedings O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
. O

Alec O
Radford O
, O
Jeffrey O
Wu O
, O
Rewon O
Child O
, O
David O
Luan O
, O
Dario O
Amodei O
, O
and O
Ilya O
Sutskever O
. O
2019 O
. O

Language O
models O
are O
unsupervised O
multitask O
learners O
. O

OpenAI O
Blog O
.Nils O

Reimers O
and O
Iryna O
Gurevych O
. O

2020 O
. O

Making O
monolingual O
sentence O
embeddings O
multilingual O
us- O

ing O
knowledge O
distillation O
. O

In O
Proceedings O
of O
Em- O
pirical O
Methods O
in O
Natural O
Language O
Processing O
. O

Maarten O
Sap O
, O
Dallas O
Card O
, O
Saadia O
Gabriel O
, O
Yejin O
Choi O
, O
and O
Noah O
A. O
Smith O
. O

2019 O
. O

The O
risk O
of O
racial O
bias O
in O
hate O
speech O
detection O
. O

In O
Proceedings O
of O
the O
Associ- O
ation O
for O
Computational O
Linguistics O
. O

Holger O
Schwenk O
and O
Xian O
Li O
. O

2018 O
. O

A O
corpus O
for O
mul- O
tilingual O
document O
classiﬁcation O
in O
eight O
languages O
. O

InProceedings O
of O
the O
Language O
Resources O
and O
Eval- O
uation O
Conference O
. O

Thomas O
Scialom O
, O
Paul O
- O
Alexis O
Dray O
, O
Sylvain O
Lamprier O
, O
Benjamin O
Piwowarski O
, O
and O
Jacopo O
Staiano O
. O

2020 O
. O

MLSUM O
: O

The O
multilingual O
summarization O
corpus O
. O

InProceedings O
of O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
. O

Libin O
Shen O
, O
Anoop O
Sarkar O
, O
and O
Franz O
Josef O
Och O
. O
2004 O
. O

Discriminative O
reranking O
for O
machine O
translation O
. O

In O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
. O

Stephanie O
Strassel O
and O
Jennifer O
Tracey O
. O

2016 O
. O

LORELEI O
language O
packs O
: O
Data O
, O
tools O
, O
and O
resources O
for O
technology O
development O
in O
low O
re- O
source O
languages O
. O

In O
Proceedings O
of O
the O
Language O
Resources O
and O
Evaluation O
Conference O
. O

Ilya O
Sutskever O
, O
Oriol O
Vinyals O
, O
and O
Quoc O
V O
Le O
. O
2014 O
. O

Sequence O
to O
sequence O
learning O
with O
neural O
networks O
. O

InProceedings O
of O
Advances O
in O
Neural O
Information O
Processing O
Systems O
. O

Erik O
F. O
Tjong O
Kim O
Sang O
. O

2002 O
. O

Introduction O
to O
the O
CoNLL-2002 O
shared O
task O
: O
Language O
- O
independent O
named O
entity O
recognition O
. O

In O
Conference O
on O
Com- O
putational O
Natural O
Language O
Learning O
. O

Chau O
Tran O
, O
Yuqing O
Tang O
, O
Xian O
Li O
, O
and O
Jiatao O
Gu O
. O
2020 O
. O

Cross O
- O
lingual O
retrieval O
for O
iterative O
self O
- O
supervised O
training O
. O

In O
Proceedings O
of O
Advances O
in O
Neural O
In- O
formation O
Processing O
Systems O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
Łukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
Proceedings O
of O
Advances O
in O
Neural O
In- O
formation O
Processing O
Systems O
. O

Ashwin O
K O
Vijayakumar O
, O
Michael O
Cogswell O
, O
Ram- O
prasath O
R O
Selvaraju O
, O
Qing O
Sun O
, O
Stefan O
Lee O
, O
David O
Crandall O
, O
and O
Dhruv O
Batra O
. O

2018 O
. O

Diverse O
beam O
search O
: O
Decoding O
diverse O
solutions O
from O
neural O
se- O
quence O
models O
. O

In O
Association O
for O
the O
Advancement O
of O
Artiﬁcial O
Intelligence O
. O

Zirui O
Wang O
, O
Zachary O
C. O
Lipton O
, O
and O
Yulia O
Tsvetkov O
. O

2020 O
. O

On O
negative O
interference O
in O
multilingual O
mod- O
els O
: O
Findings O
and O
a O
meta O
- O
learning O
treatment O
. O

In O
Pro- O
ceedings O
of O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

1219Sean O
Welleck O
, O
Ilia O
Kulikov O
, O
Stephen O
Roller O
, O
Emily O
Dinan O
, O
Kyunghyun O
Cho O
, O
and O
Jason O
Weston O
. O

2020 O
. O

Neural O
text O
generation O
with O
unlikelihood O
training O
. O

InProceedings O
of O
the O
International O
Conference O
on O
Learning O
Representations O
. O

Shijie O
Wu O
and O
Mark O
Dredze O
. O

2019 O
. O

Beto O
, O
bentz O
, O
be- O
cas O
: O
The O
surprising O
cross O
- O
lingual O
effectiveness O
of O
BERT O
. O

In O
Proceedings O
of O
Empirical O
Methods O
in O
Nat- O
ural O
Language O
Processing O
. O

Mengzhou O
Xia O
, O
Guoqing O
Zheng O
, O
Subhabrata O
Mukher- O
jee O
, O
Milad O
Shokouhi O
, O
Graham O
Newbig O
, O
and O
Ahmed O
Hassan O
Awadallah O
. O
2021 O
. O

MetaXL O
: O

Meta O
representation O
transformation O
for O
low O
- O
resource O
cross O
- O
lingual O
learning O
. O

In O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computa- O
tional O
Linguistics O
. O

Linting O
Xue O
, O
Noah O
Constant O
, O
Adam O
Roberts O
, O
Mi- O
hir O
Kale O
, O
Rami O
Al O
- O
Rfou O
, O
Aditya O
Siddhant O
, O
Aditya O
Barua O
, O
and O
Colin O
Raffel O
. O
2020 O
. O

mT5 O
: O

A O
mas- O
sively O
multilingual O
pre O
- O
trained O
text O
- O
to O
- O
text O
trans- O
former O
. O

arXiv O
preprint O
arXiv:2010.11934 O
. O

Michelle O
Yuan O
, O
Mozhi O
Zhang O
, O
Benjamin O
Van O
Durme O
, O
Leah O
Findlater O
, O
and O
Jordan O
Boyd O
- O
Graber O
. O

2020 O
. O

In- O
teractive O
reﬁnement O
of O
cross O
- O
lingual O
word O
embed- O
dings O
. O

In O
Proceedings O
of O
Empirical O
Methods O
in O
Nat- O
ural O
Language O
Processing O
. O

Mozhi O
Zhang O
, O
Yoshinari O
Fujinuma O
, O
and O
Jordan O
Boyd- O
Graber O
. O

2020a O
. O

Exploiting O
cross O
- O
lingual O
subword O
similarities O
in O
low O
- O
resource O
document O
classiﬁcation O
. O

InAssociation O
for O
the O
Advancement O
of O
Artiﬁcial O
In- O
telligence O
. O

Mozhi O
Zhang O
, O
Yoshinari O
Fujinuma O
, O
Michael O
J. O
Paul O
, O
and O
Jordan O
Boyd O
- O
Graber O
. O

2020b O
. O

Why O
overﬁtting O
is O
n’t O
always O
bad O
: O

Retroﬁtting O
cross O
- O
lingual O
word O
em- O
beddings O
to O
dictionaries O
. O

In O
Proceedings O
of O
the O
Asso- O
ciation O
for O
Computational O
Linguistics O
. O

Yizhe O
Zhang O
, O
Michel O
Galley O
, O
Jianfeng O
Gao O
, O
Zhe O
Gan O
, O
Xiujun O
Li O
, O
Chris O
Brockett O
, O
and O
Bill O
Dolan O
. O

2018 O
. O

Generating O
informative O
and O
diverse O
conversational O
responses O
via O
adversarial O
information O
maximization O
. O

InProceedings O
of O
Advances O
in O
Neural O
Information O
Processing O
Systems O
. O

Yizhe O
Zhang O
, O
Siqi O
Sun O
, O
Michel O
Galley O
, O
Yen O
- O
Chun O
Chen O
, O
Chris O
Brockett O
, O
Xiang O
Gao O
, O
Jianfeng O
Gao O
, O
Jingjing O
Liu O
, O
and O
Bill O
Dolan O
. O

2020c O
. O

DialoGPT O
: O
Large O
- O
scale O
generative O
pre O
- O
training O
for O
conversational O
response O
generation O
. O

In O
Proceedings O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
System O
Demonstrations O
. O

1220A O
Results O
for O
XLM O
- O
R O
Monolingual O
Zero O
- O
Shot O
MT O
Multilingual O
ROUGE O
Dist-1 O
Dist-2 O
ROUGE O
Dist-1 O
Dist-2 O
ROUGE O
Dist-1 O
Dist-2 O
ROUGE O
Dist-1 O
Dist-2 O
EN O
.0354 O
.0177 O
.0454 O
.0354 O
.0177 O
.0454 O

- O
- O
- O
.0319 O
.0152 O

.0398 O

ES O
.0158 O
.0069 O

.0172 O
.0140 O

.0065 O
.0160 O
.0122 O

.0079 O
.0181 O

.0155 O

.0076 O

.0182 O
DE O
.0179 O
.0098 O
.0261 O
.0141 O

.0064 O
.0162 O
.0132 O
.0071 O
.0170 O

.0171 O
.0069 O
.0170 O

PT O
.0345 O
.0088 O
.0239 O

.0126 O
.0076 O
.0209 O

.0120 O
.0071 O

.0178 O

.0332 O
.0086 O
.0230 O

FR O
.0161 O
.0062 O
.0168 O
.0143 O
.0066 O
.0177 O
.0135 O
.0073 O
.0184 O
.0161 O
.0069 O
.0185 O

JA O
.0271 O
.0132 O
.0364 O

.0181 O
.0097 O

.0277 O
.0157 O

.0106 O
.0293 O

.0166 O
.0123 O
.0328 O

IT O
.0157 O
.0123 O
.0291 O
.0144 O
.0123 O
.0306 O
.0155 O

.0156 O
.0375 O
.0143 O
.0136 O

.0337 O

SV O
.0172 O
.0129 O
.0333 O

.0165 O
.0133 O
.0333 O

.0153 O
.0140 O
.0341 O

.0168 O
.0125 O
.0321 O

NL O
.0171 O
.0142 O

.0390 O
.0161 O
.0134 O
.0371 O
.0155 O

.0134 O
.0353 O

.0162 O
.0135 O

.0370 O
RU O
.0128 O
.0259 O
.0541 O

.0123 O
.0223 O

.0467 O
.0111 O
.0248 O

.0506 O
.0130 O

.0244 O
.0510 O

Table O
5 O
: O
Results O
for O
retrieval O
model O
initialized O
with O
XLM O
-R(Conneau O
et O
al O
. O
, O
2020 O
) O
, O
The O
settings O
are O
in O
Sec- O
tion O
3.2 O
. O

Gray O
cells O
indicate O
when O
the O
model O
is O
trained O
on O
the O
target O
language O
training O
set O
. O

White O
cells O
indicate O
cross O
- O
lingual O
settings O
where O
the O
target O
language O
training O
set O
is O
not O
used O
for O
training O
. O

For O
each O
language O
, O
we O
boldface O
the O
best O
ROUGE O
scores O
in O
cross O
- O
lingual O
settings O
( O
white O
cells O
) O
. O

We O
observe O
similar O
trends O
as O
MBERT O
( O
Table O
3 O
) O
. O

