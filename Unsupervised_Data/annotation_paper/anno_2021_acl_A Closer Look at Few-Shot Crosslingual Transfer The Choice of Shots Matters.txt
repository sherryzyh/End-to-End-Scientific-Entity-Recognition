Proceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
5751–5767 O
August O
1–6 O
, O
2021 O
. O

© O
2021 O
Association O
for O
Computational O
Linguistics5751A O

Closer O
Look O
at O
Few O
- O
Shot O
Crosslingual O
Transfer O
: O
The O
Choice O
of O
Shots O
Matters O
Mengjie O
Zhao1*Yi O
Zhu2*Ehsan O
Shareghi3 O
, O
2Ivan O
Vuli O
´ O
c2 O
Roi O
Reichart4Anna O
Korhonen2Hinrich O
Sch O
¨utze1 O
1CIS O
, O
LMU O
Munich2LTL O
, O
University O
of O
Cambridge O
3Department O
of O
Data O
Science O
& O
AI O
, O
Monash O
University O
4Faculty O
of O
Industrial O
Engineering O
and O
Management O
, O
Technion O
, O
IIT O
mzhao@cis.lmu.de O
, O
fyz568,iv250,alk23 O
g@cam.ac.uk O
, O
ehsan.shareghi@monash.edu O
, O
roiri@technion.ac.il O
Abstract O
Few O
- O
shot O
crosslingual O
transfer O
has O
been O
shown O
to O
outperform O
its O
zero O
- O
shot O
counterpart O
with O
pretrained O
encoders O
like O
multilingual O
BERT O
. O

Despite O
its O
growing O
popularity O
, O
little O
to O
no O
at- O
tention O
has O
been O
paid O
to O
standardizing O
and O
an- O
alyzing O
the O
design O
of O
few O
- O
shot O
experiments O
. O

In O
this O
work O
, O
we O
highlight O
a O
fundamental O
risk O
posed O
by O
this O
shortcoming O
, O
illustrating O
that O
the O
model O
exhibits O
a O
high O
degree O
of O
sensitivity O
to O
the O
selection O
of O
few O
shots O
. O

We O
conduct O
a O
large- O
scale O
experimental O
study O
on O
40 O
sets O
of O
sampled O
few O
shots O
for O
six O
diverse O
NLP O
tasks O
across O
up O
to O
40 O
languages O
. O

We O
provide O
an O
analysis O
of O
success O
and O
failure O
cases O
of O
few O
- O
shot O
transfer O
, O
which O
highlights O
the O
role O
of O
lexical O
features O
. O

Additionally O
, O
we O
show O
that O
a O
straightforward O
full O
model O
ﬁnetuning O
approach O
is O
quite O
effec- O
tive O
for O
few O
- O
shot O
transfer O
, O
outperforming O
sev- O
eral O
state O
- O
of O
- O
the O
- O
art O
few O
- O
shot O
approaches O
. O

As O
a O
step O
towards O
standardizing O
few O
- O
shot O
crosslin- O
gual O
experimental O
designs O
, O
we O
make O
our O
sam- O
pled O
few O
shots O
publicly O
available.1 O
1 O
Introduction O
Multilingual O
pretrained O
encoders O
like O
multilingual O
BERT O
( O
mBERT O
; O
Devlin O
et O

al O
. O
( O
2019 O
) O
) O
and O
XLM- O
R O
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
are O
the O
top O
performers O
in O
crosslingual O
tasks O
such O
as O
natural O
language O
in- O
ference O
( O
Conneau O
et O
al O
. O
, O
2018 O
) O
, O
document O
clas- O
siﬁcation O
( O
Schwenk O
and O
Li O
, O
2018 O
; O

Artetxe O
and O
Schwenk O
, O
2019 O
) O
, O
and O
argument O
mining O
( O
Toledo- O
Ronen O
et O
al O
. O
, O
2020 O
) O
. O

They O
enable O
transfer O
learn- O
ing O
through O
language O
- O
agnostic O
representations O
in O
crosslingual O
setups O
( O
Hu O
et O
al O
. O
, O
2020 O
) O
. O

A O
widely O
explored O
transfer O
scenario O
is O
zero O
- O
shot O
crosslingual O
transfer O
( O
Pires O
et O

al O
. O
, O
2019 O
; O
Conneau O
and O
Lample O
, O
2019 O
; O
Artetxe O
and O
Schwenk O
, O
2019 O
) O
, O
* O
Equal O
contribution O
. O

1Code O
and O
resources O
are O
available O
at O
https://github O
. O
com O
/ O
fsxltwhere O
a O
pretrained O
encoder O
is O
ﬁnetuned O
on O
abun- O
dant O
task O
data O
in O
the O
source O
language O
( O
e.g. O
, O
English O
) O
and O
then O
directly O
evaluated O
on O
target O
- O
language O
test O
data O
, O
achieving O
surprisingly O
good O
performance O
( O
Wu O
and O
Dredze O
, O
2019 O
; O
Hu O
et O
al O
. O
, O
2020 O
) O
. O

However O
, O
there O
is O
evidence O
that O
zero O
- O
shot O
performance O
reported O
in O
the O
literature O
has O
large O
variance O
and O
is O
often O
not O
re- O
producible O
( O
Keung O
et O
al O
. O
, O
2020a O
; O
Rios O
et O
al O
. O
, O
2020 O
) O
; O
the O
results O
in O
languages O
distant O
from O
English O
fall O
far O
short O
of O
those O
similar O
to O
English O
( O
Hu O
et O
al O
. O
, O
2020 O
; O
Liang O
et O
al O
. O
, O
2020 O
) O
. O

Lauscher O
et O
al O
. O

( O
2020 O
) O
stress O
the O
importance O
of O
few O
- O
shot O
crosslingual O
transfer O
instead O
, O
where O
the O
encoder O
is O
ﬁrst O
ﬁnetuned O
on O
a O
source O
language O
and O
then O
further O
ﬁnetuned O
with O
a O
small O
amount O
( O
10–100 O
) O
of O
examples O
( O
few O
shots O
) O
of O
the O
target O
lan- O
guage O
. O

The O
few O
shots O
substantially O
improve O
model O
performance O
of O
the O
target O
language O
with O
negligi- O
ble O
annotation O
costs O
( O
Garrette O
and O
Baldridge O
, O
2013 O
; O
Hedderich O
et O
al O
. O
, O
2020 O
) O
. O

In O
this O
work O
, O
however O
, O
we O
demonstrate O
that O
the O
gains O
from O
few O
- O
shot O
transfer O
exhibit O
a O
high O
degree O
of O
sensitivity O
to O
the O
selection O
of O
few O
shots O
. O

For O
example O
, O
different O
choices O
for O
the O
few O
shots O
can O
yield O
a O
performance O
variance O
of O
over O
10 O
% O
accuracy O
in O
a O
standard O
document O
classiﬁcation O
task O
. O

Mo- O
tivated O
by O
this O
, O
we O
propose O
to O
ﬁx O
the O
few O
shots O
for O
fair O
comparisons O
between O
different O
crosslingual O
transfer O
methods O
, O
and O
provide O
a O
benchmark O
resem- O
bling O
the O
standard O
“ O
N O
- O
wayK O
- O
shot O
” O
few O
- O
shot O
learn- O
ing O
conﬁguration O
( O
Fei O
- O
Fei O
et O
al O
. O
, O
2006 O
; O

Koch O
et O
al O
. O
, O
2015 O
) O
. O

We O
also O
evaluate O
and O
compare O
several O
state- O
of O
- O
the O
- O
art O
( O
SotA O
) O
few O
- O
shot O
ﬁnetuning O
techniques O
, O
in O
order O
to O
understand O
their O
performance O
and O
sus- O
ceptibility O
to O
the O
variance O
related O
to O
few O
shots O
. O

We O
also O
demonstrate O
that O
the O
effectiveness O
of O
few O
- O
shot O
crosslingual O
transfer O
depends O
on O
the O
type O
of O
downstream O
task O
. O

For O
syntactic O
tasks O
such O
as O
named O
- O
entity O
recognition O
, O
the O
few O
shots O
can O
im- O
prove O
results O
by O
up O
to O
20F1points O
. O

For O
chal- O

5752lenging O
tasks O
like O
adversarial O
paraphrase O
identiﬁca- O
tion O
, O
the O
few O
shots O
do O
not O
help O
and O
even O
sometimes O
lead O
to O
worse O
performance O
than O
zero O
- O
shot O
transfer O
. O

To O
understand O
these O
phenomena O
, O
we O
conduct O
addi- O
tional O
in O
- O
depth O
analyses O
, O
and O
ﬁnd O
that O
the O
models O
tend O
to O
utilize O
shallow O
lexical O
hints O
( O
Geirhos O
et O
al O
. O
, O
2020 O
) O
in O
the O
target O
language O
, O
rather O
than O
leverag- O
ing O
abstract O
crosslingual O
semantic O
features O
learned O
from O
the O
source O
language O
. O

Ourcontributions O
: O
1 O
) O
We O
show O
that O
few O
- O
shot O
crosslingual O
transfer O
is O
prone O
to O
large O
variations O
in O
task O
performance O
; O
this O
property O
hinders O
unbiased O
assessments O
of O
the O
effectiveness O
of O
different O
few- O
shot O
methods O
. O

2)To O
remedy O
this O
issue O
, O
we O
publish O
ﬁxed O
and O
standardized O
few O
shots O
to O
support O
fair O
comparisons O
and O
reproducibility O
. O

3)We O
empiri- O
cally O
verify O
that O
few O
- O
shot O
crosslingual O
transfer O
has O
different O
performance O
impact O
on O
structurally O
differ- O
ent O
tasks O
; O
we O
provide O
in O
- O
depth O
analyses O
concerning O
the O
source O
of O
performance O
gains O
. O

4)We O
analyze O
several O
SotA O
few O
- O
shot O
learning O
methods O
, O
and O
show O
that O
they O
underperform O
simple O
full O
model O
ﬁnetun- O
ing O
. O

We O
hope O
that O
our O
work O
will O
shed O
new O
light O
on O
the O
potential O
and O
current O
difﬁculties O
of O
few O
- O
shot O
learning O
in O
crosslingual O
setups O
. O

2 O
Background O
and O
Related O
Work O
Zero-/Few O
- O
Shot O
Crosslingual O
Transfer O
. O

Multi- O

lingual O
pretrained O
encoders O
show O
strong O
zero O
- O
shot O
crosslingual O
transfer O
( O
ZS O
- O
XLT O
) O
ability O
in O
various O
NLP O
tasks O
( O
Pires O
et O
al O
. O
, O
2019 O
; O
Hsu O
et O
al O
. O
, O
2019 O
; O
Artetxe O
and O
Schwenk O
, O
2019 O
) O
. O

In O
order O
to O
guide O
and O
measure O
the O
progress O
, O
standardized O
bench- O
marks O
like O
XTREME O
( O
Hu O
et O
al O
. O
, O
2020 O
) O
and O
XGLUE O
( O
Liang O
et O
al O
. O
, O
2020 O
) O
have O
been O
developed O
. O

Recently O
, O
Lauscher O
et O
al O
. O

( O
2020 O
) O
and O
Hedderich O
et O
al O
. O

( O
2020 O
) O
extended O
the O
focus O
on O
few O
- O
shot O
crosslingual O
transfer O
( O
FS O
- O
XLT O
): O

They O
assume O
the O
availability O
of O
a O
handful O
of O
labeled O
examples O
in O
a O
target O
language,2which O
are O
used O
to O
further O
ﬁnetune O
a O
source O
- O
trained O
model O
. O

The O
extra O
few O
shots O
bring O
large O
performance O
gains O
at O
low O
annotation O
cost O
. O

In O
this O
work O
, O
we O
systematically O
analyze O
this O
recent O
FS O
- O
XLT O
scenario O
. O

FS O
- O
XLT O
resembles O
the O
intermediate O
- O
task O
trans- O
fer O
( O
STILT O
) O
approach O
( O
Phang O
et O
al O
. O
, O
2018 O
; O
Pruk- O
sachatkun O
et O
al O
. O
, O
2020 O
) O
. O

In O
STILT O
, O
a O
pretrained O
encoder O
is O
ﬁnetuned O
on O
a O
resource O
- O
rich O
intermedi- O
2According O
to O
Garrette O
and O
Baldridge O
( O
2013 O
) O
, O
it O
is O
possible O
to O
collect100 O
POS O
- O
annotated O
sentences O
in O
two O
hours O
even O
for O
low O
- O
resource O
languages O
such O
as O
Malagasy.ate O
task O
, O
and O
then O
ﬁnetuned O
on O
a O
( O
resource O
- O
lean O
) O
target O
task O
. O

Likewise O
, O
FS O
- O
XLT O
focuses O
on O
transfer- O
ring O
knowledge O
and O
general O
linguistic O
intelligence O
( O
Yogatama O
et O
al O
. O
, O
2019 O
) O
, O
although O
such O
transfer O
is O
between O
languages O
in O
the O
same O
task O
instead O
of O
be- O
tween O
different O
tasks O
. O

Few O
- O
shot O
learning O
was O
ﬁrst O
explored O
in O
com- O
puter O
vision O
( O
Miller O
et O
al O
. O
, O
2000 O
; O
Fei O
- O
Fei O
et O
al O
. O
, O
2006 O
; O
Koch O
et O
al O
. O
, O
2015 O
) O
; O
the O
aim O
there O
is O
to O
learn O
new O
concepts O
with O
only O
few O
images O
. O

Methods O
like O
pro- O
totypical O
networks O
( O
Snell O
et O
al O
. O
, O
2017 O
) O
and O
model- O
agnostic O
meta O
- O
learning O
( O
MAML O
; O
Finn O
et O
al O
. O
( O
2017 O
) O
) O
have O
also O
been O
applied O
to O
many O
monolingual O
( O
typi- O
cally O
English O
) O
NLP O
tasks O
such O
as O
relation O
classiﬁ- O
cation O
( O
Han O
et O
al O
. O
, O
2018 O
; O
Gao O
et O
al O
. O
, O
2019 O
) O
, O
named- O
entity O
recognition O
( O
Hou O
et O
al O
. O
, O
2020a O
) O
, O
word O
sense O
disambiguation O
( O
Holla O
et O
al O
. O
, O
2020 O
) O
, O
and O
text O
clas- O
siﬁcation O
( O
Yu O
et O
al O
. O
, O
2018 O
; O
Yin O
, O
2020 O
; O
Yin O
et O
al O
. O
, O
2020 O
; O
Bansal O
et O
al O
. O
, O
2020 O
; O
Gupta O
et O
al O
. O
, O
2020 O
) O
. O

How- O
ever O
, O
recent O
few O
- O
shot O
learning O
methods O
in O
computer O
vision O
consisting O
of O
two O
simple O
ﬁnetuning O
stages O
, O
ﬁrst O
on O
base O
- O
class O
images O
and O
then O
on O
new O
- O
class O
few O
shots O
, O
have O
been O
shown O
to O
outperform O
MAML O
and O
achieve O
SotA O
scores O
( O
Wang O
et O
al O
. O
, O
2020 O
; O

Chen O
et O
al O
. O
, O
2020 O
; O
Tian O
et O
al O
. O
, O
2020 O
; O
Dhillon O
et O
al O
. O
, O
2020 O
) O
. O

Inspired O
by O
this O
work O
, O
we O
compare O
various O
few- O
shot O
ﬁnetuning O
methods O
from O
computer O
vision O
in O
the O
context O
of O
FS O
- O
XLT O
. O

Task O
Performance O
Variance O
. O

Deep O
neural O
net- O
works O
’ O
performance O
on O
NLP O
tasks O
is O
bound O
to O
ex- O
hibit O
large O
variance O
. O

Reimers O
and O
Gurevych O
( O
2017 O
) O
and O
Dror O
et O
al O
. O

( O
2019 O
) O
stress O
the O
importance O
of O
re- O
porting O
score O
distributions O
instead O
of O
a O
single O
score O
for O
fair(er O
) O
comparisons O
. O

Dodge O
et O
al O
. O

( O
2020 O
) O
, O
Mos- O
bach O
et O
al O
. O

( O
2021 O
) O
, O
and O
Zhang O
et O
al O
. O

( O
2021 O
) O
show O
that O
ﬁnetuning O
pretrained O
encoders O
with O
different O
random O
seeds O
yields O
performance O
with O
large O
vari- O
ance O
. O

In O
this O
work O
, O
we O
examine O
a O
speciﬁc O
source O
of O
variance O
: O
We O
show O
that O
the O
choice O
of O
the O
few O
shots O
in O
crosslingual O
transfer O
learning O
also O
intro- O
duces O
large O
variance O
in O
performance O
; O
consequently O
, O
we O
offer O
standardized O
few O
shots O
for O
more O
controlled O
and O
fair O
comparisons O
. O

3 O
Method O
Following O
Lauscher O
et O
al O
. O

( O
2020 O
) O
and O
Hedderich O
et O
al O
. O

( O
2020 O
) O
, O
our O
FS O
- O
XLT O
method O
comprises O
two O
stages O
. O

First O
, O
we O
conduct O
source O
- O
training O
: O
The O
pretrained O
mBERT O
is O
ﬁnetuned O
with O
abundant O
an- O
notated O
data O
in O
the O
source O
language O
. O

Similar O
to O
Hu O
et O
al O
. O

( O
2020 O
) O
, O

Liang O
et O
al O
. O
( O
2020 O
) O
and O
due O
to O

5753Name O
Metric O
Task O
jT O
j O
TS O
# O
of O
lang O
. O

XNLI O
Acc O
. O

Natural O
language O
inference O
3 O

No O
15 O
PAWSX O
Acc O
. O

Paraphrase O
identiﬁcation O
2 O

No O
7 O
MLDoc O
Acc O
. O

News O
article O
classiﬁcation O
4 O
Yes O
8 O
MARC O
Acc O
. O

Amazon O
reviews O
5 O
Yes O
6 O
POS O
F1 O

Part O
- O
of O
- O
speech O
tagging O
17 O

Yes O
29 O
NER O
F1 O
Named O
- O
entity O
recognition O
7 O
Yes O
40 O
Table O
1 O
: O
Evaluation O
datasets O
. O

jTj O
: O
Number O
of O
classes O
( O
classiﬁcation O
tasks O
) O
and O
label O
set O
size O
( O
POS O
and O
NER O
) O
. O

TS O
: O
availability O
of O
a O
training O
split O
in O
the O
target O
language O
. O

the O
abundant O
labeled O
data O
for O
many O
NLP O
tasks O
, O
we O
choose O
English O
as O
the O
source O
in O
our O
experiments O
. O

Directly O
evaluating O
the O
source O
- O
trained O
model O
af- O
ter O
this O
stage O
corresponds O
to O
the O
widely O
studied O
ZS O
- O
XLT O
scenario O
. O

The O
second O
stage O
is O
target- O
adapting O
: O
The O
source O
- O
trained O
model O
from O
previ- O
ous O
stage O
is O
adapted O
to O
a O
target O
language O
using O
few O
shots O
. O

We O
discuss O
details O
of O
sampling O
the O
few O
shots O
in§4 O
. O

The O
development O
set O
of O
the O
target O
language O
is O
used O
for O
model O
selection O
in O
this O
stage O
. O

4 O
Experimental O
Setup O
We O
consider O
three O
types O
of O
tasks O
requiring O
vary- O
ing O
degrees O
of O
semantic O
and O
syntactic O
knowledge O
transfer O
: O
Sequence O
classiﬁcation O
( O
CLS O
) O
, O
named- O
entity O
recognition O
( O
NER O
) O
, O
and O
part O
- O
of O
- O
speech O
tag- O
ging O
( O
POS O
) O
in O
up O
to O
40 O
typologically O
diverse O
lan- O
guages O
( O
cf O
. O
, O
Appendix O
§ O
B O
) O
. O

4.1 O
Datasets O
and O
Selection O
of O
Few O
Shots O
For O
the O
CLS O
tasks O
, O
we O
sample O
few O
shots O
from O
four O
multilingual O
datasets O
: O
News O
article O
classiﬁ- O
cation O
( O
MLDoc O
; O
Schwenk O
and O
Li O
( O
2018 O
) O
) O
; O
Ama- O
zon O
review O
classiﬁcation O
( O
MARC O
; O
Keung O
et O
al O
. O
( O
2020b O
) O
) O
; O
natural O
language O
inference O
( O
XNLI O
; O
Con- O
neau O
et O

al O
. O

( O
2018 O
) O
; O

Williams O
et O
al O
. O
( O
2018 O
) O
) O
; O
and O
crosslingual O
paraphrase O
adversaries O
from O
word O
scrambling O
( O
PAWSX O
; O
Zhang O
et O

al O
. O

( O
2019 O
) O
; O

Yang O
et O
al O
. O

( O
2019 O
) O
) O
. O

We O
use O
treebanks O
in O
Universal O
Dependencies O
( O
Nivre O
et O
al O
. O
, O
2020 O
) O
for O
POS O
, O
and O
WikiANN O
dataset O
( O
Pan O
et O
al O
. O
, O
2017 O
; O
Rahimi O
et O
al O
. O
, O
2019 O
) O
for O
NER O
. O

Table O
1 O
reports O
key O
information O
about O
the O
datasets O
. O

We O
adopt O
the O
conventional O
few O
- O
shot O
sampling O
strategy O
( O
Fei O
- O
Fei O
et O
al O
. O
, O
2006 O
; O
Koch O
et O
al O
. O
, O
2015 O
; O
Snell O
et O
al O
. O
, O
2017 O
) O
, O
and O
conduct O
“ O
N O
- O
wayK O
- O
shot O
” O
sampling O
from O
the O
datasets O
; O
Nis O
the O
number O
of O
classes O
and O
Krefers O
to O
the O
number O
of O
shots O
per O
class O
. O

A O
group O
of O
N O
- O
wayK O
- O
shot O
data O
is O
referred O
to O
as O
a O
bucket O
. O

We O
setNequal O
to O
the O
number O
of O
labelsjTj O
. O

Following O
Wang O
et O
al O
. O

( O
2020 O
) O
, O
we O
sam- O
ple O
40 O
buckets O
for O
each O
target O
( O
i.e. O
, O
non O
- O
English)language O
of O
a O
task O
to O
get O
a O
reliable O
estimation O
of O
model O
performance O
. O

CLS O
Tasks O
. O

For O
MLDoc O
and O
MARC O
, O
each O
lan- O
guage O
has O
a O
train O
/ O
dev O
/ O
test O
split O
. O

We O
sample O
the O
buckets O
without O
replacement O
from O
the O
training O
set O
of O
each O
target O
language O
, O
so O
that O
buckets O
are O
dis- O
joint O
from O
each O
other O
. O

Target O
languages O
in O
XNLI O
and O
PAWSX O
only O
have O
dev O
/ O
test O
splits O
. O

We O
sam- O
ple O
the O
buckets O
from O
the O
dev O
set O
; O
the O
remaining O
data O
serves O
as O
a O
single O
new O
dev O
set O
for O
model O
selec- O
tion O
during O
target O
- O
adapting O
. O

For O
all O
tasks O
, O
we O
use O
K2f1;2;4;8 O
g. O
POS O
and O
NER O
. O

For O
the O
two O
structured O
predic- O
tion O
tasks O
, O
“ O
N O
- O
wayK O
- O
shot O
” O
is O
not O
well O
- O
deﬁned O
be- O
cause O
each O
sentence O
contains O
one O
or O
more O
labeled O
tokens O
. O

We O
use O
a O
similar O
sampling O
principle O
as O
with O
CLS O
, O
where O
Nis O
the O
size O
of O
the O
label O
set O
for O
each O
language O
and O
task O
, O
but O
Kis O
set O
to O
the O
minimum O
number O
of O
occurrences O
for O
each O
label O
. O

In O
particu- O
lar O
, O
we O
utilize O
the O
Minimum O
- O
Including O
Algorithm O
( O
Hou O
et O
al O
. O
, O
2020b O
, O
a O
) O
to O
satisfy O
the O
following O
criteria O
when O
sampling O
a O
bucket O
: O
1 O
) O
each O
label O
appears O
at O
leastKtimes O
, O
and O
2 O
) O
at O
least O
one O
label O
will O
appear O
less O
thanKtimes O
if O
any O
sentence O
is O
removed O
from O
the O
bucket O
. O

Appendix O
§ O
C O
gives O
sampling O
details O
. O

In O
contrast O
to O
sampling O
for O
CLS O
, O
we O
do O
not O
enforce O
samples O
from O
different O
buckets O
to O
be O
disjoint O
due O
to O
the O
small O
amount O
of O
data O
in O
some O
low O
- O
resource O
languages O
. O

We O
only O
use O
K2 O
f1;2;4gand O
ex- O

cludeK= O
8 O
, O
as O
8 O
- O
shot O
buckets O
already O
have O
lots O
of O
labeled O
tokens O
, O
and O
thus O
( O
arguably O
) O
might O
not O
be O
considered O
few O
- O
shot O
. O

4.2 O
Training O
Setup O
We O
use O
the O
pretrained O
cased O
mBERT O
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
and O
rely O
on O
the O
PyTorch O
- O
based O
( O
Paszke O
et O
al O
. O
, O
2019 O
) O

HuggingFace O
Transformers O
repository O
( O
Wolf O
et O
al O
. O
, O
2019 O
) O
in O
all O
experiments O
. O

Forsource O
- O
training O
, O
we O
ﬁnetune O
the O
pretrained O
encoder O
for O
10 O
epochs O
with O
batch O
size O
32 O
. O

For O
target O
- O
adapting O
toevery O
target O
language O
, O
the O
few- O
shot O
data O
is O
a O
sampled O
bucket O
in O
this O
language O
, O
and O
we O
ﬁnetune O
on O
the O
bucket O
for O
50 O
epochs O
with O
early O
- O
stopping O
of O
10 O
epochs O
. O

The O
batch O
size O
is O
set O
to O
the O
number O
of O
shots O
in O
the O
bucket O
. O

Each O
target O
- O
adapting O
experiment O
is O
repeated O
40 O
times O
us- O
ing O
the O
40 O
buckets O
. O

We O
use O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
default O
parameters O
in O
both O
stages O
with O
learning O
rates O
searched O
over O
f1e 5;3e 5;5e 5;7e 5 O
g. O

For O
CLS O
tasks O
, O
we O
use O
mBERT O
’s O

[ O
CLS O
] O
token O
as O
the O
ﬁnal O
represen- O

5754 O
48 O
49 O
50 O
51 O
52 O
53 O
Validation O
Accuracy O
( O
% O
) O
05CountPerf O
. O

Distribution O
of O
German O
in O
MARC O
78 O
80 O
82 O
84 O
86 O
88 O
90 O
Validation O
Accuracy O
( O
% O
) O
010CountPerf O
. O

Distribution O
of O
Spanish O
in O
MLDoc O
46 O
48 O
50 O
52 O
54 O
Validation O
Accuracy O
( O
% O
) O
010CountPerf O
. O

Distribution O
of O
German O
in O
MARC O
78 O
80 O
82 O
84 O
86 O
88 O
90 O
Validation O
Accuracy O
( O
% O
) O
010 O
CountPerf O
. O

Distribution O
of O
Spanish O
in O
MLDocFigure O
1 O
: O
Histograms O
of O
dev O
set O
accuracies O
. O

Top O
: O
40 O
runs O
with O
different O
random O
seeds O
. O

Bottom O
: O
40 O
runs O
with O
different O
1 O
- O
shot O
buckets O
. O

Left O
: O
DE O
MARC O
. O

Right O
: O
ES O
MLDoc O
. O

The O
variance O
due O
to O
buckets O
is O
larger O
. O

tation O
. O

For O
NER O
and O
POS O
, O
following O
Devlin O
et O
al O
. O

( O
2019 O
) O
, O
we O
use O
a O
linear O
classiﬁer O
layer O
on O
top O
of O
the O
representation O
of O
each O
tokenized O
word O
, O
which O
is O
its O
last O
wordpiece O
( O
He O
and O
Choi O
, O
2020 O
) O
. O

We O
set O
the O
maximum O
sequence O
length O
to O
128 O
after O
wordpiece O
tokenization O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
, O
in O
all O
experiments O
. O

Further O
implementation O
details O
are O
shown O
in O
our O
Reproducibility O
Checklist O
in O
Ap- O
pendix O
§ O
A. O
5 O
Results O
and O
Discussion O
5.1 O
Source O
- O
Training O
Results O
The O
ZS O
- O
XLT O
performance O
from O
English O
( O
EN O
) O
to O
target O
languages O
of O
the O
four O
CLS O
tasks O
are O
shown O
in O
theK= O
0column O
in O
Table O
2 O
. O

For O
NER O
and O
POS O
, O
the O
results O
are O
shown O
in O
Figure O
2 O
. O

For O
XTREME O
tasks O
( O
XNLI O
, O
PAWSX O
, O
NER O
, O
POS O
) O
, O
our O
implementation O
delivers O
results O
compa- O
rable O
to O
Hu O
et O
al O
. O
( O
2020 O
) O
. O

For O
MLDoc O
, O
our O
results O
are O
comparable O
to O
( O
Dong O
and O
de O
Melo O
, O
2019 O
; O
Wu O
and O
Dredze O
, O
2019 O
; O
Eisenschlos O
et O
al O
. O
, O
2019 O
) O
. O

It O
is O
worth O
noting O
that O
reproducing O
the O
exact O
results O
is O
challenging O
, O
as O
suggested O
by O
Keung O
et O

al O
. O
( O
2020a O
) O
. O

For O
MARC O
, O
our O
zero O
- O
shot O
results O
are O
worse O
than O
Keung O
et O

al O
. O

( O
2020b O
) O
’s O
who O
use O
the O
dev O
set O
of O
each O
target O
language O
for O
model O
selection O
while O
we O
use O
EN O
dev O
, O
following O
the O
common O
true O
ZS O
- O
XLT O
setup O
. O

5.2 O
Target O
- O
Adapting O
Results O
Variance O
of O
Few O
- O
Shot O
Transfer O
. O

We O
hypothesize O
that O
FS O
- O
XLT O
suffers O
from O
large O
variance O
( O
Dodge O
et O
al O
. O
, O
2020 O
) O
due O
to O
the O
large O
model O
complexity O
and O
small O
amount O
of O
data O
in O
a O
bucket O
. O

To O
test O
this O
empirically O
, O
we O
ﬁrst O
conduct O
two O
experiments O
on O
MLDoc O
and O
MARC O
. O

First O
, O
for O
a O
ﬁxed O
random O
seed O
, O
we O
repeat O
1 O
- O
shot O
target O
- O
adapting O
40 O
times O
using O
dif- O
ferent O
1 O
- O
shot O
buckets O
in O
German O
( O
DE O
) O
and O
Spanish O
( O
ES O
) O
. O

Second O
, O
for O
a O
ﬁxed O
1 O
- O
shot O
bucket O
, O
we O
repeat O
the O
same O
experiment O
40 O
times O
using O
random O
seedsinf0 O
. O
. O

.39 O
g. O
Figure O
1 O
presents O
the O
dev O
set O
perfor- O
mance O
distribution O
of O
the O
40 O
runs O
with O
40 O
random O
seeds O
( O
top O
) O
and O
40 O
1 O
- O
shot O
buckets O
( O
bottom O
) O
. O

With O
exactly O
the O
same O
training O
data O
, O
using O
differ- O
ent O
random O
seeds O
yields O
a O
1–2 O
accuracy O
difference O
of O
FS O
- O
XLT O
( O
Figure O
1 O
top O
) O
. O

A O
similar O
phenomenon O
has O
been O
observed O
in O
ﬁnetuning O
monolingual O
en- O

coders O
( O
Dodge O
et O
al O
. O
, O
2020 O
) O
and O
multilingual O
en- O
coders O
with O
ZS O
- O
XLT O
( O
Keung O
et O

al O
. O
, O
2020a O
; O
Wu O
and O
Dredze O
, O
2020b O
; O
Xia O
et O
al O
. O
, O
2020 O
) O
; O
we O
show O
this O
ob- O
servation O
also O
holds O
for O
FS O
- O
XLT O
. O

The O
key O
takeaway O
is O
that O
varying O
the O
buckets O
is O
a O
more O
severe O
problem O
. O

It O
causes O
much O
larger O
variance O
( O
Figure O
1 O
bottom O
): O
The O
maximum O
accuracy O
difference O
is O
6 O
for O
DE O
MARC O
and10 O
for O
ES O
MLDoc O
. O

This O
can O
be O
due O
to O
the O
fact O
that O
difﬁculty O
of O
individual O
examples O
varies O
in O
a O
dataset O
( O
Swayamdipta O
et O
al O
. O
, O
2020 O
) O
, O
re- O
sulting O
in O
different O
amounts O
of O
information O
encoded O
in O
buckets O
. O

This O
large O
variance O
could O
be O
an O
issue O
when O
com- O
paring O
different O
few O
- O
shot O
learning O
algorithms O
. O

The O
bucket O
choice O
is O
a O
strong O
confounding O
factor O
that O
may O
obscure O
the O
strength O
of O
a O
promising O
few O
- O
shot O
technique O
. O

Therefore O
, O
for O
fair O
comparison O
, O
it O
is O
nec- O
essary O
to O
work O
with O
a O
ﬁxed O
set O
of O
few O
shots O
. O

We O
propose O
to O
ﬁx O
the O
sampled O
buckets O
for O
unbiased O
comparison O
of O
different O
FS O
- O
XLT O
methods O
. O

We O
pub- O
lish O
the O
sampled O
buckets O
from O
the O
six O
multilingual O
datasets O
as O
a O
ﬁxed O
and O
standardized O
few O
- O
shot O
evalu- O
ation O
benchmark O
. O

In O
what O
follows O
, O
each O
FS O
- O
XLT O
experiment O
is O
re- O
peated O
40 O
times O
using O
40 O
different O
buckets O
with O
the O
same O
ﬁxed O
random O
seed O
; O
we O
report O
mean O
and O
standard O
deviation O
. O

As O
noted O
, O
the O
variance O
due O
to O
random O
seeds O
is O
smaller O
( O
cf O
. O
, O
Figure O
1 O
) O
and O
has O
been O
well O
studied O
before O
( O
Reimers O
and O
Gurevych O
, O
2017 O
; O
Dodge O
et O
al O
. O
, O
2020 O
) O
. O

In O
this O
work O
, O
we O
thus O
fo- O
cus O
our O
attention O
and O
limited O
computing O
resources O
on O
understanding O
the O
impact O
of O
buckets O
, O
the O
newly O
detected O
source O
of O
variance O
. O

However O
, O
we O
encour- O
age O
practitioners O
to O
report O
results O
with O
both O
factors O
considered O
in O
the O
future O
. O

Different O
Numbers O
of O
Shots O
. O

A O
comparison O
concerning O
the O
number O
of O
shots O
( O
K O
) O
, O
based O
on O
the O
few O
- O
shot O
results O
in O
Table O
2 O
and O
Figure O
2 O
, O
reveals O
that O
the O
buckets O
largely O
improve O
model O
performance O
on O
a O
majority O
of O
tasks O
( O
MLDoc O
, O
MARC O
, O
POS O
, O
NER O
) O
over O
zero O
- O
shot O
results O
. O

This O
is O
in O
line O
with O
prior O
work O
( O
Lauscher O
et O
al O
. O
, O
2020 O
; O
Hedderich O
et O
al O
. O
, O
2020 O
) O
and O
follows O
the O
success O
of O
work O
on O
using O
boot- O
strapped O
data O
( O
Chaudhary O
et O
al O
. O
, O
2019 O
; O
Sherborne O

5755K=0 O
K=1 O
K=2 O
K=4 O
K=8MLDocEN O
96.88 O
- O
- O
- O
- O
DE O
88.30 O
90.361.48 O
90.770.87 O
91.850.83 O
91.980.82 O
FR O
83.05 O
88.942.46 O
89.711.68 O
90.800.88 O
91.010.94 O
ES O
81.90 O
83.992.35 O
85.651.60 O
86.301.85 O
88.461.90 O
IT O
74.13 O
74.972.04 O
75.291.57 O
76.431.41 O
78.121.25 O

RU O
72.33 O
77.404.27 O
80.571.37 O
81.331.33 O
81.911.21 O
ZH O
84.38 O
87.181.45 O
87.311.53 O
88.331.11 O
88.721.05 O
JA O
74.58 O
76.231.59 O
76.712.12 O
78.602.43 O
81.171.72MARCEN O
64.52 O
- O
- O
- O
- O
DE O
49.62 O
51.501.58 O
52.760.87 O
52.781.00 O
53.320.59 O
FR O
47.30 O
49.321.34 O
49.701.43 O
50.640.94 O
51.230.76 O
ES O
48.44 O
49.721.24 O
49.961.12 O
50.451.22 O
51.250.93 O
ZH O
40.40 O
43.191.76 O
44.451.36 O
45.401.26 O
46.400.93 O
JA O
38.84 O
41.952.09 O
43.631.30 O
43.980.89 O
44.440.69XNLIEN O
82.67 O
- O
- O
- O
- O
DE O
70.32 O
70.580.36 O
70.600.34 O

70.610.39 O
70.700.50 O
FR O
73.57 O
73.410.48 O
73.740.46 O
73.570.49 O
73.770.44 O
ES O
73.71 O
73.840.40 O
73.870.44 O
73.740.48 O
73.870.46 O
RU O
68.70 O
68.810.52 O
68.760.54 O
68.870.55 O
68.810.77 O
ZH O
69.32 O
69.730.94 O
69.750.94 O
70.560.76 O
70.620.86 O
AR O
64.97 O
64.750.36 O
64.820.23 O
64.820.23 O
64.940.37 O
BG O
67.58 O
68.150.69 O
68.190.75 O
68.550.67 O
68.320.70 O
EL O
65.67 O
65.640.40 O
65.730.36 O
65.800.41 O
66.000.53 O
HI O
56.57 O
56.940.82 O
57.070.82 O
57.211.14 O
57.821.18 O
SW O
48.08 O
50.331.08 O
50.281.24 O
51.080.62 O
51.010.79 O
TH O
46.17 O
49.432.60 O
50.082.42 O
51.322.07 O
52.162.43 O
TR O
60.40 O
61.020.68 O
61.200.61 O
61.350.49 O
61.310.56 O
UR O
57.05 O
57.560.85 O
57.830.91 O
58.200.93 O
58.671.03 O
VI O
69.82 O
70.040.59 O
70.140.75 O
70.230.63 O
70.410.70PAWSXEN O
93.90 O
- O
- O
- O
- O
DE O
83.80 O
84.140.40 O
84.080.42 O
84.040.47 O
84.230.66 O
FR O
86.90 O
87.070.27 O
87.060.37 O
87.030.31 O
86.940.41 O
ES O
88.25 O
87.900.54 O
87.800.56 O
87.840.53 O
87.850.75 O
ZH O
77.75 O
77.710.37 O
77.630.47 O
77.680.51 O
77.820.64 O
JA O
73.30 O
73.780.75 O
73.711.04 O
73.480.69 O
73.791.28 O
KO O
72.05 O
73.751.30 O
73.111.05 O
73.790.92 O
73.310.61 O
Table O
2 O
: O
Zero O
- O
shot O
( O
column O
K= O
0 O
) O
and O
few O
- O
shot O
( O
columnsK O
> O
0 O
) O
results O
( O
Acc O
. O

in O
% O
) O
on O
the O
test O
set O
for O
CLS O
tasks O
. O

Green O

[ O
red O
] O
: O
few O
- O
shot O
transfer O
outper- O
forms O
[ O
underperforms O
] O
zero O
- O
shot O
transfer O
. O

et O

al O
. O
, O
2020 O
) O
. O

In O
general O
, O
we O
observe O
that O
: O
1)1 O
- O
shot O
buckets O
bring O
the O
largest O
relative O
performance O
improvement O
over O
ZS O
- O
XLT O
; O
2)the O
gains O
follow O
the O
increase O
of O
K O
, O
but O
with O
diminishing O
returns O
; O
3)the O
performance O
variance O
across O
the O
40 O
buckets O
decreases O
as O
Kin- O
creases O
. O

These O
observations O
are O
more O
pronounced O
for O
POS O
and O
NER O
; O
e.g. O
, O
1 O
- O
shot O
EN O
to O
Urdu O
( O
UR O
) O
POS O
transfer O
shows O
gains O
of O
22F1points O
( O
52.40 O
with O
zero O
- O
shot O
, O
74.95 O
with O
1 O
- O
shot O
) O
. O

For O
individual O
runs O
, O
we O
observe O
that O
models O
in O
FS O
- O
XLT O
tend O
to O
overﬁt O
the O
buckets O
quickly O
at O
small O
Kvalues O
. O

For O
example O
, O
in O
around O
32 O
% O
of O
NER O
1- O
shot O
buckets O
, O
the O
model O
achieves O
the O
best O
dev O
score O
right O
after O
the O
ﬁrst O
epoch O
; O
continuing O
the O
training O
only O
degrades O
performance O
. O

Similar O
observations O
hold O
for O
semantic O
tasks O
like O
MARC O
, O
where O
in O
10 O
out O
of O
40 O
DE O
1 O
- O
shot O
buckets O
, O
the O
dev O
set O
perfor- O
mance O
peaks O
at O
epoch O
1 O
( O
cf O
. O
learning O
curve O
in O
Ap- O
pendix O
§ O
D O
Figure O
6 O
) O
. O

This O
suggests O
the O
necessity O
of O
running O
the O
target O
- O
adapting O
experiments O
on O
multi- O
ple O
buckets O
if O
reliable O
conclusions O
are O
to O
be O
drawn O
. O

Different O
Downstream O
Tasks O
. O

The O
models O
for O
different O
tasks O
present O
various O
levels O
of O
sensitiv O
- O
ity O
to O
FS O
- O
XLT O
. O

Among O
the O
CLS O
tasks O
that O
require O
semantic O
reasoning O
, O
FS O
- O
XLT O
beneﬁts O
MLDoc O
the O
most O
. O

This O
is O
not O
surprising O
given O
the O
fact O
that O
key- O
word O
matching O
can O
largely O
solve O
MLDoc O
( O
Artetxe O
et O
al O
. O
, O
2020a O
, O
b O
): O
A O
few O
examples O
related O
to O
target O
language O
keywords O
are O
expected O
to O
signiﬁcantly O
improve O
performance O
. O

FS O
- O
XLT O
also O
yields O
promi- O
nent O
gains O
on O
the O
Amazon O
review O
classiﬁcation O
dataset O
MARC O
. O

Similar O
to O
MLDoc O
, O
we O
hypothe- O
size O
that O
just O
matching O
a O
few O
important O
opinion O
and O
sentiment O
words O
( O
Liu O
, O
2012 O
) O
in O
the O
target O
language O
brings O
large O
gains O
already O
. O

We O
provide O
further O
qual- O
itative O
analyses O
in O
§ O
5.4 O
. O

XNLI O
and O
PAWSX O
behave O
differently O
from O
MLDoc O
and O
MARC O
. O

XNLI O
requires O
higher O
level O
semantic O
reasoning O
on O
pairs O
of O
sentences O
. O

FS- O
XLT O
performance O
improves O
modestly O
( O
XNLI O
) O
or O
even O
decreases O
( O
PAWSX O
- O
ES O
) O
compared O
to O
ZS- O
XLT O
, O
even O
with O
large O
K. O
PAWSX O
requires O
a O
model O
to O
distinguish O
adversarially O
designed O
non- O
paraphrase O
sentence O
pairs O
with O
large O
lexical O
over- O
lap O
like O
“ O
Flights O
from O
New O
York O
to O
Florida O
” O
and O
“ O
Flights O
from O
Florida O
to O
New O
York O
” O
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
. O

This O
poses O
a O
challenge O
for O
FS O
- O
XLT O
, O
given O
the O
small O
amount O
of O
target O
language O
information O
in O
the O
buckets O
. O

Therefore O
, O
when O
buckets O
are O
small O
( O
e.g. O
,K= O
1 O
) O
and O
for O
challenging O
semantic O
tasks O
like O
PAWSX O
, O
the O
buckets O
do O
not O
substantially O
help O
. O

Annotating O
more O
shots O
in O
the O
target O
language O
is O
an O
intuitive O
solution O
. O

Designing O
task O
- O
speciﬁc O
pretrain- O

ing/ﬁnetuning O
objectives O
could O
also O
be O
promising O
( O
Klein O
and O
Nabi O
, O
2020 O
; O
Ram O
et O
al O
. O
, O
2021 O
) O
. O

Unlike O
CLS O
tasks O
, O
POS O
and O
NER O
beneﬁt O
from O
FS O
- O
XLT O
substantially O
. O

We O
speculate O
that O
there O
are O
two O
reasons O
: O
1)Both O
tasks O
often O
require O
little O
to O
no O
high O
- O
level O
semantic O
understanding O
or O
reasoning O
; O
2 O
) O
due O
to O
i.i.d O
. O

sampling O
, O
train O
/ O
dev O
/ O
test O
splits O
are O
likely O
to O
have O
overlapping O
vocabulary O
, O
and O
the O
labels O
in O
the O
buckets O
can O
easily O
propagate O
to O
dev O
and O
test O
. O

We O
delve O
deeper O
into O
these O
conjectures O
in O
§ O
5.4 O
. O

Different O
Languages O
. O

For O
languages O
that O
are O
more O
distant O
from O
EN O
, O
e.g. O
, O
with O
different O
scripts O
, O
small O
lexical O
overlap O
, O
or O
fewer O
common O
typological O
features O
( O
Pires O
et O
al O
. O
, O
2019 O
; O
Wu O
and O
Dredze O
, O
2020a O
) O
, O
FS O
- O
XLT O
introduces O
crucial O
lexical O
and O
structural O
information O
to O
guide O
the O
update O
of O
embedding O
and O
transformer O
layers O
in O
mBERT O
. O

We O
present O
several O
ﬁndings O
based O
on O
the O
NER O
and O
POS O
results O
for O
a O
typologically O
diverse O
lan- O
guage O
sample O
. O

Figure O
2 O
shows O
that O
for O
languages O
with O
non O
- O
Latin O
scripts O
( O
different O
from O
EN O
) O
, O
despite O

5756 O
NL O
82.8FR O
80.4DE O
79.0IT O
80.3PT O
79.3AF O
78.4ET O
71.9HU O
71.3ES O
77.2MS O
68.6SW O
68.4FI O
68.4TL O
69.2TR O
65.8VI O
64.7EU O
55.4JV O
61.2ID O
60.10510152025303540F1 O
Score O
Improvements O
Same O
Script O
= O
Yes O
EL O
75.2BG O
78.6KA O
61.3KO O
46.5ML O
46.8MR O
54.7MY O
42.5HI O
65.8TA O
46.1HE O
56.4RU O
65.2BN O
64.2TE O
50.0TH O
1.5KK O
40.3JA O
7.2AR O
39.9UR O
40.8YO O
35.5FA O
40.7ZH O
13.9 O
Same O
Script O
= O
No O
F1 O
Score O
Improvements O
1 O
- O
shot O
2 O
- O
shot O
4 O
- O
shot O
NL O
88.3PT O
86.5ID O
70.8ET O
79.2IT O
86.0DE O
86.4ES O
86.6FI O
74.5AF O
86.6TR O
57.6FR O
82.5HU O
75.1VI O
55.0EU O
49.505101520253035F1 O
Score O
Improvements O
RU O
86.4HE O
76.8TE O
67.5BG O
87.0EL O
81.9AR O
66.5TA O
53.5ZH O
63.0MR O
58.7HI O
64.3FA O
65.7KO O
42.3UR O
52.4JA O
47.6 O
Figure O
2 O
: O
Improvement O
in O
F1(mean O
and O
standard O
deviation O
) O
of O
FS O
- O
XLT O
over O
ZS O
- O
XLT O
( O
numbers O
shown O
on O
x- O
axis O
beneath O
each O
language O
) O
for O
NER O
( O
top O
) O
and O
POS O
( O
bottom O
) O
for O
three O
different O
bucket O
sizes O
. O

See O
Appendix O
§ O
D O
( O
Tables O
12 O
and O
13 O
) O
for O
absolute O
numerical O
values O
. O

Task O
Factor O
S O
P O
NERlexical O
overlap O
-0.34 O
-0.35 O
# O
of O
common O
linguistic O
features O
-0.37 O
-0.10 O
POSlexical O
overlap O
-0.63 O
-0.50 O
# O
of O
common O
linguistic O
features O
-0.57 O
-0.54 O
Table O
3 O
: O
Correlations O
between O
FS O
- O
XLT O
F1score O
gains O
and O
the O
two O
factors O
( O
lexical O
overlap O
and O
the O
number O
of O
common O
linguistic O
features O
with O
EN O
) O
when O
considered O
independently O
for O
POS O
and O
NER O
: O
S O
/ O
R O
denotes O
Spear- O
man’s O
/ O
Pearson O
’s O
. O
See O
Footnotes O
3 O
, O
4 O
for O
information O
on O
the O
two O
factors O
. O

their O
small O
to O
non O
- O
existent O
lexical O
overlap3and O
di- O
verging O
typological O
features O
( O
see O
Appendix O
§ O
D O
Ta- O
bles O
9 O
and O
14 O
) O
, O
the O
performance O
boosts O
are O
gen- O
erally O
larger O
than O
those O
in O
the O
same O
- O
script O
target O
languages O
: O
6.2 O
vs. O
3.0 O
average O
gain O
in O
NER O
and O
11.4 O
vs. O
5.4 O
in O
POS O
for O
K= O
1 O
. O

This O
clearly O
man- O
ifests O
the O
large O
information O
discrepancy O
between O
target O
- O
language O
buckets O
and O
source O
- O
language O
data O
. O

EN O
data O
is O
less O
relevant O
to O
these O
languages O
, O
so O
they O
obtain O
very O
limited O
gain O
from O
source O
- O
training O
, O
reﬂected O
by O
their O
low O
ZS O
- O
XLT O
scores O
. O

With O
a O
small O
amount O
of O
target O
- O
language O
knowledge O
in O
the O
buckets O
, O
the O
performance O
is O
improved O
dramatically O
, O
highlighting O
the O
effectiveness O
of O
FS O
- O
XLT O
. O

Table O
3 O
shows O
that O
, O
besides O
script O
form O
, O
lexical O
overlap O
and O
the O
number O
of O
linguistic O
features O
com- O
3We O
deﬁne O
lexical O
overlap O
asjVjL\jVjEN O
jVjENwhere O
Vdenotes O
vocabulary.jVjLis O
computed O
with O
the O
40 O
buckets O
of O
a O
target O
language O
L.mon O

with O
EN4also O
contribute O
directly O
to O
FS O
- O
XLT O
performance O
difference O
among O
languages O
: O
There O
is O
a O
moderate O
negative O
correlation O
between O
F1score O
gains O
vs. O
the O
two O
factors O
when O
considered O
indepen- O
dently O
for O
both O
syntactic O
tasks O
: O
The O
fewer O
over- O
laps O
/ O
features O
a O
target O
language O
shares O
with O
EN O
, O
the O
larger O
the O
gain O
FS O
- O
XLT O
achieves O
. O

This O
again O
stresses O
the O
importance O
of O
buckets O
– O
they O
contain O
target O
- O
language O
- O
speciﬁc O
knowledge O
about O
a O
task O
that O
can O
not O
be O
obtained O
by O
ZS O
- O
XLT O
, O
which O
solely O
relies O
on O
language O
similarity O
. O

Interest- O
ingly O
, O
Pearson O
’s O
indicates O
that O
common O
linguistic O
features O
are O
much O
less O
linearly O
correlated O
with O
FS- O
XLT O
gains O
in O
NER O
than O
in O
POS O
. O

5.3 O
Importance O
of O
Source O
- O
Training O
Table O
4 O
reports O
the O
performance O
drop O
when O
directly O
carrying O
out O
target O
- O
adapting O
, O
without O
any O
prior O
source O
- O
training O
of O
mBERT O
. O

We O
show O
the O
scores O
for O
MLDoc O
and O
PAWSX O
as O
a O
simple O
and O
a O
chal- O
lenging O
CLS O
task O
, O
respectively O
. O

For O
NER O
and O
POS O
, O
we O
select O
two O
high- O
( O
Russian O
( O
RU O
) O
, O
ES O
) O
, O
mid- O
( O
Viet- O
namese O
( O
VI O
) O
, O
Turkish O
( O
TR O
) O
) O
, O
and O
low O
- O
resource O
lan- O
guages O
( O
Tamil O
( O
TA O
) O
, O
Marathi O
( O
MR O
) O
) O

each.5 O
The O
results O
clearly O
indicate O
that O
omitting O
the O
4Following O
Pires O
et O
al O
. O

( O
2019 O
) O
, O
we O
use O
six O
WALS O
features O
: O
81A O
( O
Order O
of O
Subject O
, O
Object O
and O
Verb O
) O
, O
85A O
( O
Order O
of O
Ad- O
position O
and O
Noun O
) O
, O
86A O
( O
Order O
of O
Genitive O
and O
Noun O
) O
, O
87A O
( O
Order O
of O
Adjective O
and O
Noun O
) O
, O
88A O
( O
Order O
of O
Demonstrative O
and O
Noun O
) O
, O
and O
89A O
( O
Order O
of O
Numeral O
and O
Noun O
) O
. O

5The O
categorization O
based O
on O
resource O
availability O
is O
ac- O
cording O
to O
WikiSize O
( O
Wu O
and O
Dredze O
, O
2020a O
) O
. O

5757MLDoc O
PAWSX O
POS O
NER O
K=1 O
K=8 O

K=1 O
K=8 O

K=1 O
K=4 O

K=1 O
K=4 O
DE O
-37.73 O

-7.67 O
-31.11 O
-30.82 O

RU O
-15.89 O
-3.20 O
-48.19 O
-35.77 O
FR O
-38.14 O
-13.21 O
-33.02 O
-32.34 O

ES O
-9.51 O
-0.93 O
-63.98 O
-41.53 O
ES O
-33.69 O
-14.38 O
-33.76 O
-33.97 O
VI O
-7.82 O
-0.36 O
-54.41 O
-41.45 O
IT O
-33.63 O
-12.62 O
- O
- O
TR O
-15.05 O
-8.08 O
-54.35 O
-34.52 O
RU O
-30.66 O
-11.08 O
- O
- O
TA O
-13.72 O
-4.40 O
-34.70 O
-24.81 O
ZH O
-37.31 O

-12.57 O
-23.74 O
-23.65 O
MR O

-11.34 O
-3.63 O
-40.10 O
-25.68 O
JA O
-29.82 O
-14.32 O
-20.97 O
-20.82 O
- O
- O
- O
- O
- O
KO O
- O
- O
-19.83 O
-19.68 O
- O
- O
- O
- O
- O
Table O
4 O
: O
Performance O
drop O
when O
conducting O
target- O
adapting O
without O
source O
- O
training O
. O

507251341322232507251341322232Jaccard O
Index O
of O
Buckets O
and O
Improved O
Predictions O
( O
FA O
) O
9.9810.0010.0210.0410.0610.08 O
Figure O
3 O
: O
Normalized O
( O
with O
softmax O
) O

Jaccard O
index O
( O
% O
) O
of O
a O
bucket O
( O
row O
) O
and O
the O
improved O
predictions O
achieved O
with O
10 O
buckets O
( O
column O
) O
. O

source O
- O
training O
stage O
yields O
large O
performance O
drops O
. O

Even O
larger O
variance O
is O
also O
observed O
in O
this O
scenario O
( O
cf O
. O

Appendix O
§ O
D O
Table O
11 O
) O
. O

There- O

fore O
, O
the O
model O
indeed O
learns O
, O
when O
trained O
on O
the O
source O
language O
, O
some O
transferable O
crosslingual O
features O
that O
are O
beneﬁcial O
to O
target O
languages O
, O
both O
for O
semantic O
and O
syntactic O
tasks O
. O

5.4 O
Importance O
of O
Lexical O
Features O
We O
now O
investigate O
the O
sources O
of O
gains O
brought O
by O
FS O
- O
XLT O
over O
ZS O
- O
XLT O
. O

For O
syntactic O
tasks O
, O
we O
take O
Persian O
( O
FA O
) O
POS O
as O
an O
example O
. O

Figure O
3 O
visualizes O
the O
lexical O
overlap O
, O
measured O
by O
the O
Jaccard O
index O
, O
of O
10 O
1 O
- O
shot O
buck- O
ets O
( O
rows O
) O
and O
the O
improved O
word O
- O
label O
predictions O
introduced O
by O
target O
- O
adapting O
on O
each O
of O
the O
buck- O
ets O
( O
columns O
) O
. O

In O
more O
detail O
, O
for O
column O
c O
, O
we O
collect O
the O
set O
( O
denoted O
as O
Cc O
) O
of O
all O
test O
set O
words O
whose O
label O
is O
incorrectly O
predicted O
by O
the O
zero- O
shot O
model O
, O
but O
correctly O
predicted O
by O
the O
model O
trained O
on O
the O
c O
- O
th O
bucket O
. O

For O
row O
i O
, O
we O
denote O
withBithe O
set O
of O
words O
occurring O
in O
bucket O
i. O

The O
ﬁgure O
shows O
in O
cell O
( O
i O
, O
k O
) O
the O
Jaccard O
index O
of O
Bi O
andCk O
. O

The O
bright O
color O
( O
i.e. O
, O
higher O
lexical O
over- O
lap O
) O
on O
the O
diagonal O
reﬂects O
that O
the O
improvements O
0k2kCountIntersected O
False O
True O
0k2kCountIntersected O
False O
True O
Bucket O
Index0k10kCountIntersected O
False O
TrueFigure O
4 O
: O
Improvement O
of O
word O
- O
label O
predictions O
intro- O

duced O
by O
a O
bucket O
( O
x O
- O
axis O
) O
in O
FA O
( O
top O
) O
, O
UR O
( O
mid O
) O
, O
and O
HI O
( O
bottom O
) O
, O
in O
relation O
to O
the O
words O
’ O
presence O
in O
the O
bucket O
( O
True O
or O
False O
) O
. O

1 O
2 O
3 O
4 O
51 O
2 O
3 O
4 O
50 O
- O
shot O
Predictions O
DE O
1 O
2 O
3 O
4 O
51 O
- O
shot O
Predictions O
DE O
100200300400500600 O
100200300400500600 O
1 O
2 O
3 O
4 O
51 O
2 O
3 O
4 O
50 O
- O
shot O
Predictions O
ZH O
1 O
2 O
3 O
4 O
51 O
- O
shot O
Predictions O
ZH O
100200300400500 O
100200300400500 O
Figure O
5 O
: O
MARC O
( O
5 O
classes O
) O
test O
set O
prediction O
confu- O
sion O
matrices O
. O

Top O
: O
DE O
. O

Bottom O
: O
ZH O
. O

Left O
: O
zero O
- O
shot O
models O
. O

Right O
: O
1 O
- O
shot O
models O
. O

Colorbar O
numbers O
rep- O
resent O
the O
number O
of O
instances O
in O
that O
cell O
. O

introduced O
by O
a O
bucket O
are O
mainly6those O
word- O
label O
predictions O
that O
are O
lexically O
more O
similar O
to O
the O
bucket O
than O
to O
other O
buckets O
. O

We O
also O
investigate O
the O
question O
: O
How O
many O
word O
- O
label O
predictions O
that O
are O
improved O
after O
FS- O
XLT O
occur O
in O
the O
bucket O
, O
i.e. O
, O
in O
the O
training O
data O
? O

Figure O
4 O
plots O
this O
for O
the O
40 O
1 O
- O
shot O
buckets O
in O
FA O
, O
UR O
, O
and O
Hindi O
( O
HI O
) O
. O

We O
see O
that O
many O
test O
words O
do O
occur O
in O
the O
bucket O
( O
shown O
in O
orange O
) O
, O
in O
line O
with O
recent O
ﬁndings O
( O
Lewis O
et O
al O
. O
, O
2021 O
; O
Elangovan O
et O
al O
. O
, O
2021 O
) O
. O

These O
analyses O
shed O
light O
on O
why O
the O
buckets O
beneﬁt O
NER O
/ O
POS O
– O
which O
heavily O
rely O
on O
lexical O
information O
– O
more O
than O
higher O
level O
semantic O
tasks O
. O

For O
the O
CLS O
task O
MARC O
, O
which O
requires O
un- O
6Note O
that O
the O
sampled O
buckets O
for O
POS O
are O
not O
completely O
disjoint O
( O
cf O
. O
sampling O
strategy O
in O
§ O
4 O
) O
. O

5758token O
[ O
SEP O
] O
. O

nicht O
! O

Die O
sehr O
Attn O

+4.13 O
+2.91 O
+1.84 O
-1.75 O
-0.92 O
-0.81 O
Table O
5 O
: O
Tokens O
with O
the O
highest O
attention O
change O
from O
[ O
CLS O
] O
, O
comparing O
zero O
- O
shot O
with O
a O
1 O
- O
shot O
DE O
bucket O
. O

derstanding O
product O
reviews O
, O
Figure O
5 O
visualizes O
the O
confusion O
matrices O
of O
test O
set O
predictions O
for O
DE O
and O
Chinese O
( O
ZH O
) O
zero- O
and O
1 O
- O
shot O
models O
; O
axis O
ticks O
are O
review O
scores O
in O
f1;2;3;4;5 O
g. O

The O
squares O
on O
the O
diagonals O
in O
the O
two O
left O
heatmaps O
show O
that O
parameter O
initialization O
on O
EN O
is O
a O
good O
basis O
for O
well O
- O
performing O
ZS O
- O
XLT O
: O
This O
is O
particu- O
larly O
true O
for O
DE O
, O
which O
is O
linguistically O
closer O
to O
EN O
. O

Two O
extreme O
review O
scores O
– O
1 O
( O
for O
DE O
) O
and O
5 O
( O
for O
ZH O
) O
– O
have O
the O
largest O
confusions O
. O

The O
two O
right O
heatmaps O
show O
that O
improvements O
brought O
by O
the O
1 O
- O
shot O
buckets O
are O
mainly O
achieved O
by O
cor- O
rectly O
predicting O
more O
cases O
of O
the O
two O
extreme O
review O
scores O
: O
2!1 O
( O
DE O
) O
and O
4!5 O
( O
ZH O
) O
. O

But O
the O
more O
challenging O
cases O
( O
reviews O
with O
scores O
2 O
, O
3 O
, O
4 O
) O
, O
which O
require O
non O
- O
trivial O
reasoning O
, O
are O
not O
signiﬁcantly O
improved O
, O
or O
even O
become O
worse O
. O

We O
inspect O
examples O
that O
are O
incorrectly O
pre- O
dicted O
by O
the O
few O
- O
shot O
model O
( O
predicting O
1 O
) O
, O
but O
are O
correctly O
predicted O
by O
the O
zero O
- O
shot O
model O
( O
predict- O
ing O
2 O
) O
. O

Speciﬁcally O
, O
we O
compute O
the O
difference O
of O
where O
[ O
CLS O
] O
attends O
to O
, O
before O
and O
after O
adapting O
the O
model O
on O
a O
1 O
- O
shot O
DE O
bucket O
. O

We O
extract O
and O
average O
attentions O
computed O
by O
the O
12 O
heads O
from O
the O
topmost O
transformer O
layer O
. O

Table O
5 O
shows O
that O
“ O
nicht O
” O
( O
“ O
not O
” O
) O
draws O
high O
attention O
change O
from O
[ O
CLS O
] O
. O

“ O
Nicht O
” O
( O
i.e. O
, O
nega- O
tion O
) O
by O
itself O
is O
not O
a O
reliable O
indicator O
of O
senti- O
ment O
, O
so O
giving O
the O
lowest O
score O
to O
reviews O
solely O
because O
they O
contain O
“ O
nicht O
” O
is O
not O
a O
good O
strategy O
. O

The O
following O
review O
is O
classiﬁed O
as O
1 O
by O
the O
1 O
- O
shot O
model O
, O
but O
2 O
is O
the O
gold O
label O
( O
as O
the O
review O
is O
not O
entirely O
negative O
): O
“ O
Die O
Uhr O
ging O
nicht O
einmal O
eine O
Minute O
... O

Op- O
tisch O
allerdings O
sehr O
sch O
¨on O
. O

” O
( O
“ O
The O
clock O
did O
n’t O
even O
work O
one O
minute O
... O

Visually O
, O
however O
, O
very O
nice O
. O
” O
) O
Pretrained O
multilingual O
encoders O
are O
shown O
to O
learn O
and O
store O
“ O
language O
- O
agnostic O
” O
features O
( O
Pires O
et O
al O
. O
, O
2019 O
; O
Zhao O
et O

al O
. O
, O
2020 O
) O
; O
§ O
5.3 O
shows O
that O
source O
- O
training O
mBERT O
on O
EN O
substantially O
ben- O
eﬁts O
other O
languages O
, O
even O
for O
difﬁcult O
semantic O
tasks O
like O
PAWSX O
. O

Conditioning O
on O
such O
language- O
agnostic O
features O
, O
we O
expect O
that O
the O
buckets O
should O
lead O
to O
good O
understanding O
and O
reasoning O
capabili- O
ties O
for O
a O
target O
language O
. O

However O
, O
plain O
few O
- O
shot O
ﬁnetuning O
still O
relies O
heavily O
on O
unintended O
shallowlexical O
cues O
and O
shortcuts O
( O
Niven O
and O
Kao O
, O
2019 O
; O
Geirhos O
et O
al O
. O
, O
2020 O
) O
that O
generalize O
poorly O
. O

Other O
open O
research O
questions O
for O
future O
work O
arise O
: O
How O
do O
we O
overcome O
this O
excessive O
reliance O
on O
lexical O
features O
? O

How O
can O
we O
leverage O
language O
- O
agnostic O
features O
with O
few O
shots O
? O

Our O
standardized O
buckets O
, O
baseline O
results O
, O
and O
analyses O
are O
the O
initial O
step O
to- O
wards O
researching O
and O
answering O
these O
questions O
. O

5.5 O
Target O
- O
Adapting O
Methods O
SotA O
few O
- O
shot O
learning O
methods O
( O
Chen O
et O
al O
. O
, O
2019 O
; O
Wang O
et O
al O
. O
, O
2020 O
; O

Tian O
et O
al O
. O
, O
2020 O
; O
Dhillon O
et O
al O
. O
, O
2020 O
) O
from O
computer O
vision O
consist O
of O
two O
stages O
: O
1 O
) O
training O
on O
base O
- O
class O
images O
, O
and O
2 O
) O
few O
- O
shot O
ﬁnetuning O
using O
new O
- O
class O
images O
. O

Source O
- O
training O
and O
target O
- O
adapting O
stages O
of O
FS O
- O
XLT O
, O
albeit O
among O
languages O
, O
follow O
an O
approach O
very O
similar O
to O
these O
methods O
. O

Therefore O
, O
we O
test O
their O
effectiveness O
for O
crosslingual O
transfer O
. O

These O
methods O
are O
built O
upon O
cosine O
similarity O
that O
imparts O
inductive O
bias O
about O
distance O
and O
is O
more O
effective O
than O
a O
fully- O
connected O
classiﬁer O
layer O
( O
FC O
) O
with O
small O
K(Wang O
et O
al O
. O
, O
2020 O
) O
. O

Following O
( O
Chen O
et O
al O
. O
, O
2019 O
; O
Wang O
et O
al O
. O
, O
2020 O
; O
Tian O
et O
al O
. O
, O
2020 O
) O
, O
we O
freeze O
the O
em- O
bedding O
and O
transformer O
layers O
of O
mBERT O
, O
and O
explore O
four O
variants O
of O
the O
target O
- O
adapting O
stage O
using O
MARC O
. O

COS+Pooler O
. O

We O
randomly O
initialize O
a O
train- O
able O
weight O
matrix O
W2Rhcwherehis O
the O
hid- O
den O
dimension O
size O
and O
cis O
the O
number O
of O
classes O
. O

Rewriting O
Was[w1 O
; O
: O
: O
: O
; O
wi O
; O
: O
: O
: O
; O
wc O
] O
, O
we O
com- O
pute O
the O
logits O
of O
an O
input O
sentence O
representation O
x2Rh(from O
mBERT O
) O
belonging O
to O
class O
ias O

x|wi O
kxk2kwik2 O
; O
where O

is O
a O
scaling O
hyperparameter O
, O
set O
to O
10 O
in O
all O
experiments O
. O

During O
training O
, O
Wand O
mBERT O
’s O
pooler O
layer O
containing O
a O
linear O
layer O
and O
a O
tanh O
non O
- O
linearity O
are O
updated O
. O

FC+Pooler O
. O

During O
training O
, O
we O
update O
the O
lin- O
ear O
classiﬁer O
layer O
and O
mBERT O
’s O
pooler O
layer O
. O

FC O
only O
. O

During O
training O
, O
we O
only O
update O
the O
linear O
classiﬁer O
layer O
. O

This O
variant O
largely O
reduces O
model O
complexity O
and O
exhibit O
lower O
variance O
when O
Kis O
small O
. O

FC(reset)+Pooler O
. O

Similar O
to O
FC+Pooler O
, O
but O
the O
source O
- O
trained O
linear O
classiﬁer O
layer O
is O
randomly O
re O
- O
initialized O
before O
training O
. O

Table O
6 O
shows O
the O
performance O
of O
these O
methods O
along O
with O
full O
model O
ﬁnetuning O
( O
without O
freez- O
ing O
) O
. O

FC+Pooler O
performs O
the O
best O
among O
the O

5759Full O
- O
Model O
Finetuning O
FC O

only O
FC O
+ O
Pooler O
COS O
+ O
Pooler O
FC O
( O
reset O
) O
+ O

Pooler O
K=0 O
K=1 O
K=8 O

K=1 O
K=8 O

K=1 O
K=8 O

K=1 O
K=8 O

K=1 O
K=8 O

DE O
49.62 O
51.501.58 O
53.320.59 O
50.821.17 O
52.580.63 O
51.181.13 O
53.170.58 O
37.985.53 O
45.852.14 O
38.526.64 O
49.462.21 O
FR O
47.30 O
49.321.34 O
51.230.76 O
48.190.78 O
49.050.93 O
48.601.02 O
49.970.77 O
39.933.50 O
44.411.95 O
40.125.04 O
47.772.00 O
ES O
48.44 O
49.721.24 O
51.250.93 O
49.030.73 O
49.690.57 O
49.280.85 O
50.210.63 O
40.014.33 O
45.352.37 O
40.894.96 O
47.732.33 O
ZH O
40.40 O
43.191.76 O
46.400.93 O
41.901.15 O
43.340.88 O
42.301.37 O
44.420.65 O
33.105.48 O
38.311.87 O
31.837.00 O
42.072.19 O
JA O
38.84 O
41.952.09 O
44.440.69 O
40.761.76 O
43.140.76 O
41.401.74 O
43.810.56 O
34.364.19 O
38.951.80 O
32.805.17 O
41.181.68 O
Table O
6 O
: O
Accuracy O
( O
% O
) O
on O
MARC O
when O
varying O
classiﬁer O
head O
conﬁgurations O
. O

Full O
- O
Model O
Finetuning O
updates O
all O
parameters O
during O
training O
; O
the O
other O
four O
methods O
only O
update O
a O
subset O
as O
described O
in O
§ O
5.5 O
. O

The O
best O
results O
( O
excluding O
Full O
- O
Model O
Finetuning O
) O
are O
in O
bold O
. O

four O
for O
both O
K= O
1 O
andK= O
8 O
in O
all O
lan- O
guages O
. O

However O
, O
it O
underperforms O
the O
full O
model O
ﬁnetuning O
, O
especially O
when O
K= O
8.FC O
only O
is O
sub O
- O
optimal O
; O
yet O
the O
decrease O
in O
comparison O
to O
FC+Pooler O
is O
small O
, O
highlighting O
that O
EN O
- O
trained O
mBERT O
is O
a O
strong O
feature O
extractor O
. O

COS+Pooler O
andFC(reset)+Pooler O
perform O
considerably O
worse O
than O
the O
other O
two O
methods O
and O
zero O
- O
shot O
transfer O
– O
presumably O
because O
their O
new O
parameters O
need O
to O
be O
trained O
from O
scratch O
with O
few O
shots O
. O

We O
leave O
further O
exploration O
of O
other O
possibil- O
ities O
of O
exploiting O
crosslingual O
features O
through O
collapse O
- O
preventing O
regularization O
( O
Aghajanyan O
et O
al O
. O
, O
2021 O
) O
or O
contrastive O
learning O
( O
Gunel O
et O
al O
. O
, O
2021 O
) O
to O
future O
work O
. O

Integrating O
prompting O
( O
Brown O
et O
al O
. O
, O
2020 O
; O
Schick O
and O
Sch O
¨utze O
, O
2020 O
; O

Gao O
et O
al O
. O
, O
2020 O
; O
Liu O
et O
al O
. O
, O
2021 O
) O
– O
a O
strong O
per- O
forming O
few O
- O
shot O
learning O
methodology O
for O
NLP O
– O
into O
the O
crosslingual O
transfer O
learning O
pipeline O
is O
also O
a O
promising O
direction O
. O

6 O
Conclusion O
and O
Future O
Work O
We O
have O
presented O
an O
extensive O
study O
of O
few O
- O
shot O
crosslingual O
transfer O
. O

The O
focus O
of O
the O
study O
has O
been O
on O
an O
empirically O
detected O
performance O
vari- O
ance O
in O
few O
- O
shot O
scenarios O
: O
The O
models O
exhibit O
a O
high O
level O
of O
sensitivity O
to O
the O
choice O
of O
few O
shots O
. O

We O
analyzed O
and O
discussed O
the O
major O
causes O
of O
this O
variance O
across O
six O
diverse O
tasks O
for O
up O
to O
40 O
languages O
. O

Our O
results O
show O
that O
large O
language O
models O
tend O
to O
overﬁt O
to O
few O
shots O
quickly O
and O
mostly O
rely O
on O
shallow O
lexical O
features O
present O
in O
the O
few O
shots O
, O
though O
they O
have O
been O
trained O
with O
abundant O
data O
in O
English O
. O

Moreover O
, O
we O
have O
empirically O
validated O
that O
state O
- O
of O
- O
the O
- O
art O
few O
- O
shot O
learning O
methods O
in O
computer O
vision O
do O
not O
outper- O
form O
a O
conceptually O
simple O
alternative O
: O
Full O
model O
ﬁnetuning O
. O

Our O
study O
calls O
for O
more O
rigor O
and O
accurate O
re- O
porting O
of O
the O
results O
of O
few O
- O
shot O
crosslingual O
trans- O
fer O
experiments O
. O

They O
should O
include O
score O
distri- O
butions O
over O
standardized O
and O
ﬁxed O
few O
shots O
. O

Toaid O
this O
goal O
, O
we O
have O
created O
and O
provided O
such O
ﬁxed O
few O
shots O
as O
a O
standardized O
benchmark O
for O
six O
multilingual O
datasets O
. O

Few O
- O
shot O
learning O
is O
promising O
for O
crosslingual O
transfer O
, O
because O
it O
mirrors O
how O
people O
acquire O
new O
languages O
, O
and O
that O
the O
few O
- O
shot O
data O
annotation O
is O
feasible O
. O

In O
future O
work O
, O
we O
will O
investigate O
more O
sophisticated O
techniques O
and O
extend O
the O
work O
to O
more O
NLP O
tasks O
. O

Acknowledgments O
This O
work O
was O
funded O
by O
the O
European O
Research O
Council O
: O
ERC O
NonSequeToR O
( O
# O
740516 O
) O
and O
ERC O
LEXICAL O
( O
# O
648909 O
) O
. O

We O
thank O
the O
anonymous O
reviewers O
and O
Fei O
Mi O
for O
their O
helpful O
suggestions O
. O

References O
Armen O
Aghajanyan O
, O
Akshat O
Shrivastava O
, O
Anchit O
Gupta O
, O
Naman O
Goyal O
, O
Luke O
Zettlemoyer O
, O
and O
Sonal O
Gupta O
. O
2021 O
. O

Better O
ﬁne O
- O
tuning O
by O
reducing O
representa- O
tional O
collapse O
. O

In O
International O
Conference O
on O
Learning O
Representations O
. O

Mikel O
Artetxe O
, O
Sebastian O
Ruder O
, O
and O
Dani O
Yogatama O
. O
2020a O
. O

On O
the O
cross O
- O
lingual O
transferability O
of O
mono- O
lingual O
representations O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computa- O
tional O
Linguistics O
, O
pages O
4623–4637 O
, O
Online O
. O

Asso- O
ciation O
for O
Computational O
Linguistics O
. O

Mikel O
Artetxe O
, O
Sebastian O
Ruder O
, O
Dani O
Yogatama O
, O
Gorka O
Labaka O
, O
and O
Eneko O
Agirre O
. O

2020b O
. O

A O
call O
for O
more O
rigor O
in O
unsupervised O
cross O
- O
lingual O
learn- O
ing O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
7375–7388 O
, O
Online O
. O

Association O
for O
Compu- O
tational O
Linguistics O
. O

Mikel O
Artetxe O
and O
Holger O
Schwenk O
. O

2019 O
. O

Mas- O
sively O
multilingual O
sentence O
embeddings O
for O
zero- O
shot O
cross O
- O
lingual O
transfer O
and O
beyond O
. O

Transac- O
tions O
of O
the O
Association O
for O
Computational O
Linguis- O
tics O
, O
7:597–610 O
. O

Trapit O
Bansal O
, O
Rishikesh O
Jha O
, O
and O
Andrew O
McCallum O
. O
2020 O
. O

Learning O
to O
few O
- O
shot O
learn O
across O
diverse O

5760natural O
language O
classiﬁcation O
tasks O
. O

In O
Proceed- O
ings O
of O
the O
28th O
International O
Conference O
on O
Com- O
putational O
Linguistics O
, O
pages O
5108–5123 O
, O
Barcelona O
, O
Spain O
( O
Online O
) O
. O

International O
Committee O
on O
Compu- O
tational O
Linguistics O
. O

Tom O
Brown O
, O
Benjamin O
Mann O
, O
Nick O
Ryder O
, O
Melanie O
Subbiah O
, O
Jared O
D O
Kaplan O
, O
Prafulla O
Dhariwal O
, O
Arvind O
Neelakantan O
, O
Pranav O
Shyam O
, O
Girish O
Sastry O
, O
Amanda O
Askell O
, O
Sandhini O
Agarwal O
, O
Ariel O
Herbert- O
V O
oss O
, O
Gretchen O
Krueger O
, O
Tom O
Henighan O
, O
Rewon O
Child O
, O
Aditya O
Ramesh O
, O
Daniel O
Ziegler O
, O
Jeffrey O
Wu O
, O
Clemens O
Winter O
, O
Chris O
Hesse O
, O
Mark O
Chen O
, O
Eric O
Sigler O
, O
Mateusz O
Litwin O
, O
Scott O
Gray O
, O
Benjamin O
Chess O
, O
Jack O
Clark O
, O
Christopher O
Berner O
, O
Sam O
McCandlish O
, O
Alec O
Radford O
, O
Ilya O
Sutskever O
, O
and O
Dario O
Amodei O
. O

2020 O
. O

Language O
models O
are O
few O
- O
shot O
learners O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
volume O
33 O
, O
pages O
1877–1901 O
. O

Curran O
Associates O
, O
Inc. O

Aditi O
Chaudhary O
, O
Jiateng O
Xie O
, O
Zaid O
Sheikh O
, O
Graham O
Neubig O
, O
and O
Jaime O
Carbonell O
. O
2019 O
. O

A O
little O
anno- O
tation O
does O
a O
lot O
of O
good O
: O
A O
study O
in O
bootstrapping O
low O
- O
resource O
named O
entity O
recognizers O
. O

In O
Proceed- O
ings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
Inter- O
national O
Joint O
Conference O
on O
Natural O
Language O
Pro- O

cessing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
5164–5174 O
, O
Hong O
Kong O
, O
China O
. O

Association O
for O
Computational O
Lin- O
guistics O
. O

Wei O
- O
Yu O
Chen O
, O
Yen O
- O
Cheng O
Liu O
, O
Zsolt O
Kira O
, O
Yu- O

Chiang O
Frank O
Wang O
, O
and O
Jia O
- O
Bin O
Huang O
. O

2019 O
. O

A O
closer O
look O
at O
few O
- O
shot O
classiﬁcation O
. O

In O
Interna- O
tional O
Conference O
on O
Learning O
Representations O
. O

Yinbo O
Chen O
, O
Xiaolong O
Wang O
, O
Zhuang O
Liu O
, O
Huijuan O
Xu O
, O
and O
Trevor O
Darrell O
. O

2020 O
. O

A O
new O
meta- O
baseline O
for O
few O
- O
shot O
learning O
. O

arXiv O
preprint O
arXiv:2003.04390 O
. O

Alexis O
Conneau O
, O
Kartikay O
Khandelwal O
, O
Naman O
Goyal O
, O
Vishrav O
Chaudhary O
, O
Guillaume O
Wenzek O
, O
Francisco O
Guzm O
´ O
an O
, O
Edouard O
Grave O
, O
Myle O
Ott O
, O
Luke O
Zettle- O
moyer O
, O
and O
Veselin O
Stoyanov O
. O

2020 O
. O

Unsupervised O
cross O
- O
lingual O
representation O
learning O
at O
scale O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Asso- O
ciation O
for O
Computational O
Linguistics O
, O
pages O
8440 O
– O
8451 O
, O
Online O
. O

Association O
for O
Computational O
Lin- O
guistics O
. O

Alexis O
Conneau O
and O
Guillaume O
Lample O
. O

2019 O
. O
Cross- O

lingual O
language O
model O
pretraining O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
pages O
7059–7069 O
. O

Alexis O
Conneau O
, O
Ruty O
Rinott O
, O
Guillaume O
Lample O
, O
Ad- O
ina O
Williams O
, O
Samuel O
R. O
Bowman O
, O
Holger O
Schwenk O
, O
and O
Veselin O
Stoyanov O
. O

2018 O
. O

Xnli O
: O
Evaluating O
cross- O
lingual O
sentence O
representations O
. O

In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natu- O
ral O
Language O
Processing O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
under- O
standing O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
, O
Minneapolis O
, O
Minnesota O
. O

Associ- O
ation O
for O
Computational O
Linguistics O
. O

Guneet O
Singh O
Dhillon O
, O
Pratik O
Chaudhari O
, O
Avinash O
Ravichandran O
, O
and O
Stefano O
Soatto O
. O

2020 O
. O

A O
baseline O
for O
few O
- O
shot O
image O
classiﬁcation O
. O

In O
International O
Conference O
on O
Learning O
Representations O
. O

Jesse O
Dodge O
, O
Gabriel O
Ilharco O
, O
Roy O
Schwartz O
, O
Ali O
Farhadi O
, O
Hannaneh O
Hajishirzi O
, O
and O
Noah O
Smith O
. O

2020 O
. O

Fine O
- O
tuning O
pretrained O
language O
models O
: O
Weight O
initializations O
, O
data O
orders O
, O
and O
early O
stop- O
ping O
. O

Xin O
Dong O
and O
Gerard O
de O
Melo O
. O

2019 O
. O

A O
robust O
self- O
learning O
framework O
for O
cross O
- O
lingual O
text O
classiﬁca- O
tion O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natu- O
ral O
Language O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
6306–6310 O
, O
Hong O
Kong O
, O
China O
. O

Association O
for O
Computational O
Linguistics O
. O

Rotem O
Dror O
, O
Segev O
Shlomov O
, O
and O
Roi O
Reichart O
. O

2019 O
. O

Deep O
dominance O
- O
how O
to O
properly O
compare O
deep O
neural O
models O
. O

In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
, O
pages O
2773–2785 O
, O
Florence O
, O
Italy O
. O

Associa- O
tion O
for O
Computational O
Linguistics O
. O

Julian O
Eisenschlos O
, O
Sebastian O
Ruder O
, O
Piotr O
Czapla O
, O
Marcin O
Kadras O
, O
Sylvain O
Gugger O
, O
and O
Jeremy O
Howard O
. O

2019 O
. O

MultiFiT O
: O
Efﬁcient O
multi O
- O
lingual O
lan- O
guage O
model O
ﬁne O
- O
tuning O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natu- O
ral O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
5702–5707 O
, O
Hong O
Kong O
, O
China O
. O

Association O
for O
Computational O
Linguistics O
. O

Aparna O
Elangovan O
, O
Jiayuan O
He O
, O
and O
Karin O
Verspoor O
. O

2021 O
. O

Memorization O
vs. O
generalization O
: O
Quantify- O
ing O
data O
leakage O
in O
NLP O
performance O
evaluation O
. O

In O
Proceedings O
of O
the O
16th O
Conference O
of O
the O
European O
Chapter O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
: O
Main O
Volume O
, O
pages O
1325–1335 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

L. O
Fei O
- O
Fei O
, O
R. O
Fergus O
, O
and O
P. O
Perona O
. O
2006 O
. O

One- O
shot O
learning O
of O
object O
categories O
. O

IEEE O
Transac- O
tions O
on O
Pattern O
Analysis O
and O
Machine O
Intelligence O
, O
28(4):594–611 O
. O

Chelsea O
Finn O
, O
Pieter O
Abbeel O
, O
and O
Sergey O
Levine O
. O

2017 O
. O

Model O
- O
agnostic O
meta O
- O
learning O
for O
fast O
adaptation O
of O
deep O
networks O
. O

In O
Proceedings O
of O
the O
34th O
In- O
ternational O
Conference O
on O
Machine O
Learning O
, O
vol- O
ume O
70 O
of O
Proceedings O
of O

Machine O
Learning O
Re- O

5761search O
, O
pages O
1126–1135 O
, O
International O
Convention O
Centre O
, O
Sydney O
, O
Australia O
. O
PMLR O
. O

Tianyu O
Gao O
, O
Adam O
Fisch O
, O
and O
Danqi O
Chen O
. O
2020 O
. O

Making O
pre O
- O
trained O
language O
models O
better O
few O
- O
shot O
learners O
. O

arXiv O
preprint O
arXiv:2012.15723 O
. O

Tianyu O
Gao O
, O
Xu O
Han O
, O
Hao O
Zhu O
, O
Zhiyuan O
Liu O
, O
Peng O
Li O
, O
Maosong O
Sun O
, O
and O
Jie O
Zhou O
. O

2019 O
. O

FewRel O
2.0 O
: O
Towards O
more O
challenging O
few O
- O
shot O
relation O
classiﬁ- O
cation O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natu- O
ral O
Language O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
6251–6256 O
, O
Hong O
Kong O
, O
China O
. O

Association O
for O
Computational O
Linguistics O
. O

Dan O
Garrette O
and O
Jason O
Baldridge O
. O

2013 O
. O

Learning O
a O
part O
- O
of O
- O
speech O
tagger O
from O
two O
hours O
of O
annotation O
. O

InProceedings O
of O
the O
2013 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computa- O
tional O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
138–147 O
, O
Atlanta O
, O
Georgia O
. O

Association O
for O
Computational O
Linguistics O
. O

Robert O
Geirhos O
, O
J O
¨orn O
- O
Henrik O
Jacobsen O
, O
Claudio O
Michaelis O
, O
Richard O
Zemel O
, O
Wieland O
Brendel O
, O
Matthias O
Bethge O
, O
and O
Felix O
A. O
Wichmann O
. O
2020 O
. O

Shortcut O
learning O
in O
deep O
neural O
networks O
. O

Nature O
Machine O
Intelligence O
, O
2(11):665–673 O
. O

Beliz O
Gunel O
, O
Jingfei O
Du O
, O
Alexis O
Conneau O
, O
and O
Veselin O
Stoyanov O
. O

2021 O
. O

Supervised O
contrastive O
learning O
for O
pre O
- O
trained O
language O
model O
ﬁne O
- O
tuning O
. O

In O
Interna- O
tional O
Conference O
on O
Learning O
Representations O
. O

Aakriti O
Gupta O
, O
Kapil O
Thadani O
, O
and O
Neil O
O’Hare O
. O
2020 O
. O

Effective O
few O
- O
shot O
classiﬁcation O
with O
transfer O
learn- O
ing O
. O

In O
Proceedings O
of O
the O
28th O
International O
Con- O
ference O
on O
Computational O
Linguistics O
, O
pages O
1061 O
– O
1066 O
, O
Barcelona O
, O
Spain O
( O
Online O
) O
. O

International O
Com- O
mittee O
on O
Computational O
Linguistics O
. O

Xu O
Han O
, O
Hao O
Zhu O
, O
Pengfei O
Yu O
, O
Ziyun O
Wang O
, O
Yuan O
Yao O
, O
Zhiyuan O
Liu O
, O
and O
Maosong O
Sun O
. O
2018 O
. O

FewRel O
: O

A O
large O
- O
scale O
supervised O
few O
- O
shot O
relation O
classiﬁca- O
tion O
dataset O
with O
state O
- O
of O
- O
the O
- O
art O
evaluation O
. O

In O
Pro- O
ceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Meth- O
ods O
in O
Natural O
Language O
Processing O
, O
pages O
4803 O
– O
4809 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computa- O
tional O
Linguistics O
. O

Han O
He O
and O
Jinho O
D. O
Choi O
. O

2020 O
. O

Establishing O
Strong O
Baselines O
for O
the O
New O
Decade O
: O
Sequence O
Tagging O
, O
Syntactic O
and O
Semantic O
Parsing O
with O
BERT O
. O

In O
Proceedings O
of O
the O
33rd O
International O
Florida O
Ar- O
tiﬁcial O
Intelligence O
Research O
Society O
Conference O
, O
FLAIRS’20 O
. O

Best O
Paper O
Candidate O
. O

Michael O
A. O
Hedderich O
, O
David O
Adelani O
, O
Dawei O
Zhu O
, O
Je- O
sujoba O
Alabi O
, O
Udia O
Markus O
, O
and O
Dietrich O
Klakow O
. O

2020 O
. O

Transfer O
learning O
and O
distant O
supervision O
for O
multilingual O
transformer O
models O
: O
A O
study O
on O
African O
languages O
. O

In O
Proceedings O
of O
the O
2020 O
Con- O
ference O
on O
Empirical O
Methods O
in O
Natural O
LanguageProcessing O
( O
EMNLP O
) O
, O
pages O
2580–2591 O
, O
Online O
. O

As- O
sociation O
for O
Computational O
Linguistics O
. O

Nithin O
Holla O
, O
Pushkar O
Mishra O
, O
Helen O
Yannakoudakis O
, O
and O
Ekaterina O
Shutova O
. O

2020 O
. O

Learning O
to O
learn O
to O
disambiguate O
: O
Meta O
- O
learning O
for O
few O
- O
shot O
word O
sense O
disambiguation O
. O

In O
Findings O
of O
the O
Associa- O
tion O
for O
Computational O
Linguistics O
: O
EMNLP O
2020 O
, O
pages O
4517–4533 O
, O
Online O
. O

Association O
for O
Compu- O
tational O
Linguistics O
. O

Yutai O
Hou O
, O
Wanxiang O
Che O
, O
Yongkui O
Lai O
, O
Zhihan O
Zhou O
, O
Yijia O
Liu O
, O
Han O
Liu O
, O
and O
Ting O
Liu O
. O
2020a O
. O

Few O
- O
shot O
slot O
tagging O
with O
collapsed O
dependency O
transfer O
and O
label O
- O
enhanced O
task O
- O
adaptive O
projection O
network O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
As- O
sociation O
for O
Computational O
Linguistics O
, O
ACL O
2020 O
, O
Online O
, O
July O
5 O
- O
10 O
, O
2020 O
, O
pages O
1381–1393 O
. O

Associa- O
tion O
for O
Computational O
Linguistics O
. O

Yutai O
Hou O
, O
Jiafeng O
Mao O
, O
Yongkui O
Lai O
, O
Cheng O
Chen O
, O
Wanxiang O
Che O
, O
Zhigang O
Chen O
, O
and O
Ting O
Liu O
. O

2020b O
. O

Fewjoint O
: O
A O
few O
- O
shot O
learning O
benchmark O
for O
joint O
language O
understanding O
. O

CoRR O
, O
abs/2009.08138 O
. O

Tsung O
- O
Yuan O
Hsu O
, O
Chi O
- O
Liang O
Liu O
, O
and O
Hung O
- O
yi O
Lee O
. O
2019 O
. O

Zero O
- O
shot O
reading O
comprehension O
by O
cross- O
lingual O
transfer O
learning O
with O
multi O
- O
lingual O
lan- O
guage O
representation O
model O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natu- O
ral O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
5933–5940 O
, O
Hong O
Kong O
, O
China O
. O

Association O
for O
Computational O
Linguistics O
. O

Junjie O
Hu O
, O
Sebastian O
Ruder O
, O
Aditya O
Siddhant O
, O
Gra- O
ham O
Neubig O
, O
Orhan O
Firat O
, O
and O
Melvin O
Johnson O
. O

2020 O
. O

XTREME O
: O

A O
massively O
multilingual O
multi- O
task O
benchmark O
for O
evaluating O
cross O
- O
lingual O
gener- O
alisation O
. O

In O
Proceedings O
of O
the O
37th O
International O
Conference O
on O
Machine O
Learning O
, O
volume O
119 O
of O
Proceedings O
of O
Machine O
Learning O
Research O
, O
pages O
4411–4421 O
, O
Virtual O
. O

PMLR O
. O

Phillip O
Keung O
, O
Yichao O
Lu O
, O
Julian O
Salazar O
, O
and O
Vikas O
Bhardwaj O
. O
2020a O
. O

Do O
n’t O
use O
English O
dev O
: O
On O
the O
zero O
- O
shot O
cross O
- O
lingual O
evaluation O
of O
contextual O
em- O
beddings O
. O

In O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
549–554 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Phillip O
Keung O
, O
Yichao O
Lu O
, O
Gy O
¨orgy O
Szarvas O
, O
and O
Noah O
A. O
Smith O
. O

2020b O
. O

The O
multilingual O
Amazon O
reviews O
corpus O
. O

In O
Proceedings O
of O
the O
2020 O
Con- O
ference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
4563–4568 O
, O
Online O
. O

As- O
sociation O
for O
Computational O
Linguistics O
. O

Diederik O
P. O
Kingma O
and O
Jimmy O
Ba O
. O
2015 O
. O

Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O

In O
ICLR O
( O
Poster O
) O
. O

Tassilo O
Klein O
and O
Moin O
Nabi O
. O
2020 O
. O

Contrastive O
self- O
supervised O
learning O
for O
commonsense O
reasoning O
. O

In O

5762Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Asso- O
ciation O
for O
Computational O
Linguistics O
, O
pages O
7517 O
– O
7523 O
, O
Online O
. O

Association O
for O
Computational O
Lin- O
guistics O
. O

Gregory O
Koch O
, O
Richard O
Zemel O
, O
and O
Ruslan O
Salakhut- O
dinov O
. O

2015 O
. O

Siamese O
neural O
networks O
for O
one O
- O
shot O
image O
recognition O
. O

In O
ICML O
2015 O
Deep O
Learning O
Workshop O
. O

Anne O
Lauscher O
, O
Vinit O
Ravishankar O
, O
Ivan O
Vuli O
´ O
c O
, O
and O
Goran O
Glava O
ˇs O
. O

2020 O
. O

From O
zero O
to O
hero O
: O
On O
the O
limitations O
of O
zero O
- O
shot O
language O
transfer O
with O
mul- O
tilingual O
transformers O
. O

In O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Lan- O
guage O
Processing O
( O
EMNLP O
) O
, O
pages O
4483–4499 O
, O
On- O
line O
. O

Association O
for O
Computational O
Linguistics O
. O

Patrick O
Lewis O
, O
Pontus O
Stenetorp O
, O
and O
Sebastian O
Riedel O
. O
2021 O
. O

Question O
and O
answer O
test O
- O
train O
overlap O
in O
open O
- O
domain O
question O
answering O
datasets O
. O

In O
Pro- O
ceedings O
of O
the O
16th O
Conference O
of O
the O
European O
Chapter O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
: O
Main O
Volume O
, O
pages O
1000–1008 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Yaobo O
Liang O
, O
Nan O
Duan O
, O
Yeyun O
Gong O
, O
Ning O
Wu O
, O
Fen- O
fei O

Guo O
, O
Weizhen O
Qi O
, O
Ming O
Gong O
, O
Linjun O
Shou O
, O
Daxin O
Jiang O
, O
Guihong O
Cao O
, O
Xiaodong O
Fan O
, O
Ruofei O
Zhang O
, O
Rahul O
Agrawal O
, O
Edward O
Cui O
, O
Sining O
Wei O
, O
Taroon O
Bharti O
, O
Ying O
Qiao O
, O
Jiun O
- O
Hung O
Chen O
, O
Winnie O
Wu O
, O
Shuguang O
Liu O
, O
Fan O
Yang O
, O
Daniel O
Campos O
, O
Ran- O
gan O
Majumder O
, O
and O
Ming O
Zhou O
. O

2020 O
. O

XGLUE O
: O

A O
new O
benchmark O
datasetfor O
cross O
- O
lingual O
pre O
- O
training O
, O
understanding O
and O
generation O
. O

In O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
6008–6018 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Bing O
Liu O
. O

2012 O
. O

Sentiment O
analysis O
and O
opinion O
min- O
ing O
. O

Synthesis O
lectures O
on O
human O
language O
technolo- O
gies O
, O
5(1):1–167 O
. O

Xiao O
Liu O
, O
Yanan O
Zheng O
, O
Zhengxiao O
Du O
, O
Ming O
Ding O
, O
Yujie O
Qian O
, O
Zhilin O
Yang O
, O
and O
Jie O
Tang O
. O

2021 O
. O

Gpt O
understands O
, O
too O
. O

arXiv O
preprint O
arXiv:2103.10385 O
. O

Erik O
G O
Miller O
, O
Nicholas O
E O
Matsakis O
, O
and O
Paul O
A O
Viola O
. O

2000 O
. O

Learning O
from O
one O
example O
through O
shared O
densities O
on O
transforms O
. O

In O
Proceedings O
IEEE O
Con- O

ference O
on O
Computer O
Vision O
and O
Pattern O
Recogni- O
tion O
. O

CVPR O
2000 O
( O
Cat O
. O
No O
. O
PR00662 O
) O
, O
volume O
1 O
, O
pages O
464–471 O
. O

IEEE O
. O

Marius O
Mosbach O
, O
Maksym O
Andriushchenko O
, O
and O
Diet- O
rich O
Klakow O
. O

2021 O
. O

On O
the O
stability O
of O
ﬁne O
- O
tuning O
fbertg O
: O
Misconceptions O
, O
explanations O
, O
and O
strong O
baselines O
. O

In O
International O
Conference O
on O
Learning O
Representations O
. O

Timothy O
Niven O
and O
Hung O
- O
Yu O
Kao O
. O
2019 O
. O

Probing O
neu- O
ral O
network O
comprehension O
of O
natural O
language O
ar- O
guments O
. O

In O
Proceedings O
of O
the O
57th O
Annual O
Meet- O
ing O
of O
the O
Association O
for O
Computational O
Linguis- O
tics O
, O
pages O
4658–4664 O
, O
Florence O
, O
Italy O
. O

Association O
for O
Computational O
Linguistics O
. O

Joakim O
Nivre O
, O
Marie O
- O
Catherine O
de O
Marneffe O
, O
Filip O
Gin- O
ter O
, O
Jan O
Haji O
ˇc O
, O
Christopher O
D. O
Manning O
, O
Sampo O
Pyysalo O
, O
Sebastian O
Schuster O
, O
Francis O
Tyers O
, O
and O
Daniel O
Zeman O
. O

2020 O
. O

Universal O
Dependencies O
v2 O
: O
An O
evergrowing O
multilingual O
treebank O
collection O
. O

InProceedings O
of O
the O
12th O
Language O
Resources O
and O
Evaluation O
Conference O
, O
pages O
4034–4043 O
, O
Mar- O
seille O
, O
France O
. O

European O
Language O
Resources O
Asso- O
ciation O
. O

Xiaoman O
Pan O
, O
Boliang O
Zhang O
, O
Jonathan O
May O
, O
Joel O
Nothman O
, O
Kevin O
Knight O
, O
and O
Heng O
Ji O
. O
2017 O
. O
Cross- O

lingual O
name O
tagging O
and O
linking O
for O
282 O
languages O
. O

InProceedings O
of O
the O
55th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1946–1958 O
, O
Vancouver O
, O
Canada O
. O

Association O
for O
Computational O
Linguistics O
. O

Adam O
Paszke O
, O
Sam O
Gross O
, O
Francisco O
Massa O
, O
Adam O
Lerer O
, O
James O
Bradbury O
, O
Gregory O
Chanan O
, O
Trevor O
Killeen O
, O
Zeming O
Lin O
, O
Natalia O
Gimelshein O
, O
Luca O
Antiga O
, O
Alban O
Desmaison O
, O
Andreas O
Kopf O
, O
Edward O
Yang O
, O
Zachary O
DeVito O
, O
Martin O
Raison O
, O
Alykhan O
Te- O
jani O
, O
Sasank O
Chilamkurthy O
, O
Benoit O
Steiner O
, O
Lu O
Fang O
, O
Junjie O
Bai O
, O
and O
Soumith O
Chintala O
. O
2019 O
. O

Pytorch O
: O

An O
imperative O
style O
, O
high O
- O
performance O
deep O
learn- O
ing O
library O
. O

In O
Advances O
in O
Neural O
Information O
Pro- O
cessing O
Systems O
, O
volume O
32 O
, O
pages O
8026–8037 O
. O

Cur- O
ran O
Associates O
, O

Inc. O
F. O
Pedregosa O
, O
G. O
Varoquaux O
, O
A. O
Gramfort O
, O
V O
. O

Michel O
, O
B. O
Thirion O
, O
O. O
Grisel O
, O
M. O
Blondel O
, O
P. O
Prettenhofer O
, O
R. O
Weiss O
, O
V O
. O

Dubourg O
, O
J. O
Vanderplas O
, O
A. O
Passos O
, O
D. O
Cournapeau O
, O
M. O
Brucher O
, O
M. O
Perrot O
, O
and O
E. O
Duch- O
esnay O
. O
2011 O
. O

Scikit O
- O
learn O
: O
Machine O
learning O
in O
Python O
. O

Journal O
of O
Machine O
Learning O
Research O
, O
12:2825–2830 O
. O

Jason O
Phang O
, O
Thibault O
F O
´ O
evry O
, O
and O
Samuel O
R O
Bowman O
. O

2018 O
. O

Sentence O
encoders O
on O
stilts O
: O
Supplementary O
training O
on O
intermediate O
labeled O
- O
data O
tasks O
. O

arXiv O
preprint O
arXiv:1811.01088 O
. O

Telmo O
Pires O
, O
Eva O
Schlinger O
, O
and O
Dan O
Garrette O
. O

2019 O
. O

How O
multilingual O
is O
multilingual O
BERT O
? O

In O
Pro- O
ceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Asso- O
ciation O
for O
Computational O
Linguistics O
, O
pages O
4996 O
– O
5001 O
, O
Florence O
, O
Italy O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Yada O
Pruksachatkun O
, O
Jason O
Phang O
, O
Haokun O
Liu O
, O
Phu O
Mon O
Htut O
, O
Xiaoyi O
Zhang O
, O
Richard O
Yuanzhe O
Pang O
, O
Clara O
Vania O
, O
Katharina O
Kann O
, O
and O
Samuel O
R. O
Bowman O
. O
2020 O
. O

Intermediate O
- O
task O
transfer O
learning O
with O
pretrained O
language O
models O
: O
When O
and O
why O
does O
it O
work O
? O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
, O
pages O
5231–5247 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Afshin O
Rahimi O
, O
Yuan O
Li O
, O
and O
Trevor O
Cohn O
. O
2019 O
. O

Mas- O
sively O
multilingual O
transfer O
for O
NER O
. O

In O
Proceed- O
ings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
151–164 O
, O
Flo- O
rence O
, O
Italy O
. O

Association O
for O
Computational O
Linguis- O
tics O
. O

5763Ori O
Ram O
, O
Yuval O
Kirstain O
, O
Jonathan O
Berant O
, O
Amir O
Globerson O
, O
and O
Omer O
Levy O
. O

2021 O
. O

Few O
- O
shot O
ques- O
tion O
answering O
by O
pretraining O
span O
selection O
. O

arXiv O
preprint O
arXiv:2101.00438 O
. O

Nils O
Reimers O
and O
Iryna O
Gurevych O
. O

2017 O
. O

Reporting O
score O
distributions O
makes O
a O
difference O
: O
Performance O
study O
of O
LSTM O
- O
networks O
for O
sequence O
tagging O
. O

In O
Proceedings O
of O
the O
2017 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
338–348 O
, O
Copenhagen O
, O
Denmark O
. O
Association O
for O
Computational O
Linguistics O
. O

Annette O
Rios O
, O
Mathias O
M O
¨uller O
, O
and O
Rico O
Sennrich O
. O

2020 O
. O

Subword O
segmentation O
and O
a O
single O
bridge O
language O
affect O
zero O
- O
shot O
neural O
machine O
translation O
. O

InProceedings O
of O
the O
Fifth O
Conference O
on O
Machine O
Translation O
, O
pages O
526–535 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Timo O
Schick O
and O
Hinrich O
Sch O
¨utze O
. O
2020 O
. O

It O
’s O
not O
just O
size O
that O
matters O
: O
Small O
language O
mod- O
els O
are O
also O
few O
- O
shot O
learners O
. O

arXiv O
preprint O
arXiv:2009.07118 O
. O

Holger O
Schwenk O
and O
Xian O
Li O
. O

2018 O
. O

A O
corpus O
for O
mul- O
tilingual O
document O
classiﬁcation O
in O
eight O
languages O
. O

InProceedings O
of O
the O
Eleventh O
International O
Confer- O
ence O
on O
Language O
Resources O
and O
Evaluation O
( O
LREC O
2018 O
) O
, O
Paris O
, O
France O
. O

European O
Language O
Resources O
Association O
( O
ELRA O
) O
. O

Tom O
Sherborne O
, O
Yumo O
Xu O
, O
and O
Mirella O
Lapata O
. O
2020 O
. O

Bootstrapping O
a O
crosslingual O
semantic O
parser O
. O

In O
Findings O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
: O
EMNLP O
2020 O
, O
pages O
499–517 O
, O
Online O
. O

As- O
sociation O
for O
Computational O
Linguistics O
. O

Jake O
Snell O
, O
Kevin O
Swersky O
, O
and O
Richard O
Zemel O
. O

2017 O
. O

Prototypical O
networks O
for O
few O
- O
shot O
learning O
. O

In O
Ad- O
vances O
in O
Neural O
Information O
Processing O
Systems O
, O
volume O
30 O
, O
pages O
4077–4087 O
. O

Curran O
Associates O
, O
Inc. O

Swabha O
Swayamdipta O
, O
Roy O
Schwartz O
, O
Nicholas O
Lourie O
, O
Yizhong O
Wang O
, O
Hannaneh O
Hajishirzi O
, O
Noah O
A. O
Smith O
, O
and O
Yejin O
Choi O
. O

2020 O
. O

Dataset O
cartography O
: O
Mapping O
and O
diagnosing O
datasets O
with O
training O
dy- O
namics O
. O

In O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Process- O
ing O
( O
EMNLP O
) O
, O
pages O
9275–9293 O
, O
Online O
. O

Associa- O
tion O
for O
Computational O
Linguistics O
. O

Yonglong O
Tian O
, O
Yue O
Wang O
, O
Dilip O
Krishnan O
, O
Joshua O
B. O
Tenenbaum O
, O
and O
Phillip O
Isola O
. O

2020 O
. O

Rethinking O
few O
- O
shot O
image O
classiﬁcation O
: O
A O
good O
embedding O
is O
all O
you O
need O
? O

In O
Computer O
Vision O
– O
ECCV O
2020 O
, O
pages O
266–282 O
, O
Cham O
. O

Springer O
International O
Pub- O
lishing O
. O

Orith O
Toledo O
- O
Ronen O
, O
Matan O
Orbach O
, O
Yonatan O
Bilu O
, O
Artem O
Spector O
, O
and O
Noam O
Slonim O
. O

2020 O
. O

Multilin- O
gual O
argument O
mining O
: O
Datasets O
and O
analysis O
. O

In O
Findings O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
: O
EMNLP O
2020 O
, O
pages O
303–317 O
, O
Online O
. O

As- O
sociation O
for O
Computational O
Linguistics O
. O

Xin O
Wang O
, O
Thomas O
Huang O
, O
Joseph O
Gonzalez O
, O
Trevor O
Darrell O
, O
and O
Fisher O
Yu O
. O
2020 O
. O

Frustratingly O
simple O
few O
- O
shot O
object O
detection O
. O

In O
Proceedings O
of O
the O
37th O
International O
Conference O
on O
Machine O
Learning O
, O
volume O
119 O
of O
Proceedings O
of O
Machine O
Learning O
Re- O
search O
, O
pages O
9919–9928 O
, O
Virtual O
. O

PMLR O
. O

Adina O
Williams O
, O
Nikita O
Nangia O
, O
and O
Samuel O
Bowman O
. O

2018 O
. O

A O
broad O
- O
coverage O
challenge O
corpus O
for O
sen- O
tence O
understanding O
through O
inference O
. O

In O
Proceed- O
ings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
Papers O
) O
, O
pages O
1112–1122 O
, O
New O
Orleans O
, O
Louisiana O
. O

Association O
for O
Computational O
Linguis- O
tics O
. O

Thomas O
Wolf O
, O
Lysandre O
Debut O
, O
Victor O
Sanh O
, O
Julien O
Chaumond O
, O
Clement O
Delangue O
, O
Anthony O
Moi O
, O
Pier- O
ric O
Cistac O
, O
Tim O
Rault O
, O
R’emi O
Louf O
, O
Morgan O
Funtow- O
icz O
, O
and O
Jamie O
Brew O
. O

2019 O
. O

Huggingface O
’s O
trans- O
formers O
: O
State O
- O
of O
- O
the O
- O
art O
natural O
language O
process- O
ing O
. O

ArXiv O
, O
abs/1910.03771 O
. O

Shijie O
Wu O
and O
Mark O
Dredze O
. O

2019 O
. O

Beto O
, O
bentz O
, O
be- O
cas O
: O
The O
surprising O
cross O
- O
lingual O
effectiveness O
of O
BERT O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natu- O
ral O
Language O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
833–844 O
, O
Hong O
Kong O
, O
China O
. O

Association O
for O
Com- O
putational O
Linguistics O
. O

Shijie O
Wu O
and O
Mark O
Dredze O
. O

2020a O
. O

Are O
all O
languages O
created O
equal O
in O
multilingual O
bert O
? O

In O
Proceedings O
of O
the O
5th O
Workshop O
on O
Representation O
Learning O
for O
NLP O
, O
RepL4NLP@ACL O
2020 O
, O
Online O
, O
July O
9 O
, O
2020 O
, O
pages O
120–130 O
. O

Association O
for O
Computational O
Lin- O
guistics O
. O

Shijie O
Wu O
and O
Mark O
Dredze O
. O

2020b O
. O

Do O
explicit O
align- O
ments O
robustly O
improve O
multilingual O
encoders O
? O

In O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
4471–4482 O
, O
Online O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Yonghui O
Wu O
, O
Mike O
Schuster O
, O
Zhifeng O
Chen O
, O
Quoc O
V O
. O

Le O
, O
Mohammad O
Norouzi O
, O
Wolfgang O
Macherey O
, O
Maxim O
Krikun O
, O
Yuan O
Cao O
, O
Qin O
Gao O
, O
Klaus O
Macherey O
, O
Jeff O
Klingner O
, O
Apurva O
Shah O
, O
Melvin O
John- O
son O
, O
Xiaobing O
Liu O
, O
Łukasz O
Kaiser O
, O
Stephan O
Gouws O
, O
Yoshikiyo O
Kato O
, O
Taku O
Kudo O
, O
Hideto O
Kazawa O
, O
Keith O
Stevens O
, O
George O
Kurian O
, O
Nishant O
Patil O
, O
Wei O
Wang O
, O
Cliff O
Young O
, O
Jason O
Smith O
, O
Jason O
Riesa O
, O
Alex O
Rud- O
nick O
, O
Oriol O
Vinyals O
, O
Greg O
Corrado O
, O
Macduff O
Hughes O
, O
and O
Jeffrey O
Dean O
. O

2016 O
. O

Google O
’s O
neural O
machine O
translation O
system O
: O
Bridging O
the O
gap O
between O
human O
and O
machine O
translation O
. O

Patrick O
Xia O
, O
Shijie O
Wu O
, O
and O
Benjamin O
Van O
Durme O
. O

2020 O
. O

Which O
* O
BERT O
? O

A O
survey O
organizing O
contex- O
tualized O
encoders O
. O

In O
Proceedings O
of O
the O
2020 O
Con- O
ference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
7516–7533 O
, O
Online O
. O

As- O
sociation O
for O
Computational O
Linguistics O
. O

5764Yinfei O
Yang O
, O
Yuan O
Zhang O
, O
Chris O
Tar O
, O
and O
Jason O
Baldridge O
. O
2019 O
. O

PAWS O
- O
X O
: O
A O
cross O
- O
lingual O
ad- O
versarial O
dataset O
for O
paraphrase O
identiﬁcation O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Lan- O
guage O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
3687 O
– O
3692 O
, O
Hong O
Kong O
, O
China O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Wenpeng O
Yin O
. O

2020 O
. O

Meta O
- O
learning O
for O
few O
- O
shot O
natu- O
ral O
language O
processing O
: O
A O
survey O
. O

Wenpeng O
Yin O
, O
Nazneen O
Fatema O
Rajani O
, O
Dragomir O
Radev O
, O
Richard O
Socher O
, O
and O
Caiming O
Xiong O
. O

2020 O
. O

Universal O
natural O
language O
processing O
with O
limited O
annotations O
: O
Try O
few O
- O
shot O
textual O
entailment O
as O
a O
start O
. O

In O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Process- O
ing O
( O
EMNLP O
) O
, O
pages O
8229–8239 O
, O
Online O
. O

Associa- O
tion O
for O
Computational O
Linguistics O
. O

Dani O
Yogatama O
, O
Cyprien O
de O
Masson O
d’Autume O
, O
Jerome O
Connor O
, O
Tomas O
Kocisky O
, O
Mike O
Chrzanowski O
, O
Ling- O
peng O
Kong O
, O
Angeliki O
Lazaridou O
, O
Wang O
Ling O
, O
Lei O
Yu O
, O
Chris O
Dyer O
, O
and O
Phil O
Blunsom O
. O

2019 O
. O

Learning O
and O
evaluating O
general O
linguistic O
intelligence O
. O

Mo O
Yu O
, O
Xiaoxiao O
Guo O
, O
Jinfeng O
Yi O
, O
Shiyu O
Chang O
, O
Saloni O
Potdar O
, O
Yu O
Cheng O
, O
Gerald O
Tesauro O
, O
Haoyu O
Wang O
, O
and O
Bowen O
Zhou O
. O

2018 O
. O

Diverse O
few O
- O
shot O
text O
clas- O
siﬁcation O
with O
multiple O
metrics O
. O

In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chap- O
ter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
Pa- O
pers O
) O
, O
pages O
1206–1215 O
, O
New O
Orleans O
, O
Louisiana O
. O

Association O
for O
Computational O
Linguistics O
. O

Tianyi O
Zhang O
, O
Felix O
Wu O
, O
Arzoo O
Katiyar O
, O
Kilian O
Q O
Weinberger O
, O
and O
Yoav O
Artzi O
. O

2021 O
. O

Revisiting O
few- O
samplefbertgﬁne O
- O
tuning O
. O

In O
International O
Confer- O
ence O
on O
Learning O
Representations O
. O

Yuan O
Zhang O
, O
Jason O
Baldridge O
, O
and O
Luheng O
He O
. O
2019 O
. O

PAWS O
: O
Paraphrase O
adversaries O
from O
word O
scram- O
bling O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Tech- O
nologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
1298–1308 O
, O
Minneapolis O
, O
Minnesota O
. O

Association O
for O
Computational O
Linguistics O
. O

Wei O
Zhao O
, O
Steffen O
Eger O
, O
Johannes O
Bjerva O
, O
and O
Is- O
abelle O
Augenstein O
. O
2020 O
. O

Inducing O
language- O
agnostic O
multilingual O
representations O
. O

arXiv O
preprint O
arXiv:2008.09112 O
. O

5765A O
Reproducibility O
Checklist O
A.1 O
mBERT O
Architecture O
and O
Number O
of O
Parameters O
We O
use O
the O
“ O
bert O
- O
base O
- O
multilingual O
- O
cased O
” O
model7 O
. O

It O
contains O
12 O
Transformer O
blocks O
with O
768 O
hidden O
dimensions O
. O

Each O
block O
has O
12 O
self O
attention O
heads O
. O

The O
model O
is O
pretrained O
on O
the O
concatenation O
of O
the O
Wikipedia O
dump O
of O
104 O
languages O
. O

There O
are O
about O
179 O
million O
parameters O
in O
mBERT O
. O

For O
all O
the O
tasks O
, O
we O
use O
a O
linear O
output O
layer O
. O

Denoting O
the O
output O
dimension O
of O
a O
task O
as O
m O
, O
e.g. O
,m= O
2for O
PAWSX O
. O

Then O
we O
have O
in O
total O
179 O
million O
+ O
768m+mparameters O
for O
the O
task O
. O

A.2 O
Computing O
Infrastructure O
All O
experiments O
are O
conducted O
on O
GeForce O
GTX O
1080Ti O
. O

In O
the O
source O
- O
training O
stage O
, O
we O
use O
4 O
GPUs O
with O
per O
- O
GPU O
batch O
size O
32 O
. O

In O
the O
target- O
adapting O
stage O
, O
we O
use O
a O
single O
GPU O
and O
the O
batch O
size O
is O
equal O
to O
the O
number O
of O
examples O
in O
a O
bucket O
. O

A.3 O
Evaluation O
Metrics O
and O
Validation O
Performance O
We O
follow O
the O
standard O
evaluation O
metrics O
used O
in O
XTREME O
( O
Hu O
et O
al O
. O
, O
2020 O
) O
and O
they O
are O
shown O
in O
Table O
1 O
; O
evaluation O
func- O
tions O
in O
scikit O
- O
learn O
( O
Pedregosa O
et O
al O
. O
, O
2011 O
) O
and O
seqeval O
( O
https://github.com/ O
chakki O
- O
works O
/ O
seqeval O
) O
are O
used O
. O

Link O
to O
code O
: O
code O
/ O
utils O
/ O
eval O
meters.py O
. O

The O
validation O
performance O
of O
the O
English- O
trained O
models O
are O
shown O
in O
the O
ﬁrst O
row O
of O
Table O
7 O
; O
the O
optimal O
learning O
rate O
for O
each O
task O
is O
shown O
in O
the O
second O
row O
. O

MLDoc O

MARC O
XNLI O
PAWSX O

POS O
NER O
98.1 O

65.1 O
83.5 O
94.5 O
95.6 O
84.3 O
1e-5 O

1e-5 O
3e-5 O

1e-5 O

1e-5 O

1e-5 O
Table O
7 O
: O
Source O
- O
training O
validation O
performance O
( O
% O
) O
and O
the O
optimal O
learning O
rate O
. O

For O
all O
the O
FS O
- O
XLT O
experiments O
, O
we O
enclosed O
the O
validation O
scores O
in O
https://github.com/fsxlt/ O
running O
- O
logs O
. O

A.4 O
Hyperparameter O
Search O

For O
both O
source O
- O
training O
and O
target O
- O
adapting O
, O
the O
only O
hyperparameter O
we O
search O
is O
learning O
rate O
( O
fromf1e 5;3e 5;5e 5;7e 5 O
g O
) O
to O
reduce O
7https://github.com/google-research/ O
bert O
/ O
blob O
/ O
master O
/ O
multilingual.mdAlgorithm O
1 O
: O
Minimum O
- O
including O
Require O
: O
# O
of O
shot O
K O
, O
language O
dataD O
, O
label O
setLD O
1 O
: O
Initialize O
a O
bucket O
S O

= O
fg O
, O
Count O
` O
j= O
0 O
( O
8`j2LD O
) O
2 O
: O
for`inLDdo O
while O
Count O
` O
< O
K O
do O
FromD O
, O
randomly O
sample O
a O
( O
x(i);y(i))pair O
that O
y(i)includes O
` O
Add O
( O
x(i);y(i))toS O
Update O
all O
Count O
` O
j(8`j2LD O
) O
3 O
: O
foreach O
( O
x(i);y(i))inSdo O
Remove O
( O
x(i);y(i))fromS O
Update O
all O
Count O
` O
j(8`j2LD O
) O
ifany O
Count O
` O
j O
< O
Kthen O
Put(x(i);y(i))back O
toS O
Update O
all O
Count O
` O
j(8`j2LD O
) O
4 O
: O
ReturnS O
the O
sensitivity O
of O
our O
results O
to O
hyperparameter O
se- O
lection O
. O

A.5 O
Datasets O
and O
Preprocessing O
For O
tasks O
( O
XNLI O
, O
PAWSX O
, O
POS O
, O
NER O
) O
covered O
in O
XTREME O
( O
Hu O
et O
al O
. O
, O
2020 O
) O
, O
we O
utilize O
the O
provided O
preprocessed O
datasets O
. O

Our O
MLDoc O
dataset O
is O
obtained O
from O
https://github.com/ O
facebookresearch O
/ O
MLDoc O
. O

We O
retrieve O
MARC O
from O
docs.opendata.aws/amazon-reviews-ml/ O
readme.html O
. O

Table O
8 O
shows O
example O
entries O
of O
the O
datasets O
. O

It O
is O
worth O
noting O
that O
MARC O
is O
a O
single O
sentence O
review O
classiﬁcation O
task O
, O
however O
, O
we O
put O
the O
“ O
review O
title O
” O
and O
“ O
product O
category O
” O
in O
the O
“ O
Text O
B O
” O
ﬁeld O
, O
following O
Keung O
et O

al O
. O
( O
2020b O
) O
. O

We O
utilize O
the O
tokenizer O
in O
the O
HuggingFace O
Transformers O
package O
( O
Wolf O
et O
al O
. O
, O
2019 O
) O
to O
preprocess O
all O
the O
texts O
. O

In O
all O
experiments O
, O
we O
use O
128 O
maximum O
sequence O
length O
and O
truncate O
from O
the O
end O
of O
a O
sentence O
if O
its O
length O
exceeds O
the O
limit O
. O

B O
Languages O
We O
work O
on O
40 O
languages O
in O
total O
. O

They O
are O
shown O
in O
Table O
9 O
, O
together O
with O
their O
ISO O
639 O
- O
1 O
codes O
, O
writing O
script O
, O
and O
language O
features O
from O
WALs O
( O
https://wals.info/ O
) O
used O
in O
our O
experiments O
. O

C O
Minimum O
- O
Including O
Algorithm O
We O
utilize O
the O
Minimum O
- O
including O
Algorithm O
from O
Hou O
et O
al O
. O

( O
2020a O
, O
b O
) O
for O
sampling O
the O
buckets O
of O
POS O
and O
NER O
which O
have O
several O
labels O
in O
a O
sen- O
tence O
. O

Denoting O
as O
xa O
sentence O
that O
consists O
of O
an O
array O
of O
words O
( O
x1;:::;x O
n O
) O
, O
and O
the O
array O
ythat O
consists O
of O
a O
series O
of O
labels O
( O
y1;:::;y O
n O
) O
. O

We O
sam- O
ple O
the O
buckets O
by O
using O
Algorithm O
1 O
. O

Note O
that O
we O

5766MARCText O
A O
Tr O
` O
es O
mignons O
et O
de O
bonne O
qualit O
´ O
e. O
La O
ﬁgurine O
est O
assez O
imposante O
mais O
conforme O
` O
a O
la O
taille O
indiqu O
´ O
ee O
dans O
le O
descriptif O
. O

Text O
B O
Jolis O
d O
´ O
etails O
. O

home O
XNLIText O

A O
Ich O
musste O
anfagen O
Seminare O
zu O
belegen O
. O

Text O
B O
Ich O
brauchte O
keine O
V O
orbereitung O
. O

PAWSXText O

A O
Lo O
entren O
´ O
o O
John O
Vel O
´ O
azquez O
y O
en O
sus O
carreras O
m O
´ O
as O
importantes O
lo O
mont O
´ O
o O
el O
jinete O
Dale O
Romans O
. O

Text O
B O
Lo O
entren O
´ O
o O
John O
Vel O
´ O
azquez O
, O
y O
el O
jinete O
Dale O
Romans O

lo O
mont O
´ O
o O
en O
las O
carreras O
m O
´ O
as O
importantes O
. O

POS O
Text O
A O
( O
Lo O
, O
PRON O
) O
, O
( O
sanno O
, O
VERB O
) O
, O
( O
oramai O
, O
ADV O
) O
, O
( O
quasi O
, O
ADV O
) O
, O
( O
tutti O
, O
PRON O
) O
, O
( O
che O
, O
SCONJ O
) O
, O
( O
un O
, O
DET O
) O
, O
( O
respiro O
, O
NOUN O
) O
, O
( O
affannoso O
, O
ADJ O
) O
... O

NER O
Text O
A O
( O
Sempat O
, O
O O
) O
, O
( O
pindah O
, O
O O
) O
, O
( O
ke O
, O
O O
) O
, O
( O
HJK O
, O
B O
- O
ORG O
) O
, O
( O
dan O
, O
O O
) O
, O
( O
1899,B O
- O
ORG O
) O
, O
( O
Hoffenheim O
, O
I O
- O
ORG O
) O
, O
( O
yang O
, O
O O
) O
, O
( O
meminjamkannya O
, O
O O
) O
, O
( O
ke O
, O
O O
) O
... O

Table O
8 O
: O
Example O
entries O
of O
the O
datasets O
. O

We O
convert O
the O
raw O
text O
to O
the O
mBERT O
format O
“ O
Text O
A O
” O
and O
“ O
Text O
B O
” O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

For O
POS O
and O
NER O
, O
we O
list O
( O
word O
, O
tag O
) O
pairs O
in O
the O
sentence O
. O

Following O
Schwenk O
and O
Li O
( O
2018 O
) O
, O
we O
provide O
document O
indices O
of O
MLDoc O
for O
retrieving O
the O
documents O
from O
RCV1 O
and O
RCV2 O
. O

Language O
Writing O
Script81A O
85A O
86A O
87A O
88A O
89A O
Order O
of O
Subject O
, O
Object O
and O
Verb O
Order O
of O
adposition O
and O
noun O
Order O
of O
genitive O
and O
noun O
Order O
of O
adjective O
and O
noun O
Order O
of O
demonstrative O
and O
noun O
Order O
of O
numeral O
and O
noun O
English O
( O
EN O
) O
Latin O
SVO O
Prepositions O

No O
dominant O
order O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Afrikaans O
( O
AF O
) O

Latin O
- O
- O
- O
- O
- O
- O
Arabic O
( O
AR O
) O
Arabic O
VSO O
Prepositions O
Noun O
- O
genetive O
Noun O
- O
adjective O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Bulgarian O
( O
BG O
) O
Cyrillic O
SVO O
Prepositions O
No O
dominant O
order O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Bengali O
( O
BN O
) O

Brahmic O
SOV O
- O
- O
- O
- O
- O
German O
( O
DE O
) O
Latin O

No O
dominant O
order O
Prepositions O
Noun O
- O
genetive O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Greek O
( O
EL O
) O
Greek O

No O
dominant O
order O
Prepositions O
Noun O
- O
genetive O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Spanish O
( O
ES O
) O
Latin O
SVO O
Prepositions O
Noun O
- O
genetive O
Noun O
- O
adjective O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Estonian O
( O
ET O
) O
Latin O
SVO O
Postpositions O
Genetive O
- O
noun O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Basque O
( O
EU O
) O
Latin O
SOV O
Postpositions O
Genetive O
- O
noun O
Noun O
- O
adjective O
Noun O
- O
demonstrative O
Numeral O
- O
noun O
Persian O
( O
FA O
) O
Perso O
- O
Arabic O
SOV O
Prepositions O
Noun O
- O
genetive O
Noun O
- O
adjective O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Finnish O
( O
FI O
) O
Latin O
SVO O
Postpositions O
Genetive O
- O
noun O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
French O
( O
FR O
) O
Latin O
SVO O
Prepositions O
Noun O
- O
genetive O
Noun O
- O
adjective O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Hebrew O
( O
HE O
) O
Hebrew O
SVO O
Prepositions O
Noun O
- O
genetive O
Noun O
- O
adjective O
Noun O
- O
demonstrative O
Numeral O
- O
noun O
Hindi O
( O
HI O
) O
Devanagari O
SOV O
Postpositions O
Genetive O
- O
noun O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Hungarian O
( O
HU O
) O

Latin O

No O
dominant O
order O
Postpositions O
Genetive O
- O
noun O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Indonesian O
( O
ID O
) O
Latin O
SVO O
Prepositions O
Noun O
- O
genetive O
Noun O
- O
adjective O
Noun O
- O
demonstrative O
Numeral O
- O
noun O
Italian O
( O
IT O
) O
Latin O
SVO O
Prepositions O
Noun O
- O
genetive O
Noun O
- O
adjective O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Japanese O
( O
JA O
) O
Ideograms O
SOV O
Postpositions O
Genetive O
- O
noun O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Javanese O
( O
JV O
) O
Latin O
- O
- O
- O
- O
- O
- O
Georgian O
( O
KA O
) O
Georgian O
SOV O
Postpositions O
Genetive O
- O
noun O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Kazakh O
( O
KK O
) O
Cyrillic O
- O
- O
- O
- O
- O
- O
Korean O
( O
KO O
) O
Hangul O
SOV O
Postpositions O
Genetive O
- O
noun O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Malayalam O
( O
ML O
) O
Brahmic O
SOV O
- O
Genetive O
- O
noun O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Marathi O
( O
MR O
) O
Devanagari O

SOV O
Postpositions O
Genetive O
- O
noun O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Malay O
( O
MS O
) O
Latin O
- O
- O
- O
- O
- O
- O
Burmese O
( O
MY O
) O
Brahmic O
SOV O
Postpositions O
Genetive O
- O
noun O
Noun O
- O
adjective O
Demonstrative O
- O
noun O
Noun O
- O
numeral O
Dutch O
( O
NL O
) O
Latin O

No O
dominant O
order O
Prepositions O
Noun O
- O
genetive O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Portuguese O
( O
PT O
) O
Latin O
SVO O
Prepositions O
Noun O
- O
genetive O
Noun O
- O
adjective O
Demonstrative O
- O
noun O
- O
Russian O
( O
RU O
) O
Cyrillic O
SVO O
Prepositions O
Noun O
- O
genetive O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Swahili O
( O
SW O
) O
Latin O
SVO O
Prepositions O
Noun O
- O
genetive O
Noun O
- O
adjective O
Noun O
- O
demonstrative O
Noun O
- O
numeral O
Tamil O
( O
TA O
) O
Brahmic O
SOV O
Postpositions O
Genetive O
- O
noun O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Telugu O
( O
TE O
) O
Brahmic O
SOV O
Postpositions O
Genetive O
- O
noun O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Thai O
( O
TH O
) O
Brahmic O
SVO O
Prepositions O
Noun O
- O
genetive O
Noun O
- O
adjective O
Noun O
- O
demonstrative O
Noun O
- O
numeral O
Tagalog O
( O
TL O
) O
Latin O
VSO O
- O
Noun O
- O
genetive O
No O
dominant O
order O
Mixed O
Numeral O
- O
noun O
Turkish O
( O
TR O
) O
Latin O
SOV O
Postpositions O
Genetive O
- O
noun O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Urdu O
( O
UR O
) O
Perso O
- O
Arabic O
SOV O
Postpositions O
Genetive O
- O
noun O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Vietnamese O
( O
VI O
) O
Latin O
SVO O
Prepositions O
Noun O
- O
genetive O
Noun O
- O
adjective O
Noun O
- O
demonstrative O
Numeral O
- O
noun O
Yoruba O
( O
YO O
) O
Latin O
SVO O
Prepositions O
Noun O
- O
genetive O
Noun O
- O
adjective O
Noun O
- O
demonstrative O
Noun O
- O
numeral O
Chinese O
( O
ZH)Chinese O
ideograms O
SVO O
No O
dominant O
order O
Genetive O
- O
noun O
Adjective O
- O
noun O
Demonstrative O
- O
noun O
Numeral O
- O
noun O
Table O
9 O
: O
All O
languages O
for O
the O
experiments O
along O
with O
their O
ISO O
639 O
- O
1 O
codes O
, O
writing O
script O
, O
and O
linguistic O
features O
. O

“ O
- O
” O
denotes O
lacking O
feature O
information O
from O
WALS O
. O

sample O
with O
replacement O
for O
POS O
and O
NER O
. O

D O
Additional O
Results O
D.1 O
Learning O
Curve O
Figure O
6 O
visualizes O
the O
averaged O
learning O
curve O
of O
10 O
out O
of O
40 O
German O
1 O
- O
shot O
MARC O
buckets O
for O
which O
the O
best O
dev O
performance O
is O
obtained O
at O
epoch O
1 O
. O

D.2 O
Numerical O
Values O

The O
numerical O
values O
of O
the O
POS O
and O
NER O
FS O
- O
XLT O
results O
are O
shown O
in O
Table O
13 O
and O
Table O
12 O
. O

The O
absolute O
performances O
of O
few O
- O
shot O
transfer O
without O
English O
source O
- O
training O
are O
shown O
in O
Table O
11 O
. O

The O
lexical O
overlap O
of O
target O
languages O
with O
EN O
for O
NER O
and O
POS O
is O
shown O
in O
Table O
14.292 O
584 O
78 O
27 O
19 O
526 O
361 O
43 O
31 O
40 O
45 O
630 O
250 O
64 O
11 O
176 O
554 O
162 O
80 O
28 O
24 O
259 O
497 O
196 O
24 O
65 O
298 O
369 O
218 O
50 O
4 O
69 O
237 O
525 O
165 O
22 O
87 O
176 O
471 O
244 O
6 O
25 O
75 O
357 O
537 O
16 O
27 O
42 O
245 O
670 O
599 O
316 O
36 O
33 O
16 O
570 O
262 O
45 O
56 O
67 O
255 O
543 O
112 O
70 O
20 O
269 O
416 O
126 O
125 O
65 O
136 O
401 O
266 O
174 O
23 O
143 O
284 O
219 O
270 O
84 O
60 O
262 O
283 O
322 O
73 O
63 O
163 O
190 O
395 O
189 O
38 O
83 O
127 O
462 O
290 O
32 O
39 O
59 O
314 O
555 O
Table O
10 O
: O
Numerical O
value O
of O
the O
confusion O
matrices O
in O
Figure O
5 O
. O

For O
1 O
- O
shot O
confusion O
matrices O
( O
right O
) O
, O
we O
average O
results O
of O
5 O
buckets O
and O
then O
round O
to O
integers O
. O

5767MLDoc O
PAWSX O
POS O
NER O
K=1 O
K=8 O

K=1 O
K=8 O

K=1 O
K=4 O
K=1 O
K=4 O

DE O

52.638.98 O
84.313.60 O
53.031.67 O
53.411.47 O
RU O
73.184.42 O
86.651.32 O
19.116.94 O
35.576.23 O
FR O
50.808.50 O
77.804.44 O
54.051.33 O
54.600.97 O
ES O
80.544.17 O
90.260.99 O
15.215.98 O
39.375.33 O
ES O
50.308.30 O
74.086.48 O
54.141.53 O
53.881.72 O
VI O
56.975.16 O
72.001.99 O
14.364.28 O
29.635.55 O
IT O
41.346.82 O
65.504.21 O
- O
- O
TR O
48.963.15 O
59.651.83 O
15.025.58 O
37.815.63 O
RU O
46.749.48 O
70.835.63 O
- O
- O
TA O
49.124.67 O
64.962.16 O
13.114.55 O
27.424.82 O
ZH O
49.8710.44 O
76.155.10 O
53.971.79 O
54.171.38 O
MR O
60.265.72 O
73.582.39 O
15.687.09 O
33.506.02 O
JA O
46.416.59 O
66.856.54 O
52.810.96 O
52.971.15 O
- O
- O
- O
- O
- O
KO O
- O
- O
53.920.78 O
53.630.99 O
- O
- O
- O
- O
- O
Table O
11 O
: O
Target O
- O
adapting O
results O
without O
source O
- O
training O
. O

Numbers O
are O
mean O
and O
standard O
deviation O
of O
40 O
runs O
. O

K=0 O
K=1 O
K=2 O
K=4 O
EN O
95.39 O
- O
- O
- O
AF O
86.60 O
91.10 O
1.11 O
92.121.15 O
93.500.56 O
AR O
66.55 O
75.64 O
1.09 O
77.010.84 O
78.520.67 O
BG O
87.02 O
91.01 O
0.97 O
91.970.90 O
93.180.56 O
DE O
86.38 O
89.38 O
0.90 O
90.210.50 O
91.320.43 O
EL O

81.89 O
89.69 O
1.05 O
90.530.89 O
91.580.72 O
ES O
86.64 O
90.05 O
1.01 O
91.190.74 O
92.310.52 O
ET O
79.17 O
81.69 O
1.09 O
83.050.98 O
84.390.56 O
EU O
49.51 O
68.44 O
2.47 O

71.941.78 O
75.891.20 O
FA O
65.73 O
80.82 O
2.14 O
82.811.79 O
84.951.16 O

FI O
74.49 O
78.25 O
1.22 O
79.650.85 O
81.320.82 O
FR O
82.54 O
89.55 O
1.08 O
90.840.64 O
91.660.60 O

HE O
76.79 O
80.40 O
1.42 O

82.421.06 O
83.980.83 O
HI O
64.29 O
78.87 O
1.26 O
80.800.80 O
81.970.92 O
HU O
75.10 O
84.44 O
1.40 O
86.310.90 O
88.610.67 O
ID O
70.80 O
72.68 O
1.08 O
73.640.78 O
74.340.75 O
IT O
85.97 O
88.77 O
0.87 O
89.930.50 O
90.770.59 O
JA O
47.60 O
75.84 O
1.68 O
78.461.31 O
80.420.98 O
KO O
42.29 O
57.43 O
1.36 O
59.921.18 O
62.371.22 O
MR O
58.70 O
71.60 O
2.52 O
74.891.95 O

77.211.77 O
NL O
88.35 O
88.97 O
0.73 O
89.550.79 O
90.830.54 O
PT O
86.45 O
88.18 O
0.70 O
88.980.66 O
89.780.38 O

RU O
86.36 O
89.07 O
0.76 O
89.850.57 O
91.130.51 O
TA O
53.51 O
62.84 O
2.69 O
66.301.56 O
69.361.13 O
TE O
67.48 O
71.46 O
2.58 O
75.721.94 O
78.841.44 O
TR O
57.58 O
64.01 O
1.53 O

66.021.28 O
67.730.82 O
UR O
52.40 O
74.95 O
2.15 O
78.531.38 O
79.571.24 O
VI O
54.96 O
64.79 O
2.33 O
69.391.73 O
72.361.51 O
ZH O
63.01 O
74.15 O

1.96 O
76.621.39 O
79.420.83 O

Table O
12 O
: O
Zero- O
( O
column O
K O
= O
0 O
) O
and O
few- O
( O
columns O
K>0 O
) O
shot O
cross O
- O
lingual O
transfer O
results O
( O
% O
) O
on O
POS O
test O
set O
. O

K=0 O
K=1 O
K=2 O
K=4 O
EN O
83.65 O
- O
- O
- O
AF O

78.36 O
79.07 O
1.47 O
79.691.40 O
80.241.16 O
AR O
39.91 O
54.44 O
6.74 O
60.514.30 O
63.612.65 O
BG O
78.59 O
78.65 O
0.38 O
78.700.39 O
78.870.48 O
BN O
64.17 O
66.37 O
1.69 O
66.661.57 O
65.982.11 O
DE O
79.00 O
79.33 O
0.71 O
79.610.76 O
79.740.73 O
EL O
75.20 O
74.93 O
0.79 O
75.180.95 O
75.400.93 O
ES O
77.16 O
79.19 O
1.97 O
80.281.71 O
80.901.94 O
ET O
71.88 O
72.58 O
1.17 O
73.601.65 O
74.601.59 O
EU O
55.35 O
59.60 O
3.32 O

61.593.84 O
64.682.96 O
FA O
40.73 O
59.20 O
5.34 O
68.554.04 O
71.133.45 O
FI O
68.43 O
71.43 O
2.61 O
73.922.44 O
75.812.15 O
FR O
80.38 O
80.54 O
0.93 O
81.080.85 O
81.220.93 O

HE O
56.36 O
58.24 O
2.25 O
59.432.29 O
60.272.43 O
HI O
65.84 O
67.16 O
1.61 O
67.562.18 O
68.291.76 O
HU O
71.28 O
72.23 O
1.33 O
73.031.44 O
74.141.61 O
ID O
60.10 O
77.87 O
6.31 O
78.574.14 O
81.071.50 O
IT O
80.30 O
80.68 O
0.79 O
81.000.92 O
80.901.12 O
JA O
7.16 O
20.71 O
7.07 O
28.235.32 O
32.936.03 O
JV O
61.18 O
67.80 O
4.72 O
69.793.37 O
72.123.34 O
KA O
61.26 O
61.62 O
1.09 O
62.251.56 O
63.681.66 O
KK O
40.29 O
50.42 O
5.49 O
54.976.81 O
62.944.55 O
KO O
46.50 O
47.25 O
1.36 O
48.691.82 O
51.762.30 O
ML O
46.77 O
47.83 O
2.30 O
49.513.01 O
51.413.31 O
MR O
54.70 O
55.78 O
2.54 O

57.222.43 O
59.183.13 O
MS O
68.61 O
71.04 O
3.07 O
74.514.28 O
76.253.04 O
MY O
42.45 O
43.55 O
3.88 O
46.034.48 O
47.814.28 O
NL O
82.77 O
82.73 O
0.43 O
82.830.54 O
82.820.46 O
PT O
79.28 O
79.89 O
0.99 O
80.390.98 O
80.490.95 O
RU O
65.20 O
67.30 O
2.38 O
68.782.73 O
71.342.82 O
SW O
68.36 O
71.07 O
4.28 O
70.083.15 O
74.335.25 O
TA O
46.12 O
47.81 O
1.81 O
49.862.99 O
52.232.63 O

TE O
50.02 O
52.57 O
1.91 O
54.022.65 O
55.752.72 O
TH O
1.53 O
4.56 O
4.87 O

6.084.88 O
5.874.14 O

TL O
69.23 O
72.34 O
2.25 O
72.632.43 O
73.552.25 O
TR O
65.78 O
69.37 O
2.24 O

69.532.07 O
72.332.85 O
UR O
40.77 O
58.48 O
6.51 O
63.384.88 O
66.494.64 O
VI O
64.67 O
68.77 O
3.54 O
69.643.63 O
71.083.28 O
YO O
35.48 O
53.55 O
6.19 O
58.225.47 O
65.467.10 O

ZH O
13.95 O
32.84 O
7.10 O
40.345.32 O
48.494.30 O

Table O
13 O
: O
Zero- O
( O
column O
K O
= O
0 O
) O
and O
few- O
( O
columns O
K>0 O
) O
shot O
cross O
- O
lingual O
transfer O
results O
( O
% O
) O
on O
NER O
test O
set O
. O

2 O
4 O
6 O
8 O
10 O
Epoch O
Index404244464850Validation O
Accuracy O
( O
% O
) O
Learning O
Curve O
of O
1 O
- O
shot O
German O
MARC O
1 O
- O
shot O
0 O
- O
shotFigure O
6 O
: O
Early O
stopped O
1 O
- O
shot O
transfer O
( O
EN O
! O
DE O
) O
learning O
curve O
. O

The O
English O
- O
trained O
model O
overﬁts O
the O
1 O
- O
shot O
bucket O
quickly O
, O
showing O
decreasing O
dev O
perfor- O
mance O
during O
training O
. O

NER O
POS O
K=1 O
K=2 O
K=4 O
K=1 O
K=2 O
K=4 O

AF O
4.54 O
8.75 O
13.44 O
4.97 O
6.11 O
7.90 O
AR O
0.65 O
0.95 O
1.57 O
3.51 O
4.49 O
5.30 O
BG O
0.98 O
2.19 O
3.23 O
- O
- O
- O
BN O
0.39 O
0.77 O
0.80 O
- O
- O
- O
DE O
8.75 O
13.20 O
20.61 O
9.36 O
15.33 O
21.48 O
EL O
1.45 O
1.84 O
3.59 O
1.96 O
2.87 O
3.04 O
ES O
6.29 O
10.59 O
19.66 O
10.00 O
17.53 O
22.63 O
ET O
4.80 O
5.96 O
11.24 O
5.81 O
9.22 O
13.17 O
EU O
3.77 O
5.55 O
12.31 O
2.60 O
3.45 O
4.69 O
FA O
0.27 O
0.44 O
1.01 O
0.37 O
0.37 O
0.41 O

FI O
5.61 O
9.05 O
15.66 O
4.59 O
7.03 O
8.78 O
FR O
6.26 O
10.83 O
19.01 O
15.60 O
25.23 O
37.39 O
HE O
0.86 O
1.90 O
3.23 O
1.22 O
1.93 O
2.26 O
HI O
0.95 O
1.16 O
1.99 O
0.44 O
0.27 O
0.51 O
HU O
5.07 O
9.19 O
14.35 O
3.18 O
3.92 O
4.15 O
ID O
5.34 O
9.82 O
16.94 O
9.39 O
13.78 O
21.75 O
IT O
7.89 O
10.94 O
21.27 O
11.99 O
16.15 O
21.35 O
JA O
1.75 O
2.02 O
2.14 O
2.60 O
3.68 O
5.00 O

JV O
2.49 O
3.05 O
3.44 O
- O
- O
- O
KA O
1.99 O
4.00 O
5.78 O
- O
- O
- O
KK O
0.89 O
1.22 O
2.11 O
- O
- O
- O
KO O
1.48 O
1.54 O
3.32 O
2.33 O
3.85 O
5.67 O
ML O
0.36 O
1.04 O
1.30 O
- O
- O
- O
MR O
0.53 O
0.56 O
0.71 O
0.24 O
0.24 O
0.24 O
MS O
4.86 O
7.44 O
13.70 O
- O
- O
- O
MY O
0.21 O
0.36 O
0.42 O
- O
- O
- O
NL O
7.18 O
10.65 O
20.14 O
7.94 O
11.42 O
16.79 O
PT O
6.29 O
11.00 O
19.13 O
8.88 O
13.38 O
20.13 O
RU O
1.60 O
2.34 O
3.77 O
4.15 O
6.11 O
9.32 O
SW O
5.90 O
8.10 O
12.37 O
- O
- O
- O
TA O
0.65 O
1.54 O
2.08 O
1.32 O
1.28 O
1.62 O
TE O
0.77 O
0.80 O
1.19 O
0.20 O
0.20 O
0.20 O
TH O
1.63 O
1.87 O
2.08 O
- O
- O
- O
TL O
4.83 O
8.96 O
14.98 O
- O
- O
- O
TR O
4.89 O
8.48 O
16.43 O
2.09 O
2.26 O
3.01 O
UR O
0.30 O
0.27 O
0.68 O
0.74 O
1.35 O
2.16 O
VI O
4.33 O
8.39 O
13.41 O
1.62 O
2.16 O
2.90 O
YO O
1.90 O
2.58 O
2.88 O
- O
- O
- O
ZH O
1.81 O
1.99 O
2.14 O
3.04 O
4.86 O
7.33 O
Table O
14 O
: O
Lexical O
overlap O
( O
per O
- O
mille O
) O
of O
target O
lan- O
guages O
with O
EN O
for O
NER O
and O
POS O
using O
different O
K- O
shot O
buckets O
. O

