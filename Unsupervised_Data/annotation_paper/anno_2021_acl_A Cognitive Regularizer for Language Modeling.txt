Proceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
5191–5202 O
August O
1–6 O
, O
2021 O
. O

© O
2021 O
Association O
for O
Computational O
Linguistics5191A O
Cognitive O
Regularizer O
for O
Language O
Modeling O
Jason O
Wei O
  O
Clara O
Meister O
  O
Ryan O
Cotterell O
, O
Google O
AI O
Language O
  O
ETH O
Zürich O
  O
University O
of O
Cambridge O
jasonwei@google.com O
clara.meister@inf.ethz.ch O
ryan.cotterell@inf.ethz.ch O

Abstract O

The O
uniform O
information O
density O
( O
UID O
) O
hy- O
pothesis O
, O
which O
posits O
that O
speakers O
behaving O
optimally O
tend O
to O
distribute O
information O
uni- O
formly O
across O
a O
linguistic O
signal O
, O
has O
gained O
traction O
in O
psycholinguistics O
as O
an O
explanation O
for O
certain O
syntactic O
, O
morphological O
, O
and O
prosodic O
choices O
. O

In O
this O
work O
, O
we O
explore O
whether O
the O
UID O
hypothesis O
can O
be O
opera- O
tionalized O
as O
an O
inductive O
bias O
for O
statistical O
language O
modeling O
. O

Speciﬁcally O
, O
we O
augment O
the O
canonical O
MLE O
objective O
for O
training O
lan- O
guage O
models O
with O
a O
regularizer O
that O
encodes O
UID O
. O

In O
experiments O
on O
ten O
languages O
span- O
ning O
ﬁve O
language O
families O
, O
we O
ﬁnd O
that O
using O
UID O
regularization O
consistently O
improves O
perplexity O
in O
language O
models O
, O
having O
a O
larger O
effect O
when O
training O
data O
is O
limited O
. O

Moreover O
, O
via O
an O
analysis O
of O
generated O
sequences O
, O
we O
ﬁnd O
that O
UID O
- O
regularized O
language O
models O
have O
other O
desirable O
properties O
, O
e.g. O
, O
they O
gen- O
erate O
text O
that O
is O
more O
lexically O
diverse O
. O

Our O
results O
not O
only O
suggest O
that O
UID O
is O
a O
reason- O
able O
inductive O
bias O
for O
language O
modeling O
, O
but O
also O
provide O
an O
alternative O
validation O
of O
the O
UID O
hypothesis O
using O
modern O
- O
day O
NLP O
tools O
. O

1 O
Introduction O
Language O
has O
been O
hypothesized O
to O
follow O
certain O
information O
- O
theoretic O
constraints O
. O

One O
of O
the O
most O
famous O
of O
these O
constraints O
is O
the O
uniform O
infor- O
mation O
density O
( O
UID O
) O
hypothesis O
( O
Fenk O
and O
Fenk O
, O
1980 O
; O
Jaeger O
, O
2010 O
) O
, O
which O
states O
that O
, O
subject O
to O
the O
rules O
of O
the O
grammar O
, O
speakers O
aim O
to O
distribute O
information O
density O
across O
a O
linguistic O
signal O
as O
uniformly O
as O
possible O
. O

That O
is O
, O
speakers O
behav- O
ing O
optimally O
should O
structure O
their O
utterances O
such O
that O
the O
differences O
between O
the O
peaks O
and O
troughs O
in O
information O
are O
minimized O
. O

In O
the O
psycholinguistics O
literature O
, O
the O
UID O
hy- O
pothesis O
has O
been O
used O
to O
explain O
a O
variety O
of O
lin- O
guistic O
phenomena O
ranging O
from O
how O
we O
shorten O
the O
phonetic O
duration O
of O
more O
- O
predictable O
linguistic(a O
) O
( O
b O
) O
Figure O
1 O
: O
Graphical O
illustration O
of O
two O
examples O
regard- O
ing O
UID O
. O

In O
( O
a O
) O
, O
many O
speakers O
will O
prefer O
the O
version O
with O
the O
relativizer O
that O
( O
dotted O
blue O
line O
) O
. O

The O
UID O
hypothesis O
posits O
that O
this O
is O
because O
, O
without O
the O
rela- O
tivizer O
, O
the O
ﬁrst O
word O
of O
the O
relative O
clause O
, O
we O
, O
has O
high O
information O
density O
; O
and O
so O
including O
the O
relativizer O
distributes O
the O
per O
- O
word O
information O
density O
more O
uni- O
formly O
. O

In O
( O
b O
) O
, O
the O
relativizer O
that O
is O
often O
omitted O
be- O
cause O
, O
at O
the O
onset O
of O
the O
relative O
clause O
, O
the O
informa- O
tion O
density O
of O
Iis O
lower O
and O
therefore O
the O
distribution O
of O
information O
density O
is O
already O
relatively O
uniform O
. O

Il- O
lustration O
based O
on O
Jaeger O
( O
2010 O
) O
. O

units O
( O
Aylett O
and O
Turk O
, O
2004 O
) O
to O
when O
we O
decide O
to O
use O
optional O
syntactic O
relativizers O
( O
Levy O
and O
Jaeger O
, O
2007 O
) O
, O
among O
other O
phenomena O
( O
Bell O
et O
al O
. O
, O
2003 O
; O
Frank O
and O
Jaeger O
, O
2008 O
) O
. O

These O
studies O
often O
use O
language O
models O
to O
estimate O
the O
information O
den- O
sity O
of O
linguistic O
units O
, O
taking O
observations O
of O
low O
variation O
of O
information O
density O
in O
well O
- O
formed O
ut- O
terances O
as O
evidence O
for O
the O
UID O
hypothesis O
. O

5192In O
this O
paper O
, O
we O
propose O
a O
new O
experimental O
paradigm O
that O
uses O
modern O
- O
day O
NLP O
models O
to O
test O
the O
UID O
hypothesis O
. O

Whereas O
prior O
work O
has O
used O
language O
modeling O
as O
a O
tool O
for O
observing O
UID,1 O
we O
explore O
the O
converse O
— O
can O
UID O
be O
used O
as O
a O
tool O
to O
train O
better O
language O
models O
? O

Speciﬁcally O
, O
if O
the O
UID O
hypothesis O
is O
true O
, O
then O
we O
should O
be O
able O
to O
operationalize O
UID O
as O
a O
regularizer O
to O
help O
train O
language O
models O
. O

Moreover O
, O
observing O
lower O
perplexity O
in O
language O
models O
trained O
with O
this O
regularization O
would O
imply O
that O
the O
concept O
of O
UID O
is O
a O
good O
inductive O
bias O
for O
language O
modeling O
, O
thereby O
providing O
a O
new O
type O
of O
evidence O
for O
the O
UID O
hypothesis O
at O
scale O
. O

In O
experiments O
, O
we O
indeed O
ﬁnd O
such O
evidence O
: O
across O
a O
variety O
of O
languages O
and O
dataset O
sizes O
, O
UID O
regularization O
consistently O
improves O
perfor- O
mance O
, O
having O
a O
larger O
effect O
when O
training O
data O
is O
limited O
. O

Moreover O
, O
we O
observe O
that O
— O
in O
compar- O
ison O
with O
their O
unregularized O
counterparts O
— O
UID- O
regularized O
language O
models O
are O
( O
1 O
) O
higher O
entropy O
while O
achieving O
the O
same O
( O
or O
better O
) O
test O
set O
perplex- O
ity O
and O
( O
2 O
) O
generate O
text O
that O
is O
longer O
and O
more O
lexically O
diverse O
. O

Our O
work O
is O
the O
ﬁrst O
to O
explore O
the O
interaction O
between O
UID O
and O
training O
modern- O
day O
neural O
language O
models O
, O
and O
our O
ﬁndings O
— O
that O
a O
cognitively O
motivated O
objective O
can O
improve O
lan- O
guage O
model O
performance O
— O
open O
up O
new O
avenues O
for O
testing O
other O
psycholinguistic O
hypotheses O
in O
a O
similar O
framework O
. O

2 O
Preliminaries O
: O
Language O
Modeling O
The O
task O
of O
language O
modeling O
aims O
to O
estimate O
a O
model O
of O
the O
probability O
of O
observing O
any O
given O
string O
in O
( O
a O
subset O
of O
) O
natural O
language O
. O

For- O
mally O
, O
a O
language O
model O
pis O
an O
( O
unconditional O
) O
probability O
distribution O
over O
sequences O
of O
words O
w O
= O
hw1;w2;:::i O
, O
where O
wconsists O
of O
tokens O
from O
some O
vocabulary O
and O
begins O
and O
ends O
with O
special O
tokens O
BOS O
and O
EOS O
, O
respectively O
. O

Today O
’s O
language O
models O
are O
typically O
param- O
eterized O
by O
neural O
networks O
( O
e.g. O
, O
transformers O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
) O
, O
that O
follow O
a O
local- O
normalization O
scheme O
. O

Speciﬁcally O
, O
the O
model O
pro- O
vides O
a O
conditional O
distribution O
over O
the O
vocabulary O
at O
each O
time O
step O
; O
we O
can O
then O
compute O
the O
proba- O
1On O
its O
own O
, O
the O
term O
‘ O
UID O
’ O
is O
formally O
an O
attribute O
of O
a O
linguistic O
signal O
. O

We O
also O
use O
it O
throughout O
this O
work O
to O
refer O
to O
the O
concept O
that O
UID O
is O
a O
desirable O
property.bility O
of O
an O
entire O
sequence O
was O
: O
p(w O
) O

= O
jwjY O
t=1p(wtjw O
< O
t O
) O
( O
1 O
) O
where O
are O
the O
parameters O
of O
the O
model O
and O
we O
usew O
< O
tto O
represent O
the O
ﬁrst O
t 1tokens O
of O
w. O
Parameters O
are O
estimated O
by O
optimizing O
over O
some O
objectiveL( O
) O
. O

The O
standard O
objective O
for O
lan- O
guage O
modeling O
is O
the O
negative O
log O
- O
likelihood O
of O
a O
datasetWunder O
the O
model O
: O
L( O
) O

= O
 X O
w2Wlogp(w O
) O
( O
2 O
) O
Subsequently O
, O
we O
drop O
explicit O
dependence O
on O
 O
when O
it O
is O
obvious O
from O
context O
. O

To O
assess O
the O
goodness O
of O
ﬁt O
of O
a O
model O
p O
, O
we O
typically O
evaluate O
its O
perplexity O
on O
some O
held O
- O
out O
datasetWtest O
, O
where O
perplexity O
( O
PPL O
) O
is O
deﬁned O
as O
PPL(p O
) O

= O
exp O
  O
 X O
w2Wtest1 O
jwjlogp(w O
) O
! O

( O
3 O
) O
Note O
that O
under O
this O
deﬁnition O
of O
perplexity O
, O
our O
evaluation O
metric O
is O
slightly O
different O
than O
the O
train- O
ing O
objective O
; O
the O
former O
computes O
an O
average O
over O
each O
sequence O
while O
the O
later O
treats O
all O
tokens O
equally O
, O
regardless O
of O
the O
length O
of O
the O
sequence O
in O
which O
they O
are O
present O
. O

3 O
Uniform O
Information O
Density O
Communication O
via O
natural O
language O
is O
a O
compli- O
cated O
and O
nuanced O
process O
that O
takes O
place O
under O
a O
host O
of O
cognitive O
and O
environmental O
constraints O
. O

As O
a O
result O
, O
speakers O
have O
to O
make O
( O
perhaps O
subcon- O
scious O
) O
choices O
to O
best O
navigate O
this O
communicative O
dance O
. O

A O
rational O
speaker O
would O
use O
these O
choices O
to O
optimize O
the O
communicative O
properties O
of O
their O
utterances O
. O

One O
such O
locus O
of O
optimization O
is O
out- O
lined O
by O
the O
Uniform O
Information O
Density O
( O
UID O
) O
hypothesis O
. O

3.1 O
The O
UID O
Hypothesis O
At O
its O
core O
, O
the O
UID O
hypothesis O
aims O
to O
explain O
certain O
phenomena O
in O
human O
language O
processing O
using O
an O
information O
- O
theoretic O
approach O
: O
we O
can O
view O
language O
as O
a O
transfer O
of O
information O
, O
which O
is O
transmitted O
with O
a O
certain O
density O
through O
a O
com- O
munication O
channel O
. O

The O
UID O
hypothesis O
posits O
that O
speakers O
that O
behave O
optimally O
will O
structure O

5193their O
utterances O
to O
avoid O
peaks O
and O
troughs O
in O
this O
information O
density O
( O
Aylett O
and O
Turk O
, O
2004 O
; O
Levy O
and O
Jaeger O
, O
2007 O
; O
Jaeger O
, O
2010 O
) O
. O

More O
formally O
stated O
: O
“ O
Within O
the O
bounds O
deﬁned O
by O
grammar O
, O
speakers O
prefer O
utterances O
that O
distribute O
informa- O
tion O
uniformly O
across O
the O
signal O
( O
information O
den- O
sity O
) O
. O

Where O
speakers O
have O
a O
choice O
between O
sev- O
eral O
variants O
to O
encode O
their O
message O
, O
they O
prefer O
the O
variant O
with O
more O
- O
uniform O
information O
density O
( O
ceteris O
paribus O
) O
” O
( O
Jaeger O
, O
2010 O
) O
. O

3.2 O
Example O
: O
UID O
in O
syntactic O
reduction O
To O
better O
understand O
the O
UID O
hypothesis O
, O
consider O
the O
concrete O
example O
of O
syntactic O
reduction O
( O
that- O
mentioning O
) O
from O
Jaeger O
( O
2010 O
) O
, O
which O
we O
show O
graphically O
in O
Figure O
1 O
and O
also O
describe O
below O
. O

Ex O
. O

A O
.My O
boss O
conﬁrmed O
[ O
that O
] O
we O
are O
crazy O
. O

Ex O
. O
B O
.My O
boss O
thinks O
[ O
that O
] O
I O
am O
crazy O
. O

In O
both O
these O
sentences O
, O
the O
use O
of O
the O
relativizer O
thatis O
syntactically O
optional O
— O
at O
the O
onset O
of O
a O
rel- O
ative O
clause O
( O
RC O
) O
, O
speakers O
can O
, O
but O
do O
not O
have O
to O
, O
include O
the O
relativizer O
. O

Many O
speakers O
, O
how- O
ever O
, O
would O
argue O
that O
the O
sentence O
ﬂows O
better O
with O
the O
relativizer O
included O
in O
Example O
A O
and O
the O
relativizer O
omitted O
in O
Example O
B. O
The O
UID O
hypothesis O
provides O
a O
potential O
expla- O
nation O
for O
this O
phenomenon O
. O

When O
a O
RC O
is O
used O
without O
a O
relativizer O
, O
the O
ﬁrst O
word O
of O
the O
RC O
con- O
veys O
two O
pieces O
of O
information O
: O
both O
the O
onset O
of O
the O
RC O
, O
as O
well O
as O
part O
of O
the O
RC O
’s O
internal O
con- O
tents O
. O

In O
Example O
A O
, O
many O
speakers O
would O
ﬁnd O
that O
the O
information O
density O
of O
the O
ﬁrst O
word O
in O
the O
RC O
, O
we O
, O
is O
high O
, O
and O
so O
adding O
in O
the O
relative O
clause O
distributes O
the O
information O
over O
two O
words O
, O
making O
it O
easier O
to O
parse O
. O

In O
Example O
B O
, O
the O
information O
density O
of O
the O
ﬁrst O
word O
in O
the O
RC O
, O
I O
, O
is O
lower O
rel- O
atively O
, O
and O
so O
we O
do O
not O
need O
to O
( O
or O
it O
is O
not O
as O
beneﬁcial O
to O
) O
include O
the O
relativizer O
. O

3.3 O
Measuring O
UID O
Now O
that O
we O
better O
understand O
what O
the O
UID O
hy- O
pothesis O
attempts O
to O
explain O
, O
how O
might O
we O
opera- O
tionalize O
UID O
and O
ﬁnd O
quantitative O
evidence O
of O
the O
pressure O
for O
it O
in O
language O
? O

First O
, O
to O
quantify O
the O
amount O
of O
information O
conveyed O
by O
a O
word O
, O
we O
turn O
to O
the O
most O
basic O
information O
- O
theoretic O
deﬁnition O
: O
the O
information O
conveyed O
by O
a O
word O
win O
context O
is O
its O
Shannon O
information O
content O
( O
Shannon O
, O
1948 O
) O
, O
also O
called O
surprisal O
. O

Ideally O
, O
this O
surprisal O
would O
be O
measured O
using O
the O
“ O
true O
” O
distribution O
over O
hu- O
man O
language O
. O

Because O
we O
do O
not O
have O
access O
tosuch O
a O
distribution O
, O
we O
often O
estimate O
it O
using O
a O
sta- O
tistical O
language O
model O
. O

That O
is O
, O
given O
a O
statistical O
language O
model O
p O
, O
which O
estimates O
the O
probability O
of O
a O
word O
given O
its O
context O
, O
the O
surprisal O
u(wt)of O
wordwtis O
deﬁned O
as O
the O
following O
: O
u(wt O
) O
= O
 logp(wtjw O
< O
t O
) O
( O
4 O
) O
This O
setup O
provides O
a O
natural O
approach O
to O
exploring O
how O
UID O
might O
manifest O
— O
if O
the O
UID O
hypothesis O
is O
true O
, O
then O
we O
should O
observe O
that O
variation O
in O
surprisal O
, O
as O
estimated O
by O
a O
language O
model O
, O
is O
minimized O
in O
natural O
language O
. O

Using O
this O
approach O
, O
prior O
work O
has O
accumulated O
evidence O
for O
UID O
across O
various O
levels O
of O
linguistic O
representation O
( O
Pluymaekers O
et O
al O
. O
, O
2005 O
; O
Bell O
et O
al O
. O
, O
2009 O
, O
inter O
alia O
) O
. O

As O
some O
of O
the O
earliest O
exam- O
ples O
, O
Aylett O
and O
Turk O
( O
2004 O
) O
showed O
that O
linguistic O
units O
that O
had O
high O
surprisal O
according O
to O
a O
tri O
- O
gram O
language O
model O
were O
uttered O
with O
longer O
syllable O
durations O
, O
and O
Levy O
and O
Jaeger O
( O
2007 O
) O
found O
that O
for O
RCs O
in O
which O
the O
ﬁrst O
word O
had O
higher O
surprisal O
, O
relativizers O
were O
more O
likely O
to O
be O
used O
in O
the O
RC O
during O
actual O
speech O
. O

Further O
examples O
are O
given O
in O
our O
related O
work O
section O
( O
§ O
7 O
) O
. O

4 O
UID O
- O
Regularized O
Language O
Modeling O
While O
prior O
work O
has O
shown O
evidence O
that O
UID O
can O
help O
explain O
many O
of O
the O
choices O
we O
make O
when O
generating O
language O
, O
to O
the O
best O
of O
our O
knowledge O
, O
operationalizations O
of O
UID O
have O
not O
been O
explic- O
itly O
employed O
as O
part O
of O
the O
training O
objective O
in O
modern O
- O
day O
NLP O
models O
. O

This O
raises O
the O
simple O
question O
that O
is O
central O
to O
our O
paper O
: O
Can O
UID O
serve O
as O
an O
inductive O
bias O
for O
training O
statistical O
lan- O
guage O
models O
? O

In O
an O
effort O
to O
answer O
this O
question O
, O
we O
present O
a O
scheme O
for O
incorporating O
operationalizations O
of O
UID O
into O
the O
language O
model O
training O
objective O
. O

Formally O
, O
we O
augment O
the O
canonical O
maximum O
like- O
lihood O
estimation O
objective2 O
in O
eq O
. O

( O
2 O
) O
with O
UID O
2Note O
that O
the O
maximum O
likelihood O
estimation O
objective O
minimizes O
( O
over O
w2W O
) O
 logp(wtjw O
< O
t O
) O
, O
i.e. O
, O
surprisal O
. O

Although O
such O
an O
objective O
may O
indirectly O
minimize O
peaks O
and O
dips O
in O
surprisal O
across O
a O
sequence O
simply O
by O
pushing O
them O
towards O
0 O
, O
it O
does O
not O
explicitly O
include O
any O
sequence O
level O
penalty O
for O
even O
surprisal O
distribution O
. O

5194operationalizations O
as O
regularizers O
R. O
Under O
this O
new O
objective O
, O
we O
minimize O
LR( O
) O
= O
L( O
) O
+ O

R( O
) O
( O
5 O
) O
where O

  O
> O
0is O
the O
strength O
coefﬁcient O
of O
the O
regu- O
larizer O
. O

We O
consider O
two O
natural O
operationalizations O
of O
UID O
— O
inspired O
by O
Collins O
( O
2014)—as O
regulariz- O
ers O
for O
training O
language O
models O
: O
Variance O
Regularizer O
. O

UID O
concerns O
the O
distri- O
bution O
of O
information O
in O
language O
production O
, O
and O
so O
a O
natural O
measure O
of O
this O
behavior O
is O
the O
variance O
of O
surprisals O
. O

Thus O
, O
we O
ﬁrst O
consider O
a O
regularizer O
that O
penalizes O
high O
variance O
among O
the O
surprisals O
of O
words O
in O
a O
given O
sequence O
: O
R( O
) O

= O
1 O
jwjjwjX O
t=1(u(wt) )2(6 O
) O
where=1 O
jwjPjwj O
t=1u(wt O
) O
. O

Note O
that O
here O
, O
and O
in O
our O
subsequent O
regularizers O
, O
we O
estimate O
u()via O
eq O
. O

( O
4 O
) O
using O
our O
model O
p. O
Local O
Consistency O
. O

Next O
, O
we O
consider O
a O
local O
consistency O
regularizer O
that O
encourages O
the O
sur- O
prisals O
of O
adjacent O
words O
to O
have O
similar O
magnitude O
: O
R( O
) O

= O
1 O
jwj 1jwj 1X O
t=1 O
u(wt) u(wt+1)2 O
( O
7 O
) O
This O
regularizer O
is O
also O
a O
reasonable O
operational- O
ization O
of O
UID O
— O
if O
every O
surprisal O
is O
similar O
to O
its O
neighbor O
, O
then O
the O
density O
of O
information O
in O
the O
sequence O
will O
be O
close O
to O
uniform O
. O

Though O
we O
focus O
on O
these O
two O
regularizers O
, O
other O
operationalizations O
of O
UID O
certainly O
exist O
. O

For O
ex- O
ample O
, O
a O
similar O
variant O
of O
the O
above O
regularizers O
is O
the O
max O
regularizer O
( O
Meister O
et O
al O
. O
, O
2020a O
) O
, O
which O
penalizes O
the O
highest O
surprisal O
in O
a O
sentence.3Fur- O
thermore O
, O
UID O
may O
also O
be O
deﬁned O
in O
terms O
of O
parse O
steps O
( O
Hale O
, O
2001 O
) O
or O
structural O
integrations O
( O
Gibson O
, O
2000 O
) O
, O
as O
well O
as O
in O
spoken O
language O
in O
the O
form O
of O
ﬁller O
words O
like O
uhandumor O
word O
repetition O
during O
challenging O
lexical O
retrieval O
. O

We O
consider O
these O
operationalizations O
( O
as O
well O
as O
the O
broader O
discussion O
of O
how O
to O
operationalize O
UID O
) O
as O
future O
work O
. O

3We O
also O
tried O
this O
operationalization O
in O
preliminary O
exper- O
iments O
, O
but O
results O
were O
not O
as O
strong O
as O
the O
variance O
or O
local O
consistency O

regularizers.5 O
Experimental O
Setup O
To O
empirically O
evaluate O
UID O
regularization O
, O
we O
train O
various O
language O
models O
with O
the O
UID- O
regularized O
objective O
( O
eq O
. O

( O
5 O
) O
) O
using O
the O
following O
experimental O
setup O
. O

Datasets O
. O

We O
employ O
datasets O
from O
multiple O
lan- O
guages O
and O
of O
varying O
sizes O
. O

We O
use O
the O
EuroParl O
corpus O
( O
Koehn O
, O
2005)—a O
multi O
- O
lingual O
dataset O
of O
discussions O
from O
the O
European O
Parliament O
that O
has O
been O
commonly O
used O
for O
language O
modeling O
( O
Cot- O
terell O
et O
al O
. O
, O
2018 O
; O

Mielke O
et O
al O
. O
, O
2019)—since O
it O
is O
roughly O
semantically O
controlled O
in O
that O
all O
utter- O
ances O
are O
presumably O
about O
the O
same O
topics O
. O

We O
use O
EuroParl O
v7 O
download O
from O
the O
ACL O
2014 O
SMT O
Workshop4and O
perform O
a O
80–10–10 O
train- O

dev O
- O
test O
split O
on O
all O
ﬁve O
languages O
— O
Czech O
, O
En- O
glish O
, O
French O
, O
German O
, O
and O
Spanish O
— O
which O
yields O
46.7 O
, O
42.2 O
, O
47.2 O
, O
51.3 O
, O
and O
12.4 O
million O
training O
tokens O
for O
each O
language O
respectively O
. O

Moreover O
, O
we O
experiment O
on O
languages O
from O
several O
language O
families O
; O
the O
ﬁve O
languages O
in O
Europarl O
that O
we O
consider O
are O
all O
Indo O
- O
European O
, O
and O
so O
we O
look O
to O
Wiki-40B O
( O
Guo O
et O
al O
. O
, O
2020 O
) O
, O
which O
contains O
Wikipedia O
dumps O
of O
a O
wide O
range O
of O
languages O
. O

We O
choose O
a O
set O
of O
diverse O
languages O
with O
training O
set O
sizes O
relatively O
similar O
to O
that O
of O
EuroParl O
: O
Finnish O
( O
a O
Uralic O
language O
; O
59.3 O
M O
train- O
ing O
tokens O
) O
, O
Indonesian O
( O
an O
Austronesian O
language O
; O
45.7 O
M O
training O
tokens O
) O
, O
and O
Turkish O
( O
a O
Turkic O
lan- O
guage O
; O
38.1 O
M O
training O
tokens O
) O
. O

To O
explore O
per- O
formance O
on O
lower O
- O
resource O
languages O
, O
we O
addi- O
tionally O
experiment O
with O
Swahili5(a O
Niger O
- O
Congo O
language O
; O
6.3 O
M O
training O
tokens O
) O
and O
Tagalog O
( O
an O
Austronesian O
language O
; O
4.2 O
M O
training O
tokens O
) O
. O

For O
all O
languages O
, O
we O
performed O
tokenization O
using O
the O
MosesTokenizer.6Train O
, O
dev O
, O
and O
test O
set O
splits O
are O
shown O
in O
Table O
5 O
in O
the O
Appendix O
. O

Model O
Framework O
and O
Architecture O
. O

For O
our O
experiments O
, O
we O
use O
the O
fairseq O
library O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
, O
a O
standard O
sequence O
modeling O
toolkit O
in O
PyTorch O
. O

As O
our O
model O
, O
we O
use O
fairseq O
’s O
de- O
fault O
transformer O
( O
with O
six O
decoder O
layers O
and O
eight O
4http://statmt.org/wmt14/ O
translation-task.html O
5Since O
there O
are O
no O
Niger O
- O
Congo O
languages O
in O
Wiki-40B O
, O
we O
perform O
a O
80 O
- O
10 O
- O
10 O
split O
on O
Swahili O
Wikidumps O
( O
see O
https://github.com/google-research/bert/ O
blob O
/ O
master O
/ O
multilingual.md O
) O
. O

6https://pypi.org/project/ O
mosestokenizer/ O

5195attention O
heads O
) O
, O
which O
achieves O
competitive7lan- O
guage O
modeling O
performance O
( O
although O
the O
purpose O
of O
our O
paper O
is O
not O
to O
achieve O
or O
compare O
with O
the O
state O
of O
the O
art O
) O
. O

For O
all O
experiments O
, O
we O
followed O
the O
data O
- O
preprocessing O
scripts O
and O
recommended O
hyperparameters O
provided O
in O
fairseq O
’s O
language O
modeling O
module O
; O
more O
detailed O
information O
can O
be O
found O
on O
the O
Github O
page.8 O
UID O
Regularizers O
. O

For O
UID O
regularization O
, O
we O
experiment O
with O
the O
variance O
( O
eq O
. O

( O
6 O
) O
) O
and O
local O
consistency O
regularizers O
( O
eq O
. O
( O
7 O
) O
) O
. O

We O
found O
in O
pre- O
liminary O
experiments O
that O
effective O
regularization O
strengths O
were O
often O
near O

= O
0:01 O
, O
and O
so O
we O
performed O
a O
grid O
search O
over O
values O
within O
an O
or- O
der O
of O
magnitude O
around O

= O
0:01 O
: O

2f0:006 O
, O
0:008,0:01,0:02,0:03,0:04,0:05 O

g. O

We O
choose O
the O
model O
with O
the O
lowest O
dev O
loss O
to O
evaluate O
on O
the O
test O
set O
. O

6 O
Results O
In O
this O
section O
, O
we O
report O
results O
for O
models O
trained O
under O
the O
UID O
- O
regularized O
objective O
. O

We O
ﬁnd O
that O
UID O
regularization O
consistently O
improves O
perplexity O
for O
models O
trained O
on O
various O
languages O
( O
§ O
6.1 O
) O
and O
dataset O
sizes O
( O
§ O
6.2 O
) O
. O

Additionally O
, O
we O
examine O
properties O
of O
text O
generated O
by O
UID O
- O
regularized O
models O
( O
§ O
6.3 O
) O
and O
analyze O
the O
relationship O
between O
our O
operationalization O
of O
UID O
and O
perplexity O
( O
§ O
6.4 O
) O
. O

6.1 O
Languages O
Table O
1 O
shows O
the O
results O
of O
UID O
- O
regularized O
lan- O
guage O
models O
trained O
on O
various O
languages O
from O
EuroParl O
and O
Wiki-40B O
, O
and O
includes O
statistical O
signiﬁcance O
of O
changes O
in O
perplexity O
, O
as O
compared O
with O
baselines O
, O
computed O
using O
permutation O
tests9 O
( O
Efron O
and O
Tibshirani O
, O
1994 O
) O
. O

For O
all O
languages O
, O
UID O
regularization O
signiﬁcantly O
improves O
perplex- O
ity O
for O
at O
least O
one O
of O
the O
two O
regularizers O
. O

Further- O

7On O
Wikitext-103 O
, O
the O
largest O
dataset O
we O
train O
on O
( O
103 O
million O
tokens O
) O
, O
we O
achieve O
a O
competitive O
perplexity O
of O
29.89 O
( O

c.f O
. O
Merity O
et O
al O
. O

( O
2018 O
) O
) O

. O

For O
smaller O
datasets O
, O
we O
tried O
a O
smaller O
transformer O
architecture O
of O
four O
decoder O
layers O
and O
four O
attention O
heads O
, O
but O
it O
did O
not O
perform O
better O
than O
the O
six O
decoder O
layer O
and O
eight O
attention O
heads O
version O
, O
suggesting O
that O
this O
architecture O
was O
not O
too O
large O
for O
the O
datasets O
we O
use O
in O
this O
paper O
( O
even O
the O
Tagalog O
dataset O
we O
use O
is O
larger O
than O
the O
commonly O
used O
Penn O
Treebank O
and O
WikiText-2 O
) O
. O

8https://github.com/pytorch/fairseq/ O
tree O
/ O
master O
/ O
examples O
/ O
language_model O
9http://www2.stat.duke.edu/~ar182/rr/ O
examples O
- O
gallery O
/ O
PermutationTest.htmlLANGUAGE O
( O
# O
train O
tokens O
) O

Perplexity O
CZECH O
( O
12.4 O
M O
) O
Baseline O
( O
no O
UID O
) O
47.47 O
+ O
UID O
: O
variance O
47.24 O
( O
# O
0:5 O
% O
) O
+ O
UID O
: O
local O
consistency O
47.08 O
( O
# O
0:8%)y O
ENGLISH O
( O
46.7 O
M O
) O
Baseline O
( O
no O
UID O
) O
21.34 O
+ O
UID O
: O
variance O
21.08 O
( O
# O
1:2%)y O
+ O
UID O
: O
local O
consistency O
21.19 O
( O
# O
0:7%)y O
FINNISH O
( O
59.3 O
M O
) O
Baseline O
( O
no O
UID O
) O
51.58 O
+ O
UID O
: O
variance O
51.30 O
( O
# O
0:5%)y O
+ O
UID O
: O
local O
consistency O
51.49 O
( O
# O
0:2 O
% O
) O
FRENCH O
( O
51.3 O
M O
) O
Baseline O
( O
no O
UID O
) O
17.08 O
+ O
UID O
: O
variance O
17.02 O
( O
# O
0:4%)y O
+ O
UID O
: O
local O
consistency O
17.03 O
( O
# O
0:3%)y O
GERMAN O
( O
42.3 O
M O
) O
Baseline O
( O
no O
UID O
) O
26.62 O
+ O
UID O
: O
variance O
26.50 O
( O
# O
0:4%)y O
+ O
UID O
: O
local O
consistency O
26.45 O
( O
# O
0:6%)y O
INDONESIAN O
( O
45.7 O
M O
) O
Baseline O
( O
no O
UID O
) O
53.96 O
+ O
UID O
: O
variance O
53.66 O
( O
# O
0:6%)y O
+ O
UID O
: O
local O
consistency O
53.70 O
( O
# O
0:5 O
% O
) O
SPANISH O
( O
47.2 O
M O
) O
Baseline O
( O
no O
UID O
) O
22.54 O
+ O
UID O
: O
variance O
22.37 O
( O
# O
0:8%)y O
+ O
UID O
: O
local O
consistency O
22.44 O
( O
# O
0:4%)y O
SWAHILI O
( O
6.3 O
M O
) O
Baseline O
( O
no O
UID O
) O
40.45 O
+ O
UID O
: O
variance O
39.79 O
( O
# O
1:6%)y O
+ O
UID O
: O
local O
consistency O
39.44 O
( O
# O
2:5%)y O
TAGALOG O
( O
4.2 O
M O
) O
Baseline O
( O
no O
UID O
) O
80.48 O
+ O
UID O
: O
variance O
78.40 O
( O
# O
2:5%)y O
+ O
UID O
: O
local O
consistency O
78.12 O
( O
# O
2:9%)y O
TURKISH O
( O
38.1 O
M O
) O
Baseline O
( O
no O
UID O
) O
66.13 O
+ O
UID O
: O
variance O
65.70 O
( O
# O
0:7%)y O
+ O
UID O
: O
local O
consistency O
66.06 O
( O
# O
0:1 O
% O
) O
Table O
1 O
: O
UID O
regularizers O
improve O
perplexity O
for O
mul- O
tiple O
languages.yindicates O
statistical O
signiﬁcance O
com- O
pared O
with O
the O
baseline O
( O
p<0:05 O
) O
. O

more O
, O
UID O
regularization O
( O
under O
the O
best O
perform- O
ing O

) O
never O
leads O
to O
worse O
perplexity O
. O

These O
re- O

sults O
suggest O
that O
incorporating O
UID O
operational- O
izations O
into O
a O
model O
’s O
training O
objective O
leads O
to O
a O
better O
model O
of O
language O
, O
substantiating O
uniform O
information O
density O
as O
a O
valid O
inductive O
bias O
. O

More- O
over O
, O
the O
improvement O
for O
many O
languages O
corrob- O
orates O
the O
expectation O
that O
UID O
should O
, O
due O
to O
its O
information O
theoretic O
nature O
, O
hold O
across O
languages O
( O
Jaeger O
and O
Tily O
, O
2011 O
) O
. O

5196WMT’06 O
EuroParl O
WT-103 O
# O
training O
tokens O
16.0 O
M O
47.0 O
M O
103.2 O
M O
Baseline O
( O
no O
UID O
) O
49.70 O
21.34 O
29.89 O

+ O
UID O
: O
variance O
48.25y21.08y29.58 O
+ O
UID O
: O
local O
consistency O
48.79 O
21.19 O
29.73 O
Table O
2 O
: O
UID O
regularizers O
improve O
perplexity O
on O
lan- O
guage O
models O
trained O
on O
English O
datasets O
of O
vary- O
ing O
size O
. O

Improvements O
tend O
to O
be O
larger O
on O
smaller O
datasets.yindicates O
statistical O
signiﬁcance O
compared O
with O
the O
baseline O
( O
p<0:05 O
) O
. O

6.2 O
Dataset O
Size O
Notably O
, O
we O
observe O
the O
largest O
improvements O
( O
1.6–2.9 O
% O
) O
in O
perplexity O
in O
Table O
1 O
for O
the O
low- O
est O
resource O
languages O
, O
Tagalog O
and O
Swahili O
( O
with O
4.2 O
and O
6.3 O
million O
training O
tokens O
respectively O
) O
. O

Conversely O
, O
improvement O
was O
most O
marginal O
( O
0.2 O
– O
0.5 O
% O
) O
on O
the O
highest O
- O
resource O
languages O
, O
French O
and O
Finnish O
( O
51.3 O
and O
59.3 O
million O
training O
tokens O
respectively O
) O
. O

To O
remove O
language O
as O
a O
confound- O
ing O
factor O
from O
this O
observation O
, O
we O
perform O
a O
con- O
trolled O
analysis O
of O
the O
effects O
of O
UID O
regularization O
as O
a O
function O
of O
dataset O
size O
. O

We O
focus O
on O
English O
; O
in O
addition O
to O
the O
result O
on O
English O
EuroParl O
2014 O
from O
Table O
1 O
, O
which O
con- O
tains O
47.0 O
million O
training O
tokens O
, O
we O
experiment O
with O
the O
smaller O
monolingual O
English O
dataset O
from O
the O
2006 O
NAACL O
Workshop O
on O
Statistical O
Machine O
Translation O
( O
WMT’06),10which O
has O
17.0 O
M O
tokens O
in O
its O
training O
set O
, O
as O
well O
as O
the O
larger O
Wikitext-103 O
benchmark O
( O
Merity O
et O
al O
. O
, O
2017 O
) O
, O
which O
contains O
103 O
million O
tokens O
in O
its O
training O
set O
. O

Table O
2 O
shows O
the O
perplexities O
for O
models O
with O
and O
without O
UID O
regulariztion O
for O
these O
three O
datasets O
. O

As O
suggested O
by O
earlier O
results O
, O
improve- O
ments O
were O
strongest O
for O
the O
WMT’06 O
dataset O
, O
with O
an O
improvement O
of O
1.4 O
perplexity O
points O
for O
the O
variance O
regularizer O
and O
0.9 O
PPL O
points O
for O
local O
consistency O
. O

For O
the O
larger O
EuroParl O
and O
WT-103 O
datasets O
, O
on O
the O
other O
hand O
, O
improvement O
was O
more O
modest O
, O
ranging O
from O
0.1 O
to O
0.3 O
perplexity O
points O
. O

As O
further O
conﬁrmation O
that O
UID O
regularization O
has O
a O
greater O
impact O
on O
smaller O
datasets O
, O
we O
per- O
form O
an O
ablation O
study O
that O
roughly O
controls O
for O
language O
content O
by O
training O
models O
on O
the O
subsets O
of O
the O
same O
dataset O
. O

For O
this O
ablation O
, O
we O
take O
sub- O
sets O
of O
2 O
, O
4 O
, O
8 O
, O
12 O
, O
16 O
, O
24 O
, O
and O
32 O
million O
sentences O
from O
the O
47 O
million O
sentences O
in O
English O
EuroParl O
, O
10We O
downloaded O
the O
given O
train O
- O
dev O
- O
test O
splits O
from O
https://www.statmt.org/wmt06/ O
.020406080Baseline O
perplexity O
2816 O
24 O
32 O
4700:511:522:5 O
Training O
tokens O
( O
millions)Improvement O
in O
perplexityUID O
: O
variance O
UID O
: O
local O
consistency O
Figure O
2 O
: O
Improvement O
in O
perplexity O
for O
UID O
regular- O
ized O
models O
trained O
on O
subsets O
of O
varying O
size O
sampled O
from O
the O
EuroParl O
English O
dataset O
( O
full O
dataset O
size O
47.0 O
million O
tokens O
) O
. O

UID O
regularization O
helped O
more O
when O
training O
data O
was O
more O
limited O
. O

and O
observe O
how O
much O
the O
UID O
regularizers O
im- O
prove O
perplexity O
for O
each O
training O
dataset O
size O
. O

As O
shown O
in O
Figure O
2 O
, O
the O
results O
tell O
the O
same O
story O
as O
Table O
2 O
— O
UID O
regularization O
improves O
perplexity O
more O
for O
smaller O
datasets O
. O

These O
results O
are O
consistent O
with O
the O
expectation O
that O
models O
trained O
on O
smaller O
datasets O
are O
more O
likely O
to O
overﬁt O
and O
could O
therefore O
beneﬁt O
more O
from O
regularization O
( O
Melis O
et O
al O
. O
, O
2018 O
) O
. O

As O
it O
is O
possible O
that O
the O
models O
trained O
on O
smaller O
datasets O
could O
beneﬁt O
from O
any O
kind O
of O
regularization O
, O
we O
experiment O
with O
label O
smoothing O
( O
Szegedy O
et O
al O
. O
, O
2016 O
) O
, O
another O
regularization O
technique O
that O
similarly O
augments O
the O
training O
objective O
with O
a O
penalty O
. O

Table O
4 O
shows O
these O
results O
for O
models O
trained O
on O
WMT’06 O
and O
EuroParl O
with O
label O
smoothing O
— O
our O
experiments O
indicate O
that O
, O
across O
the O
board O
, O
label O
smoothing O
leads O
to O
worse O
perplexity O
compared O
with O
baseline O
models.11 O
We O
take O
this O
result O
as O
further O
evidence O
that O
the O
improvement O
from O
UID O
regularization O
stems O
from O
the O
UID O
hypothesis O
as O
a O
valid O
inductive O
bias O
, O
rather O
than O
simply O
a O
need O
for O
any O
kind O
of O
regularization O
when O
training O
on O
smaller O
datasets O
. O

11This O
negative O
result O
for O
applying O
label O
smoothing O
to O
lan- O
guage O
modeling O
is O
consistent O
with O
prior O
empirical O
ﬁndings O
( O
Müller O
et O
al O
. O
, O
2019 O
; O
Gao O
et O

al O
. O
, O
2020 O
; O
Meister O
et O
al O
. O
, O
2020b O
) O
. O

5197Sequence O
Model O
% O
unique O
n O
- O
grams O
length O
entropy O
n= O
2n= O

3n= O
4 O
Baseline O
( O
no O
UID O
) O
22.9 O
69.6 O
37.7 O
73.5 O
90.9 O
+ O
UID O
: O
variance O
24.0 O
79.4 O
40.7 O
77.8 O
93.3 O
+ O
UID O
: O
local O
consistency O
23.3 O
73.9 O
39.1 O
75.7 O
92.1 O
Table O
3 O
: O
Text O
generated O
by O
UID O
- O
regularized O
language O
models O
is O
longer O
( O
higher O
average O
sequence O
length O
) O
, O
higher O
entropy O
( O
computed O
via O
monte O
- O
carlo O
estimation O
) O
, O
and O
more O
lexically O
diverse O
( O
a O
higher O
ratio O
of O
unique O
n O
- O
grams O
) O
. O

WMT’06 O
EuroParl O
# O
training O
tokens O
16.0 O
M O
47.0 O
M O
Baseline O

35.75 O
23.22 O
+ O
label O
smoothing O
, O

= O
0:01 O
36.15 O
26.26 O
+ O
label O
smoothing O
, O

= O
0:05 O
55.56 O
40.79 O
+ O
label O
smoothing O
, O

= O
0:1 O
90.57 O
68.26 O
Table O
4 O
: O
Label O
smoothing O
, O
another O
form O
of O
regulariza- O
tion O
that O
similarly O
augments O
the O
cross O
- O
entropy O
objective O
with O
a O
penalty O
, O
does O
not O
improve O
perplexity O
. O

( O
Results O
shown O
on O
dev O
set O
) O
. O

6.3 O
Evaluating O
Generated O
Text O
Unconditional O
models O
of O
language O
have O
been O
ob- O
served O
to O
produce O
generic O
text O
that O
can O
be O
short O
, O
bland O
, O
or O
repetitive O
( O
Fan O
et O
al O
. O
, O
2018 O
; O
Kulikov O
et O
al O
. O
, O
2019 O
; O
Holtzman O
et O
al O
. O
, O
2020 O
) O
, O
and O
so O
in O
this O
subsection O
we O
investigate O
how O
UID O
regularization O
might O
affect O
these O
characteristics O
in O
generated O
text O
. O

For O
these O
experiments O
, O
we O
consider O
the O
baseline O
model O
, O
the O
variance O
- O
regularized O
model O
, O
and O
the O
lo- O
cal O
consistency O
- O
regularized O
model O
trained O
on O
En- O
glish O
EuroParl O
. O

To O
obtain O
text O
samples O
, O
we O
generate O
samples O
by O
sequentially O
sampling O
tokens O
according O
to O
the O
model O
’s O
predicted O
distribution O
until O
the O
end- O
of O
- O
sequence O
( O
EOS O
) O
token O
is O
sampled O
, O
i.e. O
, O
ancestral O
sampling O
. O

Note O
that O
for O
language O
model O
p O
, O
this O
sampling O
scheme O
is O
equivalent O
to O
directly O
sampling O
yp O
. O

We O
obtain O
10,000 O
samples O
for O
each O
model O
and O
report O
statistics O
in O
Table O
3 O
. O

We O
analyze O
each O
set O
of O
generated O
sentences O
for O
several O
metrics O
. O

First O
, O
we O
compute O
the O
average O
length O
of O
generated O
sentences O
. O

Next O
, O
we O
evaluate O
the O
lexical O
diversity O
of O
generated O
texts O
by O
comput- O
ing O
the O
percent O
of O
unique O
n O
- O
grams O
forn2f2;3;4 O
g. O
Finally O
, O
sampling O
from O
a O
model O
also O
gives O
us O
a O
means O
for O
estimating O
the O
language O
model O
’s O
entropy O
: O
H(p O
) O
= O
 X O
y2supp O
( O
p)p(y O
) O
logp(y O
) O
( O
8) O
= O
 Eyp(logp(y O
) O
) O
( O
9 O
) O
In O
the O
case O
of O
language O
models O
, O
supp(p)is O
the O
set O
of O
all O
strings O
that O
can O
be O
generated O
from O
the O
model’svocabularyV. O
As O
this O
is O
exponentially O
large O
in O
jVj O
, O
directly O
computing O
H(p)is O
intractable O
. O

We O
can O
use O
its O
equivalence O
to O
eq O
. O

( O
9 O
) O
, O
however O
, O
to O
estimate O
H(p O
) O
with O
a O
simple O
Monte O
- O
Carlo O
estimator O
: O
^H(p O
) O
= O
 1 O
KKX O
k=1logp(y(k O
) O
) O
( O
10 O
) O
where O
we O
sample O
y(k)pfork= O
1;:::;K O
. O

Table O
3 O
shows O
results O
from O
UID O
- O
regularized O
models O
compared O
with O
the O
baseline O
. O

The O
models O
trained O
with O
the O
variance O
and O
local O
consistency O
reg- O
ularizers O
exhibit O
a O
preference O
for O
longer O
sequence O
length O
and O
higher O
lexical O
diversity O
. O

Additionally O
, O
the O
entropy O
estimates O
of O
these O
models O
are O
notably O
higher O
, O
which O
, O
following O
the O
principle O
of O
maximum O
entropy O
( O
Jaynes O
, O
1957),12can O
be O
seen O
as O
an O
addi- O
tional O
advantage O
of O
UID O
- O
regularized O
models O
over O
their O
unregularized O
counterparts O
. O

6.4 O
UID O
Behavior O
To O
take O
a O
closer O
look O
at O
how O
UID O
regularization O
affects O
language O
models O
, O
we O
examine O
the O
relation- O
ship O
between O
minimizing O
perplexity O
and O
UID O
be- O
havior O
, O
where O
we O
quantify O
UID O
behavior O
as O
the O
variance O
of O
models O
’ O
surprisals O
. O

We O
consider O
mod- O
els O
trained O
on O
the O
English O
EuroParl O
dataset O
with O
the O
variance O
regularizer O
at O
strengths O

2f0:01,0:03 O
, O
0:05,0:07,0:09gand O
our O
baseline O
( O
which O
is O
equiv- O
alent O
to O

= O
0 O
) O
, O
For O
further O
comparison O
, O
we O
also O
train O
a O
model O
with O

= O
 0:01to O
observe O
the O
ef- O
fects O
of O
penalizing O
UID O
behavior O
. O

We O
report O
results O
on O
the O
EuroParl O
test O
set O
in O
Figure O
3 O
. O

We O
observe O
that O
the O
model O
trained O
with O
a O
UID O
penalty O
( O
negative O

) O
indeed O
exhibits O
worse O
perplex- O
ity O
and O
UID O
behavior O
( O
variance O
of O
surprisals O
) O
on O
the O
test O
set O
. O

And O
as O
we O
might O
expect O
, O
models O
trained O
with O
higher O

exhibit O
UID O
behavior O
more O
strongly O
, O
as O
our O
quantiﬁcation O
is O
part O
of O
their O
training O
objec- O
tive O
. O

Overall O
, O
from O

= O
0:01to O

= O
0:05 O
, O
both O
12The O
principle O
of O
maximum O
entropy O
states O
that O
the O
proba- O
bility O
distribution O
that O
best O
represents O
the O
current O
knowledge O
state O
is O
the O
one O
with O
the O
largest O
entropy O
. O

519821 O
21:221:421:621:8 O
221515:51616:51717:51818:5 O

= O
 0:01 O

= O
0(baseline O
) O

= O
0:01 O

= O
0:03 O

= O
0:05 O

= O
0:07 O

= O
0:09 O
PerplexityUID O
behavior O
( O
variance O
of O
surprisals O
) O

Figure O
3 O
: O
A O
trade O
- O
off O
between O
perplexity O
( O
x O
- O
axis O
) O
and O
variance O
of O
surprisals O
( O
a O
measure O
of O
UID O
behavior O
; O
y- O
axis O
) O
. O

The O
black O
pentagon O
indicates O
the O

that O
yielded O
the O
best O
perplexity O
( O

= O
0:03 O
) O
. O

perplexity O
and O
UID O
behavior O
are O
positively O
corre- O
lated O
with O

, O
but O
when O
we O
optimize O
too O
much O
for O
UID O
( O

0:07 O
) O
, O
there O
is O
a O
trade O
- O
off O
in O
which O
model O
perplexity O
begins O
to O
increase O
. O

We O
also O
observe O
an O
intriguing O
phenomenon O
in O
Figure O
3 O
. O

Models O
that O
achieve O
similar O
perplexity O
can O
have O
substantially O
different O
UID O
behavior O
val- O
ues O
on O
the O
test O
set O
. O

Speciﬁcally O
, O
the O

= O
0 O
and O

= O
0:07models O
, O
which O
have O
almost O
the O
same O
perplexity O
, O
have O
variance O
of O
surprisals O
of O
17.8 O
and O
15.8 O
— O
a O
difference O
of O
more O
than O
ten O
percent O
! O

If O
such O
models O
with O
similar O
perplexity O
can O
have O
varying O
deﬁnitions O
of O
what O
constitutes O
good O
UID O
behav- O
ior O
, O
then O
prior O
work O
, O
which O
has O
drawn O
conclusions O
on O
UID O
based O
on O
surprisals O
computed O
by O
a O
single O
model O
( O
Aylett O
and O
Turk O
, O
2004 O
; O
Levy O
and O
Jaeger O
, O
2007 O
; O
Jain O
et O
al O
. O
, O
2018 O
) O
, O
may O
need O
revisiting O
. O

As O
this O
direction O
is O
outside O
the O
scope O
of O
the O
present O
paper O
, O
we O
leave O
it O
as O
future O
work O
. O

7 O
Discussion O
and O
Related O
Work O

We O
discussed O
how O
operationalizing O
UID O
for O
lan- O
guage O
modeling O
leads O
to O
better O
models O
in O
a O
wide O
variety O
of O
settings O
. O

These O
results O
both O
provide O
a O
new O
form O
of O
evidence O
for O
the O
UID O
hypothesis O
and O
build O
on O
prior O
work O
exploring O
UID O
in O
modern O
- O
day O
NLP O
models O
. O

Evidence O
for O
the O
UID O
hypothesis O
. O

Our O
work O
ex- O
tends O
the O
body O
of O
psycholinguistic O
research O
on O
uni- O
form O
information O
density O
, O
which O
has O
largely O
corrob- O
orated O
the O
UID O
hypothesis O
by O
providing O
evidence O
that O
variation O
in O
surprisal O
, O
as O
estimated O
by O
a O
lan O
- O
guage O
model O
, O
is O
minimized O
in O
natural O
language O
. O

In O
addition O
to O
early O
studies O
that O
used O
this O
approach O
to O
ﬁnd O
evidence O
for O
UID O
in O
syntactic O
reduction O
( O
Levy O
and O
Jaeger O
, O
2007 O
) O
, O
morphosyntactic O
contractions O
( O
Frank O
and O
Jaeger O
, O
2008 O
) O
, O
and O
prosodic O
structure O
( O
Aylett O
and O
Turk O
, O
2004 O
) O
, O
the O
same O
line O
of O
reasoning O
has O
been O
used O
by O
more O
recent O
work O
exploring O
a O
variety O
of O
other O
linguistic O
properties O
. O

These O
studies O
have O
found O
that O
word O
duration O
can O
be O
predicted O
by O
syntactic O
surprisal O
( O
Demberg O
et O
al O
. O
, O
2012 O
; O
Moore- O
Cantwell O
, O
2013 O
) O
, O
construction O
probability O
( O
Kuper- O
man O
and O
Bresnan O
, O
2012 O
) O
, O
informativity O
( O
Seyfarth O
, O
2014 O
) O
, O
and O
contextual O
predictability O
( O
Jurafsky O
et O
al O
. O
, O
2001 O
; O
Bell O
et O
al O
. O
, O
2003 O
; O
Gahl O
and O
Garnsey O
, O
2004 O
) O
. O

They O
have O
also O
observed O
that O
word O
length O
is O
re- O
ﬂected O
by O
conceptual O
complexity O
( O
Lewis O
and O
Frank O
, O
2016 O
) O
; O
word O
order O
choice O
can O
be O
predicted O
by O
pro- O
cessing O
cost O
( O
Bloem O
, O
2016 O
; O
Sikos O
et O

al O
. O
, O
2017 O
) O
; O
phonological O
patterns O
can O
be O
shaped O
by O
word O
pre- O
dictability O
( O
Hall O
et O
al O
. O
, O
2018 O
) O
; O
and O
UID O
computed O
at O
the O
sequence O
level O
predicts O
human O
preferences O
for O
syntactic O
alternatives O
of O
the O
same O
sentence O
. O

Whereas O
the O
above O
prior O
work O
has O
used O
language O
modeling O
as O
a O
tool O
for O
measuring O
UID O
, O
our O
paper O
has O
explored O
the O
exact O
converse O
— O
we O
have O
asked O
whether O
UID O
, O
operationalized O
as O
a O
regularizer O
, O
can O
be O
used O
as O
a O
tool O
for O
training O
better O
language O
mod- O
els O
. O

We O
argue O
that O
if O
the O
UID O
hypothesis O
holds O
as O
a O
general O
principle O
, O
then O
we O
should O
be O
able O
to O
exploit O
it O
as O
a O
training O
criterion O
that O
improves O
lan- O
guage O
modeling O
. O

And O
accordingly O
, O
our O
results O
show O
that O
— O
across O
a O
variety O
of O
languages O
and O
dataset O
sizes O
— O
regularization O
for O
UID O
did O
indeed O
improve O
perplexity O
, O
which O
we O
view O
as O
an O
alternative O
kind O
of O
evidence O
for O
the O
UID O
hypothesis O
at O
scale O
. O

Notably O
, O
Figure O
3 O
at O
ﬁrst O
could O
appear O
to O
contra- O
dict O
the O
UID O
hypothesis O
, O
since O
models O
with O
better O
UID O
behavior O
did O
not O
always O
achieve O
better O
perplex- O
ity O
. O

We O
do O
not O
consider O
this O
as O
evidence O
against O
the O
UID O
hypothesis O
, O
however O
. O

Rather O
, O
we O
posit O
that O
when O

is O
too O
large O
, O
we O
may O
be O
optimizing O
for O
UID O
to O
the O
point O
of O
tending O
towards O
unnatu- O
ral O
language O
— O
a O
perfectly O
uniform O
dispersion O
of O
information O
across O
an O
utterance O
may O
come O
at O
the O
cost O
of O
strange O
lexical O
choices O
. O

In O
this O
light O
, O
such O
a O
trade O
- O
off O
should O
be O
somewhat O
expected O
. O

UID O
in O
modern O
NLP O
. O

In O
addition O
to O
the O
tradi- O
tional O
line O
of O
psycholinguistic O
work O
, O
there O
have O
also O
been O
more O
- O
recent O
studies O
on O
UID O
in O
the O
con- O
text O
of O
modern O
NLP O
, O
although O
this O
work O
is O
rela- O
tively O
sparse O
. O

Rubino O
et O

al O
. O

( O
2016 O
) O
leverage O
infor- O

5199mation O
density O
encoded O
as O
surprisal O
at O
the O
word O
, O
part O
of O
speech O
, O
and O
syntax O
levels O
to O
help O
build O
a O
state O
- O
of O
- O
the O
- O
art O
model O
for O
mixed O
- O
domain O
transla- O
tionese O
detection O
. O

Jain O
et O
al O
. O

( O
2018 O
) O
incorporate O
UID O
measures O
across O
sentences O
into O
models O
de- O
signed O
to O
detect O
natural O
versus O
manipulated O
text O
. O

Perhaps O
the O
work O
that O
is O
most O
related O
to O
ours O
, O
Meis- O
ter O
et O
al O
. O
( O
2020a O
) O
, O
leverages O
UID O
to O
explain O
why O
beam O
search O
is O
an O
effective O
decoding O
algorithm O
and O
uses O
operationalizations O
of O
UID O
during O
beam O
search O
to O
alleviate O
problems O
with O
decoding O
poorly O
calibrated O
machine O
translation O
models O
. O

Whereas O
Meister O
et O
al O
. O
( O
2020a O
) O
focuses O
on O
decoding O
, O
our O
work O
shows O
the O
ﬁrst O
evidence O
that O
UID O
can O
be O
op- O
erationalized O
to O
aid O
training O
. O

8 O
Conclusions O
In O
closing O
, O
we O
have O
proposed O
encoding O
uniform O
information O
density O
as O
a O
regularizer O
for O
training O
lan- O
guage O
models O
— O
a O
novel O
manner O
of O
incorporating O
an O
established O
psycholinguistic O
theory O
into O
modern O
statistical O
language O
modeling O
. O

In O
experiments O
on O
a O
range O
of O
languages O
and O
dataset O
sizes O
, O
UID O
reg- O
ularization O
consistently O
improves O
perplexity O
over O
baselines O
. O

Our O
results O
suggest O
that O
UID O
is O
a O
valid O
inductive O
bias O
for O
improving O
the O
canonical O
maxi- O
mum O
likelihood O
objective O
in O
language O
modeling O
, O
providing O
a O
new O
, O
alternative O
type O
of O
evidence O
that O
supports O
the O
UID O
hypothesis O
at O
scale O
. O

Our O
work O
opens O
the O
door O
to O
future O
research O
directions O
such O
as O
using O
similar O
techniques O
to O
validate O
other O
psy- O
cholinguistic O
phenomena O
, O
applying O
UID O
regulariza- O
tion O
in O
conditional O
language O
generation O
tasks O
, O
and O
exploring O
how O
UID O
regularized O
models O
perform O
in O
downstream O
NLP O
applications O
. O

Ethical O
Concerns O
Language O
models O
have O
various O
ethical O
, O
environmen- O
tal O
, O
and O
ﬁnancial O
concerns O
. O

We O
can O
not O
do O
justice O
to O
them O
here O
, O
but O
do O
see O
Bender O
et O
al O
. O

( O
2021 O
) O
for O
a O
pointer O
. O

We O
do O
not O
foresee O
any O
additional O
ethical O
concerns O
with O
the O
contributions O
made O
in O
our O
work O
beyond O
those O
discussed O
in O
Bender O
et O
al O
. O

( O
2021 O
) O
. O

Acknowledgements O
We O
thank O
Roger O
Levy O
for O
feedback O
in O
the O
middle O
stages O
of O
our O
work O
and O
Tiago O
Pimentel O
, O
David O
Re- O
itter O
, O
Tal O
Linzen O
, O
and O
Slav O
Petrov O
for O
feedback O
on O
the O
manuscript O
. O

References O
Matthew O
Aylett O
and O
Alice O
Turk O
. O

2004 O
. O

The O
smooth O
signal O
redundancy O
hypothesis O
: O
A O
functional O
ex- O
planation O
for O
relationships O
between O
redundancy O
, O
prosodic O
prominence O
, O
and O
duration O
in O
spontaneous O
speech O
. O

Language O
and O
Speech O
, O
47(1):31–56 O
. O

PMID O
: O
15298329 O
. O

Alan O
Bell O
, O
Jason O
M. O
Brenier O
, O
Michelle O
Gregory O
, O
Cyn- O
thia O
Girand O
, O
and O
Dan O
Jurafsky O
. O
2009 O
. O

Predictability O
effects O
on O
durations O
of O
content O
and O
function O
words O
in O
conversational O
English O
. O

Journal O
of O
Memory O
and O
Language O
, O
60(1):92–111 O
. O

Alan O
Bell O
, O
Daniel O
Jurafsky O
, O
Eric O
Fosler O
- O
Lussier O
, O
Cyn- O
thia O
Girand O
, O
Michelle O
Gregory O
, O
and O
Daniel O
Gildea O
. O

2003 O
. O

Effects O
of O
disﬂuencies O
, O
predictability O
, O
and O
ut- O
terance O
position O
on O
word O
form O
variation O
in O
English O
conversation O
. O

The O
Journal O
of O
the O
Acoustical O
Society O
of O
America O
, O
113(2):1001–1024 O
. O

Emily O
M. O
Bender O
, O
Timnit O
Gebru O
, O
Angelina O
McMillan- O
Major O
, O
and O
Shmargaret O
Shmitchell O
. O

2021 O
. O

On O
the O
dangers O
of O
stochastic O
parrots O
: O
Can O
language O
models O
be O
too O
big O
? O

In O
Proceedings O
of O
the O
2021 O
ACM O
Confer- O
ence O
on O
Fairness O
, O
Accountability O
, O
and O
Transparency O
, O
FAccT O
’ O
21 O
, O
page O
610–623 O
, O
New O
York O
, O
NY O
, O
USA O
. O

As- O
sociation O
for O
Computing O
Machinery O
. O

Jelke O
Bloem O
. O

2016 O
. O

Testing O
the O
processing O
hypoth- O
esis O
of O
word O
order O
variation O
using O
a O
probabilistic O
language O
model O
. O

In O
Proceedings O
of O
the O
Workshop O
on O
Computational O
Linguistics O
for O
Linguistic O
Com- O
plexity O
( O
CL4LC O
) O
, O
pages O
174–185 O
, O
Osaka O
, O
Japan O
. O

The O
COLING O
2016 O
Organizing O
Committee O
. O

Michael O
Xavier O
Collins O
. O

2014 O
. O

Information O
density O
and O
dependency O
length O
as O
complementary O
cogni- O
tive O
models O
. O

Journal O
of O
Psycholinguistic O
Research O
, O
43(5):651–681 O
. O

Ryan O
Cotterell O
, O
Sabrina O
J. O
Mielke O
, O
Jason O
Eisner O
, O
and O
Brian O
Roark O
. O

2018 O
. O

Are O
all O
languages O
equally O
hard O
to O
language O
- O
model O
? O

In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
2 O
( O
Short O
Papers O
) O
, O
pages O
536–541 O
, O
New O
Orleans O
, O
Louisiana O
. O

Associa- O
tion O
for O
Computational O
Linguistics O
. O

Vera O
Demberg O
, O
Asad O
Sayeed O
, O
Philip O
Gorinski O
, O
and O
Nikolaos O
Engonopoulos O
. O

2012 O
. O

Syntactic O
surprisal O
affects O
spoken O
word O
duration O
in O
conversational O
con- O
texts O
. O

In O
Proceedings O
of O
the O
2012 O
Joint O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Process- O
ing O
and O
Computational O
Natural O
Language O
Learning O
, O
pages O
356–367 O
, O
Jeju O
Island O
, O
Korea O
. O

Association O
for O
Computational O
Linguistics O
. O

Bradley O
Efron O
and O
Robert O
J. O
Tibshirani O
. O

1994 O
. O

An O
In- O
troduction O
to O
the O
Bootstrap O
. O

CRC O
Press O
. O

Angela O
Fan O
, O
Mike O
Lewis O
, O
and O
Yann O
Dauphin O
. O

2018 O
. O

Hi- O
erarchical O
neural O
story O
generation O
. O

In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O

5200Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
889–898 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O

August O
Fenk O
and O
Gertraud O
Fenk O
. O

1980 O
. O

Konstanz O
i O
m O

kurzzeitgedächtnis O
- O
konstanz O
i O
m O
sprachlichen O
in- O
formationsﬂuß O
. O

Zeitschrift O
für O
Experimentelle O
und O
Angewandte O
Psychologie O
, O
27:400–414 O
. O

Austin O
F. O
Frank O
and O
T. O
Florian O
Jaeger O
. O

2008 O
. O

Speaking O
rationally O
: O
Uniform O
information O
density O
as O
an O
opti- O
mal O
strategy O
for O
language O
production O
. O

In O
Proceed- O
ings O
of O
the O
Annual O
Meeting O
of O
the O
Cognitive O
Science O
Society O
, O
volume O
30 O
. O

Susanne O
Gahl O
and O
Susan O
M. O
Garnsey O
. O

2004 O
. O

Knowl- O
edge O
of O
grammar O
, O
knowledge O
of O
usage O
: O
Syntactic O
probabilities O
affect O
pronunciation O
variation O
. O

Lan- O
guage O
, O
pages O
748–775 O
. O

Yingbo O
Gao O
, O
Weiyue O
Wang O
, O
Christian O
Herold O
, O
Zijian O
Yang O
, O
and O
Hermann O
Ney O
. O
2020 O
. O

Towards O
a O
bet- O
ter O
understanding O
of O
label O
smoothing O
in O
neural O
ma- O
chine O
translation O
. O

In O
Proceedings O
of O
the O
1st O
Con- O
ference O
of O
the O
Asia O
- O
Paciﬁc O
Chapter O
of O
the O
Associa- O
tion O
for O
Computational O
Linguistics O
and O
the O
10th O
In- O
ternational O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
212–223 O
, O
Suzhou O
, O
China O
. O
Associ- O
ation O
for O
Computational O
Linguistics O
. O

Edward O
Gibson O
. O

2000 O
. O

The O
dependency O
locality O
the- O
ory O
: O
A O
distance O
- O
based O
theory O
of O
linguistic O
complex- O
ity O
. O

Image O
, O
language O
, O
brain O
: O
Papers O
from O
the O
ﬁrst O
mind O
articulation O
project O
symposium O
, O
2000:95–126 O
. O

Mandy O
Guo O
, O
Zihang O
Dai O
, O
Denny O
Vrande O
ˇci´c O
, O
and O
Rami O
Al O
- O
Rfou O
. O
2020 O
. O

Wiki-40B O
: O
Multilingual O
language O
model O
dataset O
. O

In O
Proceedings O
of O
the O
12th O
Lan- O
guage O
Resources O
and O
Evaluation O
Conference O
, O
pages O
2440–2452 O
, O
Marseille O
, O
France O
. O

European O
Language O
Resources O
Association O
. O

John O
Hale O
. O

2001 O
. O

A O
probabilistic O
Earley O
parser O
as O
a O
psy- O
cholinguistic O
model O
. O

In O
Second O
Meeting O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computa- O
tional O
Linguistics O
. O

Kathleen O
Currie O
Hall O
, O
Elizabeth O
Hume O
, O
T. O
Florian O
Jaeger O
, O
and O
Andrew O
Wedel O
. O

2018 O
. O

The O
role O
of O
pre- O
dictability O
in O
shaping O
phonological O
patterns O
. O

Lin- O
guistics O
Vanguard O
, O
4(s2 O
) O
. O

Ari O
Holtzman O
, O
Jan O
Buys O
, O
Maxwell O
Forbes O
, O
and O
Yejin O
Choi O
. O

2020 O
. O

The O
curious O
case O
of O
neural O
text O
degen- O
eration O
. O

In O
Proceedings O
of O
the O
International O
Confer- O
ence O
on O
Learning O
Representations O
. O

T. O
Florian O
Jaeger O
. O

2010 O
. O

Redundancy O
and O
reduc- O
tion O
: O
Speakers O
manage O
syntactic O
information O
den- O
sity O
. O

Cognitive O
Psychology O
, O
61(1 O
) O
. O

T. O
Florian O
Jaeger O
and O
Harry O
Tily O
. O

2011 O
. O

On O
language O
‘ O
utility O
’ O
: O
Processing O
complexity O
and O
communicative O
efﬁciency O
. O

Wiley O
Interdisciplinary O
Reviews O
: O
Cogni- O
tive O
Science O
, O
2.Ayush O
Jain O
, O
Vishal O
Singh O
, O
Sidharth O
Ranjan O
, O
Rajakrish- O
nan O
Rajkumar O
, O
and O
Sumeet O
Agarwal O
. O

2018 O
. O

Uniform O
Information O
Density O
effects O
on O
syntactic O
choice O
in O
Hindi O
. O

In O
Proceedings O
of O
the O
Workshop O
on O
Linguis- O

tic O
Complexity O
and O
Natural O
Language O
Processing O
, O
pages O
38–48 O
, O
Santa O
Fe O
, O
New O
- O
Mexico O
. O

Association O
for O
Computational O
Linguistics O
. O

Edwin O
T. O
Jaynes O
. O

1957 O
. O

Information O
Theory O
and O
Statis- O
tical O
Mechanics O
. O

Physical O
Review O
, O
106(4):620 O
. O

Daniel O
Jurafsky O
, O
Alan O
Bell O
, O
Michelle O
Gregory O
, O
and O
William O
D. O
Raymond O
. O
2001 O
. O

Probabilistic O
relations O
between O
words O
: O
Evidence O
from O
reduction O
in O
lexi- O
cal O
production O
. O

Typological O
Studies O
in O
Language O
, O
45:229–254 O
. O

Philipp O
Koehn O
. O

2005 O
. O

Europarl O
: O
A O
parallel O
corpus O
for O
statistical O
machine O
translation O
. O

In O
MT O
Summit O
, O
pages O
79–86 O
. O

Ilia O
Kulikov O
, O
Alexander O
Miller O
, O
Kyunghyun O
Cho O
, O
and O
Jason O
Weston O
. O

2019 O
. O

Importance O
of O
search O
and O
eval- O
uation O
strategies O
in O
neural O
dialogue O
modeling O
. O

In O
Proceedings O
of O
the O
12th O
International O
Conference O
on O
Natural O
Language O
Generation O
, O
pages O
76–87 O
, O
Tokyo O
, O
Japan O
. O

Association O
for O
Computational O
Linguistics O
. O

Victor O
Kuperman O
and O
Joan O
Bresnan O
. O

2012 O
. O

The O
effects O
of O
construction O
probability O
on O
word O
durations O
during O
spontaneous O
incremental O
sentence O
production O
. O

Jour- O
nal O
of O
Memory O
and O
Language O
, O
66(4):588–611 O
. O

Roger O
P. O
Levy O
and O
T. O
F. O
Jaeger O
. O

2007 O
. O

Speakers O
op- O
timize O
information O
density O
through O
syntactic O
reduc- O
tion O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
. O

Molly O
L. O
Lewis O
and O
Michael O
C. O
Frank O
. O

2016 O
. O

The O
length O
of O
words O
reﬂects O
their O
conceptual O
complexity O
. O

Cognition O
, O
153:182–195 O
. O

Clara O
Meister O
, O
Ryan O
Cotterell O
, O
and O
Tim O
Vieira O
. O
2020a O
. O

If O
beam O
search O
is O
the O
answer O
, O
what O
was O
the O
question O
? O

InProceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
2173–2185 O
, O
Online O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Clara O
Meister O
, O
Elizabeth O
Salesky O
, O
and O
Ryan O
Cotterell O
. O

2020b O
. O

Generalized O
entropy O
regularization O
or O
: O
There O
’s O
nothing O
special O
about O
label O
smoothing O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Asso- O
ciation O
for O
Computational O
Linguistics O
, O
Online O
. O

Asso- O
ciation O
for O
Computational O
Linguistics O
. O

Gábor O
Melis O
, O
Chris O
Dyer O
, O
and O
Phil O
Blunsom O
. O

2018 O
. O

On O
the O
state O
of O
the O
art O
of O
evaluation O
in O
neural O
language O
models O
. O

In O
Proceedings O
of O
the O
International O
Confer- O
ence O
on O
Learning O
Representations O
. O

Stephen O
Merity O
, O
Nitish O
Shirish O
Keskar O
, O
and O
Richard O
Socher O
. O

2018 O
. O

An O
analysis O
of O
neural O
language O
mod- O
eling O
at O
multiple O
scales O
. O

CoRR O
, O
abs/1803.08240 O
. O

5201Stephen O
Merity O
, O
Caiming O
Xiong O
, O
James O
Bradbury O
, O
and O
Richard O
Socher O
. O

2017 O
. O

Pointer O
sentinel O
mixture O
mod- O
els O
. O

In O
Proceedings O
of O
the O
International O
Conference O
on O
Learning O
Representations O
. O

Sabrina O
J. O
Mielke O
, O
Ryan O
Cotterell O
, O
Kyle O
Gorman O
, O
Brian O
Roark O
, O
and O
Jason O
Eisner O
. O

2019 O
. O

What O
kind O
of O
lan- O
guage O
is O
hard O
to O
language O
- O
model O
? O

In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Com- O
putational O
Linguistics O
, O
pages O
4975–4989 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O

Claire O
Moore O
- O
Cantwell O
. O

2013 O
. O

Syntactic O
predictability O
inﬂuences O
duration O
. O

In O
Proceedings O
of O
Meetings O
on O
Acoustics O
. O

Acoustical O
Society O
of O
America O
. O

Rafael O
Müller O
, O
Simon O
Kornblith O
, O
and O
Geoffrey O
E. O
Hin- O
ton O
. O
2019 O
. O

When O
does O
label O
smoothing O
help O
? O

In O
Ad- O
vances O
in O
Neural O
Information O
Processing O
Systems O
. O

Myle O
Ott O
, O
Sergey O
Edunov O
, O
Alexei O
Baevski O
, O
Angela O
Fan O
, O
Sam O
Gross O
, O
Nathan O
Ng O
, O
David O
Grangier O
, O
and O
Michael O
Auli O
. O

2019 O
. O

fairseq O
: O

A O
fast O
, O
extensible O
toolkit O
for O
sequence O
modeling O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chap- O
ter O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Demonstrations O
) O
, O
pages O
48–53 O
, O
Minneapolis O
, O
Min- O
nesota O
. O

Association O
for O
Computational O
Linguistics O
. O

Mark O
Pluymaekers O
, O
Mirjam O
Ernestus O
, O
and O
R. O
Harald O
Baayen O
. O

2005 O
. O

Lexical O
frequency O
and O
acoustic O
re- O
duction O
in O
spoken O
dutch O
. O

The O
Journal O
of O
the O
Acous- O
tical O
Society O
of O
America O
, O
118(4):2561–2569 O
. O
Raphael O
Rubino O
, O
Ekaterina O
Lapshinova O
- O
Koltunski O
, O
and O
Josef O
van O
Genabith O
. O

2016 O
. O

Information O
density O
and O
quality O
estimation O
features O
as O
translationese O
indica- O
tors O
for O
human O
translation O
classiﬁcation O
. O

In O
Pro- O
ceedings O
of O
the O
2016 O
Conference O
of O
the O
North O
Amer- O
ican O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
960–970 O
, O
San O
Diego O
, O
California O
. O

Association O
for O
Computational O
Linguistics O
. O

Scott O
Seyfarth O
. O

2014 O
. O

Word O
informativity O
inﬂuences O
acoustic O
duration O
: O

Effects O
of O
contextual O
predictabil- O
ity O
on O
lexical O
representation O
. O

Cognition O
, O
133(1):140 O
– O
155 O
. O

Claude O
E. O
Shannon O
. O

1948 O
. O

A O
mathematical O
theory O
of O
communication O
. O

The O
Bell O
System O
Technical O
Journal O
, O
27(3):379–423 O
. O

Les O
Sikos O
, O
Clayton O
Greenberg O
, O
Heiner O
Drenhaus O
, O
and O
Matthew O
W. O
Crocker O
. O
2017 O
. O

Information O
density O
of O
encodings O
: O
The O
role O
of O
syntactic O
variation O
in O
compre- O
hension O
. O

In O
Proceedings O
of O
the O
39th O
Annual O
Meeting O
of O
the O
Cognitive O
Science O
Society O
. O

Christian O
Szegedy O
, O
Vincent O
Vanhoucke O
, O
Sergey O
Ioffe O
, O
Jon O
Shlens O
, O
and O
Zbigniew O
Wojna O
. O

2016 O
. O

Rethinking O
the O
inception O
architecture O
for O
computer O
vision O
. O

In O
Proceedings O
of O
the O
IEEE O
Conference O
on O
Computer O
Vision O
and O
Pattern O
Recognition O
, O
pages O
2818–2826.Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N. O
Gomez O
, O
Łukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O

2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
Advances O
in O
Neural O
Information O
Pro- O
cessing O
Systems O
. O

5202A O
Appendix O
Datasets O
. O

Table O
5 O
shows O
the O
train O
, O
dev O
, O
and O
test O
set O
splits O
for O
the O
language O
modeling O
datasets O
we O
use O
. O

V O
ocab O
Train O
Dev O
Test O
Language O
Family O
Source O
Split O
size O
Sentences O
Tokens O
Sentences O
Tokens O
Sentences O
Tokens O
English O
Indo O
- O
European O
EuroParl O
80–10–10 O
64k O
1.6 O
M O
46.7 O
M O
201k O
5.8 O
M O
201k O
5.8 O
M O
WMT’06 O
80–10–10 O
62k O
751k O
17.0 O
M O
2.0k O
61k O
3.1k O
90k O
WT-103 O
provided O
268k O
1.8 O
M O
103.2 O
M O
3.8k O
217k O
4.4k O
246k O
Czech O
Indo O
- O
European O
EuroParl O
80–10–10 O
64k O

517k O
12.4 O
M O
65k O
1.6 O
M O
65k O
1.6 O
M O
French O
Indo O
- O
European O
EuroParl O
80–10–10 O
64k O

1.6 O
M O
51.3 O
M O
201k O
6.4 O
M O
201k O
6.3 O
M O
German O
Indo O
- O
European O
EuroParl O
80–10–10 O
64k O
1.5 O
M O
42.3 O
M O
192k O
5.4 O
M O
192k O
5.2 O
M O
Spanish O
Indo O
- O
European O
EuroParl O
80–10–10 O
64k O
1.6 O
M O
47.2 O
M O
197k O
6.0 O
M O
197k O
5.9 O
M O
Finnish O
Uralic O
Wiki-40B O
provided O
128k O
256k O
59.3 O
M O
14.1k O
3.9 O
M O
14.0k O
3.2 O
M O
Indonesian O
Austronesian O

Wiki-40B O
provided O
128k O
156k O
45.7 O
M O
8.7k O
3.1 O
M O
8.6k O
2.5 O
M O
Tagalog O
Austronesian O
Wiki-40B O
provided O
128k O
26k O
4.2 O
M O
1.5k O
270k O
1.4k O
220k O
Turkish O
Turkic O
Wiki-40B O
provided O
128k O
143k O
38.1 O
M O

7.8k O
2.5 O
M O
7.7k O
1.9 O
M O
Swahili O
Niger O
- O
Congo O
Wikipedia O
80–10–10 O
128k O
406k O
6.3 O
M O
51k O
800k O
51k O
803k O
Table O
5 O
: O
Train O
, O
dev O
, O
and O
test O
splits O
, O
as O
well O
as O
vocab O
size O
, O
for O
the O
language O
modeling O
datasets O
that O
we O
use O
in O
this O
paper O
. O

If O
train O
- O
dev O
- O
test O
splits O
were O
provided O
, O
then O
we O
used O
them O
. O

Otherwise O
, O
we O
performed O
a O
80–10–10 O
train O
- O
dev O
- O
test O
split O
. O

We O
found O
a O
vocab O
size O
of O
64k O
to O
cover O
more O
than O
98 O
% O
of O
the O
training O
set O
for O
the O
Indo O
- O
European O
languages O
, O
and O
a O
vocab O
size O
of O
62k O
allowed O
us O
to O
cover O
100 O
% O
in O
the O
training O
set O
of O
English O
WMT’06 O
. O

For O
the O
remaining O
languages O
, O
which O
had O
larger O
vocabularies O
, O
we O
followed O
Wiki-40B O
( O
Guo O
et O
al O
. O
, O
2020 O
) O
and O
increased O
the O
vocab O
size O
to O
128k O
. O

Hyperparameters O
. O

Table O
6 O
shows O
the O
optimized O

hyperparameter O
from O
a O
grid O
- O
search O
over O

2f0:006 O
, O
0:008,0:01,0:02,0:03,0:04,0:05gfor O
both O
regularizers O
on O
all O
datasets O
we O
use O
. O

Notably O
, O
the O
best O

for O
variance O
ranged O
from O
1 O
10 2to O
510 2 O
, O
and O
the O
best O

for O
local O
consistency O
ranged O
from O
6 O
10 3to O
210 2 O
. O

For O
use O
on O
a O
new O
dataset O
, O
we O
recommend O
starting O
with O
1 O
10 2 O
, O
which O
we O
found O
almost O
always O
improved O
perplexity O
for O
both O
regularizers O
( O
on O
these O
datasets O
, O
at O
least O
) O
. O

UID O
Regularizer O
Variance O
Local O
Consistency O
Language O
Source O
Best O

  O
Dev O
Loss O
Best O

  O
Dev O
Loss O
English O
EuroParl O
( O
full O
dataset O
) O
2 O
10 24.519 O
810 34.529 O
EuroParl O
( O
2 O
M O
subset O
) O
2 O
10 26.497 O
110 26.497 O
EuroParl O
( O
4 O
M O
subset O
) O
2 O
10 25.940 O
110 25.948 O
EuroParl O
( O
8 O
M O
subset O
) O
2 O
10 25.500 O
810 35.511 O
EuroParl O
( O
12 O
M O
subset O
) O
2 O
10 25.236 O
810 35.230 O
EuroParl O
( O
16 O
M O
subset O
) O
5 O
10 25.084 O
210 25.089 O
EuroParl O
( O
24 O
M O
subset O
) O
4 O
10 24.841 O
210 24.843 O
EuroParl O
( O
32 O
M O
subset O
) O
1 O
10 24.747 O
110 24.742 O
WMT’06 O
3 O
10 24.974 O
110 24.991 O
WT-103 O
1 O
10 24.933 O
810 34.939 O
Czech O
EuroParl O
3 O
10 25.388 O
110 25.391 O
French O
EuroParl O
1 O
10 24.161 O
610 34.162 O
German O
EuroParl O
2 O
10 24.782 O
810 34.779 O
Spanish O
EuroParl O
3 O
10 24.539 O
110 24.550 O
Finnish O
Wiki-40B O
1 O
10 25.811 O
610 35.819 O
Indonesian O
Wiki-40B O
3 O
10 25.808 O
810 35.809 O
Tagalog O
Wiki-40B O
4 O
10 26.319 O
810 36.319 O
Turkish O
Wiki-40B O
3 O
10 26.119 O
810 36.121 O
Swahili O
Wikipedia O
2 O
10 25.555 O
610 35.546 O
Table O
6 O
: O
Best O

hyperparameters O
and O
dev O
losses O
for O
all O
experiments O
. O

