Proceedings O
of O
NAACL O
- O
HLT O
2019 O
, O
pages O
2772â€“2785 O
Minneapolis O
, O
Minnesota O
, O
June O
2 O
- O
June O
7 O
, O
2019 O
. O

c O

2019 O
Association O
for O
Computational O
Linguistics2772A O
Dynamic O
Speaker O
Model O
for O
Conversational O
Interactions O
Hao O
Cheng O
Hao O
Fang O
Mari O
Ostendorf O
University O
of O
Washington O
fchenghao O
, O
hfang O
, O
ostendorf O
g@uw.edu O

Abstract O
Individual O
differences O
in O
speakers O
are O
reï¬‚ected O
in O
their O
language O
use O
as O
well O
as O
in O
their O
inter- O
ests O
and O
opinions O
. O

Characterizing O
these O
differ- O
ences O
can O
be O
useful O
in O
human O
- O
computer O
inter- O
action O
, O
as O
well O
as O
analysis O
of O
human O
- O
human O
conversations O
. O

In O
this O
work O
, O
we O
introduce O
a O
neural O
model O
for O
learning O
a O
dynamically O
up- O
dated O
speaker O
embedding O
in O
a O
conversational O
context O
. O

Initial O
model O
training O
is O
unsuper- O
vised O
, O
using O
context O
- O
sensitive O
language O
gen- O
eration O
as O
an O
objective O
, O
with O
the O
context O
be- O

ing O
the O
conversation O
history O
. O

Further O
ï¬ne- O
tuning O
can O
leverage O
task O
- O
dependent O
supervised O
training O
. O

The O
learned O
neural O
representation O
of O
speakers O
is O
shown O
to O
be O
useful O
for O
content O
ranking O
in O
a O
socialbot O
and O
dialog O
act O
prediction O
in O
human O
- O
human O
conversations.1 O
1 O
Introduction O
Representing O
language O
in O
context O
is O
key O
to O
improv- O
ing O
natural O
language O
processing O
( O
NLP O
) O
. O

There O
are O
a O
variety O
of O
useful O
contexts O
, O
including O
word O
his- O
tory O
, O
related O
documents O
, O
author O
/ O
speaker O
informa- O
tion O
, O
social O
context O
, O
knowledge O
graphs O
, O
visual O
or O
situational O
grounding O
, O
etc O
. O

This O
paper O
addresses O
the O
problem O
of O
modeling O
the O
speaker O
. O

Account- O
ing O
for O
author O
/ O
speaker O
variations O
has O
been O
shown O
to O
be O
useful O
in O
many O
NLP O
tasks O
, O
including O
lan- O
guage O
understanding O
( O
Hovy O
and O
SÃ¸gaard O
, O
2015 O
; O
V O
olkova O
et O
al O
. O
, O
2013 O
) O
, O
language O
generation O
( O
Mirkin O
et O
al O
. O
, O
2015 O
; O
Li O
et O

al O
. O
, O
2016 O
) O
, O
human O
- O
computer O
di- O
alog O
policy O
( O
Bowden O
et O
al O
. O
, O
2018 O
) O
, O
query O
comple- O
tion O
( O
Jaech O
and O
Ostendorf O
, O
2018 O
; O
Shokouhi O
, O
2013 O
) O
, O
comment O
recommendation O
( O
Agarwal O
et O
al O
. O
, O
2011 O
) O
and O
more O
. O

In O
this O
work O
, O
we O
speciï¬cally O
focus O
on O
dialogs O
, O
including O
both O
human O
- O
computer O
( O
social- O
bot O
) O
and O
human O
- O
human O
conversations O
. O

1The O
implementation O
of O
code O
is O
available O
at O
https://github.com/hao-cheng/dynamic O
_ O
speaker_model.gitWhile O
many O
studies O
rely O
only O
on O
discrete O
meta- O
data O
and/or O
demographic O
information O
, O
such O
infor- O
mation O
is O
not O
always O
available O
. O

Thus O
, O
it O
is O
of O
in- O
terest O
to O
learn O
about O
the O
speaker O
from O
the O
language O
directly O
, O
as O
it O
relates O
to O
the O
person O
â€™s O
interests O
and O
speaking O
style O
. O

Motivated O
by O
the O
success O
of O
un- O
supervised O
contextualized O
representation O
learning O
for O
words O
and O
documents O
( O
Mikolov O
et O
al O
. O
, O
2013 O
; O
Kiros O
et O
al O
. O
, O
2015 O
; O

McCann O
et O
al O
. O
, O
2017 O
; O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
our O
approach O
is O
to O
use O
unsupervised O
learning O
with O
a O
neural O
model O
of O
a O
speaker O
â€™s O
dialog O
history O
. O

The O
model O
uses O
latent O
speaker O
mode O
vectors O
for O
representing O
a O
speaker O
turn O
as O
in O
( O
Cheng O
et O
al O
. O
, O
2017 O
) O
, O
which O
provides O
a O
framework O
for O
analysis O
of O
what O
the O
model O
learns O
about O
speaking O
style O
. O

Further O
, O
the O
model O
is O
struc- O
tured O
to O
allow O
a O
dynamic O
update O
of O
the O
speaker O
vector O
at O
each O
turn O
in O
a O
dialog O
, O
in O
order O
to O
capture O
changes O
over O
time O
and O
improve O
the O
speaker O
repre- O
sentation O
with O
added O
data O
. O

The O
speaker O
embeddings O
can O
be O
used O
as O
context O
in O
conversational O
language O
understanding O
tasks O
, O
e.g. O
, O
as O
an O
additional O
input O
in O
dialog O
policy O
predic- O
tion O
in O
human O
- O
computer O
dialogs O
or O
in O
understand- O

ing O
dialog O
acts O
in O
human O
- O
human O
dialogs O
. O

In O
the O
su- O
pervised O
training O
of O
such O
tasks O
, O
the O
speaker O
model O
can O
be O
ï¬ne O
- O
tuned O
. O

This O
work O
makes O
two O
primary O
contributions O
. O

First O
, O
we O
propose O
a O
neural O
model O
for O
learning O
dy- O
namically O
updated O
speaker O
embeddings O
in O
conver- O
sational O
interactions O
. O

The O
model O
training O
is O
un- O
supervised O
, O
relying O
on O
only O
the O
speaker O
â€™s O
conver- O
sation O
history O
rather O
than O
meta O
information O
( O
e.g. O
, O
age O
, O
gender O
) O
or O
audio O
signals O
which O
may O
not O
be O
available O
in O
a O
privacy O
- O
sensitive O
situation O
. O

The O
model O
also O
has O
a O
learnable O
component O
for O
analyz- O
ing O
the O
latent O
modes O
of O
the O
speaker O
, O
which O
can O
be O
helpful O
for O
aligning O
the O
learned O
characteristics O
of O
a O
speaker O
with O
the O
human O
- O
interpretable O
factors O
. O

Sec- O
ond O
, O
we O
use O
the O
learned O
dynamic O
speaker O
embed- O

2773 O
Speaker O
State O
Tracker O
ğ’‰1 O
ğ’‰0 O
ğ’‰ğ‘¡ğ’‰ğ‘¡+1 O
Latent O
Mode O
Analyzer O
ğ’–1Global O
Mode O
Vectors O
ğ’–ğ¾Attentionà·¥ğ’–ğ‘¡ O
ğ’”ğ‘¡ O
ğ’˜ğ‘¡,ğ‘ğ‘¡ğ’˜ğ‘¡,1 O
ğ’…ğ‘¡,ğ‘ğ‘¡+1ğ’…ğ‘¡,1ğ’…ğ‘¡,0 O
ğ’˜ğ‘¡,0 O
ğ’˜ğ‘¡,ğ‘ğ‘¡Speaker O
Language O
Predictor O
ğ’†ğ‘¡,1 O
ğ’†ğ‘¡,ğ‘ğ‘¡ğ‘¤ğ‘¡,1 O

< O
/s O
> O
Conversation O
-Level O
Turn O
-LevelFigure O
1 O
: O
The O
dynamic O
speaker O
model O
. O

The O
speaker O
state O
tracker O
operates O
at O
the O
conversation O
level O
. O

The O
la- O
tent O
model O
analyzer O
and O
speaker O
language O
predictor O
op- O
erate O
at O
the O
turn O
level O
. O

The O
ï¬gure O
only O
shows O
processes O
in O
those O
two O
components O
for O
the O
turn O
t. O
dings O
in O
two O
representative O
tasks O
in O
dialogs O
: O
pre- O
dicting O
user O
topic O
decisions O
in O
socialbot O
dialogs O
, O
and O
classifying O
dialog O
acts O
in O
human O
- O
human O
di- O
alogs O
. O

Empirical O
results O
show O
that O
using O
the O
dy- O
namic O
speaker O
embeddings O
signiï¬cantly O
outper- O
forms O
the O
baselines O
in O
both O
tasks O
. O

In O
the O
public O
dialog O
act O
classiï¬cation O
task O
, O
the O
proposed O
model O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
results O
. O

2 O
Dynamic O
Speaker O
Model O
In O
this O
section O
, O
we O
start O
with O
an O
overview O
of O
the O
proposed O
model O
for O
learning O
speaker O
embeddings O
that O
are O
dynamically O
reï¬ned O
over O
the O
course O
of O
a O
conversation O
. O

Details O
about O
individual O
compo- O
nents O
are O
described O
in O
subsequent O
subsections O
. O

The O
model O
is O
based O
on O
two O
motivations O
. O

First O
, O
a O
speaker O
â€™s O
utterances O
reï¬‚ect O
intents O
, O
speaking O
style O
, O
etc O
. O

Thus O
, O
we O
may O
build O
speaker O
embeddings O
by O
analyzing O
latent O
modes O
that O
characterize O
utter- O
ances O
in O
terms O
of O
such O
characteristics O
, O
apart O
from O
topic O
- O
related O
interests O
a O
user O
might O
have O
. O

Second O
, O
the O
information O
about O
a O
speaker O
is O
accumulated O
as O
the O
conversation O
evolves O
, O
which O
allows O
us O
to O
grad- O
ually O
reï¬ne O
and O
update O
the O
speaker O
embeddings O
. O

The O
speaker O
embeddings O
can O
be O
directly O
used O
as O
features O
or O
ï¬ne O
- O
tuned O
based O
on O
the O
downstream O
tasks O
. O

We O
design O
the O
dynamic O
speaker O
model O
to O
focus O
on O
learning O
cues O
from O
the O
speaker O
â€™s O
utter- O
ances O
, O
and O
leave O
the O
modeling O
of O
different O
speaker- O
addressee O
interactions O
for O
supervised O
downstream O
tasks O
. O

The O
model O
consists O
of O
three O
components O
as O
il- O
lustrated O
in O
Fig O
. O

1 O
. O

First O
, O
a O
latent O
mode O
analyzerreads O
in O
an O
utterance O
and O
analyzes O
its O
latent O
modes O
. O

It O
processes O
the O
speaker O
â€™s O
turns O
independently O
of O
each O
other O
and O
builds O
a O
local O
speaker O
mode O
vector O
for O
each O
turn O
. O

To O
accumulate O
speaker O
information O
as O
the O
conversation O
evolves O
, O
we O
build O
a O
speaker O
state O
tracker O
that O
maintains O
speaker O
states O
at O
in- O
dividual O
turns O
. O

At O
each O
turn O
, O
it O
takes O
two O
input O
vectors O
to O
update O
the O
speaker O
state O
: O
1 O
) O
the O
local O
speaker O
mode O
vector O
for O
the O
current O
turn O
from O
the O
latent O
mode O
analyzer O
, O
and O
2 O
) O
the O
speaker O
state O
at O
the O
previous O
turn O
from O
the O
tracker O
itself O
. O

Finally O
, O
we O
employ O
a O
speaker O
language O
predictor O
to O
drive O
the O
learning O
of O
the O
latent O
model O
analyzer O
and O
the O
speaker O
state O
tracker O
. O

It O
reconstructs O
the O
utterance O
using O
the O
corresponding O
speaker O
state O
. O

Intuitively O
, O
the O
speaker O
language O
predictor O
models O
overall O
lin- O
guistic O
regularities O
itself O
and O
uses O
the O
speaker O
state O
to O
supply O
information O
related O
to O
speaker O
char- O
acteristics O
. O

For O
sequence O
modeling O
in O
all O
three O
components O
, O
we O
use O
the O
long O
short O
- O
term O
memory O
( O
LSTM O
) O
recurrent O
neural O
network O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

In O
our O
experiments O
, O
the O
three O
components O
are O
trained O
jointly O
. O

2.1 O
Latent O
Mode O
Analyzer O
At O
each O
turn O
t O
, O
the O
latent O
mode O
analyzer O
constructs O
a O
local O
speaker O
mode O
vector O
~ut2Rcthat O
captures O
salient O
characteristics O
of O
the O
speaker O
â€™s O
current O
ut- O
terance O
for O
use O
in O
the O
dynamic O
speaker O
model O
. O

First O
, O
the O
utterance O
word O
sequence O
wt;1;;wt;Nt O
is O
mapped O
to O
an O
embedding O
sequence O
, O
where O
wt;n O
is O
represented O
with O
wt;n2Rdaccording O
a O
lookup O
with O
dictionary O
W2RjVjdassociated O
with O
vo- O
cabularyV. O
Then O
, O
the O
latent O
mode O
analyzer O
goes O
through O
two O
stages O
to O
construct O
~ut O
. O

In O
the O
ï¬rst O
stage O
, O
a O
bi O
- O
directional O
LSTM O
( O
Bi- O
LSTM O
) O
, O
which O
consists O
of O
a O
forward O
LSTM O
and O
a O
backward O
LSTM O
, O
is O
used O
to O
encode O
the O
word O
em- O
bedding O
sequence O
into O
a O
ï¬xed O
- O
size O
utterance O
sum- O
mary O
vector O
st2R2 O
m O
, O
wheremis O
the O
dimension O
of O
the O
hidden O
layer O
in O
the O
forward O
and O
backward O
LSTMs O
. O

Formally O
, O
the O
forward O
LSTM O
computes O
its O
hidden O
states O
as O
eF O
t;n O
= O
gF(wt;n;eF O
t;n 1)2Rm O
forn= O
1;:::;Nt O
, O
wheregF(;)denotes O
the O
for- O
ward O
LSTM O
function O
. O

The O
backward O
LSTM O
com- O
putes O
its O
hidden O
states O
eB O
t;n2Rmsimilarly O
. O

The O
initial O
hidden O
states O
eF O
t;0andeB O
t;Nt+1are O
set O
to O
ze- O

ros O
. O

The O
summary O
vector O
stis O
the O
concatenation O
of O
the O
two O
ï¬nal O
hidden O
states O
, O
st= O

[ O
eF O
t;Nt;eB O
t;1 O
] O
. O

In O
the O
second O
stage O
, O
the O
utterance O
summary O
vec- O
torstis O
compared O
with O
Kglobal O
mode O
vectors O

2774u1;:::;uK2Rcwhich O
are O
learned O
as O
part O
of O
the O
model O
. O

The O
association O
score O
at;kbetween O
st O
andukis O
computed O
using O
the O
dot O
- O
product O
atten- O
tion O
mechanism O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
as O
follows O
, O
at;k O
= O
exp(hPst;Quki)PK O
k0=1exp(hPst;Quk0i O
) O
; O
( O
1 O
) O
where O
P2Rc2mandQ2Rccare O
learnable O
weights O
, O
andh;iindicates O
the O
dot O
- O
product O
of O
two O
vectors O
. O

The O
local O
speaker O
mode O
vector O
is O
then O
constructed O
as O
~ut O
= O
PK O
k=1at;kuk O
. O

2.2 O
Speaker O
State O
Tracker O
The O
speaker O
state O
tracker O
provides O
a O
dynamic O
sum- O
mary O
of O
speaker O
language O
features O
observed O
in O
the O
conversation O
history O
, O
using O
an O
LSTM O
to O
en- O
code O
the O
sequence O
of O
local O
speaker O
mode O
vec- O
tors O
~ O
ut;1;;~ut;Nt O
. O

At O
turnt O
, O
this O
LSTM O
up- O
dates O
its O
hidden O
state O
ht2Rmusing O
the O
local O
speaker O
mode O
vector O
~utand O
its O
previous O
hidden O
stateht 12Rm O
, O
i.e. O
,ht O
= O
gS(~ut;ht 1 O
) O
, O
where O
gS(;)is O
the O
speaker O
LSTM O
function O
. O

The O
hidden O
statehtprovides O
the O
speaker O
state O
vector O
at O
turn O
t. O
2.3 O
Speaker O
Language O
Predictor O
The O
speaker O
language O
predictor O
is O
a O
conditional O
LSTM O
language O
model O
( O
LM O
) O
that O
reconstructs O
the O
word O
sequence O
in O
the O
current O
turn O
. O

Language O
mod- O
eling O
is O
a O
way O
to O
provide O
a O
training O
signal O
for O
un- O
supervised O
learning O
that O
models O
the O
conditional O
probability O
Pr(wt;njwt;<n O
) O
, O
wherewt;<n O
denotes O
all O
preceding O
words O
of O
wt;nin O
the O
turnt O
. O

The O
speaker O
language O
predictor O
uses O
the O
same O
dictionary O
Wfor O
word O
embeddings O
as O
the O
latent O
mode O
analyzer O
to O
represent O
words O
at O
time O
t. O

The O
initial O
hidden O
state O
dt;02Rmof O
the O
LSTM O
is O
set O
totanh(Lht O
) O
, O
where O
L2Rmmis O
a O
learnable O
matrix O
and O
tanh()is O
the O
hyperbolic O
tangent O
func- O
tion O
. O

Subsequent O
LSTM O
hidden O
states O
are O
com- O
puted O
as O
dt;n O
= O
gLM(rI(wt;n 1;ht);dt;n 1 O
) O
; O
forn= O
1;:::;Nt+ O
1 O
, O
whererI(wt;n 1;ht O
) O

= O
RI O
wwt;n 1+RI O
hhtis O
a O
linear O
transformation O
with O
learned O
parameters O
RI O
w2RmdandRI O
h2 O
Rmm O
, O
gLM(;)is O
a O
forward O
LSTM O
function O
, O
andwt;0is O
the O
word O
embedding O
for O
the O
start O
- O
of- O
sentence O
token O
. O

By O
injecting O
the O
speaker O
state O
vec- O
tor O
at O
every O
time O
step O
nin O
the O
turnt O
, O
the O
model O
is O
more O
likely O
to O
favor O
directly O
using O
the O
speaker O
state O
vector O
( O
vs. O
the O
word O
history O
) O
for O
predictingthe O
speaker O
language O
. O

The O
conditional O
probability O
is O
then O
computed O
as O
Pr(wt;njwt;<n O
) O
= O
softmax O
( O
VrO(ht;dt;n));(2 O
) O
where O
V2RjVjmis O
the O
weight O
matrix O
, O
and O
rO(ht;dt;n O
) O

= O
RO O
hht+RO O
ddt;nis O
another O
lin- O
ear O
function O
with O
learnable O
parameters O
RO O
h;RO O
d2 O
Rmm O
. O

The O
last O
word O
wt;Nt+1is O
always O
the O
end- O
of O
- O
sentence O
token O
. O

2.4 O
Model O
Training O
and O
Tuning O
The O
training O
objective O
for O
a O
given O
conversation O
is O
the O
log O
- O
likelihoodP O
tP O
nlog O
Pr(wt;njwt;<n O
) O
; O
where O
the O
conditional O
probability O
is O
deï¬ned O
in O
( O
2 O
) O
. O

The O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
is O
used O
with O
a O
conï¬guration O
of O

1= O
0:9and O

2= O
0:97 O
. O

The O
initial O
learning O
rate O
is O
set O
to O
0:002 O
. O

We O
halve O
the O
learning O
rate O
at O
each O
epoch O
once O
the O
de- O
velopment O
log O
- O
likelihood O
decreases O
, O
and O
terminate O
the O
training O
when O
it O
decreases O
for O
the O
second O
time O
. O

This O
validation O
protocol O
is O
used O
throughout O
the O
pa- O
per O
for O
training O
the O
proposed O
model O
. O

In O
our O
experiments O
, O
the O
embedding O
dictio- O
nary O
Wis O
initialized O
using O
pre O
- O
trained O
300- O
dimensional O
word O
embeddings O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
for O
words O
within O
the O
vocabulary O
of O
this O
resource O
. O

The O
remaining O
part O
of O
Wand O
other O
model O
parameters O
are O
randomly O
initialized O
based O
onN(0;0:01 O
) O
. O

The O
mode O
vector O
dimension O
cis O
set O
to64 O
. O

We O
tune O
the O
number O
of O
global O
mode O
vectors O
Kfromf16;32gand O
the O
hidden O
layer O
size O
mfrom O
f128;160 O
g. O
The O
ï¬nal O
model O
is O
selected O
based O
on O
the O
log O
- O
likelihood O
on O
the O
development O
set O
. O

3 O
User O
Topic O
Decision O
Prediction O
We O
ï¬rst O
study O
a O
prediction O
task O
that O
estimates O
whether O
the O
user O
engaged O
in O
a O
socialbot O
conversa- O
tion O
would O
accept O
a O
suggested O
topic O
. O

Speciï¬cally O
, O
we O
use O
a O
corpus O
of O
human O
- O
socialbot O
conversations O
collected O
during O
the O
2017 O
Alexa O
Prize O
competi- O
tion O
( O
Ram O
et O
al O
. O
, O
2017 O
) O
from O
the O
Sounding O
Board O
system O
( O
Fang O
et O
al O
. O
, O
2018 O
; O
Fang O
, O
2019 O
) O
. O

Due O
to O
privacy O
concerns O
, O
the O
socialbot O
does O
not O
have O
ac- O
cess O
to O
any O
identity O
information O
about O
users O
. O

Also O
, O
since O
each O
device O
may O
be O
used O
by O
multiple O
users O
, O
the O
device O
address O
is O
not O
a O
reliable O
indicator O
of O
the O
user O
ID O
. O

Therefore O
, O
the O
ability O
to O
proï¬le O
the O
user O
through O
one O
conversational O
interaction O
is O
desirable O
for O
guiding O
the O
socialbot O
â€™s O
dialog O
policy O
. O

2775train O
dev O
test O
# O
conversations O
19,076 O
6,321 O
6,465 O
# O
topic O
decisions O
85,340 O
28,060 O
29,561 O
Table O
1 O
: O
Data O
statistics O
of O
the O
topic O
decision O
dataset O
. O

3.1 O
Data O
Each O
conversation O
begins O
with O
a O
greeting O
and O
ends O
when O
the O
user O
makes O
a O
stop O
command O
. O

The O
so- O
cialbot O
engages O
the O
user O
in O
the O
conversation O
using O
a O
wide O
range O
of O
content O
indexed O
by O
topics O
, O
where O
a O
topic O
corresponds O
to O
a O
noun O
or O
noun O
phrase O
that O
refers O
to O
a O
named O
entity O
( O
e.g. O
, O
Google O
) O
or O
a O
concept O
( O
e.g. O
, O
artiï¬cial O
intelligence O
) O
. O

These O
topics O
are O
ex- O
tracted O
using O
both O
constituency O
parsing O
results O
of O
the O
textual O
content O
and O
content O
meta O
- O
information O
. O

During O
the O
conversation O
, O
the O
socialbot O
sometimes O
negotiates O
the O
topic O
with O
the O
user O
using O
an O
explicit O
conï¬rmation O
turn O
and O
records O
the O
user O
â€™s O
binary O
de- O
cision O
( O
accept O
or O
reject O
) O
on O
the O
topic O
. O

In O
socialbot O
conversations O
, O
a O
system O
turn O
is O
al- O
ways O
followed O
by O
a O
user O
turn O
and O
vice O
versa O
. O

We O
tag O
system O
turns O
making O
explicit O
conï¬rmation O
about O
a O
topic O
and O
attach O
the O
corresponding O
binary O
user O
decisions O
with O
them O
. O

To O
curate O
the O
dataset O
for O
the O
topic O
decision O
prediction O
task O
, O
we O
use O
a O
total O
of O
31,862 O
conversations O
with O
more O
than O
5 O
user O
turns O
. O

On O
average O
there O
are O
around O
22 O
user O
turns O
per O
con- O
versation O
. O

Not O
every O
system O
turn O
makes O
a O
topic O
suggestion O
, O
and O
the O
average O
number O
of O
topic O
deci- O
sions O
per O
conversation O
is O
4.5 O
. O

We O
randomly O
split O
the O
conversations O
into O
training O
, O
development O
, O
and O
test O
sets O
by O
3=1=1 O
. O

The O
data O
statistics O
are O
shown O
in O
Table O
1 O
. O

In O
our O
experiments O
, O
we O
directly O
use O
the O
speech O
recognition O
output O
of O
user O
utterances O
. O

The O
vocabularyVconsists O
of O
roughly O
11 O
K O
words O
that O
appear O
at O
least O
5 O
times O
in O
the O
training O
set O
. O

3.2 O
Topic O
Decision O
Classiï¬er O
We O
use O
a O
feed O
- O
forward O
neural O
network O
( O
FFNN O
) O
to O
make O
binary O
predictions O
( O
accept O
vs. O
reject O
) O
for O
in- O
dividual O
topic O
suggestions O
. O

For O
each O
topic O
sugges- O
tion O
, O
the O
FFNN O
takes O
two O
inputs O
: O
1 O
) O
an O
embedding O
xt0for O
the O
suggested O
topic O
at O
system O
turn O
t0 O
, O
and O
2 O
) O
a O
user O
embedding O
vector O
ztat O
user O
turnt O
. O

Note O
the O
model O
does O
not O
have O
information O
about O
user O
turns O
after O
the O
system O
turn O
t0when O
making O
the O
predic- O
tion O
, O
i.e. O
, O
the O
user O
turn O
tappears O
before O
the O
system O
turnt0 O
. O

The O
topic O
embedding O
xt0 O
â€™s O
are O
looked O
up O
from O
the O
embedding O
dictionary O
learned O
by O
the O
FFNN.They O
are O
initialized O
by O
averaging O
the O
embeddings O
of O
their O
component O
words O
using O
the O
public O
pre- O
trained O
300 O
- O
dimensional O
word O
embeddings O
( O
Bo- O
janowski O

et O

al O
. O
, O
2017 O
) O

. O

For O
the O
user O
embedding O
vector O
, O
we O
explore O
two O
settings O
that O
use O
different O
numbers O
of O
user O
turns O
as O
context O
. O

In O
both O
settings O
, O
topic O
decisions O
occurring O
in O
the O
ï¬rst O
5 O
user O
turns O
are O
not O
used O
for O
evaluations O
. O

Static O
User O
Embeddings O
: O
Motivated O
by O
the O
ï¬nd- O
ings O
that O
most O
user O
characteristics O
can O
be O
inferred O
from O
initial O
interactions O
( O
Ravichander O
and O
Black O
, O
2018 O
) O
, O
we O
derive O
a O
static O
user O
embedding O
vector O
for O
a O
conversation O
using O
the O
ï¬rst O
5 O
user O
turns O
and O
apply O
it O
for O
predicting O
topic O
decisions O
afterwards O
. O

Dynamic O
User O
Embeddings O
: O

Alternatively O
, O
we O
build O
a O
user O
embedding O
vector O
for O
user O
turn O
tus- O
ing O
all O
previous O
user O
turns O
. O

Here O
, O
a O
topic O
decision O
for O
system O
turn O
t0is O
aligned O
with O
its O
preceding O
user O
turnt O
. O

In O
our O
experiments O
, O
we O
compare O
different O
un- O
supervised O
models O
with O
our O
proposed O
dynamic O
speaker O
model O
. O

For O
both O
settings O
, O
all O
unsupervised O
models O
are O
pre O
- O
trained O
on O
alluser O
turns O
in O
training O
conversations O
. O

They O
are O
ï¬xed O
when O
training O
the O
FFNN O
classiï¬er O
. O

The O
FFNN O
classiï¬er O
is O
trained O
with O
the O
logistic O
loss O
using O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
. O

The O
training O
protocol O
is O
similar O
to O
that O
described O
in O
Â§ O
2.4 O
. O

We O
tune O
the O
hid- O
den O
layer O
size O
from O
f64;128gand O
the O
number O
of O
hidden O
layers O
from O
f0;1 O
g. O

The O
model O
is O
selected O
based O
on O
the O
loss O
on O
the O
development O
set O
. O

In O
addition O
, O
we O
use O
a O
user O
- O
agnostic O
TopicPrior O
baseline O
. O

It O
builds O
a O
probability O
lookup O
for O
each O
topic O
using O
its O
acceptance O
rate O
on O
the O
training O
set O
. O

We O
tune O
a O
universal O
probability O
threshold O
for O
all O
topics O
based O
on O
the O
development O
set O
accuracy O
. O

In O
all O
experiments O
, O
three O
evaluation O
metrics O
are O
used O
: O
accuracy O
, O
area O
under O
the O
receiver O
operating O
characteristic O
curve O
( O
AUC O
) O
, O
and O
normalized O
cross- O
entropy O
( O
N O
- O
CE O
) O
. O

N O
- O
CE O
is O
computed O
as O
the O
relative O
cross O
- O
entropy O
reduction O
of O
the O
model O
over O
the O
Top- O
icPrior O
baseline O
. O

3.3 O
Experiments O
: O
Static O
User O
Embeddings O
As O
described O
in O
Â§ O
3.2 O
, O
we O
use O
the O
ï¬rst O
5 O
user O
turns O
to O
derive O
the O
user O
embedding O
vector O
for O
a O
conver- O
sation O
. O

We O
compare O
our O
dynamic O
speaker O
model O
with O
three O
other O
unsupervised O
models O
. O

DynamicSpeakerModel O
: O

For O
the O
proposed O
dy- O
namic O
speaker O
model O
, O
we O
concatenate O
the O
speaker O
state O
vector O
htand O
the O
local O
speaker O
mode O
vector O

2776Model O
Acc O
AUC O
N O
- O
CE O
TopicPrior O
68.8 O
72.5 O
0 O
UtteranceLDA O
68.8 O
73.1 O
12.6 O
UtteranceAE O
68.8 O
73.4 O
12.8 O
TopicDecisionEncoder O
68.9 O
73.8 O
13.4 O
DynamicSpeakerModel O
69.5 O
74.2 O
13.7 O
Table O
2 O
: O
Test O
set O
results O
( O
in O
% O
) O
for O
topic O
decision O
pre- O
dictions O
using O
static O
user O
embeddings O
. O

~utfor O
each O
of O
the O
ï¬rst O
5 O
user O
turns O
. O

Then O
, O
we O
apply O
the O
max O
- O
pooling O
operation O
over O
the O
5 O
concatenated O
vectors O
to O
summarize O
all O
the O
information O
. O

The O
re- O
sulting O
vector O
~his O
used O
as O
the O
user O
embedding O
vec- O
tor O
. O

UtteranceLDA O
: O

The O
latent O
Dirichlet O
allocation O
( O
LDA O
) O
model O
( O
Blei O
et O
al O
. O
, O
2003 O
) O
is O
trained O
with O
16 O
latent O
groups O
by O
treating O
all O
user O
utterances O
in O
a O
conversation O
as O
a O
document.2The O
trained O
LDA O
model O
builds O
a O
16 O
- O
dimensional O
probability O
vector O
as O
the O
user O
embedding O
vector O
by O
loading O
the O
ï¬rst O
5 O
user O
turns O
as O
a O
single O
document O
. O

UtteranceAE O
: O

The O
utterance O
auto O
- O
encoder O
model O
is O
built O
upon O
the O
sequence O
auto O
- O
encoder O
( O
Dai O
and O
Le O
, O
2015 O
) O
. O

We O
replace O
the O
original O
encoder O
by O
a O
BiLSTM O
that O
encodes O
the O
utterance O
at O
user O
turn O
t O
into O
a O
summary O
vector O
stin O
the O
same O
way O
as O
the O
ï¬rst O
stage O
of O
the O
latent O
mode O
analyzer O
described O
in O
Â§ O
2.1 O
. O

The O
auto O
- O
encoder O
is O
trained O
on O
all O
user O
ut- O
terances O
in O
the O
training O
data O
, O
using O
the O
same O
train- O
ing O
protocol O
described O
in O
Â§ O
2.4 O
. O

We O
set O
the O
hidden O
layer O
size O
to O
128 O
. O

The O
user O
embedding O
vector O
is O
constructed O
by O
applying O
the O
max O
- O
pooling O
opera- O
tion O
over O
the O
summary O
vectors O
s1;:::;s5for O
the O
ï¬rst O
5 O
user O
turns O
. O

TopicDecisionEncoder O
: O

This O
model O
encodes O
the O
topic O
decisions O
occurred O
in O
the O
ï¬rst O
5 O
user O
turns O
. O

The O
user O
embedding O
vector O
is O
the O
concatenation O
of O
two O
vectors O
. O

One O
is O
max O
- O
pooled O
from O
the O
topic O
embeddings O
for O
accepted O
topics O
, O
and O
the O
other O
for O
rejected O
topics O
, O
both O
include O
a O
dummy O
topic O
vector O
as O
default O
. O

The O
topic O
embeddings O
are O
composed O
by O
averaging O
the O
public O
pre O
- O
trained O
300 O
- O
dimensional O
embeddings O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
for O
words O
in O
the O
topic O
. O

Experiment O
results O
are O
summarized O
in O
Table O
2 O
. O

The O
TopicPrior O
is O
a O
very O
strong O
predictor O
, O
with O
an O
2To O
allow O
the O
LDA O
model O
to O
take O
into O
account O
bi O
- O
grams O
, O
we O
replace O
the O
uni O
- O
gram O
token O
wiwith O
its O
bi O
- O
gram O
( O
wi O
, O
wi+1 O
) O
concatenated O
as O
a O
single O
token O
if O
the O
bi O
- O
gram O
is O
among O
the O
top O
500 O
frequent O
bi O
- O
grams O
. O

Model O
Acc O
AUC O
N O
- O
CE O
TopicDecisionLSTM O
69.3 O
74.8 O
14.6 O
UtteranceAE O
+ O
LSTM O
69.9 O
75.4 O
15.3 O
DynamicSpeakerModel O
72.4 O
79.0 O
20.0 O
Table O
3 O
: O
Test O
set O
results O
( O
in O
% O
) O
for O
topic O
decision O
pre- O
dictions O
using O
dynamic O
user O
embeddings. O
: O
The O
im- O
provement O
of O
DynamicSpeakerModel O
over O
both O
Top- O
icDecisionLSTM O
and O
UtteranceAE O

+ O
LSTM O
is O
statis- O
tically O
signiï¬cant O
based O
on O
both O
t O
- O
test O
and O
McNemar O
â€™s O
test(p O
< O
: O
001 O
) O
. O

accuracy O
on O
par O
with O
other O
user O
embeddings O
. O

This O
indicates O
that O
the O
popularity O
- O
based O
approach O
is O
a O
good O
start O
for O
content O
ranking O
in O
socialbots O
when O
there O
is O
little O
user O
information O
. O

Nevertheless O
, O
we O
can O
still O
observe O
some O
improvement O
over O
the O
Top- O
icPrior O
in O
terms O
of O
AUC O
and O
N O
- O
CE O
, O
which O
suggests O
using O
information O
from O
initial O
interactions O
reduces O
the O
uncertainty O
of O
predictions O
. O

The O
proposed O
dy- O
namic O
speaker O
model O
performs O
the O
best O
among O
the O
compared O
models O
, O
reducing O
the O
cross O
- O
entropy O
by O
13.7 O
% O
over O
the O
TopicPrior O
baseline O
. O

3.4 O
Experiments O
: O
Dynamic O
User O
Embeddings O
Here O
, O
we O
use O
all O
information O
accumulated O
before O
the O
system O
turn O
of O
suggesting O
the O
topic O
to O
build O
the O
corresponding O
user O
embedding O
vector O
. O

Since O
the O
UtteranceLDA O
is O
not O
as O
effective O
based O
on O
static O
embedding O
experiments O
, O
we O
only O
consider O
extend- O

ing O
UtteranceAE O
and O
TopicDecisionEncoder O
mod- O
els O
for O
comparison O
here O
. O

DynamicSpeakerModel O
: O

The O
speaker O
state O
tracker O
in O
our O
model O
accumulates O
the O
user O
infor- O
mation O
as O
the O
conversation O
evolves O
. O

Thus O
, O
we O
directly O
concatenate O
the O
speaker O
state O
vector O
ht O
and O
the O
local O
speaker O
mode O
vector O
~utas O
the O
user O
embedding O
vector O
at O
user O
turn O
t. O
Other O
than O
us- O
ing O
more O
turns O
, O
this O
is O
the O
same O
DynamicSpeaker- O
Model O
conï¬guration O
as O
in O
Â§ O
3.3 O
. O

UtteranceAE+LSTM O
: O

This O
model O
uses O
an O
LSTM O
to O
encode O
the O
summary O
vector O
sequence O
derived O
from O
the O
same O
utterance O
auto O
- O
encoder O
used O
in O
Â§ O
3.3 O
. O

The O
LSTM O
hidden O
states O
are O
treated O
as O
user O
embedding O
vectors O
at O
individual O
user O
turns O
. O

TopicDecisionLSTM O
: O

Similarly O
, O
an O
LSTM O
is O
used O
to O
encode O
the O
topic O
decision O
sequence O
. O

At O
each O
time O
step O
, O
the O
LSTM O
reads O
the O
concatenation O
of O
the O
topic O
embedding O
and O
the O
one O
- O
hot O
vector O
en- O
coding O
the O
topic O
decision O
. O

We O
use O
the O
same O
topic O
embeddings O
as O
the O
TopicDecisionEncoder O
in O
Â§ O
3.3 O
. O

Since O
not O
every O
user O
turn O
is O
associated O
with O
a O
topic O

2777decision O
, O
the O
time O
steps O
of O
this O
LSTM O
are O
aligned O
to O
a O
sequence O
of O
non O
- O
consecutive O
user O
turns O
. O

The O
LSTM O
hidden O
states O
are O
treated O
as O
user O
embedding O
vectors O
at O
corresponding O
user O
turns O
. O

For O
UtteranceAE+LSTM O
and O
TopicDecision- O
LSTM O
, O
the O
hidden O
layer O
size O
of O
the O
LSTM O
is O
set O
to O
128 O
. O

While O
the O
utterance O
auto O
- O
encoder O
and O
topic O
embeddings O
are O
pre O
- O
trained O
, O
the O
LSTM O
compo- O
nents O
are O
jointly O
learned O
with O
the O
FFNN O
for O
com- O
posing O
dynamic O
user O
embeddings O
. O

Experiment O
results O
are O
shown O
in O
Table O
3 O
. O

The O
DynamicSpeakerModel O
performs O
the O
best O
. O

Com- O
paring O
to O
results O
in O
Table O
2 O
, O
all O
three O
unsupervised O
models O
outperform O
their O
static O
counterparts O
, O
which O
suggests O
the O
advantage O
of O
using O
dynamic O
context O
for O
predicting O
user O
topic O
decisions O
as O
conversation O
evolves O
. O

Statistical O
signiï¬cance O
tests O
of O
the O
difference O
in O
performance O
of O
two O
systems O
were O
conducted O
un- O
der O
both O
the O
t O
- O
test O
using O
the O
predicted O
probabilities O
and O
McNemar O
â€™s O
test O
using O
the O
binary O
predictions O
. O

Under O
both O
tests O
, O
the O
predictions O
from O
the O
Top- O
icDecisionLSTM O
and O
the O
DynamicSpeakerModel O
are O
highly O
signiï¬cation O
( O
p O
< O
: O
001 O
) O
. O

Predictions O
from O
UtteranceAE O
+ O
LSTM O
and O
DynamicSpeak- O
erModel O
are O
also O
signiï¬cantly O
different O
based O
on O
both O
tests O
( O
p O
< O
: O
001 O
) O
. O

3.5 O
Qualitative O
Analysis O
First O
, O
we O
manually O
inspect O
the O
predictions O
from O
the O
TopicDecisionLSTM O
and O
DynamicSpeakerModel O
used O
in O
Â§ O
3.4 O
and O
the O
static O
baseline O
TopicPrior O
in O
Â§ O
3.3 O
. O

Compared O
with O
TopicPrior O
, O
we O
ï¬nd O
that O
Top- O
icDecisionLSTM O
is O
able O
to O
utilize O
the O
semantic O
relatedness O
between O
neighboring O
topics O
and O
cor- O
responding O
user O
decisions O
. O

For O
example O
, O
â€œ O
Elon O
Musk O
â€ O
( O
the O
CEO O
) O
is O
likely O
to O
be O
rejected O
if O
â€œ O
Tesla O
â€ O
( O
the O
company O
) O
has O
been O
rejected O
earlier O
, O
though O
both O
are O
popular O
topics O
with O
high O
acceptance O
rates O
. O

In O
addition O
, O
it O
seems O
that O
the O
DynamicSpeaker- O
Model O
is O
able O
to O
make O
use O
of O
user O
reactions O
. O

In O
the O
anecdotal O
example O
illustrated O
in O
Table O
4 O
, O
the O
user O
accepts O
the O
topic O
â€œ O
Arnold O
Schwarzenegger O
â€ O
which O
is O
correctly O
predicted O
by O
both O
TopicDeci- O
sionLSTM O
and O
DynamicSpeakerModel O
, O
but O
only O
the O
DynamicSpeakerModel O
correctly O
predicts O
the O
rejection O
of O
â€œ O
politics O
â€ O
later O
. O

We O
then O
analyze O
what O
language O
features O
are O
learned O
by O
latent O
modes O
in O
our O
dynamic O
speaker O
model O
. O

For O
each O
mode O
, O
we O
extract O
top O
utterances O
sorted O
by O
their O
association O
scores O
as O
computed O
inBot O
: O
Do O
you O
like O
the O
actor O
Arnold O
Schwarzenegger O
? O

User O
: O

yeah O
before O
he O
got O
into O
politics O
Bot O
: O
Super O
, O
would O
you O
like O
to O
know O
a O
fun O
fact O
about O
Arnold O
Schwarzenegger O
? O

â€¢TopicDecisionLSTM O
: O
accept O
â€¢DynamicSpeakerModel O
: O
accept O
User O
: O
why O
not O
sure O
. O
. O
. O

Bot O
: O
I O
â€™m O
running O
out O
of O
things O
to O
say O
about O
him O
. O

Do O
you O
wanna O
hear O
some O
news O
about O
politics O
? O

â€¢TopicDecisionLSTM O
: O
accept O
â€¢DynamicSpeakerModel O
: O
reject O
User O
: O
no O
Table O
4 O
: O
A O
dialog O
snippet O
showing O
topic O
decision O
predictions O
from O
TopicDecisionLSTM O
and O
DynamicS- O
peakerModel O
. O

Topics O
are O
shown O
with O
underscores O
. O

( O
1 O
) O
. O

Examples O
from O
the O
most O
representative O
modes O
are O
provided O
in O
Appendix O
A. O

In O
brief O
, O
we O
ï¬nd O
two O
separate O
modes O
related O
to O
positive O
and O
nega- O
tive O
reactions O
; O
other O
modes O
correspond O
to O
classes O
of O
dialog O
acts O
, O
such O
as O
yes O
/ O
no O
answers O
, O
topic O
re- O
quests O
and O
conversation O
- O
closing O
. O

Within O
topic O
re- O
quest O
modes O
, O
some O
involve O
short O
topic O
phrases O
( O
e.g. O
, O
â€œ O
holidays O
â€ O
) O
while O
others O
use O
complete O
re- O
quests O
( O
e.g. O
â€œ O
can O
we O
talk O
about O
cats O
â€ O
) O
. O

Along O
this O
line O
, O
some O
modes O
are O
associated O
with O
relatively O
terse O
users O
and O
others O
with O
talkative O
users O
. O

These O
ï¬ndings O
indicate O
that O
our O
model O
cpatures O
various O
user O
characteristics O
that O
might O
be O
useful O
for O
pre- O
dicting O
their O
interaction O
preferences O
. O

4 O
Dialog O
Act O
Classiï¬cation O
Dialog O
act O
analysis O
is O
widely O
used O
for O
conversa- O
tions O
, O
which O
identiï¬es O
the O
illocutionary O
force O
of O
a O
speaker O
â€™s O
utterance O
following O
the O
speech O
act O
the- O
ory O
( O
Austin O
, O
1975 O
; O
Searle O
, O
1969 O
) O
. O

In O
this O
section O
, O
we O
apply O
the O
proposed O
dynamic O
speaker O
model O
to O
the O
dialog O
act O
classiï¬cation O
task O
. O

4.1 O
Data O
We O
use O
the O
Switchboard O
Dialog O
Act O
Corpus O
( O
SwDA O
) O
, O
which O
has O
dialog O
act O
annotations O
on O
two- O
party O
human O
- O
human O
speech O
conversations O
( O
Juraf- O
sky O
et O
al O
. O
, O
1997 O
; O
Stolcke O
et O
al O
. O
, O
2000 O
) O
. O

In O
total O
, O
there O
are O
1155 O
open O
- O
domain O
conversations O
with O
manual O
transcripts O
. O

Following O
recent O
work O
, O
we O
use O
1115 O
conversations O
for O
training O
, O
19 O
for O
test- O
ing O
, O
and O
the O
rest O
21 O
for O
development.3The O
origi- O
nal O
ï¬ne O
- O
grained O
dialog O
act O
labels O
are O
mapped O
to O
42 O
3The O
training O
and O
test O
split O
ï¬les O
are O
downloaded O
from O
https://web.stanford.edu/ O
Ëœjurafsky O
/ O
ws97/ O
. O

2778 O
ğ’™ğ­ğ’›ğ­âˆ’ğŸ O
ğ’™ğ­âˆ’ğŸğ’›ğ­ğ’št O
ğ’›ğ­âˆ’ğŸ O
ğ’™ğ­âˆ’ğŸ O
Speaker O
  O
A O
ğ’‰ğ›½(ğ‘¡âˆ’1)ğµğ’‰ğ›½(ğ‘¡+1)ğµğ’‰ğ›¼(ğ‘¡)ğ´ O
Speaker O
  O
BAttentionğ’ˆ1Dialog O
Act O
Vectorsğ’ˆğ¿à·¥ğ’ˆğ‘¡Figure O
2 O
: O
The O
attention O
- O
based O
LSTM O
tagging O
model O
for O
dialog O
act O
classiï¬cation O
. O

The O
ï¬gure O
only O
shows O
the O
at- O
tention O
operation O
for O
turn O
t. O

The O
lower O
two O
boxes O
rep- O
resent O
two O
speaker O
state O
trackers O
. O

classes.4For O
this O
set O
of O
experiments O
, O
we O
use O
the O
golden O
segmentation O
and O
manual O
transcripts O
pro- O
vided O
in O
the O
dataset O
. O

Motivated O
by O
the O
recent O
success O
of O
unsupervised O
models O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
we O
also O
study O
whether O
the O
dynamic O
speaker O
model O
can O
beneï¬t O
from O
training O
on O
external O
unlabelled O
data O
. O

Thus O
, O
we O
use O
speech O
transcripts O
from O
5850 O
conver- O
sations O
from O
the O
Fisher O
English O
Training O
Speech O
Part O
1 O
Transcripts O
( O
Cieri O
et O
al O
. O
, O
2004 O
) O
, O
which O
( O
like O
Switchboard O
) O
consists O
of O
two O
- O
party O
human- O
to O
- O
human O
telephone O
conversations O
but O
without O
an- O
notations O
for O
dialog O
acts O
. O

4.2 O
Dialog O
Act O
Tagging O
Model O
We O
use O
an O
attention O
- O
based O
LSTM O
tagging O
model O
for O
the O
dialog O
act O
classiï¬cation O
. O

As O
shown O
in O
Fig O
. O

2 O
, O
the O
tagging O
LSTM O
is O
stacked O
on O
two O
speaker O
state O
trackers O
. O

Note O
the O
two O
trackers O
share O
the O
same O
parameters O
as O
well O
as O
the O
underlying O
la- O
tent O
mode O
analyzer O
and O
speaker O
language O
predic- O
tor O
. O

They O
generate O
speaker O
embeddings O
by O
track- O

ing O
corresponding O
speakers O
separately O
. O

Let O

( O
t)and O

( O
t)denote O
the O
mappings O
from O
the O
global O
turn O
index O
tto O
the O
speaker O
- O
speciï¬c O
turn O
in- O
dices O
for O
speaker O
A O
and O
speaker O
B O
, O
respectively O
. O

The O
mapping O
returns O
a O
null O
value O
if O
the O
turn O
t O
is O
not O
associated O
with O
the O
corresponding O
speaker O
. O

The O
speaker O
state O
vectors O
are O
used O
as O
the O
input O
to O
the O
tagging O
LSTM O
for O
corresponding O
turns O
, O
i.e. O
, O
xt O

= O
I(hA O

( O
t);hB O

( O
t))whereI(;)is O
a O
switcher O
that O
chooses O
hA O

( O
t)orhB O

( O
t)depending O
on O
whether O

( O
t O
) O
4Dialog O
act O
labels O
are O
mapped O
using O
scripts O
from O
http://compprag.christopherpotts.net/ O
swda.html O
. O

Utterances O
labelled O
as O
â€œ O
segment O
â€ O
are O
merged O
with O
corresponding O
previous O
utterance O
by O
the O
same O
speaker.and O

( O
t)return O
a O
non O
- O
null O
value O
. O

The O
tagging O
LSTM O
also O
maintains O
a O
dictionary O
ofLdialog O
act O
vectors O
g1;:::;gL. O

The O
dialog O
act O
probabilities O
yt2RLat O
turntare O
computed O
using O
the O
dot O
- O
product O
attention O
mechanism O
, O
i.e. O
, O
yt= O
f(zt;[g1;:::;gL O
] O
) O
, O
wheref(;)is O
deï¬ned O
as O
in O
( O
1 O
) O
, O
and O
ztis O
the O
hidden O
state O
vector O
of O
the O
LSTM O
. O

The O
tagging O
LSTM O
computes O
hidden O
states O
as O
zt+1 O
= O
gDA  O
rDA(~gt;xt+1);zt O
where O
~gt O
= O
PL O
l=1yt;lgl O
, O
gDA(;)is O
the O
LSTM O
function O
, O
and O
rDA(;)is O
a O
linear O
function O
with O
learnable O
parameters O
. O

In O
this O
way O
, O
both O
the O
history O
dialog O
act O
predictions O
and O
the O
utterance O
informa- O
tion O
are O
encoded O
in O
the O
hidden O
states O
. O

The O
training O
objective O
of O
the O
tagging O
LSTM O
is O
the O
sum O
of O
the O
cross O
- O
entropy O
between O
the O
dialog O
act O
label O
and O
the O
probabilities O
ytat O
each O
turn O
. O

The O
training O
conï¬guration O
is O
the O
same O
as O
the O
topic O
de- O
cision O
classiï¬er O
described O
in O
Â§ O
3.2 O
. O

We O
tune O
the O
size O
of O
hidden O
states O
ztand O
dialog O
act O
embed- O
dings O
glfromf64;128gwith O
arbitrary O
combina- O
tions O
, O
and O
vary O
the O
number O
of O
LSTM O
hidden O
layers O
fromf1;2 O
g. O

The O
best O
model O
is O
selected O
according O
to O
the O
development O
set O
accuracy O
. O

4.3 O
Experiment O
Results O
In O
our O
experiments O
, O
we O
compare O
three O
settings O
for O
using O
the O
dynamic O
speaker O
model O
. O

In O
the O
pre O
- O
train O
setting O
, O
the O
dynamic O
speaker O
model O
is O
trained O
on O
the O
SwDA O
data O
without O
the O
dialog O
act O
labels O
. O

We O
then O
freeze O
the O
model O
when O
training O
the O
tagging O
LSTM O
. O

In O
contrast O
, O
in O
the O
pretrain O
+ O
ï¬ne O
- O
tune O
setting O
, O
the O
dynamic O
speaker O
model O
is O
ï¬ne O
- O
tuned O
together O
with O
the O
tagging O
LSTM O
. O

Finally O
, O
in O
the O
pre O
- O
train O
w O
/ O
Fisher O
+ O
ï¬ne O
- O
tune O
setting O
, O
the O
dy- O
namic O
speaker O
model O
is O
pre O
- O
trained O
on O
the O
com- O
bination O
of O
SwDA O
and O
Fisher O
datasets O
, O
and O
then O
ï¬ne O
- O
tuned O
together O
with O
the O
tagging O
LSTM O
on O
the O
SwDA O
dataset O
. O

For O
all O
three O
settings O
, O
we O
use O
the O
same O
vocabularyVof O
size O
21 O
K O
which O
combines O
all O
tokens O
from O
the O
SwDA O
training O
set O
and O
those O
appearing O
at O
least O
5 O
time O
in O
the O
Fisher O
corpus O
. O

We O
compare O
our O
results O
to O
best O
published O
re- O
sults O
. O

In O
( O
Kalchbrenner O
and O
Blunsom O
, O
2013 O
) O
, O
a O
convolutional O
neural O
network O
( O
CNN O
) O
is O
used O
to O
encode O
utterances O
. O

A O
recurrent O
neural O
network O
( O
RNN O
) O
is O
then O
applied O
on O
top O
of O
the O
CNN O
to O
en- O
code O
both O
utterances O
and O
speaker O
label O
informa- O
tion O
for O
predicting O
the O
dialog O
acts O
. O

Ji O
et O
al O
. O

( O
2016 O
) O
propose O
a O
discourse O
- O
aware O
RNN O
LM O
by O
treating O

2779Model O
Acc O
( O
% O
) O
( O
Kalchbrenner O
and O
Blunsom O
, O
2013 O
) O
73.9 O
( O
Tran O
et O
al O
. O
, O
2017a O
) O
74.2 O
( O
Tran O
et O
al O
. O
, O
2017b O
) O
74.5 O

( O
Tran O
et O
al O
. O
, O
2017c O
) O
75.6 O
( O
Ji O
et O
al O
. O
, O
2016 O
) O
77.0 O
pre O
- O
train O
75.6 O
pre O
- O
train O
+ O
ï¬ne O
- O
tune O
77.2 O
pre O
- O
train O
w/ O
Fisher O
+ O
ï¬ne O
- O
tune O
78.6 O
Table O
5 O
: O
Test O
set O
accuracy O
for O
SwDA O
dialog O
act O
clas- O
siï¬cation. O
: O
The O
improvement O
of O
pre O
- O
train O
w/ O
Fisher O
+ O
ï¬ne O
- O
tune O
is O
statistically O
signiï¬cant O
over O
pre O
- O
train O
+ O
ï¬ne O
- O
tune O
based O
on O
McNemar O
â€™s O
test O
( O
p O
< O
: O
001 O
) O
. O

the O
dialog O
act O
as O
a O
conditional O
variable O
to O
the O
LM O
. O

Tran O
et O

al O
. O
( O
2017a O
, O
b O
, O
c O
) O
focus O
on O
building O
hierar- O
chies O
of O
RNNs O
to O
model O
the O
dialog O
context O
using O
previous O
utterances O
or O
dialog O
act O
predictions O
. O

Re- O
sults O
from O
( O
Lee O
and O
Dernoncourt O
, O
2016 O
) O
and O
( O
Liu O
et O
al O
. O
, O
2017 O
) O
are O
not O
directly O
comparable O
due O
to O
dif- O
ferent O
experiment O
settings O
. O

Experiment O
results O
are O
summarized O
in O
Table O
5 O
. O

Our O
pre O
- O
train O
setting O
performs O
on O
par O
with O
previ- O
ous O
state O
- O
of O
- O
the O
- O
art O
supervised O
models O
except O
( O
Ji O
et O
al O
. O
, O
2016 O
) O
. O

Fine O
- O
tuning O
signiï¬cantly O
improves O
the O
performance O
and O
allows O
the O
model O
to O
achieve O
a O
similar O
accuracy O
as O
( O
Ji O
et O
al O
. O
, O
2016 O
) O
. O

The O
best O
result O
is O
achieved O
by O
pre O
- O
training O
the O
dynamic O
speaker O
model O
with O
both O
SwDA O
and O
Fisher O
datasets O
. O

The O
improvement O
of O
pre O
- O
train O
w/ O
Fisher O
+ O
ï¬ne O
- O
tune O
is O
statistically O
signiï¬cant O
over O
pre O
- O
train O
+ O
ï¬ne O
- O
tune O
based O
on O
McNemar O
â€™s O
test O
( O
p O
< O
: O
001 O
) O
. O

This O
illus- O
trates O
the O
advantage O
of O
the O
unsupervised O
learning O
approach O
for O
the O
proposed O
model O
as O
it O
can O
exploit O
a O
large O
amount O
of O
unlabelled O
data O
. O

4.4 O
Qualitative O
Analysis O
We O
analyze O
the O
latent O
modes O
learned O
on O
SwDA O
us- O
ing O
the O
same O
approach O
as O
in O
Â§ O
3.5 O
. O

Again O
, O
speciï¬c O
examples O
are O
included O
in O
Appendix O
A. O
Overall O
, O
there O
are O
several O
modes O
corresponding O
to O
coarse- O
grained O
dialog O
acts O
, O
such O
as O
statements O
, O
questions O
, O
agreement O
, O
backchannel O
and O
conversation O
- O
closing O
. O

Many O
modes O
characterize O
statements O
, O
probably O
due O
to O
their O
high O
relative O
frequency O
in O
the O
corpus O
. O

Among O
the O
statement O
modes O
, O
there O
are O
two O
dis- O
tinct O
groups O
, O
one O
containing O
multiple O
ï¬lled O
pauses O
, O
such O
as O
uh O
, O
you O
know O
, O
well O
, O
and O
the O
other O
one O
with O
because O
-clauses O
. O

The O
fact O
that O
coarse O
- O
grained O
dia- O
log O
act O
information O
is O
partly O
encoded O
in O
the O
modesmay O
be O
helping O
with O
recognizing O
the O
dialog O
act O
. O

In O
addition O
, O
we O
use O
the O
speaker O
gender O
infor- O
mation O
available O
in O
the O
SwDA O
data O
to O
determine O
whether O
the O
latent O
modes O
in O
the O
dynamic O
speaker O
model O
pick O
up O
gender O
- O
related O
language O
variation O
. O

Speciï¬c O
examples O
and O
statistics O
are O
included O
in O
Appendix O
B. O
The O
Cohen O
- O
d O
score O
( O
Cohen O
, O
1988 O
) O
is O
used O
to O
measure O
the O
strength O
of O
the O
difference O
be- O
tween O
association O
score O
distributions O
of O
male O
vs. O
female O
utterances O
for O
individual O
modes O
. O

Based O
on O
the O
Cohen O
- O
d O
score O
, O
we O
identiï¬ed O
two O
modes O
that O
have O
a O
strong O
association O
with O
male O
speak- O
ers O
, O
and O
two O
with O
female O
speakers O
. O

All O
have O
sig- O
niï¬cantly O
different O
( O
p O
< O
0:001 O
) O

distributions O
of O
association O
scores O
for O
female O
vs. O
male O
speakers O
using O
Mann O
- O
Whitney O
U O
test O
. O

In O
the O
top O
associ- O
ated O
utterances O
for O
the O
male O
modes O
, O
we O
ï¬nd O
utter- O
ances O
with O
several O
ï¬lled O
pauses O
, O
which O
has O
been O
found O
to O
be O
indicative O
of O
male O
speakers O
in O
pre- O
vious O
work O
on O
Switchboard O
( O
Boulis O
and O
Osten- O
dorf O
, O
2005 O
) O
. O

The O
female O
modes O
are O
mostly O
agree- O
ment O
, O
acknowledgement O
and O
backchannel O
, O
which O
aligns O
with O
a O
popular O
sociolinguistic O
theory O
that O
fe- O

males O
are O
more O
responsive O
( O
Coates O
, O
1998 O
) O
. O

Based O
on O
this O
, O
we O
conclude O
that O
some O
speaker O
gender O
language O
variations O
are O
indeed O
captured O
by O
the O
learned O
modes O
. O

5 O
Related O
Work O
As O
reviewed O
by O
Zukerman O
and O
Litman O
( O
2001 O
) O
, O
user O
modeling O
for O
conversational O
systems O
has O
a O
long O
history O
. O

The O
research O
can O
be O
tracked O
back O
to O
the O
GRUNDY O
system O
( O
Rich O
, O
1979 O
) O
which O
catego- O
rizes O
users O
in O
terms O
of O
hand O
- O
crafted O
sets O
of O
user O
properties O
for O
book O
recommendation O
. O

Other O
sys- O
tems O
have O
focused O
on O
different O
aspects O
of O
users O
, O
e.g. O
, O
the O
expertise O
level O
of O
the O
user O
in O
a O
speciï¬c O
domain O
( O
Chin O
, O
1986 O
; O
Sleeman O
, O
1985 O
; O
Paris O
, O
1987 O
; O

Hovy O
, O
1987 O
) O
, O
the O
user O
â€™s O
intent O
and O
plan O
( O
Allen O
and O
Perrault O
, O
1980 O
; O
Carberry O
, O
1983 O
; O
Litman O
, O
1986 O
; O
Moore O
and O
Paris O
, O
1992 O
) O
, O
and O
the O
user O
â€™s O
personality O
( O
Mairesse O
and O
Walker O
, O
2006 O
; O
DeVault O
et O
al O
. O
, O
2014 O
; O
Fung O
et O
al O
. O
, O
2016 O
; O
Fang O
et O
al O
. O
, O
2017 O
) O
. O

User O
model- O
ing O
has O
also O
been O
employed O
for O
personalized O
topic O
suggestion O
in O
recent O
Alexa O
Prize O
socialbots O
, O
using O
a O
pre O
- O
deï¬ned O
mapping O
between O
personality O
types O
and O
topics O
( O
Fang O
et O
al O
. O
, O
2017 O
) O
, O
or O
a O
conditional O
ran- O
dom O
ï¬eld O
sequence O
model O
with O
hand O
- O
crafted O
user O
and O
context O
features O
( O
Ahmadvand O
et O
al O
. O
, O
2018 O
) O
. O

Modeling O
speakers O
with O
continuous O
embeddings O
for O
neural O
conversation O
models O
is O
studied O
in O
( O
Li O

2780et O
al O
. O
, O
2016 O
) O
, O
where O
the O
model O
directly O
learns O
a O
dictionary O
of O
speaker O
embeddings O
. O

Our O
unsuper- O
vised O
dynamic O
speaker O
model O
differs O
from O
previ- O
ous O
work O
in O
that O
we O
build O
speaker O
embeddings O
as O
a O
weighted O
combination O
of O
latent O
modes O
with O
weights O
computed O
based O
on O
the O
utterance O
. O

Thus O
, O
the O
model O
can O
construct O
embeddings O
for O
any O
new O
users O
and O
dynamically O
update O
the O
embeddings O
as O
the O
conversation O
evolves O
. O

Speaker O
language O
variances O
have O
been O
ana- O
lyzed O
by O
previous O
work O
and O
incorporated O
in O
NLP O
models O
. O

Preot O
Â¸iuc O
- O
Pietro O
et O
al O
. O
( O
2016 O
) O
and O
Jo- O
hannsen O
et O
al O
. O
( O
2015 O
) O
ï¬nd O
that O
speaker O
- O
level O
lan- O
guage O
variance O
affects O
lexical O
choices O
and O
even O
syntactic O
structure O
based O
on O
psycholinguistic O
hy- O
potheses O
. O

Speaker O
demographics O
are O
used O
to O
im- O
prove O
both O
low O
- O
level O
tasks O
such O
as O
part O
- O
of O
- O
speech O
tagging O
( O
Hovy O
and O
SÃ¸gaard O
, O
2015 O
) O
and O
high O
- O
level O
applications O
such O
as O
sentiment O
analysis O
( O
V O
olkova O
et O
al O
. O
, O
2013 O
) O
and O
machine O
translation O
( O
Mirkin O
et O
al O
. O
, O
2015 O
) O
. O

Lynn O
et O
al O
. O

( O
2017 O
) O
introduce O
a O
continu- O
ous O
adaptation O
method O
to O
include O
user O
age O
, O
gender O
, O
personality O
traits O
and O
language O
features O
for O
person- O
alizing O
several O
supervised O
NLP O
models O
. O

Different O
from O
previous O
work O
, O
we O
study O
the O
use O
of O
speaker O
embeddings O
learned O
from O
utterances O
in O
an O
unsu- O
pervised O
fashion O
and O
analyze O
the O
possible O
inter- O
pretability O
of O
the O
latent O
modes O
. O

6 O
Conclusion O
In O
this O
paper O
, O
we O
address O
the O
problem O
of O
mod- O
eling O
speakers O
from O
their O
language O
using O
an O
un- O
supervised O
approach O
. O

A O
dynamic O
speaker O
model O
is O
proposed O
to O
learn O
speaker O
embeddings O
that O
are O
updated O
as O
the O
conversation O
evolves O
. O

The O
model O
achieves O
promising O
results O
on O
two O
representative O
tasks O
in O
dialogs O
: O
user O
topic O
decision O
prediction O
in O
human O
- O
socialbot O
conversations O
and O
dialog O
act O
classiï¬cation O
in O
human O
- O
human O
conversations O
. O

In O
particular O
, O
we O
demonstrate O
that O
the O
model O
can O
ben- O
eï¬t O
from O
unlabelled O
data O
in O
the O
dialog O
act O
classi- O
ï¬cation O
task O
, O
where O
we O
achieve O
the O
state O
- O
of O
- O
the- O
art O
results O
. O

Finally O
, O
we O
carry O
out O
analysis O
on O
the O
learned O
latent O
modes O
on O
both O
tasks O
, O
and O
ï¬nd O
cues O
that O
suggest O
the O
model O
captures O
speaker O
character- O
istics O
such O
as O
intent O
, O
speaking O
style O
, O
and O
gender O
. O

For O
future O
work O
, O
it O
could O
be O
interesting O
to O
explore O
guiding O
some O
latent O
modes O
with O
a O
few O
examples O
to O
pick O
up O
speciï¬c O
user O
features O
such O
as O
personality O
traits O
. O

Acknowledgements O
We O
thank O
the O
anonymous O
reviewers O
for O
their O
help- O
ful O
feedback O
. O

We O
also O
thank O
Trang O
Tran O
for O
her O
feedback O
on O
the O
paper O
. O

This O
research O
is O
supported O
by O
Amazon O
Alexa O
Fellowship O
and O
Tencent O
AI O
Lab O
Rhino O
- O
Bird O
Gift O
Fund O
. O

The O
conclusions O
and O
ï¬nd- O
ings O
are O
those O
of O
the O
authors O
and O
do O
not O
necessarily O
reï¬‚ect O
the O
views O
of O
sponsors O
. O

References O
Deepak O
Agarwal O
, O
Bee O
- O
Chung O
Chen O
, O
and O
Bo O
Pang O
. O

2011 O
. O

Personalized O
recommendation O
of O
user O
com- O
ments O
via O
factor O
models O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Methods O
Natural O
Language O
Process O
. O

( O
EMNLP O
) O
. O

Ali O
Ahmadvand O
, O
Ingyu O
Choi O
, O
Harshita O
Sahijwani O
, O
Jus- O
tus O
Schmidt O
, O
Mingyang O
Sun O
, O
Sergey O
V O
olokhin O
, O
Zi- O
hao O
Wang O
, O
and O
Eugene O
Agichtein O
. O

2018 O
. O

Emory O
IrisBot O
: O

An O
open O
- O
domain O
conversational O
bot O
for O
per- O
sonalized O
information O
access O
. O

In O
Proc O
. O

Alexa O
Prize O
2018 O
. O

James O
F. O
Allen O
and O
C. O
Raymond O
Perrault O
. O

1980 O
. O

Ana- O
lyzing O
intention O
in O
utterances O
. O

Artiï¬cial O
Intelligence O
, O
15:143â€“178 O
. O

John O
L. O
Austin O
. O

1975 O
. O

How O
To O
Do O
Things O
with O
Words O
, O
2nd O
edition O
. O

Harvard O
University O
Press O
, O
Cambridge O
, O
MA O
. O

David O
M. O
Blei O
, O
Andrew O
Y O
. O

Ng O
, O
and O
Michael O
I. O
Jordan O
. O

2003 O
. O

Latent O
dirichlet O
allocation O
. O

J. O
Machine O
Learn- O
ing O
Research O
, O
3:993â€“1022 O
. O

Piotr O
Bojanowski O
, O
Edouard O
Grave O
, O
Armand O
Joulin O
, O
and O
Tomas O
Mikolov O
. O

2017 O
. O

Enriching O
word O
vectors O
with O
subword O
information O
. O

Transactions O
of O
the O
Associa- O
tion O
for O
Computational O
Linguistics O
, O
5:135â€“146 O
. O

Constantinos O
Boulis O
and O
Mari O
Ostendorf O
. O

2005 O
. O

A O
quantitative O
analysis O
of O
lexical O
differences O
between O
genders O
in O
telephone O
conversations O
. O

In O
Proc O
. O

Annu O
. O

Meeting O
Assoc O
. O

for O
Computational O
Linguistics O
( O
ACL O
) O
, O
pages O
435â€“442 O
. O

Kevin O
K. O
Bowden O
, O
Jiaqi O
Wu O
, O
Wen O
Cui O
, O
Juraj O
Juraska O
, O
Vrindavan O
Harrison O
, O
Brian O
Schwarzmann O
, O
Nick O
San- O
ter O
, O
and O
Marilyn O
Walker O
. O

2018 O
. O

SlugBot O
: O
Develop- O
ing O
a O
computational O
model O
and O
framework O
of O
a O
novel O
dialogue O
genre O
. O

In O
Proc O
. O

Alexa O
Prize O
2018 O
. O

Sandra O
Carberry O
. O
1983 O
. O

Tracking O
user O
goals O
in O
an O
information O
- O
seeking O
environment O
. O

In O
Proc O
. O

AAAI O
Conf O
. O

Artiï¬cial O
Intelligence O
, O
pages O
59â€“63 O
. O

Hao O
Cheng O
, O
Hao O
Fang O
, O
and O
Mari O
Ostendorf O
. O
2017 O
. O

A O
factored O
neural O
network O
model O
for O
characteriz- O
ing O
online O
discussions O
in O
vector O
space O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Methods O
Natural O
Language O
Pro- O
cess O
. O

( O
EMNLP O
) O
, O
pages O
2296â€“2306 O
. O

2781David O
N. O
Chin O
. O

1986 O
. O

User O
modeling O
in O
UC O
, O
the O
UNIX O
consultant O
. O

In O
Proc O
. O

Computer O
Human O
Interactions O
( O
CHI O
) O
, O
pages O
24â€“28 O
. O

Christopher O
Cieri O
, O
David O
Graff O
, O
Owen O
Kimball O
, O
Dave O
Miller O
, O
and O
Kevin O
Walker O
. O

2004 O
. O

Fisher O
english O
training O
speech O
part O
1 O
transcripts O
LDC2004T19 O
. O

Web O
Download O
. O

Jennifer O
Coates O
, O
editor O
. O

1998 O
. O

Language O
and O
gender O
: O
a O
reader O
. O

Wiley O
- O
Blackwell O
. O

Jacob O
Cohen O
. O

1988 O
. O

Statistical O
Power O
Analysis O
for O
the O
Behavioral O
Sciences O
. O

Lawrence O
Erlbaum O
Asso- O
ciates O
. O

Andrew O
M O
Dai O
and O
Quoc O
V O
Le O
. O
2015 O
. O

Semi O
- O
supervised O
sequence O
learning O
. O

In O
Proc O
. O

Annu O
. O

Conf O
. O

Neural O
In- O
form O
. O

Process O
. O

Syst O
. O

( O
NIPS O
) O
, O
pages O
3079â€“3087 O
. O

David O
DeVault O
, O
Ron O
Artstein O
, O
Grace O
Benn O
, O
Teresa O
Dey O
, O
Ed O
Fast O
, O
Alesia O
Gainer O
, O
Kallirroi O
Georgila O
, O
Jon O
Gratch O
, O
Arno O
Hartholt O
, O
Margaux O
Lhommet O
, O
et O
al O
. O
2014 O
. O

SimSensei O
kiosk O
: O

A O
virtual O
human O
inter- O
viewer O
for O
healthcare O
decision O
support O
. O

In O
Proc O
. O

Int O
. O

Conf O
. O

Autonomous O
Agents O
and O
Multi O
- O
agent O
Systems O
, O
pages O
1061â€“1068 O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

Bert O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understand- O
ing O
. O

In O
Proc O
. O

Conf O
. O

North O
American O
Chapter O
Assoc O
. O

for O
Computational O
Linguistics O
( O
NAACL O
) O
. O

Hao O
Fang O
. O

2019 O
. O

Building O
a O
User O
- O
Centric O
and O
Content- O
Driven O
Socialbot O
. O

Ph.D. O
thesis O
, O
University O
of O
Wash- O
ington O
. O

Hao O
Fang O
, O
Hao O
Cheng O
, O
Elizabeth O
Clark O
, O
Ariel O
Holtz- O
man O
, O
Maarten O
Sap O
, O
Mari O
Ostendorf O
, O
Yejin O
Choi O
, O
and O
Noah O
Smith O
. O
2017 O
. O

Sounding O
Board O
â€“ O
University O
of O
Washington O
â€™s O
Alexa O
Prize O
submission O
. O

In O
Proc O
. O

Alexa O
Prize O
. O

Hao O
Fang O
, O
Hao O
Cheng O
, O
Maarten O
Sap O
, O
Elizabeth O
Clark O
, O
Ariel O
Holtzman O
, O
Yejin O
Choi O
, O
Noah O
Smith O
, O
and O
Mari O
Ostendorf O
. O

2018 O
. O

Sounding O
Board O
â€“ O
a O
user O
- O
centric O
and O
content O
- O
driven O
social O
chatbot O
. O

In O
Proc O
. O

Conf O
. O

North O
American O
Chapter O
Assoc O
. O

for O
Computational O
Linguistics O
( O
NAACL O
) O
( O
System O
Demonstrations O
) O
. O

Pascale O
Fung O
, O
Anik O
Dey O
, O
Farhad O
Bin O
Siddique O
, O
Ruixi O
Lin O
, O
Yang O
Yang O
, O
Yan O
Wan O
, O
and O
Ricky O
Ho O
Yin O
Chan O
. O

2016 O
. O

Zara O
the O
supergirl O
: O
An O
empathetic O
personal- O
ity O
recognition O
system O
. O

In O
Proc O
. O

Conf O
. O

North O
Amer- O
ican O
Chapter O
Assoc O
. O

for O
Computational O
Linguistics O
( O
NAACL O
) O
( O
System O
Demonstrations O
) O
. O

Sepp O
Hochreiter O
and O
J O
Â¨urgen O
Schmidhuber O
. O

1997 O
. O

Long O
short O
- O
term O
memory O
. O

Neural O
Computation O
, O
9(8):1735â€“1780 O
. O

Dirk O
Hovy O
and O
Anders O
SÃ¸gaard O
. O

2015 O
. O

Tagging O
per- O
formance O
correlates O
with O
author O
age O
. O

In O
Proc O
. O

Annu O
. O

Meeting O
Assoc O
. O

for O
Computational O
Linguistics O
( O
ACL O
) O
, O
pages O
483â€“488.Eduard O
Hovy O
. O
1987 O
. O

Generating O
natural O
language O
un- O
der O
pragmatic O
constraints O
. O

Journal O
of O
Pragmatics O
, O
11:689â€“710 O
. O

Aaron O
Jaech O
and O
Mari O
Ostendorf O
. O

2018 O
. O

Personalized O
language O
model O
for O
query O
auto O
- O
completion O
. O

In O
Proc O
. O

Annu O
. O

Meeting O
Assoc O
. O

for O
Computational O
Linguistics O
( O
ACL O
) O
, O
pages O
700â€“705 O
. O

Yangfeng O
Ji O
, O
Gholamreza O
Haffari O
, O
and O
Jacob O
Eisen- O
stein O
. O

2016 O
. O

A O
latent O
variable O
recurrent O
neural O
net- O
work O
for O
discourse O
relation O
language O
models O
. O

In O
Proc O
. O

Conf O
. O

North O
American O
Chapter O
Assoc O
. O

for O
Computational O
Linguistics O
( O
NAACL O
) O
, O
pages O
332 O
â€“ O
342 O
. O

Anders O
Johannsen O
, O
Dirk O
Hovy O
, O
and O
Anders O
SÃ¸gaard O
. O
2015 O
. O

Cross O
- O
lingual O
syntactic O
variation O
over O
age O
and O
gender O
. O

In O
Proc O
. O

Conf O
. O

Computational O
Natural O
Lan- O
guage O
Learning O
( O
CoNLL O
) O
, O
pages O
103â€“112 O
. O

Dan O
Jurafsky O
, O
Elizabeth O
Shriberg O
, O
, O
and O
Debra O
Biasca O
. O

1997 O
. O

Switchboard O
swbd O
- O
damsl O
shallow O
- O
discourse- O
function O
annotation O
coders O
manual O
, O
draft O
13 O
. O

Tech- O
nical O
report O
, O
University O
of O
Colorado O
, O
Boulder O
. O

Nal O
Kalchbrenner O
and O
Phil O
Blunsom O
. O

2013 O
. O

Recurrent O
convolutional O
neural O
networks O
for O
discourse O
compo- O
sitionality O
. O

In O
Proceedings O
of O
the O
Workshop O
on O
Con- O
tinuous O
Vector O
Space O
Models O
and O
their O
Composition- O
ality O
, O
pages O
119â€“126 O
. O

Diederik O
Kingma O
and O
Jimmy O
Ba O
. O
2015 O
. O

Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O

In O
Proc O
. O

Int O
. O

Conf O
. O

Learning O
Representations O
( O
ICLR O
) O
. O

Ryan O
Kiros O
, O
Yukun O
Zhu O
, O
Ruslan O
R O
Salakhutdinov O
, O
Richard O
Zemel O
, O
Raquel O
Urtasun O
, O
Antonio O
Torralba O
, O
and O
Sanja O
Fidler O
. O
2015 O
. O

Skip O
- O
thought O
vectors O
. O

In O
Proc O
. O

Annu O
. O

Conf O
. O

Neural O
Inform O
. O

Process O
. O

Syst O
. O

( O
NIPS O
) O
, O
pages O
3294â€“3302 O
. O

Ji O
Young O
Lee O
and O
Franck O
Dernoncourt O
. O

2016 O
. O

Sequen- O

tial O
short O
- O
text O
classiï¬cation O
with O
recurrent O
and O
con- O
volutional O
neural O
networks O
. O

In O
Proc O
. O

Conf O
. O

North O
American O
Chapter O
Assoc O
. O

for O
Computational O
Lin- O
guistics O
( O
NAACL O
) O
, O
pages O
515â€“520 O
. O

Jiwei O
Li O
, O
Michel O
Galley O
, O
Chris O
Brockett O
, O
Georgios O
P. O
Spithourakis O
, O
Jianfeng O
Gao O
, O
and O
William O
B. O
Dolan O
. O

2016 O
. O

A O
persona O
- O
based O
neural O
conversation O
model O
. O

InProc O
. O

Annu O
. O

Meeting O
Assoc O
. O

for O
Computational O
Linguistics O
( O
ACL O
) O
, O
pages O
994â€“1003 O
. O

Diane O
J. O
Litman O
. O

1986 O
. O

Linguistic O
coherence O
: O
a O
plan- O
based O
alternative O
. O

In O
Proc O
. O

Annu O
. O

Meeting O
Assoc O
. O

for O
Computational O
Linguistics O
( O
ACL O
) O
, O
pages O
215â€“223 O
. O

Yang O
Liu O
, O
Kun O
Han O
, O
Zhao O
Tan O
, O
and O
Yun O
Lei O
. O
2017 O
. O

Using O
context O
information O
for O
dialog O
act O
classiï¬- O
cation O
in O
dnn O
framework O
. O

In O
Proc O
. O

Conf O
. O

Empiri- O
cal O
Methods O
Natural O
Language O
Process O
. O

( O
EMNLP O
) O
, O
pages O
2170â€“2178 O
. O

2782Veronica O
Lynn O
, O
Youngseo O
Son O
, O
Vivek O
Kulkarni O
, O
Ni- O
ranjan O
Balasubramanian O
, O
and O
H. O
Andrew O
Schwartz O
. O

2017 O
. O

Human O
centered O
nlp O
with O
user O
- O
factor O
adap- O
tation O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Methods O
Natural O
Language O
Process O
. O

( O
EMNLP O
) O
, O
pages O
1146â€“1155 O
. O

Franc O
Â¸ois O
Mairesse O
and O
Marilyn O
Walker O
. O

2006 O
. O

Au- O
tomatic O
recognition O
of O
personality O
in O
conversation O
. O

InProc O
. O

Conf O
. O

North O
American O
Chapter O
Assoc O
. O

for O
Computational O
Linguistics O
( O
NAACL O
) O
, O
pages O
85â€“88 O
. O

Bryan O
McCann O
, O
James O
Bradbury O
, O
Caiming O
Xiong O
, O
and O
Richard O
Socher O
. O

2017 O
. O

Learned O
in O
translation O
: O
Con- O
textualized O
word O
vectors O
. O

In O
Proc O
. O

Annu O
. O

Conf O
. O

Neu- O
ral O
Inform O
. O

Process O
. O

Syst O
. O

( O
NIPS O
) O
, O
pages O
6294â€“6305 O
. O

Tomas O
Mikolov O
, O
Ilya O
Sutskever O
, O
Kai O
Chen O
, O
Greg O
S O
Cor- O
rado O
, O
and O
Jeff O
Dean O
. O

2013 O
. O

Distributed O
representa- O
tions O
of O
words O
and O
phrases O
and O
their O
compositional- O
ity O
. O

In O
Proc O
. O

Annu O
. O

Conf O
. O

Neural O
Inform O
. O

Process O
. O

Syst O
. O

( O
NIPS O
) O
, O
pages O
3111â€“3119 O
. O

Shachar O
Mirkin O
, O
Scott O
Nowson O
, O
Caroline O
Brun O
, O
and O
Julien O
Perez O
. O
2015 O
. O

Motivating O
personality O
- O
aware O
machine O
translation O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Meth- O
ods O
Natural O
Language O
Process O
. O

( O
EMNLP O
) O
, O
pages O
1102â€“1108 O
. O

Johanna O
D. O
Moore O
and O
Cecile O
Paris O
. O

1992 O
. O

Exploiting O
user O
feedback O
to O
compensate O
for O
the O
unreliability O
of O
user O
models O
. O

User O
Modeling O
and O
User O
- O
Adapted O
In- O
teraction O
, O
2:287â€“330 O
. O

CÂ´ecile O
L. O
Paris O
. O

1987 O
. O

The O
Use O
of O
Explicit O
User O
Models O
in O
a O
Generation O
System O
for O
Tailoring O
Answers O
to O
the O
User O
â€™s O
Level O
of O
Expertise O
. O

Ph.D. O
thesis O
, O
Columbia O
University O
. O

Matthew O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O

2018 O
. O

Deep O
contextualized O
word O
rep- O
resentations O
. O

In O
Proc O
. O

Conf O
. O

North O
American O
Chap- O
ter O
Assoc O
. O

for O
Computational O
Linguistics O
( O
NAACL O
) O
, O
pages O
2227â€“2237 O
. O

Daniel O
Preot O
Â¸iuc O
- O
Pietro O
, O
Wei O
Xu O
, O
and O
Lyle O
Ungar O
. O

2016 O
. O

Discovering O
user O
attribute O
stylistic O
differences O
via O
paraphrasing O
. O

In O
Proceedings O
of O
the O
Thirtieth O
AAAI O
Conference O
on O
Artiï¬cial O
Intelligence O
, O
pages O
3030 O
â€“ O
3037 O
. O

Ashwin O
Ram O
, O
Rohit O
Prasad O
, O
Chandra O
Khatri O
, O
Anu O
Venkatesh O
, O
Raefer O
Gabriel O
, O
Qing O
Liu O
, O
Jeff O
Nunn O
, O
Behnam O
Hedayatnia O
, O
Ming O
Cheng O
, O
Ashish O
Nagar O
, O
Eric O
King O
, O
Kate O
Bland O
, O
Amanda O
Wartick O
, O
Yi O
Pan O
, O
Han O
Song O
, O
Sk O
Jayadevan O
, O
Gene O
Hwang O
, O
and O
Art O
Pet- O
tigrue O
. O

2017 O
. O

Conversational O
AI O
: O
The O
science O
behind O
the O
alexa O
prize O
. O

In O
Proc O
. O

Alexa O
Prize O
2017 O
. O

Abhilasha O
Ravichander O
and O
Alan O
Black O
. O

2018 O
. O

An O
em- O
pirical O
study O
of O
self O
- O
disclosure O
in O
spoken O
dialogue O
systems O
. O

In O
Proc O
. O

SIGdial O
Meeting O
Discourse O
and O
Dialogue O
, O
pages O
253â€“263 O
. O

Elaine O
Rich O
. O

1979 O
. O

User O
modeling O
via O
stereotypes O
. O

Cognitive O
Science O
, O
3:329â€“354.John O
R. O
Searle O
. O

1969 O
. O

Speech O
Acts O
: O
An O
Essay O
in O
the O
Philosophy O
of O
Language O
. O

Cambridge O
University O
Press O
. O

Milad O
Shokouhi O
. O

2013 O
. O

Learning O
to O
personalize O
query O
auto O
- O
completion O
. O

In O
SIGIR O
, O
pages O
103â€“112 O
. O

ACM O
. O

Derek O
Sleeman O
. O
1985 O
. O

UMFE O
: O

A O
user O
modelling O
front- O
end O
subsystem O
. O

Int O
. O

J. O
Man O
- O
Machine O
Studies O
, O
23:71 O
â€“ O
88 O
. O

Andreas O
Stolcke O
, O
Klaus O
Ries O
, O
Noah O
Coccaro O
, O
Eliza- O
beth O
Shriberg O
, O
Rebecca O
Bates O
, O
Daniel O
Jurafsky O
, O
Paul O
Taylor O
, O
Rachel O
Martin O
, O
Carol O
Van O
Ess O
- O
Dykema O
, O
and O
Marie O
Meteer O
. O
2000 O
. O

Dialogue O
act O
modeling O
for O
au- O
tomatic O
tagging O
and O
recognition O
of O
conversational O
speech O
. O

Computational O
Linguistics O
, O
26(3):339â€“373 O
. O

Quan O
Hung O
Tran O
, O
Gholamreza O
Haffari O
, O
and O
Ingrid O
Zuk- O
erman O
. O

2017a O
. O

A O
generative O
attentional O
neural O
net- O
work O
model O
for O
dialogue O
act O
classiï¬cation O
. O

In O
Proc O
. O

Annu O
. O

Meeting O
Assoc O
. O

for O
Computational O
Linguistics O
( O
ACL O
) O
, O
pages O
524â€“529 O
. O

Quan O
Hung O
Tran O
, O
Ingrid O
Zukerman O
, O
and O
Gholamreza O
Haffari O
. O

2017b O
. O

A O
hierarchical O
neural O
model O
for O
learning O
sequences O
of O
dialogue O
acts O
. O

In O
Proc O
. O

Euro- O
pean O
Chapter O
Assoc O
. O

for O
Computational O
Linguistics O
( O
EACL O
) O
, O
pages O
428â€“437 O
. O

Quan O
Hung O
Tran O
, O
Ingrid O
Zukerman O
, O
and O
Gholam- O
reza O
Haffari O
. O

2017c O
. O

Preserving O
distributional O
in- O
formation O
in O
dialogue O
act O
classiï¬cation O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Methods O
Natural O
Language O
Pro- O
cess O
. O

( O
EMNLP O
) O
, O
pages O
2151â€“2156 O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
Åukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
Proc O
. O

Annu O
. O

Conf O
. O

Neural O
Inform O
. O

Pro- O
cess O
. O

Syst O
. O

( O
NIPS O
) O
, O
pages O
5998â€“6008 O
. O

Svitlana O
V O
olkova O
, O
Theresa O
Wilson O
, O
and O
David O
Yarowsky O
. O

2013 O
. O

Exploring O
demographic O
language O
variations O
to O
improve O
multilingual O
sentiment O
anal- O
ysis O
in O
social O
media O
. O

In O
Proc O
. O

Conf O
. O

Empirical O
Methods O
Natural O
Language O
Process O
. O

( O
EMNLP O
) O
, O
pages O
1815â€“1827 O
. O

Ingrid O
Zukerman O
and O
Diane O
Litman O
. O

2001 O
. O

Natural O
lan- O
guage O
processing O
and O
user O
modeling O
. O

User O
Model- O
ing O
and O
User O
- O
Adapted O
Interaction O
, O
11:129â€“158 O
. O

2783A O
Examples O
for O
Mode O
Analysis O
For O
each O
mode O
, O
we O
list O
top O
associated O
user O
utter- O
ances O
in O
Table O
6 O
and O
Table O
7 O
for O
the O
user O
topic O
de- O
cision O
corpus O
and O
SwDA O
corpus O
, O
respectively O
. O

For O
modes O
learned O
in O
the O
user O
topic O
decision O
corpus O
, O
mode O
4 O
seems O
to O
include O
positive O
reac- O
tions O
, O
while O
mode O
2 O
involves O
slightly O
negative O
re- O
actions O
. O

Modes O
0 O
and O
6 O
are O
mostly O
yes O
/ O
no O
answers O
. O

Utterances O
associated O
with O
mode O
3 O
are O
mostly O
con- O
versation O
ending O
. O

Modes O
9 O
, O
14 O
, O
and O
10 O
are O
mostly O
set O
topic O
commands O
, O
differing O
in O
style O
. O

Mode O
10 O
is O
associated O
with O
complete O
requests O
( O
e.g. O
, O
â€œ O
letâ€™s O
/ O
can O
we O
talk O
about O
cats O
) O
, O
â€ O
while O
mode O
9 O
and O
Mode O
14 O
involve O
short O
topic O
phrases O
( O
e.g. O
, O
â€œ O
holidays O
â€ O
) O
. O

Modes O
8 O
and O
11 O
capture O
talkative O
users O
, O
whereas O
modes O
1 O
and O
7 O
capture O
relatively O
terse O
users O
. O

For O
latent O
modes O
learned O
in O
the O
SwDA O
corpus O
, O
there O
are O
several O
modes O
corresponding O
to O
coarse- O
grained O
dialog O
acts O
, O
such O
as O
statements O
( O
modes O
2 O
, O
4 O
, O
6 O
, O
16 O
, O
19 O
) O
, O
questions O
( O
modes O
8 O
, O
9 O
) O
, O
agreement O
( O
modes O
12 O
, O
20 O
) O
, O
backchannel O
( O
modes O
0 O
, O
28 O
) O
, O
and O
conversation O
- O
closing O
( O
mode O
13 O
) O
. O

Among O
the O
state- O
ment O
modes O
, O
there O
are O
two O
distinct O
groups O
, O
one O
( O
modes O
4 O
, O
6 O
, O
16 O
, O
19 O
) O
containing O
multiple O
ï¬lled O
pauses O
, O
such O
as O
uh O
, O
you O
know O
, O
well O
, O
and O
the O
other O
one O
( O
mode O
2 O
) O
with O
because O
-clauses O
. O

The O
fact O
that O
coarse O
- O
grained O
dialog O
act O
information O
is O
partly O
en- O
coded O
in O
the O
modes O
may O
be O
helping O
with O
recogniz- O
ing O
the O
dialog O
act O
. O

B O
Speaker O
Gender O
Analysis O
We O
use O
the O
speaker O
gender O
information O
from O
the O
SwDA O
data O
and O
analyze O
whether O
latent O
modes O
unsupervisedly O
learned O
in O
the O
dynamic O
speaker O
model O
could O
pick O
up O
some O
gender O
language O
vari- O
ations O
. O

First O
, O
we O
gather O
the O
latent O
mode O
associa- O
tion O
scores O
for O
each O
of O
the O
32 O
modes O
for O
all O
utter- O
ances O
as O
computed O
in O
( O
1 O
) O
. O

Then O
we O
carry O
out O
the O
group O
mean O
tests O
for O
individual O
modes O
to O
test O
the O
associate O
score O
distributions O
of O
male O
vs O
female O
ut- O
terances O
. O

The O
Cohen O
- O
d O
score O
is O
used O
to O
measure O
the O
strength O
of O
the O
difference O
( O
Cohen O
, O
1988 O
) O
. O

We O
also O
compute O
the O
p O
- O
value O
using O
the O
Mann O
- O
Whitney O
U O
test O
. O

Previous O
work O
has O
observed O
larger O
gender O
language O
differences O
when O
the O
two O
speakers O
have O
the O
same O
gender O
( O
Boulis O
and O
Ostendorf O
, O
2005 O
) O
. O

Thus O
, O
we O
carry O
out O
the O
group O
mean O
tests O
on O
the O
following O
three O
sets O
: O
1 O
) O
all O
conversations O
, O
2 O
) O
con- O
versations O
involving O
only O
males O
or O
females O
, O
and O
3 O
) O
conversations O
involving O
both O
genders O
. O

The O
Cohen- O
d O
scores O
for O
overall O
, O
same O
- O
gender O
and O
cross O
- O
gender O
( O
a O
) O
all O
conversations O
( O
b O
) O
same O
- O
gender O
conversations O
( O
c O
) O
cross O
- O
gender O
conversations O
Figure O
3 O
: O
Cohen O
- O
d O
scores O
for O
gender O
group O
tests O
. O

The O
x O
- O
axis O
is O
the O
mode O
index O
. O

The O
y O
- O
axis O
is O
the O
Cohen O
- O
d O
score O
, O
with O
a O
larger O
magnitude O
suggesting O
a O
large O
effect O
size O
, O
and O
a O
positive O
value O
for O
a O
more O
female O
- O
like O
mode O
. O

The O
red O
dash O
lines O
indicate O
the O
0.20 O
threshold O
. O

conversations O
are O
shown O
in O
Fig O
. O

3 O
. O
For O
each O
set O
, O
we O
identify O
the O
most O
female O
- O
like O
mode O
( O
with O
the O
most O
positive O
Cohen O
- O
d O
score O
) O
and O
the O
most O
male- O
like O
mode O
( O
with O
the O
most O
negative O
Cohen O
- O
d O
score O
) O
. O

For O
female O
- O
like O
modes O
, O
modes O
15 O
and O
17 O
are O
iden- O
tiï¬ed O
in O
this O
way O
, O
whereas O
modes O
4 O
and O
19 O
are O
identiï¬ed O
for O
male O
- O
like O
modes O
. O

By O
examining O
rep- O
resentative O
patterns O
in O
modes O
15 O
and O
17 O
, O
they O
are O
mostly O
backchannel O
, O
acknowledgement O
, O
or O
agree- O
ment O
. O

For O
modes O
4 O
and O
19 O
, O
ï¬lled O
pauses O
are O
preva- O
lent O
. O

2784Mode-0â€¢ O

no O
no O
no O
no O
no O
no O
go O
back O
to O
my O
alexa O
. O
. O
. O

â€¢ O

no O

no O
no O

no O
let O
â€™s O
stop O
talking O
now O
goodbye O
. O
. O
. O

â€¢ O

no O
let O
â€™s O
chat O
let O
â€™s O
chat O
about O
donald O
trump O
. O
. O
. O

Mode-1â€¢ O
gotcha O
â€¢ O
hiya O
â€¢ O
possibly O
Mode-2â€¢ O
serious O
â€¢ O
are O
you O
serious O
â€¢ O
that O
is O
a O
paradox O
Mode-3â€¢ O
alexa O
resume O
pandora O
â€¢ O
alexa O
connect O
bluetooth O
â€¢ O

no O

bye O
bye O
alexa O
Mode-4â€¢ O
that O
is O
fascinating O
â€¢ O
whoa O
â€¢ O
that O
that O
â€™s O
cool O

Mode-5â€¢ O
i O
did O
not O
that O
â€™s O
not O
surprising O
â€¢ O

i O
did O
not O
i O
did O
not O
knew O
that O
â€¢ O
unfortunately O
Mode-6â€¢ O

somewhat O
â€¢ O

yes O

yes O
yes O

yes O

yes O
â€¢ O

yes O
i O
did O
it O
was O
on O
the O
news O

Mode-7â€¢ O

mhm O
â€¢ O
ok O
â€¢ O
fascinating O
Mode-8â€¢ O

yes O
it O
was O
very O
much O
was O

i O
saw O
it O

i O
i O
was O
there O
i O
choose O
to O
the O
dark O
side O
did O
you O
choose O
that O
via O
uh O
right O
. O
. O
. O

â€¢ O
the O
online O
selanne O
jungle O
the O
mighty O
jungle O
the O
line O
the O
jungle O
in O
the O
jungle O
the O
mighty O
jungle O
the O
mighty O
jungle O
. O
. O
. O

â€¢ O
no O
if O
your O
life O
was O
narrated O
by O
someone O
and O
the O
choice O
was O
either O
â€¢ O

i O
was O
curious O
if O
you O
â€™d O
rather O
have O
your O
life O
narrated O
by O
regis O
philbin O
or O
by O
morgan O
freeman O
â€¢ O
did O
you O
know O
the O
answer O
rogers O
because O
like O
a O
better O
go O
bike O

and O
probably O
i O
just O
do O
nâ€™t O
know O
it O
was O
just O
a O
long O
time O
ago O
â€¢ O

i O
thought O
bill O
murray O
was O
very O
very O
funny O
Mode-9â€¢ O
meow O
â€¢ O
award O
shows O
â€¢ O
celebrity O
Mode-10â€¢ O
no O
let O
â€™s O
talk O
about O
butterï¬‚ies O
â€¢ O
no O
let O
â€™s O
talk O
about O
snakes O
â€¢ O
can O
we O
talk O
about O
kardashians O
Mode-11â€¢ O
is O
king O
kong O
real O
or O
is O
he O
bake O
but O
is O
he O
awesome O
or O
. O
. O
. O

â€¢ O
that O
is O
so O
true O
the O
concept O
of O
pencils O
are O
really O
stupid O
and O
should O
i O
even O
exist O
imagine O
if O
we O
have O
pencil O
do O
we O
wanna O
be O
able O
to O
write O
on O
paper O
so O
that O
makes O
you O
stupid O
â€¢ O
is O
this O
randomly O
talking O
to O
this O
is O
the O
dawning O
alligators O

okay O
so O
did O
we O
get O
bored O
i O
do O
nâ€™t O
know O
you O
somehow O
or O
. O
. O
. O

Mode-12â€¢ O
do O
you O
know O
alexa O
how O
do O
you O
how O
do O
you O
know O
all O
the O
stuff O
you O
â€™re O
an O
a. O
i. O
â€¢ O
what O
what O
alexa O
what O
how O
do O
you O
talk O
about O

â€¢ O

alexa O
do O
you O
know O
alexa O
do O
you O
know O
a O
joke O
today O
â€¢ O
alexa O
do O
you O
tell O
me O
what O
you O
know O
about O
the O
new O
vision O
nuclear O
plant O
Mode-13â€¢ O
ten O
million O
â€¢ O
thirty O
percent O
â€¢ O
what O
â€™s O
p. O
r. O
Mode-14â€¢ O
dog O

â€¢ O
dogs O
â€¢ O
tv O
Mode-15â€¢ O
now O
â€¢ O
not O
now O
Table O
6 O
: O
User O
utterances O
in O
socialbot O
conversations O
that O
have O
top O
association O
scores O
for O
individual O
latent O
modes O
. O

2785StatementsMode-2â€¢ O
cause O
i O
know O
there O
â€™s O
one O
not O
too O
far O
from O
from O
me O
here O
in O
dallas O
â€¢ O
because O
they O
really O
had O
no O
idea O
NONVERBAL O
what O
was O
involved O
once O
i O
got O
home O
â€¢ O
because O
like O
i O
said O
i O
worked O
with O
a O
lot O
of O
those O
â€¢ O
because O
he O
left O
home O
at O
ï¬ve O
thirty O
in O
the O
morning O
â€¢ O

and O
then O
she O
would O
like O
to O
turn O
in O
half O
of O
the O
parents O
that O
drop O
their O
kids O
off O
because O
of O
the O
condition O
the O
kids O
are O
in O
you O
know O
Mode-4â€¢ O
uh O
some O
more O
in O
interest O
type O
topics O
in O
in O
other O
countries O
â€¢ O
uh O
the O
uh O
the O
credit O
union O
has O
got O
a O
deal O
now O
where O
you O
decide O
what O
you O
want O
â€¢ O

well O
it O
would O
be O
lower O
middle O
class O
housing O
here O
â€¢ O
uh O
the O
only O
other O
thing O
i O
have O
noticed O
though O
is O
that O
uh O
it O
seems O
that O
there O
â€™s O
been O
a O
lot O
of O
or O
more O
empha O
emphasis O
at O
least O
in O
what O
we O
â€™ O
ve O
been O
dealing O
with O
Mode-6â€¢ O
and O
i O
know O
that O
uh O
you O
know O
it O
can O
be O
freezing O
cold O
in O
the O
wintertime O
and O
hot O
and O
uh O
sticky O
in O
the O
summertime O
â€¢ O

it O
â€™s O
uh O
it O
â€™s O
uh O
it O
â€™s O
uh O
plywood O
uh O
face O
i O
guess O
â€¢ O

but O
i O
NONVERBAL O

i O

i O
i O
think O
you O
know O
the O
biggest O
causes O
even O
then O
a O
lot O
of O
times O
are O
uh O
uh O
like O
when O
i O
was O
up O
in O
boston O
just O
all O
the O
cars O
you O
know O
just O
all O
over O
the O
place O
â€¢ O

and O
so O
i O

i O
it O
â€™s O

i O
think O
i O
to O
me O

i O
think O
uh O
something O
that O
â€™s O
going O
to O
help O
our O
medical O
uh O
arena O
is O
for O
um O
â€¢ O
you O
know O
it O
â€™s O
like O
it O
â€™s O
like O
a O
luxury O
car O
except O
that O
it O
â€™s O
the O
dodge O
aries O
NONVER- O

BAL O
you O
know O
Mode-16â€¢ O

but O
uh O
this O
last O
ski O
trip O
they O
took O
uh O
she O
had O
in O
contracted O
chicken O
pox O

ï¬rst O
â€¢ O

but O
uh O
we O
lived O
in O
malaysia O
for O
t O
i O
in O
nineteen O
uh O
eighty O
one O
two O
three O
and O
four O
â€¢ O

well O
my O
uh O
my O
sister O
lives O
in O
houston O
â€¢ O

i O
i O
was O
only O
twenty O
ï¬ve O
years O
old O
or O
something O
â€¢ O
it O
â€™s O
uh O
uh O
c O
n O
n O
has O
been O
a O
welcome O
addition O
to O
NONVERBAL O
the O
t O
v O
scene O
here O
in O
the O
last O
uh O
number O
of O
years O
Mode-19â€¢ O
uh O
i O
traded O
off O
an O
eighty O
two O
oldsmobile O
for O
the O
eighty O
nine O
mazda O
â€¢ O
because O
i O
mean O
after O
i O
ï¬gured O
out O
i O
was O
getting O
eighty O
cents O
an O
hour O
i O
said O
bag O
it O
â€¢ O
uh O
we O
have O
a O
a O
mazda O
nine O
twenty O
nine O
and O
a O
ford O
crown O
victoria O
and O
a O
little O
two O
seater O
c O
r O
x. O
â€¢ O

and O
uh O
you O
know O

i O
i O
was O
amazed O
cause O
i O
â€™d O
pick O
up O
a O
local O
paper O

and O
i O
â€™d O
read O
about O
all O
of O
these O
you O
know O
really O
interesting O
things O
going O
on O

â€¢ O
well O
a O
friend O
of O
mine O
at O
work O
here O
said O
that O
he O
tried O
it O
with O
his O
dog O
BackchannelMode-0â€¢ O
yes O
â€¢ O
yes O
NONVERBAL O
Mode-15â€¢ O
see O
â€¢ O
probably O
â€¢ O
like O
Mode-17â€¢ O
uh O
â€¢ O
um O
Mode-18â€¢ O

oh O
oh O

yeah O
â€¢ O

oh O
well O
â€¢ O

oh O
okay O
Mode-28â€¢ O

uh O
huh O
NONVERBAL O
â€¢ O
uh O
huh O
NONVERBAL O
NONVERBAL O
â€¢ O

uh O
huh O
ery O
faint O
AgreementMode-12 O
â€¢ O
exactly O
Mode-20â€¢ O

yep O
ause O
â€¢ O
deï¬nitely O
â€¢ O
absolutely O
â€¢ O

i O
agree O
QuesetionMode-8â€¢ O
are O
you O
and O
your O
roommate O
a O
similar O
size O
â€¢ O
did O
you O
do O
the O
diagnosis O
or O
was O
it O
just O
an O
assumption O
that O
that O
â€™s O
probably O
the O
part O
that O
failed O
â€¢ O
or O
do O
you O
have O
powered O
you O
know O
a O
â€¢ O
NONVERBAL O
what O
kind O
of O
a O
car O
do O
you O
have O
now O
â€¢ O

did O
they O
know O
that O
all O
along O
Mode-9â€¢ O
so O
what O
do O
you O
think O
about O
uh O
what O
do O
you O
think O
about O
what O
you O
see O
on O
t O
v O
about O
them O
like O
in O
the O
news O
or O
on O
the O
ads O
â€¢ O
what O
do O
you O
think O
about O
what O
do O
you O
think O
about O
the O
the O
lower O
grades O
you O
know O
k O
through O
seven O
â€¢ O

so O
uh O
what O
do O
you O
think O
about O
our O
involvement O
in O
the O
middle O
east O
â€¢ O
you O
are O
talking O
about O
p O
o O
w O
s O
or O
missing O
in O
actions O
Conversation O
- O
closing O
Mode-13â€¢ O

bye O
â€¢ O

bye O
bye O
â€¢ O
appreciation O
talking O
to O
you O
Table O
7 O
: O
Utterances O
for O
each O
mode O
in O
SwDA O
dataset O
. O

