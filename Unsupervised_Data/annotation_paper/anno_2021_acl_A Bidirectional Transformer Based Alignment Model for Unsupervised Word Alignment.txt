Proceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
283–292 O
August O
1–6 O
, O
2021 O
. O

© O
2021 O
Association O
for O
Computational O
Linguistics283A O
Bidirectional O
Transformer O
Based O
Alignment O
Model O
for O
Unsupervised O
Word O
Alignment O
Jingyi O
Zhang1and O
Josef O
van O
Genabith1;2 O
1German O
Research O
Center O
for O
Artiﬁcial O
Intelligence O
( O
DFKI O
) O
, O
Saarland O
Informatics O
Campus O
, O
Saarbr O
¨ucken O
, O
Germany O
2Department O
of O
Language O
Science O
and O
Technology O
, O
Saarland O
University O
, O
Saarland O
Informatics O
Campus O
, O
Saarbr O
¨ucken O
, O
Germany O
Jingyi O
. O

Zhang@dfki.de O
, O
Josef O
. O

Van O
Genabith@dfki.de O
Abstract O
Word O
alignment O
and O
machine O
translation O
are O
two O
closely O
related O
tasks O
. O

Neural O
transla- O
tion O
models O
, O
such O
as O
RNN O
- O
based O
and O
Trans- O
former O
models O
, O
employ O
a O
target O
- O
to O
- O
source O
at- O
tention O
mechanism O
which O
can O
provide O
rough O
word O
alignments O
, O
but O
with O
a O
rather O
low O
accu- O
racy O
. O

High O
- O
quality O
word O
alignment O
can O
help O
neural O
machine O
translation O
in O
many O
different O
ways O
, O
such O
as O
missing O
word O
detection O
, O
anno- O
tation O
transfer O
and O
lexicon O
injection O
. O

Existing O
methods O
for O
learning O
word O
alignment O
include O
statistical O
word O
aligners O
( O
e.g. O
GIZA++ O
) O
and O
re- O
cently O
neural O
word O
alignment O
models O
. O

This O
pa- O
per O
presents O
a O
bidirectional O
Transformer O
based O
alignment O
( O
BTBA O
) O
model O
for O
unsupervised O
learning O
of O
the O
word O
alignment O
task O
. O

Our O
BTBA O
model O
predicts O
the O
current O
target O
word O
by O
attending O
the O
source O
context O
and O
both O
left- O
side O
and O
right O
- O
side O
target O
context O
to O
produce O
accurate O
target O
- O
to O
- O
source O
attention O
( O
alignment O
) O
. O

We O
further O
ﬁne O
- O
tune O
the O
target O
- O
to O
- O
source O
atten- O
tion O
in O
the O
BTBA O
model O
to O
obtain O
better O
align- O
ments O
using O
a O
full O
context O
based O
optimization O
method O
and O
self O
- O
supervised O
training O
. O

We O
test O
our O
method O
on O
three O
word O
alignment O
tasks O
and O
show O
that O
our O
method O
outperforms O
both O
previ- O
ous O
neural O
word O
alignment O
approaches O
and O
the O
popular O
statistical O
word O
aligner O
GIZA++ O
. O

1 O
Introduction O
Neural O
machine O
translation O
( O
NMT O
) O
( O
Bahdanau O
et O
al O
. O
, O
2014 O
; O
Vaswani O
et O

al O
. O
, O
2017 O
) O
achieves O
state- O
of O
- O
the O
- O
art O
results O
for O
various O
translation O
tasks O
( O
Bar- O
rault O
et O
al O
. O
, O
2019 O
, O
2020 O
) O
. O

Neural O
translation O
models O
, O
such O
as O
RNN O
- O
based O
( O
Bahdanau O
et O
al O
. O
, O
2014 O
) O
and O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
models O
, O
gen- O
erally O
have O
an O
encoder O
- O
decoder O
structure O
with O
a O
target O
- O
to O
- O
source O
attention O
mechanism O
. O

The O
target- O
to O
- O
source O
attention O
in O
NMT O
can O
provide O
rough O
word O
alignments O
but O
with O
a O
rather O
low O
accuracy O
( O
Koehn O
and O
Knowles O
, O
2017 O
) O
. O

High O
- O
quality O
word O
alignmentcan O
be O
used O
to O
help O
NMT O
in O
many O
different O
ways O
, O
such O
as O
detecting O
source O
words O
that O
are O
missing O
in O
the O
translation O
( O
Lei O
et O
al O
. O
, O
2019 O
) O
, O
integrating O
an O
external O
lexicon O
into O
NMT O
to O
improve O
translation O
for O
domain O
- O
speciﬁc O
terminology O
or O
low O
- O
frequency O
words O
( O
Chatterjee O
et O
al O
. O
, O
2017 O
; O
Chen O
et O
al O
. O
, O
2020 O
) O
, O
transferring O
word O
- O
level O
annotations O
( O
e.g. O
under- O
line O
and O
hyperlink O
) O
from O
source O
to O
target O
for O
docu- O
ment O
/ O
webpage O
translation O
( O
M O
¨uller O
, O
2017 O
) O
. O

A O
number O
of O
approaches O
have O
been O
proposed O
to O
learn O
the O
word O
alignment O
task O
, O
including O
both O
statis- O
tical O
models O
( O
Brown O
et O
al O
. O
, O
1993 O
) O
and O
recently O
neu- O
ral O
models O
( O
Zenkel O
et O
al O
. O
, O
2019 O
; O
Garg O
et O
al O
. O
, O
2019 O
; O

Zenkel O
et O
al O
. O
, O
2020 O
; O
Chen O
et O
al O
. O
, O
2020 O
; O
Stengel- O
Eskin O
et O
al O
. O
, O
2019 O
; O
Nagata O
et O
al O
. O
, O
2020 O
) O
. O

The O
pop- O
ular O
word O
alignment O
tool O
GIZA++ O
( O
Och O
and O
Ney O
, O
2003 O
) O
is O
based O
on O
statistical O
IBM O
models O
( O
Brown O
et O
al O
. O
, O
1993 O
) O
which O
learn O
the O
word O
alignment O
task O
through O
unsupervised O
learning O
and O
do O
not O
require O
gold O
alignments O
from O
humans O
as O
training O
data O
. O

As O
deep O
neural O
networks O
have O
been O
successfully O
ap- O
plied O
to O
many O
natural O
language O
processing O
( O
NLP O
) O
tasks O
, O
neural O
word O
alignment O
approaches O
have O
de- O
veloped O
rapidly O
and O
outperformed O
statistical O
word O
aligners O
( O
Zenkel O
et O
al O
. O
, O
2020 O
; O

Garg O
et O
al O
. O
, O
2019 O
) O
. O

Neural O
word O
alignment O
approaches O
include O
both O
su- O
pervised O
and O
unsupervised O
approaches O
: O
supervised O
approaches O
( O
Stengel O
- O
Eskin O
et O
al O
. O
, O
2019 O
; O
Nagata O
et O
al O
. O
, O
2020 O
) O
use O
gold O
alignments O
from O
human O
an- O
notators O
as O
training O
data O
and O
train O
neural O
models O
to O
learn O
word O
alignment O
through O
supervised O
learning O
; O
unsupervised O
approaches O
do O
not O
use O
gold O
human O
alignments O
for O
model O
training O
and O
mainly O
focus O
on O
improving O
the O
target O
- O
to O
- O
source O
attention O
in O
NMT O
models O
to O
produce O
better O
word O
alignment O
, O
such O
as O
performing O
attention O
optimization O
during O
infer- O
ence O
( O
Zenkel O
et O
al O
. O
, O
2019 O
) O
, O
encouraging O
contiguous O
alignment O
connections O
( O
Zenkel O
et O
al O
. O
, O
2020 O
) O
or O
us- O
ing O
alignments O
from O
GIZA++ O
to O
supervise O
/ O
guide O
the O
attention O
in O
NMT O
models O
( O
Garg O
et O
al O
. O
, O
2019 O
) O
. O

284We O
propose O
a O
bidirectional O
Transformer O
based O
alignment O
( O
BTBA O
) O
model O
for O
unsupervised O
learn- O
ing O
of O
the O
word O
alignment O
task O
. O

Our O
BTBA O
model O
predicts O
the O
current O
target O
word O
by O
paying O
atten- O
tion O
to O
the O
source O
context O
and O
both O
left O
- O
side O
and O
right O
- O
side O
target O
context O
to O
produce O
accurate O
target- O
to O
- O
source O
attention O
( O
alignment O
) O
. O

Compared O
to O
the O
original O
Transformer O
translation O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
which O
computes O
target O
- O
to O
- O
source O
at- O
tention O
based O
on O
only O
the O
left O
- O
side O
target O
context O
due O
to O
left O
- O
to O
- O
right O
autoregressive O
decoding O
, O
our O
BTBA O
model O
can O
exploit O
both O
left O
- O
side O
and O
right- O
side O
target O
context O
to O
compute O
more O
accurate O
target- O
to O
- O
source O
attention O
( O
alignment O
) O
. O

We O
further O
ﬁne- O
tune O
the O
BTBA O
model O
to O
produce O
better O
alignments O
using O
a O
full O
context O
based O
optimization O
method O
and O
self O
- O
supervised O
training O
. O

We O
test O
our O
method O
on O
three O
word O
alignment O
tasks O
and O
show O
that O
our O
method O
outperforms O
previous O
neural O
word O
align- O
ment O
approaches O
and O
also O
beats O
the O
popular O
statisti- O
cal O
word O
aligner O
GIZA++ O
. O

2 O
Background O
2.1 O
Word O
Alignment O
Task O

The O
goal O
of O
the O
word O
alignment O
task O
( O
Och O
and O
Ney O
, O
2003 O
) O
is O
to O
ﬁnd O
word O
- O
level O
alignments O
for O
paral- O
lel O
source O
and O
target O
sentences O
. O

Given O
a O
source O
sentence O
sI 1 O
0 O
= O
s0 O
; O
: O
: O
: O
; O
s O
i O
; O
: O
: O
: O
; O
s O
I 1and O
its O
parallel O
target O
sentence O
tJ 1 O
0 O

= O
t0 O
; O
: O
: O
: O
; O
t O
j O
; O
: O
: O
: O
; O
t O
J 1 O
, O
the O
word O
alignment O
Gis O
deﬁned O
as O
a O
set O
of O
links O
that O
link O
the O
corresponding O
source O
and O
target O
words O
as O
shown O
in O
Equation O
1 O
. O
Gf(i O
; O
j O
) O
: O
i= O
0 O
; O
: O
: O
: O
; O
I 1;j= O
0 O
; O
: O
: O
: O
; O
J 1g(1 O
) O

The O
word O
alignment O
Gallows O
one O
- O
to O
- O
one O
, O
one O
- O
to- O
many O
, O
many O
- O
to O
- O
one O
, O
many O
- O
to O
- O
many O
alignments O
and O
also O
unaligned O
words O
( O
Och O
and O
Ney O
, O
2003 O
) O
. O

Due O
to O
the O
lack O
of O
labelled O
training O
data O
( O
gold O
alignments O
annotated O
by O
humans O
) O
for O
the O
word O
alignment O
task O
, O
most O
word O
alignment O
methods O
learn O
the O
word O
align- O
ment O
task O
through O
unsupervised O
learning O
( O
Brown O
et O
al O
. O
, O
1993 O
; O

Zenkel O
et O
al O
. O
, O
2020 O
; O
Chen O
et O
al O
. O
, O
2020 O
) O
. O

2.2 O
Neural O
Machine O
Translation O
Neural O
translation O
models O
( O
Bahdanau O
et O
al O
. O
, O
2014 O
; O
Vaswani O
et O
al O
. O
, O
2017 O
) O
generally O
have O
an O
encoder- O
decoder O
structure O
with O
a O
target O
- O
to O
- O
source O
attention O
mechanism O
: O
the O
encoder O
encodes O
the O
source O
sen- O
tence O
; O
the O
decoder O
generates O
the O
target O
sentence O
by O
attending O
the O
source O
context O
and O
performingleft O
- O
to O
- O
right O
autoregressive O
decoding O
. O

The O
target O
- O
to- O
source O
attention O
learned O
in O
NMT O
models O
can O
pro- O
vide O
rough O
word O
alignments O
between O
source O
and O
target O
words O
. O

Among O
various O
translation O
models O
, O
the O
Transformer O
translation O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
on O
various O
translation O
tasks O
and O
is O
based O
solely O
on O
attention O
: O
source O
- O
to O
- O
source O
attention O
in O
the O
encoder O
; O
target O
- O
to- O
target O
and O
target O
- O
to O
- O
source O
attention O
in O
the O
decoder O
. O

The O
attention O
networks O
used O
in O
the O
Transformer O
model O
are O
called O
multi O
- O
head O
attention O
which O
per- O
forms O
attention O
using O
multiple O
heads O
as O
shown O
in O
Equation O
2 O
. O
MultiHead O
( O
Q O
; O
K O
; O
V O
) O

= O
Concat O
( O
head O
0 O
; O
: O
: O
: O
; O
head O
N 1)Wo O
Head O
n O
= O
AnVn O

An O
= O
softmax O
QnKT O
np O
dk O

Qn O
= O
QWQ O
n O
; O
Kn O
= O
KWK O
n O
; O
Vn O
= O
V O
WV O
n(2 O
) O
where O
Q O
, O
KandVare O
query O
, O
keys O
, O
values O
for O
the O
attention O
function O
; O
Wo O
, O
WQ O
n O
, O
WK O
nandWV O
n O
are O
model O
parameters O
; O
dkis O
the O
dimension O
of O
the O
keys O
. O

Based O
on O
parallelizable O
attention O
networks O
, O
the O
Transformer O
can O
be O
trained O
much O
faster O
than O
RNN O
- O
based O
translation O
models O
( O
Bahdanau O
et O
al O
. O
, O
2014 O
) O
. O

3 O
Related O
Work O
3.1 O
Statistical O
Alignment O
Models O
Word O
alignment O
is O
a O
key O
component O
in O
traditional O
statistical O
machine O
translation O
( O
SMT O
) O
, O
such O
as O
phrase O
- O
based O
SMT O
( O
Koehn O
et O
al O
. O
, O
2003 O
) O
which O
ex- O
tracts O
phrase O
- O
based O
translation O
rules O
based O
on O
word O
alignments O
. O

The O
popular O
statistical O
word O
alignment O
tool O
GIZA++ O
( O
Och O
and O
Ney O
, O
2003 O
) O
implements O
the O
statistical O
IBM O
models O
( O
Brown O
et O
al O
. O
, O
1993 O
) O
. O

The O
statistical O
IBM O
models O
are O
mainly O
based O
on O
lexical O
translation O
probabilities O
. O

Words O
that O
co O
- O
occur O
fre- O
quently O
in O
parallel O
sentences O
generally O
have O
higher O
lexical O
translation O
probabilities O
and O
are O
more O
likely O
to O
be O
aligned O
. O

The O
statistical O
IBM O
models O
are O
trained O
using O
parallel O
sentence O
pairs O
with O
no O
word- O
level O
alignment O
annotations O
and O
therefore O
learn O
the O
word O
alignment O
task O
through O
unsupervised O
learn- O
ing O
. O

Based O
on O
a O
reparameterization O
of O
IBM O
Model O
2 O
, O
Dyer O
et O
al O
. O

( O
2013 O
) O
presented O
another O
popular O
sta- O
tistical O
word O
alignment O
tool O
fast O
align O
which O
can O
be O
trained O
faster O
than O
GIZA++ O
, O
but O
GIZA++ O
generally O
produces O
better O
word O
alignments O
than O
fast O
align O
. O

2853.2 O
Neural O
Alignment O
Models O
With O
neural O
networks O
being O
successfully O
applied O
to O
many O
NLP O
tasks O
, O
neural O
word O
alignment O
ap- O

proaches O
have O
received O
much O
attention O
. O

The O
ﬁrst O
neural O
word O
alignment O
models O
are O
based O
on O
feed- O
forward O
neural O
networks O
( O
Yang O
et O
al O
. O
, O
2013 O
) O
and O
recurrent O
neural O
networks O
( O
Tamura O
et O
al O
. O
, O
2014 O
) O
which O
can O
be O
trained O
in O
an O
unsupervised O
manner O
by O
noise O
- O
contrastive O
estimation O
( O
NCE O
) O
( O
Gutmann O
and O
Hyv¨arinen O
, O
2010 O
) O
or O
in O
a O
supervised O
manner O
by O
us- O
ing O
alignments O
from O
human O
annotators O
or O
existing O
word O
aligners O
as O
labelled O
training O
data O
. O

As O
NMT O
( O
Bahdanau O
et O
al O
. O
, O
2014 O
; O
Vaswani O
et O
al O
. O
, O
2017 O
) O
achieves O
great O
success O
, O
the O
target O
- O
to O
- O
source O
attention O
in O
NMT O
models O
can O
be O
used O
to O
infer O
rough O
word O
alignments O
, O
but O
with O
a O
rather O
low O
accuracy O
. O

A O
number O
of O
recent O
works O
focus O
on O
improving O
the O
target O
- O
to O
- O
source O
attention O
in O
NMT O
to O
produce O
better O
word O
alignments O
( O
Garg O
et O
al O
. O
, O
2019 O
; O

Zenkel O
et O
al O
. O
, O
2019 O
; O
Chen O
et O
al O
. O
, O
2020 O
; O

Zenkel O
et O
al O
. O
, O
2020 O
) O
. O

Garg O
et O

al O
. O

( O
2019 O
) O
trained O
the O
Transformer O
translation O
model O
to O
jointly O
learn O
translation O
and O
word O
align- O
ment O
through O
multi O
- O
task O
learning O
using O
word O
align- O
ments O
from O
existing O
word O
aligners O
such O
as O
GIZA++ O
as O
labelled O
training O
data O
. O

Chen O
et O
al O
. O

( O
2020 O
) O
pro- O
posed O
a O
method O
to O
infer O
more O
accurate O
word O
align- O
ments O
from O
the O
Transformer O
translation O
model O
by O
choosing O
the O
appropriate O
decoding O
step O
and O
layer O
for O
word O
alignment O
inference O
. O

Zenkel O
et O

al O
. O

( O
2019 O
) O
proposed O
an O
alignment O
layer O
for O
the O
Transformer O
translation O
model O
and O
they O
only O
used O
the O
output O
of O
the O
alignment O
layer O
for O
target O
word O
prediction O
which O
forces O
the O
alignment O
layer O
to O
produce O
bet- O
ter O
alignment O
( O
attention O
) O
. O

Zenkel O
et O

al O
. O

( O
2019 O
) O
also O
proposed O
an O
attention O
optimization O
method O
which O
directly O
optimizes O
the O
attention O
for O
the O
test O
set O
to O
produce O
better O
alignment O
. O

Zenkel O
et O

al O
. O

( O
2020 O
) O
pro- O
posed O
to O
improve O
the O
attention O
in O
NMT O
by O
using O
a O
contiguity O
loss O
to O
encourage O
contiguous O
alignment O
connections O
and O
performing O
direct O
attention O
opti- O

mization O
to O
maximize O
the O
translation O
probability O
for O
both O
the O
source O
- O
to O
- O
target O
and O
target O
- O
to O
- O
source O
translation O
models O
. O

Compared O
to O
these O
methods O
that O
infer O
word O
alignments O
based O
on O
NMT O
target O
- O
to- O
source O
attention O
which O
is O
computed O
by O
considering O
only O
the O
left O
- O
side O
target O
context O
, O
our O
BTBA O
model O
can O
exploit O
both O
left O
- O
side O
and O
right O
- O
side O
target O
con- O
text O
to O
compute O
better O
target O
- O
to O
- O
source O
attention O
( O
alignment O
) O
. O

There O
are O
also O
a O
number O
of O
supervised O
neural O
approaches O
that O
require O
gold O
alignments O
from O
hu O
- O
mans O
for O
learning O
the O
word O
alignment O
task O
( O
Stengel- O
Eskin O
et O
al O
. O
, O
2019 O
; O
Nagata O
et O
al O
. O
, O
2020 O
) O
. O

Because O
gold O
alignments O
from O
humans O
are O
scarce O
, O
Stengel- O
Eskin O
et O
al O
. O

( O
2019 O
) O
; O

Nagata O
et O
al O
. O
( O
2020 O
) O
’s O
models O
only O
have O
a O
small O
size O
of O
task O
- O
speciﬁc O
training O
data O
and O
exploit O
representations O
from O
pre O
- O
trained O
NMT O
and O
BERT O
models O
. O

Compared O
to O
these O
supervised O
methods O
, O
our O
method O
does O
not O
require O
gold O
human O
alignments O
for O
model O
training O
. O

4 O
Our O
Approach O
We O
present O
a O
bidirectional O
Transformer O
based O
align- O
ment O
( O
BTBA O
) O
model O
for O
unsupervised O
learning O
of O
the O
word O
alignment O
task O
. O

Motivated O
by O
BERT O
which O
learns O
a O
masked O
language O
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
we O
randomly O
mask O
10 O
% O
of O
the O
words O
in O
the O
target O
sentence O
and O
then O
train O
our O
BTBA O
model O
to O
predict O
the O
masked O
target O
words O
by O
pay- O
ing O
attention O
to O
the O
source O
context O
and O
both O
left- O
side O
and O
right O
- O
side O
target O
context O
. O

Therefore O
, O
our O
BTBA O
model O
can O
exploit O
both O
left O
- O
side O
and O
right- O
side O
target O
context O
to O
compute O
more O
accurate O
target- O
to O
- O
source O
attention O
( O
alignment O
) O
compared O
to O
the O
original O
Transformer O
translation O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
which O
computes O
the O
target O
- O
to O
- O
source O
attention O
based O
on O
only O
the O
left O
- O
side O
target O
context O
due O
to O
left O
- O
to O
- O
right O
autoregressive O
decoding O
. O

We O
further O
ﬁne O
- O
tune O
the O
target O
- O
to O
- O
source O
attention O
in O
the O
BTBA O
model O
to O
produce O
better O
alignments O
us- O
ing O
a O
full O
context O
based O
optimization O
method O
and O
self O
- O
supervised O
training O
. O

4.1 O
Bidirectional O
Transformer O
Based O
Alignment O
( O
BTBA O
) O

Figure O
1 O
shows O
the O
architecture O
of O
the O
proposed O
BTBA O
model O
. O

The O
encoder O
is O
used O
to O
encode O
the O
source O
sentence1and O
has O
the O
same O
structure O
as O
the O
original O
Transformer O
encoder O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O

The O
input O
of O
the O
decoder O
is O
the O
masked O
target O
sentence O
and O
10 O
% O
of O
the O
words O
in O
the O
tar- O
get O
sentence O
are O
randomly O
masked2 O
. O

As O
shown O
in O
Figure O
1 O
, O
the O
target O
sentence O
contains O
a O
masked O
word O
< O
x O
> O
. O

The O
decoder O
contains O
6 O
layers O
. O

Each O
of O
the O
ﬁrst O
5 O
layers O
of O
the O
decoder O
has O
3 O
sub O
- O
layers O
: O
1Following O
Och O
and O
Ney O
( O
2003 O
) O
’s O
work O
, O
we O
add O
a O
< O
bos O
> O
token O
at O
the O
beginning O
of O
the O
source O
sentence O
for O
target O
words O
that O
are O
not O
aligned O
with O
any O
source O
words O
. O

2During O
training O
, O
we O
randomly O
mask O
10 O
% O
of O
the O
words O
in O
the O
target O
sentences O
for O
each O
training O
epoch O
, O
i.e. O
, O
one O
target O
sentence O
is O
masked O
differently O
for O
different O
training O
epochs O
. O

If O
a O
target O
sentence O
contains O
less O
than O
10 O
words O
, O
then O
we O
just O
randomly O
mask O
one O
word O
in O
this O
sentence O
. O

286 O
Multi O
- O
HeadAttentionAdd O
& O
NormFeedForwardAdd O
& O
Norm O
Multi O
- O
HeadAttentionAdd O
& O
NormFeedForwardAdd O
& O
Norm O
Multi O
- O
HeadAttentionAdd O
& O
Norm O

Embedding O
Embedding6 O
x5 O
xMulti O
- O
HeadAttention O
Multi O
- O
HeadAttentionAdd O
& O
Norm O
PositionalEncodingPositionalEncodingV0V1V2V3V4Aijncake O
Target O

: O
the O
< O
x O
> O
is O
very O
delicious O
Source:<bos O
> O
derkuchenistsehrleckerFigure O
1 O
: O
Architecture O
of O
our O
BTBA O
model O
. O

a O
multi O
- O
head O
self O
- O
attention O
sub O
- O
layer O
, O
a O
target O
- O
to- O
source O
multi O
- O
head O
attention O
sub O
- O
layer O
and O
a O
feed O
forward O
sub O
- O
layer O
, O
like O
a O
standard O
Transformer O
de- O
coder O
layer O
except O
that O
the O
self O
- O
attention O
sub O
- O
layer O
in O
the O
standard O
Transformer O
decoder O
can O
only O
at- O
tend O
left O
- O
side O
target O
context O
while O
the O
self O
- O
attention O
sub O
- O
layer O
in O
our O
BTBA O
decoder O
can O
attend O
all O
target O
words O
and O
make O
use O
of O
both O
left O
- O
side O
and O
right O
- O
side O
target O
context O
to O
compute O
better O
target O
- O
to O
- O
source O
attention O
( O
alignment O
) O
. O

The O
last O
layer O
of O
the O
BTBA O
decoder O
contains O
a O
self O
- O
attention O
sub O
- O
layer O
and O
a O
target O
- O
to O
- O
source O
attention O
sub O
- O
layer O
like O
the O
ﬁrst O
5 O
layers O
of O
the O
BTBA O
decoder O
but O
without O
the O
feed- O
forward O
sub O
- O
layer O
. O

We O
use O
the O
output O
of O
the O
last O
target O
- O
to O
- O
source O
attention O
sub O
- O
layer O
for O
predicting O
the O
masked O
target O
words O
and O
we O
use O
the O
attention O
of O
the O
last O
target O
- O
to O
- O
source O
attention O
sub O
- O
layer O
for O
in- O
ferring O
word O
alignments O
between O
source O
and O
target O
words O
. O

Our O
design O
that O
only O
uses O
the O
last O
target- O
to O
- O
source O
attention O
sub O
- O
layer O
output O
for O
predicting O
the O
masked O
target O
words O
is O
motivated O
by O
the O
align- O
ment O
layer O
of O
Zenkel O
et O
al O
. O

( O
2019 O
) O
in O
order O
to O
forceOriginal O
the O
cake O
is O
very O
delicious O
< O
x O
> O
cake O
is O
very O
delicious O
the O
< O
x O
> O
is O
very O
delicious O
Masked O
the O
cake O
< O
x O
> O
very O
delicious O
the O
cake O
is O
< O
x O
> O
delicious O
the O
cake O
is O
very O
< O
x O
> O
Table O
1 O
: O
Masking O
target O
sentences O
in O
the O
test O
set O
. O

the O
last O
target O
- O
to O
- O
source O
attention O
sub O
- O
layer O
to O
pay O
attention O
to O
the O
most O
important O
source O
words O
for O
predicting O
the O
target O
word O
and O
therefore O
produce O
better O
word O
alignments O
. O

In O
Figure O
1 O
, O
Aijnis O
the O
attention O
value O
of O
the O
jth O
target O
word O
paying O
to O
the O
ith O
source O
word O
using O
thenth O
head O
in O
the O
last O
target O
- O
to O
- O
source O
multi O
- O
head O
attention O
sub O
- O
layer O
. O

V0 O
; O
V1 O
; O
V2 O
; O
V3 O
; O
V4are O
the O
out- O
puts O
of O
the O
decoder O
for O
the O
5 O
target O
words O
and O
V1 O
is O
used O
to O
predict O
the O
masked O
target O
word O
“ O
cake O
” O
. O

Because O
V1is O
used O
to O
predict O
“ O
cake O
” O
, O
the O
attention O
value O
A21nshould O
be O
learned O
to O
be O
high O
in O
order O
to O
make O
V1contain O
the O
most O
useful O
source O
infor- O
mation O
( O
“ O
kuchen O
” O
) O
. O

Therefore O
, O
Aijncan O
be O
used O
to O
infer O
word O
alignment O
for O
the O
target O
word O
“ O
cake O
” O
effectively O
. O

However O
, O
Aijncannot O
provide O
good O
word O
alignments O
for O
unmasked O
target O
words O
such O
as O
“ O
delicious O
” O
in O
Figure O
1 O
because O
V4is O
not O
used O
to O
predict O
any O
target O
word O
and O
A54nis O
not O
necessarily O
learned O
to O
be O
high O
. O

Because O
Aijncan O
only O
be O
used O
to O
infer O
accu- O
rate O
word O
alignment O
for O
masked O
target O
words O
but O
we O
want O
to O
get O
alignments O
for O
all O
target O
words O
in O
the O
test O
set O
, O
we O
mask O
a O
target O
sentence O
tJ 1 O
0 O
in O
the O
test O
set O
Jtimes O
and O
each O
time O
we O
mask O
one O
target O
word O
as O
shown O
in O
Table O
1 O
. O

Each O
masked O
target O
sen- O
tence O
is O
fed O
into O
the O
BTBA O
model O
together O
with O
the O
source O
sentence O
and O
then O
we O
collect O
the O
attention O
Aijnfor O
the O
masked O
target O
words O
. O

Suppose O
the O
j0th O
target O
word O
is O
masked O
, O
then O
we O
compute O
the O
source O
position O
that O
it O
should O
be O
aligned O
to O
as O
, O
i0= O
arg O
max O
iN 1X O
n=0Aij0n O
( O
3 O
) O
4.2 O
Full O
Context O
Based O
Optimization O
In O
Equation O
3 O
, O
the O
attention O
Aij0nfor O
the O
j0tar- O
get O
word O
is O
computed O
by O
considering O
both O
left O
- O
side O
and O
right O
- O
side O
target O
context O
, O
but O
information O
about O
the O
current O
target O
word O
is O
not O
used O
since O
the O
j0tar- O
get O
word O
is O
masked O
. O

For O
example O
in O
Figure O
1 O
, O
the O
BTBA O
model O
does O
not O
know O
that O
the O
second O
target O
word O
is O
“ O
cake O
” O
because O
it O
is O
masked O
, O
therefore O
the O
BTBA O
model O
computes O
the O
attention O
( O
alignment O
) O

287for O
“ O
cake O
” O
only O
using O
the O
left O
- O
side O
and O
right O
- O
side O
context O
of O
“ O
cake O
” O
without O
knowing O
that O
the O
word O
that O
needs O
to O
be O
aligned O
is O
“ O
cake O
” O
. O

We O
propose O
a O
novel O
full O
context O
based O
optimization O
method O
to O
use O
full O
target O
context O
, O
including O
the O
current O
target O
word O
information O
, O
to O
improve O
the O
target O
- O
to O
- O
source O
atten- O
tion O
in O
the O
BTBA O
model O
to O
produce O
better O
align- O
ments O
. O

That O
is O
for O
the O
last O
50 O
training O
steps O
of O
the O
BTBA O
model O
, O
we O
do O
not O
mask O
the O
target O
sentence O
any O
more O
and O
we O
only O
optimize O
parameters O

WQ O
n O
andWK O
nin O
the O
last O
target O
- O
to O
- O
source O
multi O
- O
head O
at- O
tention O
sub O
- O
layer O
. O

As O
shown O
in O
Equation O
2 O
, O
WQ O
n O
andWK O
nare O
parameters O
that O
are O
used O
to O
compute O
the O
attention O
values O
in O
multi O
- O
head O
attention O
. O

Opti- O
mizing O
WQ O
nandWK O
nbased O
on O
full O
target O
context O
can O
help O
the O
BTBA O
model O
to O
produce O
better O
atten- O
tion O
( O
alignment O
) O
while O
at O
the O
same O
time O
freezing O
other O
parameters O
can O
make O
the O
BTBA O
model O
keep O
the O
knowledge O
learned O
from O
masked O
target O
word O
prediction O
. O

After O
full O
target O
context O
based O
optimiza- O
tion O
, O
we O
do O
not O
need O
to O
mask O
target O
sentences O
in O
the O
test O
set O
as O
shown O
in O
Table O
1 O
any O
more O
. O

We O
can O
directly O
feed O
the O
original O
source O
and O
target O
test O
sentences O
into O
the O
BTBA O
model O
and O
compute O
atten- O
tion O
( O
alignment O
) O
for O
all O
target O
words O
in O
the O
sentence O
. O

The O
full O
context O
based O
optimization O
method O
can O
be O
seen O
as O
a O
ﬁne O
- O
tuning O
of O
the O
original O
BTBA O
model O
, O
i.e. O
we O
ﬁne O
- O
tune O
the O
two O
parameters O
WQ O
nandWK O
n O
in O
the O
last O
target O
- O
to O
- O
source O
attention O
layer O
based O
on O
full O
target O
context O
to O
compute O
more O
accurate O
word O
alignments O
. O

4.3 O
Self O
- O
Supervised O
Training O
The O
BTBA O
model O
learns O
word O
alignment O
through O
unsupervised O
learning O
and O
does O
not O
require O
labelled O
data O
for O
the O
word O
alignment O
task O
. O

We O
train O
two O
unsupervised O
BTBA O
models O
, O
one O
for O
the O
forward O
direction O
( O
source O
- O
to O
- O
target O
) O
and O
one O
for O
the O
back- O
ward O
direction O
( O
target O
- O
to O
- O
source O
) O
, O
and O
then O
sym- O
metrize O
the O
alignments O
using O
heuristics O
such O
as O
grow O
- O
diagonal-ﬁnal O
- O
and O
( O
Och O
and O
Ney O
, O
2003 O
) O
as O
the O
symmetrized O
alignments O
have O
better O
quality O
than O
the O
alignments O
from O
a O
single O
forward O
or O
back- O
ward O
model O
. O

After O
unsupervised O
learning O
, O
we O
use O
the O
symmetrized O
word O
alignments O
Gainferred O
from O
our O
unsupervised O
BTBA O
models O
as O
labelled O
data O
to O
further O
ﬁne O
- O
tune O
each O
BTBA O
model O
for O
the O
word O
alignment O
task O
through O
supervised O
training O
using O
the O
alignment O
loss O
in O
Equation O
4 O
following O
Garget O
al O
. O

( O
2019 O
) O
’s O
work.3During O
supervised O
train- O
ing O
, O
the O
BTBA O
model O
is O
trained O
to O
learn O
the O
align- O
ment O
task O
instead O
of O
masked O
target O
word O
prediction O
, O
therefore O
the O
target O
sentence O
does O
not O
need O
to O
be O
masked O
. O

La(A O
) O

= O
 1 O
jGajX O
( O
p;q)2GaN 1X O
n=0log O
( O
Apqn O
) O
( O
4 O
) O
Note O
that O
we O
apply O
byte O
pair O
encoding O
( O
BPE O
) O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
for O
both O
source O
and O
tar- O
get O
sentences O
before O
we O
feed O
them O
into O
the O
BTBA O
model O
. O

Therefore O
the O
alignments O
inferred O
from O
the O
BTBA O
model O
is O
on O
BPE O
- O
level O
. O

We O
convert4 O
BPE O
- O
level O
alignments O
to O
word O
- O
level O
alignments O
be- O

fore O
we O
perform O
alignment O
symmetrization O
. O

Af- O
ter O
alignment O
symmetrization O
, O
we O
want O
to O
use O
the O
symmetrized O
alignments O
to O
further O
ﬁne O
- O
tune O
each O
BTBA O
model O
through O
supervised O
learning O
and O
therefore O
we O
convert5the O
word O
- O
level O
alignments O
back O
to O
BPE O
- O
level O
for O
supervised O
training O
of O
the O
BTBA O
models O
. O

5 O
Experiments O
5.1 O
Settings O
In O
order O
to O
compare O
with O
previous O
work O
, O
we O
used O
the O
same O
datesets6as O
Zenkel O
et O
al O
. O

( O
2020 O
) O
’s O
work O
and O
conducted O
word O
alignment O
experiments O
for O
three O
language O
pairs O
: O
German O
$ O
English O
( O
DeEn O
) O
, O
English O
$ O
French O
( O
EnFr O
) O
and O
Romanian O
$ O
En- O
glish O
( O
RoEn O
) O
. O

Each O
language O
pair O
contains O
a O
test O
set O
and O
a O
training O
set O
: O
the O
test O
set O
contains O
paral- O
lel O
sentences O
with O
gold O
word O
alignments O
annotated O
by O
humans O
; O
the O
training O
set O
contains O
only O
parallel O
sentences O
with O
no O
word O
alignments O
. O

Table O
2 O
gives O
numbers O
of O
sentence O
pairs O
contained O
in O
the O
train- O
ing O
and O
test O
sets O
. O

Parallel O
sentences O
from O
both O
the O
training O
set O
and O
the O
test O
set O
can O
be O
used O
to O
train O
3We O
optimize O
all O
model O
parameters O
during O
supervised O
ﬁne- O
tuning O
. O

4To O
convert O
BPE O
- O
level O
alignments O
to O
word O
- O
level O
align- O
ments O
, O
we O
add O
an O
alignment O
between O
a O
source O
word O
and O
a O
target O
word O
if O
any O
parts O
of O
these O
two O
words O
are O
aligned O
. O

Align- O
ments O
between O
the O
source O
< O
bos O
> O
token O
and O
any O
target O
word O
are O
deleted O
; O
alignments O
between O
the O
last O
source O
word O
“ O
. O
” O

( O
full O
stop O
) O
and O
a O
target O
word O
which O
is O
not O
the O
last O
target O
word O
are O
also O
deleted O
. O

5To O
convert O
word O
- O
level O
alignments O
to O
BPE O
- O
level O
align- O
ments O
, O
we O
add O
an O
alignment O
between O
a O
source O
BPE O
token O
and O
a O
target O
BPE O
token O
if O
the O
source O
word O
and O
the O
target O
word O
that O
contain O
these O
two O
BPE O
tokens O
are O
aligned O
; O
we O
add O
an O
alignment O
between O
the O
source O
< O
bos O
> O
token O
and O
a O
target O
BPE O
token O
if O
the O
target O
word O
that O
contains O
this O
target O
BPE O
token O
is O
not O
aligned O
with O
any O
source O
words O
. O

6https://github.com/lilt/alignment-scripts O

288DeEn O
EnFr O
RoEn O
TRAIN O
1.91 O
M O
1.13 O
M O
447k O
TEST O
508 O
447 O
248 O
Table O
2 O
: O
Numbers O
of O
sentence O
pairs O
in O
the O
datasets O
. O

unsupervised O
word O
alignment O
models O
. O

We O
use O
BPE O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
to O
learn O
a O
joint O
source O
and O
tar- O
get O
vocabulary O
of O
40k O
. O

After O
BPE O
, O
we O
train O
BTBA O
models O
to O
learn O
the O
word O
alignment O
tasks O
. O

We O
use O
a O
word O
embedding O
size O
of O
512 O
. O

The O
feed O
forward O
layer O
contains O
2048 O
hidden O
units O
. O

The O
multi O
- O
head O
attention O
layer O
contains O
8 O
heads O
. O

We O
use O
the O
Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
algorithm O
for O
optimiza- O
tion O
and O
set O
the O
learning O
rate O
to O
0.0002 O
. O

We O
use O
a O
dropout O
of O
0.3 O
. O

Each O
training O
batch O
contains O
40k O
masked O
target O
words O
. O

Since O
the O
word O
alignment O
tasks O
do O
not O
provide O
validation O
data O
, O
we O
trained O
all O
BTBA O
models O
for O
a O
ﬁxed O
number O
of O
training O
epochs O
: O
50 O
for O
DeEn O
, O
100 O
for O
EnFr O
and O
200 O
for O
RoEn.7For O
the O
last O
50 O
training O
steps O
of O
each O
BTBA O
model O
, O
we O
performed O
full O
context O
based O
optimiza- O
tion O
. O

For O
each O
language O
pair O
, O
we O
trained O
two O
BTBA O
models O
, O
one O
for O
the O
forward O
direction O
and O
one O
for O
the O
backward O
direction O
, O
and O
then O
symmetrized O
the O
alignments O
. O

We O
tested O
different O
heuristics O
for O
alignment O
symmetrization O
, O
including O
the O
standard O
Moses O
heuristics O
, O
grow O
- O
diagonal O
, O
grow O
- O
diagonal- O
ﬁnal O
, O
grow O
- O
diagonal-ﬁnal O
- O
and O
. O

We O
also O
tested O
an- O
other O
heuristic O
grow O
- O
diagonal O
- O
and O
which O
is O
slightly O
different O
from O
grow O
- O
diagonal O
: O
the O
grow O
- O
diagonal- O
and O
heuristic O
only O
adds O
a O
new O
alignment O
( O
i O
; O
j O
) O
when O
both O
siandtjare O
unaligned O
while O
grow- O
diagonal O
adds O
a O
new O
alignment O
( O
i O
; O
j)when O
any O
of O
the O
two O
words O
( O
siandtj O
) O
are O
unaligned O
. O

We O
ﬁnd O
that O
the O
Moses O
heuristic O
grow O
- O
diagonal-ﬁnal O
- O
and O
generally O
achieved O
the O
best O
results O
for O
symmetriz- O
ing O
the O
BTBA O
alignments O
, O
but O
grow O
- O
diagonal O
- O
and O
worked O
particularly O
good O
for O
the O
EnFr O
task O
. O

Finally O
, O
we O
used O
the O
symmetrized O
alignments O
inferred O
from O
our O
unsupervised O
BTBA O
models O
as O
labelled O
data O
to O
further O
ﬁne O
- O
tune O
each O
BTBA O
model O
to O
learn O
the O
alignment O
task O
through O
supervised O
train- O
ing O
. O

We O
ﬁne O
- O
tuned O
each O
BTBA O
model O
for O
50 O
train- O
ing O
steps O
using O
the O
alignment O
loss O
in O
Equation O
4 O
. O

In O
addition O
, O
we O
also O
tested O
to O
use O
alignments O
from O
GIZA++ O
instead O
of O
alignments O
inferred O
from O
our O
7The O
training O
time O
( O
time O
of O
one O
training O
epoch O
number O
of O
training O
epochs O
) O
of O
one O
BTBA O
model O
for O
different O
tasks O
( O
DeEn O
, O
EnFr O
and O
RoEn O
) O
is O
roughly O
the O
same O
, O
30 O
hours O
using O
4 O
GPUs O
. O

Method O
DeEn O
EnFr O
RoEn O
Zenkel O
et O

al O
. O

( O
2019 O
) O
21.2 O
% O
10.0 O
% O
27.6 O
% O
Garg O
et O
al O
. O

( O
2019 O
) O
16.0 O
% O
4.6 O
% O
23.1 O
% O
Zenkel O
et O

al O
. O

( O
2020 O
) O
16.3 O
% O
5.0 O
% O
23.4 O
% O
Chen O
et O
al O
. O

( O
2020 O
) O
15.4 O
% O
4.7 O
% O
21.2 O
% O
GIZA++ O
18.4 O
% O
5.2 O
% O
24.2 O
% O
OursBTBA O
- O
left O
30.3 O
% O
20.2 O
% O
33.0 O
% O
BTBA O
- O
right O
32.3 O
% O
14.9 O
% O
38.6 O
% O
BTBA O
17.8 O
% O
9.5 O
% O
22.9 O
% O
+ O
FCBO O
16.3 O
% O
8.9 O
% O
20.6 O
% O
+ O
SST O
14.3 O
% O
6.7 O
% O
18.5 O
% O
+ O
GST O
14.5 O
% O
4.2 O
% O
19.7 O
% O
Table O
3 O
: O
AER O
Results O
. O

FCBO O
: O
full O
context O
based O
opti- O
mization O
; O
SST O
: O
self O
- O
supervised O
training O
; O
GST O
: O
GIZA++ O
supervised O
training O
. O

unsupervised O
BTBA O
models O
as O
labelled O
data O
for O
supervised O
ﬁne O
- O
tuning O
of O
the O
BTBA O
models O
. O

5.2 O
Results O
Table O
3 O
gives O
alignment O
error O
rate O
( O
AER O
) O
( O
Och O
and O
Ney O
, O
2000 O
) O
results O
of O
our O
BTBA O
model O
and O
com- O
parison O
with O
previous O
work O
. O

Table O
3 O
also O
gives O
results O
of O
BTBA O
- O
left O
and O
BTBA O
- O
right O
: O
BTBA O
- O
left O
means O
that O
the O
BTBA O
decoder O
only O
attends O
left- O
side O
target O
context O
; O
BTBA O
- O
right O
means O
that O
the O
BTBA O
decoder O
only O
attends O
right O
- O
side O
target O
con- O
text O
. O

As O
shown O
in O
Table O
3 O
, O
the O
BTBA O
model O
, O
which O
uses O
both O
left O
- O
side O
and O
right O
- O
side O
target O
context O
, O
signiﬁcantly O
outperformed O
BTBA O
- O
left O
and O
BTBA- O
right O
. O

Results O
also O
show O
that O
the O
performance O
of O
our O
BTBA O
model O
can O
be O
further O
improved O
by O
full O
context O
based O
optimization O
( O
FCBO O
) O
and O
supervised O
training O
including O
both O
self O
- O
supervised O
training O
and O
GIZA++ O
supervised O
training O
. O

For O
DeEn O
and O
RoEn O
tasks O
, O
the O
self O
- O
supervised O
BTBA O
( O
S O
- O
BTBA O
) O
model O
achieved O
the O
best O
results O
, O
outperforming O
previous O
neural O
and O
statistical O
methods O
. O

For O
the O
EnFr O
task O
, O
as O
the O
statistical O
aligner O
GIZA++ O
performed O
well O
and O
achieved O
better O
results O
than O
our O
unsupervised O
BTBA O
model O
, O
the O
GIZA++ O
supervised O
BTBA O
( O
G- O
BTBA O
) O
model O
achieved O
better O
results O
than O
the O
S- O
BTBA O
model O
and O
also O
outperformed O
the O
original O
GIZA++ O
and O
previous O
neural O
models O
. O

Tables O
4 O
, O
5 O
and O
6 O
give O
results O
of O
using O
differ- O
ent O
heuristics O
for O
symmetrizing O
alignments O
pro- O
duced O
by O
BTBA O
, O
GIZA++ O
and O
G O
- O
BTBA O
, O
respec- O
tively O
. O

For O
our O
unsupervised O
and O
self O
- O
supervised O
BTBA O
models O
, O
grow O
- O
diagonal-ﬁnal O
- O
and O
achieved O
the O
best O
results O
on O
DeEn O
and O
RoEn O
tasks O
while O
grow O
- O
diagonal O
- O
and O
achieved O
the O
best O
results O
on O
the O
EnFr O
task O
. O

For O
GIZA++ O
and O
G O
- O
BTBA O
, O
the O
best O
heuristics O
for O
different O
language O
pairs O
are O
quite O
dif- O
ferent O
, O
though O
grow O
- O
diagonal-ﬁnal O
- O
and O
generally O

289DeEn O
EnFr O
RoEn O
BTBA O
+ O
FCBO O
+ O
SST O
BTBA O
+ O
FCBO O
+ O
SST O
BTBA O
+ O
FCBO O
+ O
SST O
forward O
20.2 O
% O
18.3 O
% O
14.3 O
% O
13.6 O
% O
12.8 O
% O
7.3 O
% O
24.7 O
% O
22.4 O
% O
20.5 O
% O
backward O
23.8 O
% O
23.3 O
% O
17.2 O
% O
14.6 O
% O
13.3 O
% O
7.5 O
% O
27.3 O
% O
26.1 O
% O
22.0 O
% O
union O
20.6 O
% O
18.3 O
% O
14.5 O
% O
15.7 O
% O
14.3 O
% O
7.5 O
% O
24.1 O
% O
21.2 O
% O
18.9 O
% O
intersection O
23.7 O
% O
23.9 O
% O
17.1 O
% O
11.6 O
% O
11.2 O
% O
7.4 O
% O
28.3 O
% O
27.9 O
% O
24.0 O
% O
grow O
- O
diagonal O
19.9 O
% O
18.5 O
% O
14.3 O
% O
11.2 O
% O
10.7 O
% O
6.9 O
% O
23.6 O
% O
21.6 O
% O
18.6 O
% O
grow O
- O
diagonal O
- O
and O
21.0 O
% O
20.6 O
% O
17.3 O
% O
9.5 O
% O
8.9 O
% O
6.7 O
% O
26.1 O
% O
25.4 O
% O
23.6 O
% O
grow O
- O
diagonal-ﬁnal O
19.5 O
% O
17.3 O
% O
14.4 O
% O
14.4 O
% O
13.4 O
% O
7.4 O
% O
23.4 O
% O
20.8 O
% O
18.6 O
% O
grow O
- O
diagonal-ﬁnal O
- O
and O
17.8 O
% O
16.3 O
% O
14.3 O
% O
11.9 O
% O
11.2 O
% O
7.0 O
% O
22.9 O
% O
20.6 O
% O
18.5 O
% O
Table O
4 O
: O
Comparison O
of O
different O
heuristics O
for O
symmetrizing O
the O
BTBA O
alignments O
. O

FCBO O
: O
full O
context O
based O
optimization O
. O

SST O
: O
self O
- O
supervised O
training O
. O

DeEn O
EnFr O
RoEn O
forward O
19.0 O
% O
10.3 O
% O
25.6 O
% O
backward O
22.5 O
% O
9.1 O
% O
29.7 O
% O
union O
22.1 O
% O
12.9 O
% O
27.5 O
% O
intersection O
19.0 O
% O
5.2 O
% O
27.8 O
% O
grow O
- O
diagonal O

18.4 O
% O
7.7 O
% O
24.5 O
% O
grow O
- O
diagonal O
- O
and O
18.9 O
% O
5.7 O
% O
26.1 O
% O
grow O
- O
diagonal-ﬁnal O
21.1 O
% O
11.7 O
% O
26.0 O
% O
grow O
- O
diagonal-ﬁnal O
- O
and O
18.9 O
% O
8.5 O
% O
24.2 O
% O
Table O
5 O
: O
Comparison O
of O
different O
heuristics O
for O
sym- O
metrizing O
GIZA++ O
alignments O
. O

1618202224262830 O
0102030405060freeze O
no O
freezeAER O
Training O
step O
Figure O
2 O
: O
DeEn O
test O
AER O
per O
training O
step O
during O
FCBO O
with O
/ O
without O
parameter O
freezing O
. O

obtained O
good O
( O
best O
or O
close O
to O
best O
) O
results O
on O
DeEn O
and O
RoEn O
tasks O
while O
grow O
- O
diagonal O
- O
and O
generally O
obtained O
good O
( O
close O
to O
best O
) O
results O
on O
the O
EnFr O
task O
. O

FCBO O
with O
/ O
without O
Parameter O
Freezing O
As O
we O
explained O
in O
Section O
4.2 O
, O
during O
full O
context O
based O
optimization O
( O
FCBO O
) O
, O
we O
only O
optimize O
WQ O
nandWK O

nin O
the O
last O
target O
- O
to O
- O
source O
attention O
sub O
- O
layer O
and O
freeze O
all O
other O
parameters O
so O
the O
BTBA O
model O
can O
keep O
the O
knowledge O
learned O
from O
masked O
target O
word O
prediction O
. O

We O
also O
tested O
to O
optimize O
all O
parameters O
of O
the O
BTBA O
model O
without O
parameter O
freezing O
during O
FCBO O
. O

Figure O
2 O
shows O
how O
the O
AER O
results O
on O
the O
DeEn O
test O
set O
changed O
during O
FCBO O
with O
and O
without O
param- O
eter O
freezing O
. O

Without O
freezing O
any O
parametersDeEn O
EnFr O
RoEn O
forward O
14.5 O
% O
5.8 O
% O
21.4 O
% O
backward O
17.6 O
% O
4.2 O
% O
21.9 O
% O
union O
15.1 O
% O
5.3 O
% O
19.9 O
% O
intersection O
17.2 O
% O
4.7 O
% O
23.6 O
% O
grow O
- O
diagonal O
14.7 O
% O
4.6 O
% O
19.7 O
% O
grow O
- O
diagonal O
- O
and O
17.5 O
% O
4.4 O
% O
23.7 O
% O
grow O
- O
diagonal-ﬁnal O
15.1 O
% O
5.3 O
% O
19.8 O
% O
grow O
- O
diagonal-ﬁnal O
- O
and O
14.8 O
% O
4.7 O
% O
19.8 O
% O
Table O
6 O
: O
Comparison O
of O
different O
heuristics O
for O
sym- O
metrizing O
G O
- O
BTBA O
alignments O
. O

during O
FCBO O
, O
the O
AER O
result O
( O
the O
red O
curve O
) O
ﬁrst O
increased O
a O
little O
, O
then O
decreased O
sharply O
, O
and O
soon O
increased O
again O
. O

In O
contrast O
, O
when O
we O
freeze O
most O
of O
the O
parameters O
, O
the O
AER O
result O
( O
the O
blue O
curve O
) O
decreased O
stably O
and O
eventually O
got O
better O
results O
( O
16.3 O
% O
) O
than O
no O
parameter O
freezing O
( O
16.7 O
% O
) O
. O

Note O
that O
the O
results O
in O
Figure O
2 O
are O
computed O
based O
on O
full O
target O
context O
, O
i.e. O
, O
the O
target O
sentence O
is O
not O
masked O
. O

As O
we O
explained O
in O
Section O
4.1 O
, O
the O
BTBA O
model O
without O
FCBO O
should O
only O
be O
used O
to O
infer O
word O
alignments O
for O
masked O
target O
words O
. O

Without O
FCBO O
, O
using O
the O
BTBA O
model O
to O
infer O
word O
alignments O
for O
unmasked O
target O
words O
pro- O
duces O
poor O
AER O
results O
( O
26.9 O
% O
as O
shown O
in O
Fig- O
ure O
2 O
) O
compared O
to O
using O
the O
BTBA O
model O
to O
infer O
word O
alignments O
for O
masked O
target O
words O
( O
17.8 O
% O
as O
shown O
in O
Table O
3 O
) O
. O

FCBO O
can O
quickly O
improve O
the O
results O
of O
using O
the O
BTBA O
model O
for O
inferring O
word O
alignments O
for O
unmasked O
target O
words O
, O
and O
eventually O
after O
FCBO O
, O
the O
BTBA O
model O
can O
effec- O
tively O
use O
full O
target O
context O
to O
compute O
better O
word O
alignment O
compared O
to O
the O
original O
BTBA O
model O
without O
FCBO O
( O
16.3 O
% O
versus O
17.8 O
% O
as O
shown O
in O
Table O
3 O
) O
. O

Training O
Data O
for O
Supervised O
Learning O
Be- O
cause O
the O
symmetrized O
BTBA O
alignments O
have O
bet- O
ter O
quality O
compared O
to O
alignments O
from O
a O
single O
unidirectional O
( O
forward O
or O
backward O
) O
BTBA O
model O
as O
shown O
in O
Table O
4 O
, O
we O
used O
the O
symmetrized O

290 O
Gold O
Ours O
, O
. O

dürfen O
werden O
abgefüllt O
schaumweinflaschen O
in O
getränke O
welche O
klar O
definiert O
berichtsvorschlag O
i O
m O
es O
wird O
may O
beverages O
which O
defines O
clearly O
report O
the O
. O

bottles O
wine O
sparkling O
in O
bottled O
bemay O
beverages O
which O
defines O
clearly O
report O
the O
. O

bottles O
wine O
sparkling O
in O
bottled O
beFigure O
3 O
: O
An O
example O
of O
gold O
alignments O
and O
alignments O
produced O
by O
our O
S O
- O
BTBA O
model O
. O

141618202224 O
0102030405060UNI O
SYM O
Training O
  O
stepAER O
Figure O
4 O
: O
AER O
results O
of O
the O
forward O
BTBA O
model O
dur- O
ing O
self O
- O
supervised O
training O
. O

UNI O
: O
using O
unidirectional O
BTBA O
alignments O
as O
labelled O
training O
data O
. O

SYM O
: O
us- O
ing O
symmetrized O
BTBA O
alignments O
as O
labelled O
training O
data O
. O

word O
alignments O
inferred O
from O
our O
unsupervised O
BTBA O
models O
as O
labelled O
data O
to O
further O
ﬁne O
- O
tune O
each O
unidirectional O
BTBA O
model O
for O
the O
alignment O
task O
through O
supervised O
training O
. O

We O
also O
tested O
to O
use O
unidirectional O
BTBA O
alignments O
instead O
of O
symmetrized O
BTBA O
alignments O
as O
labelled O
data O
for O
supervised O
training O
. O

Figure O
4 O
( O
the O
blue O
curve O
) O
shows O
how O
the O
performance O
of O
the O
forward O
BTBA O
model O
of O
the O
DeEn O
task O
changes O
during O
supervised O
training O
when O
using O
unidirectional O
alignments O
in- O
ferred O
from O
itself O
( O
the O
forward O
BTBA O
model O
) O
as O
labelled O
training O
data O
, O
which O
demonstrates O
that O
the O
forward O
BTBA O
model O
can O
be O
signiﬁcantly O
im- O
proved O
through O
supervised O
training O
even O
when O
the O
training O
data O
is O
inferred O
from O
itself O
and O
not O
im- O
proved O
by O
alignment O
symmetrization O
. O

Figure O
4 O
also O
shows O
that O
using O
symmetrized O
alignments O
for O
su- O
pervised O
training O
( O
the O
red O
curve O
) O
did O
achieve O
better O
results O
than O
using O
unidirectional O
alignments O
for O
su- O
pervised O
training O
. O

In O
addition O
, O
it O
is O
worth O
noting O
that O
supervised O
training O
can O
improve O
the O
BTBA O
model O
even O
if O
the O
quality O
of O
the O
labelled O
training O
data O
is O
somewhat O
worse O
than O
the O
BTBA O
model O
itself O
, O
e.g. O
for O
the O
RoEn O
task O
, O
using O
the O
GIZA++ O
alignments O
for O
ﬁne O
- O
tuning O
the O
forward O
BTBA O
model O
through O
supervised O
training O
improved O
the O
result O
of O
the O
for- O
ward O
BTBA O
model O
( O
22.4 O
% O
! O
21.4 O
% O
as O
shown O
inDeEn O
EnFr O
RoEn O
S O
- O
BTBA O
FF O
12.3 O
11.3 O
18.2 O
CC O
6.1 O
3.3 O
7.8 O
FC O
44.4 O
12.8 O
41.1 O
G O
- O
BTBA O
FF O
13.2 O
5.1 O
18.6 O
CC O
7.1 O
2.9 O
8.3 O
FC O
43.3 O
9.3 O
46.1 O
Table O
7 O
: O
AER O
for O
different O
types O
of O
alignments O
. O

Table O
4 O
and O
Table O
6 O
) O
even O
though O
GIZA++ O
pro- O

duced O
worse O
alignments O
( O
24.2 O
% O
in O
Table O
3 O
) O
than O
the O
forward O
BTBA O
model O
. O

Alignment O
Error O
Analysis O
We O
analyze O
the O
alignment O
errors O
produced O
by O
our O
system O
and O
ﬁnd O
that O
most O
of O
the O
alignment O
errors O
are O
caused O
by O
function O
words O
. O

As O
shown O
in O
the O
alignment O
exam- O
ple O
in O
Figure O
3 O
, O
source O
and O
target O
corresponding O
content O
words O
( O
e.g. O
“ O
deﬁniert O
” O
and O
“ O
deﬁnes O
” O
) O

are O
all O
correctly O
aligned O
by O
our O
model O
, O
but O
function O
words O
such O
as O
“ O
the O
” O
, O
“ O
i O
m O
” O
and O
“ O
wird O
” O
are O
not O
cor- O
rectly O
aligned O
. O

To O
give O
a O
more O
detailed O
analysis O
, O
we O
compute O
AER O
results O
of O
our O
model O
for O
3 O
different O
types O
of O
alignments O
: O
FF O
( O
alignments O
between O
two O
function O
words O
) O
, O
CC O
( O
alignments O
between O
two O
con- O
tent O
words O
) O
and O
FC O
( O
alignments O
between O
a O
function O
word O
and O
a O
content O
word).8Table O
7 O
shows O
that O
our O
models O
achieved O
signiﬁcantly O
better O
results O
for O
CC O
alignments O
than O
for O
FF O
and O
FC O
alignments O
. O

Func- O
tion O
words O
are O
more O
difﬁcult O
to O
align O
than O
content O
words O
most O
likely O
because O
content O
words O
in O
a O
paral- O
lel O
sentence O
pair O
usually O
have O
very O
clear O
correspond- O
ing O
relations O
( O
such O
as O
“ O
deﬁnes O
” O
clearly O
corresponds O
to O
“ O
deﬁniert O
” O
in O
Figure O
3 O
) O
, O
but O
function O
words O
( O
such O
as O
“ O
the O
” O
, O
“ O
es O
” O
and O
“ O
i O
m O
” O
) O
are O
used O
more O
ﬂexibly O
and O
frequently O
do O
not O
have O
clear O
corresponding O
words O
in O
parallel O
sentences O
, O
which O
increases O
the O
alignment O
difﬁculty O
signiﬁcantly O
. O

8For O
each O
language O
, O
we O
judge O
whether O
a O
word O
is O
a O
function O
word O
or O
a O
content O
word O
using O
a O
list O
of O
stopwords O
from O
nltk O
, O
https://www.nltk.org/ O

291de!en O
en!de O
SHIFT O
- O
AET O
34.8 O
28.0 O
Ours O
35.1 O
28.7 O
Table O
8 O
: O
Translation O
results O
( O
BLEU O
) O
for O
dictionary- O
guided O
NMT O
. O

5.3 O
Dictionary O
- O
Guided O
NMT O
via O
Word O
Alignment O
For O
downstream O
tasks O
, O
word O
alignment O
can O
be O
used O
to O
improve O
dictionary O
- O
guided O
NMT O
( O
Song O
et O
al O
. O
, O
2020 O
; O
Chen O
et O
al O
. O
, O
2020 O
) O
. O

Speciﬁcally O
, O
at O
each O
de- O
coding O
step O
in O
NMT O
, O
Chen O
et O
al O
. O

( O
2020 O
) O
used O
a O
SHIFT O
- O
AET O
method O
to O
compute O
word O
alignment O
for O
the O
newly O
generated O
target O
word O
and O
then O
re- O
vised O
the O
newly O
generated O
target O
word O
by O
encour- O
aging O
the O
pre O
- O
speciﬁed O
translation O
from O
the O
dictio- O
nary O
. O

The O
SHIFT O
- O
AET O
alignment O
method O
adds O
a O
separate O
alignment O
module O
to O
the O
original O
Trans- O
former O
translation O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
and O
trains O
the O
separate O
alignment O
module O
using O
alignments O
induced O
from O
the O
attention O
weights O
of O
the O
original O
Transformer O
. O

To O
test O
the O
effec- O
tiveness O
of O
our O
alignment O
method O
for O
improving O
dictionary O
- O
guided O
NMT O
, O
we O
used O
the O
alignments O
inferred O
from O
our O
BTBA O
models O
as O
labelled O
data O
for O
supervising O
the O
SHIFT O
- O
AET O
alignment O
module O
and O
performed O
dictionary O
- O
guided O
translation O
for O
the O
German O
$ O
English O
language O
pair O
following O
Chen O
et O
al O
. O

( O
2020 O
) O
’s O
work O
. O

Table O
8 O
gives O
the O
translation O
re- O
sults O
of O
dictionary O
- O
guided O
NMT O
and O
shows O
that O
our O
alignment O
method O
led O
to O
higher O
translation O
quality O
compared O
to O
the O
original O
SHIFT O
- O
AET O
method O
. O

6 O
Conclusion O
This O
paper O
presents O
a O
novel O
BTBA O
model O
for O
unsu- O
pervised O
learning O
of O
the O
word O
alignment O
task O
. O

Our O
BTBA O
model O
predicts O
the O
current O
target O
word O
by O
paying O
attention O
to O
the O
source O
context O
and O
both O
left O
- O
side O
and O
right O
- O
side O
target O
context O
to O
produce O
accurate O
target O
- O
to O
- O
source O
attention O
( O
alignment O
) O
. O

We O
further O
ﬁne O
- O
tune O
the O
target O
- O
to O
- O
source O
attention O
in O
the O
BTBA O
model O
to O
obtain O
better O
alignments O
using O
a O
full O
context O
based O
optimization O
method O
and O
self- O
supervised O
training O
. O

We O
test O
our O
method O
on O
three O
word O
alignment O
tasks O
and O
show O
that O
our O
method O
out- O
performs O
both O
previous O
neural O
alignment O
methods O
and O
the O
popular O
statistical O
word O
aligner O
GIZA++ O
. O

Acknowledgments O
This O
work O
is O
supported O
by O
the O
German O
Federal O
Min- O
istry O
of O
Education O
and O
Research O
( O
BMBF O
) O
underfunding O
code O
01IW20010 O

( O
CORA4NLP O
) O
. O

References O
Dzmitry O
Bahdanau O
, O
Kyunghyun O
Cho O
, O
and O
Yoshua O
Ben- O
gio O
. O

2014 O
. O

Neural O
machine O
translation O
by O
jointly O
learning O
to O
align O
and O
translate O
. O

arXiv O
preprint O
arXiv:1409.0473 O
. O

Loc O
Barrault O
, O
Magdalena O
Biesialska O
, O
Ondej O
Bo- O
jar O
, O
Marta O
R. O
Costa O
- O
juss O
, O
Christian O
Federmann O
, O
Yvette O
Graham O
, O
Roman O
Grundkiewicz O
, O
Barry O
Had- O
dow O
, O
Matthias O
Huck O
, O
Eric O
Joanis O
, O
Tom O
Kocmi O
, O
Philipp O
Koehn O
, O
Chi O
- O
kiu O
Lo O
, O
Nikola O
Ljubei O
, O
Christof O
Monz O
, O
Makoto O
Morishita O
, O
Masaaki O
Nagata O
, O
Toshi- O
aki O
Nakazawa O
, O
Santanu O
Pal O
, O
Matt O
Post O
, O
and O
Marcos O
Zampieri O
. O

2020 O
. O

Findings O
of O
the O
2020 O
conference O
on O
machine O
translation O
( O
wmt20 O
) O
. O

In O
Proceedings O
of O
the O
Fifth O
Conference O
on O
Machine O
Translation O
, O
pages O
1 O
– O
55 O
, O
Online O
. O

Association O
for O
Computational O
Linguis- O
tics O
. O

Loc O
Barrault O
, O
Ondej O
Bojar O
, O
Marta O
R. O
Costa O
- O
juss O
, O
Chris- O
tian O
Federmann O
, O
Mark O
Fishel O
, O
Yvette O
Graham O
, O
Barry O
Haddow O
, O
Matthias O
Huck O
, O
Philipp O
Koehn O
, O
Shervin O
Malmasi O
, O
Christof O
Monz O
, O
Mathias O
Mller O
, O
Santanu O
Pal O
, O
Matt O
Post O
, O
and O
Marcos O
Zampieri O
. O

2019 O
. O

Find- O
ings O
of O
the O
2019 O
conference O
on O
machine O
translation O
( O
wmt19 O
) O
. O

In O
Proceedings O
of O
the O
Fourth O
Conference O
on O
Machine O
Translation O
( O
Volume O
2 O
: O
Shared O
Task O
Pa- O
pers O
, O
Day O
1 O
) O
, O
pages O
1–61 O
, O
Florence O
, O
Italy O
. O

Associa- O
tion O
for O
Computational O
Linguistics O
. O

Peter O
F. O
Brown O
, O
Stephen O
A. O
Della O
Pietra O
, O
Vincent O
J. O
Della O
Pietra O
, O
and O
Robert O
L. O
Mercer O
. O

1993 O
. O

The O
math- O
ematics O
of O
statistical O
machine O
translation O
: O
Parameter O
estimation O
. O

Computational O
Linguistics O
, O
19(2):263 O
– O
311 O
. O

Rajen O
Chatterjee O
, O
Matteo O
Negri O
, O
Marco O
Turchi O
, O
Mar- O
cello O
Federico O
, O
Lucia O
Specia O
, O
and O
Fr O
´ O
ed´eric O
Blain O
. O

2017 O
. O

Guiding O
neural O
machine O
translation O
decoding O
with O
external O
knowledge O
. O

In O
Proceedings O
of O
the O
Sec- O
ond O
Conference O
on O
Machine O
Translation O
, O
pages O
157 O
– O
168 O
, O
Copenhagen O
, O
Denmark O
. O

Association O
for O
Com- O
putational O
Linguistics O
. O

Yun O
Chen O
, O
Yang O
Liu O
, O
Guanhua O
Chen O
, O
Xin O
Jiang O
, O
and O
Qun O
Liu O
. O
2020 O
. O

Accurate O
word O
alignment O
induction O
from O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natu- O
ral O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
566–576 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
under- O
standing O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
, O
Minneapolis O
, O
Minnesota O
. O

Associ- O
ation O
for O
Computational O
Linguistics O
. O

292Chris O
Dyer O
, O
Victor O
Chahuneau O
, O
and O
Noah O
A. O
Smith O
. O

2013 O
. O

A O
simple O
, O
fast O
, O
and O
effective O
reparameter- O
ization O
of O
IBM O
model O
2 O
. O

In O
Proceedings O
of O
the O
2013 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Hu- O
man O
Language O
Technologies O
, O
pages O
644–648 O
, O
At- O
lanta O
, O
Georgia O
. O

Association O
for O
Computational O
Lin- O
guistics O
. O

Sarthak O
Garg O
, O
Stephan O
Peitz O
, O
Udhyakumar O
Nallasamy O
, O
and O
Matthias O
Paulik O
. O

2019 O
. O

Jointly O
learning O
to O
align O
and O
translate O
with O
transformer O
models O
. O

In O
Proceed- O
ings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
Inter- O
national O
Joint O
Conference O
on O
Natural O
Language O
Pro- O

cessing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
4453–4462 O
, O
Hong O
Kong O
, O
China O
. O

Association O
for O
Computational O
Lin- O
guistics O
. O

Michael O
Gutmann O
and O
Aapo O
Hyv O
¨arinen O
. O
2010 O
. O

Noise- O
contrastive O
estimation O
: O
A O
new O
estimation O
principle O
for O
unnormalized O
statistical O
models O
. O

In O
Proceedings O
of O
the O
Thirteenth O
International O
Conference O
on O
Artiﬁ- O

cial O
Intelligence O
and O
Statistics O
, O
pages O
297–304 O
. O

Diederik O
P O
Kingma O
and O
Jimmy O
Ba O
. O
2014 O
. O

Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O

arXiv O
preprint O
arXiv:1412.6980 O
. O

Philipp O
Koehn O
and O
Rebecca O
Knowles O
. O

2017 O
. O

Six O
chal- O
lenges O
for O
neural O
machine O
translation O
. O

In O
Proceed- O
ings O
of O
the O
First O
Workshop O
on O
Neural O
Machine O
Trans- O
lation O
, O
pages O
28–39 O
, O
Vancouver O
. O
Association O
for O
Computational O
Linguistics O
. O

Philipp O
Koehn O
, O
Franz O
J. O
Och O
, O
and O
Daniel O
Marcu O
. O

2003 O
. O

Statistical O
phrase O
- O
based O
translation O
. O

In O
Proceedings O
of O
the O
2003 O
Human O
Language O
Technology O
Confer- O
ence O
of O
the O
North O
American O
Chapter O
of O
the O
Associa- O
tion O
for O
Computational O
Linguistics O
, O
pages O
127–133 O
. O

Wenqiang O
Lei O
, O
Weiwen O
Xu O
, O

Ai O
Ti O

Aw O
, O
Yuanxin O
Xiang O
, O
and O
Tat O
Seng O
Chua O
. O

2019 O
. O

Revisit O
automatic O
error O
detection O
for O
wrong O
and O
missing O
translation O
– O
a O
su- O
pervised O
approach O
. O

In O
Proceedings O
of O
the O
2019 O
Con- O
ference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Confer- O
ence O
on O
Natural O
Language O
Processing O
( O
EMNLP- O
IJCNLP O
) O
, O
pages O
942–952 O
, O
Hong O
Kong O
, O
China O
. O

As- O
sociation O
for O
Computational O
Linguistics O
. O

Mathias O
M O
¨uller O
. O

2017 O
. O

Treatment O
of O
markup O
in O
sta- O
tistical O
machine O
translation O
. O

In O
Proceedings O
of O
the O
Third O
Workshop O
on O
Discourse O
in O
Machine O
Transla- O
tion O
, O
pages O
36–46 O
, O
Copenhagen O
, O
Denmark O
. O

Associa- O
tion O
for O
Computational O
Linguistics O
. O

Masaaki O
Nagata O
, O
Chousa O
Katsuki O
, O
and O
Masaaki O
Nishino O
. O
2020 O
. O

A O
supervised O
word O
alignment O
method O
based O
on O
cross O
- O
language O
span O
predic- O
tion O
using O
multilingual O
bert O
. O

arXiv O
preprint O
arXiv:2004.14516 O
. O

Franz O
Josef O
Och O
and O
Hermann O
Ney O
. O
2000 O
. O

Improved O
statistical O
alignment O
models O
. O

In O
Proceedings O
of O
the38th O
Annual O
Meeting O
of O
the O
Association O
for O
Com- O
putational O
Linguistics O
, O
pages O
440–447 O
, O
Hong O
Kong O
. O
Association O
for O
Computational O
Linguistics O
. O

Franz O
Josef O
Och O
and O
Hermann O
Ney O
. O

2003 O
. O

A O
systematic O
comparison O
of O
various O
statistical O
alignment O
models O
. O

Computational O
Linguistics O
, O
29(1):19–51 O
. O

Rico O
Sennrich O
, O
Barry O
Haddow O
, O
and O
Alexandra O
Birch O
. O
2016 O
. O

Neural O
machine O
translation O
of O
rare O
words O
with O
subword O
units O
. O

In O
Proceedings O
of O
the O
54th O

An- O
nual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1715 O
– O
1725 O
, O
Berlin O
, O
Germany O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Kai O
Song O
, O
Kun O
Wang O
, O
Heng O
Yu O
, O
Yue O
Zhang O
, O
Zhongqiang O
Huang O
, O
Weihua O
Luo O
, O
Xiangyu O
Duan O
, O
and O
Min O
Zhang O
. O

2020 O
. O

Alignment O
- O
enhanced O
trans- O
former O
for O
constraining O
nmt O
with O
pre O
- O
speciﬁed O
trans- O
lations O
. O

In O
Proceedings O
of O
the O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
volume O
34 O
, O
pages O
8886–8893 O
. O

Elias O
Stengel O
- O
Eskin O
, O
Tzu O
- O
ray O
Su O
, O
Matt O
Post O
, O
and O
Ben- O
jamin O
Van O
Durme O
. O

2019 O
. O

A O
discriminative O
neural O
model O
for O
cross O
- O
lingual O
word O
alignment O
. O

In O
Proceed- O
ings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
Interna- O
tional O
Joint O
Conference O
on O
Natural O
Language O
Pro- O
cessing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
910–920 O
, O
Hong O
Kong O
, O
China O
. O

Association O
for O
Computational O
Lin- O
guistics O
. O

Akihiro O
Tamura O
, O
Taro O
Watanabe O
, O
and O
Eiichiro O
Sumita O
. O

2014 O
. O

Recurrent O
neural O
networks O
for O
word O
align- O
ment O
model O
. O

In O
Proceedings O
of O
the O
52nd O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1470 O
– O
1480 O
, O
Baltimore O
, O
Maryland O
. O

Association O
for O
Compu- O
tational O
Linguistics O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
Łukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
Advances O
in O
neural O
information O
pro- O
cessing O
systems O
, O
pages O
5998–6008 O
. O

Nan O
Yang O
, O
Shujie O
Liu O
, O
Mu O
Li O
, O
Ming O
Zhou O
, O
and O
Neng- O
hai O
Yu O
. O

2013 O
. O

Word O
alignment O
modeling O
with O
con- O
text O
dependent O
deep O
neural O
network O
. O

In O
Proceed- O
ings O
of O
the O
51st O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Pa- O
pers O
) O
, O
pages O
166–175 O
, O
Soﬁa O
, O
Bulgaria O
. O

Association O
for O
Computational O
Linguistics O
. O

Thomas O
Zenkel O
, O
Joern O
Wuebker O
, O
and O
John O
DeNero O
. O
2019 O
. O

Adding O
interpretable O
attention O
to O
neural O
trans- O
lation O
models O
improves O
word O
alignment O
. O

arXiv O
preprint O
arXiv:1901.11359 O
. O

Thomas O
Zenkel O
, O
Joern O
Wuebker O
, O
and O
John O
DeNero O
. O
2020 O
. O

End O
- O
to O
- O
end O
neural O
word O
alignment O
outper- O
forms O
GIZA++ O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
, O
pages O
1605–1617 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

