Proceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
5795–5807 O
August O
1–6 O
, O
2021 O
. O

© O
2021 O
Association O
for O
Computational O
Linguistics5795A O
Conditional O
Splitting O
Framework O
for O
Efﬁcient O
Constituency O
Parsing O
Thanh O
- O
Tung O
Nguyeny O
{ O
, O
Xuan O
- O
Phi O
Nguyeny O
{ O
, O
Shaﬁq O
Joty{x O
, O
Xiaoli O
Liy O
{ O
{ O
Nanyang O
Technological O
University O
xSalesforce O
Research O
Asia O
yInstitute O
for O
Infocomm O
Research O
, O
A O
- O
STAR O
Singapore O
{ O
ng0155ng@e.;nguyenxu002@e.;srjoty@}ntu.edu.sg O

xlli@i2r.a-star.edu.sg O
Abstract O
We O
introduce O
a O
generic O
seq2seq O
parsing O
frame- O
work O
that O
casts O
constituency O
parsing O
problems O
( O
syntactic O
and O
discourse O
parsing O
) O
into O
a O
series O
of O
conditional O
splitting O
decisions O
. O

Our O
pars- O
ing O
model O
estimates O
the O
conditional O
probabil- O
ity O
distribution O
of O
possible O
splitting O
points O
in O
a O
given O
text O
span O
and O
supports O
efﬁcient O
top- O
down O
decoding O
, O
which O
is O
linear O
in O
number O
of O
nodes O
. O

The O
conditional O
splitting O
formulation O
together O
with O
efﬁcient O
beam O
search O
inference O
facilitate O
structural O
consistency O
without O
rely- O
ing O
on O
expensive O
structured O
inference O
. O

Cru- O
cially O
, O
for O
discourse O
analysis O
we O
show O
that O
in O
our O
formulation O
, O
discourse O
segmentation O
can O
be O
framed O
as O
a O
special O
case O
of O
parsing O
which O
allows O
us O
to O
perform O
discourse O
parsing O
without O
requiring O
segmentation O
as O
a O
pre O
- O
requisite O
. O

Ex- O
periments O
show O
that O
our O
model O
achieves O
good O
results O
on O
the O
standard O
syntactic O
parsing O
tasks O
under O
settings O
with O
/ O
without O
pre O
- O
trained O
repre- O
sentations O
and O
rivals O
state O
- O
of O
- O
the O
- O
art O
( O
SoTA O
) O
methods O
that O
are O
more O
computationally O
ex- O
pensive O
than O
ours O
. O

In O
discourse O
parsing O
, O
our O
method O
outperforms O
SoTA O
by O
a O
good O
margin O
. O

1 O
Introduction O
A O
number O
of O
formalisms O
have O
been O
introduced O
to O
analyze O
natural O
language O
at O
different O
linguistic O
lev- O
els O
. O

This O
includes O
syntactic O
structures O
in O
the O
form O
of O
phrasal O
and O
dependency O
trees O
, O
semantic O
struc- O
tures O
in O
the O
form O
of O
meaning O
representations O
( O
Ba- O
narescu O
et O
al O
. O
, O
2013 O
; O
Artzi O
et O

al O
. O
, O
2013 O
) O
, O
and O
dis- O
course O
structures O
with O
Rhetorical O
Structure O
Theory O
( O
RST O
) O
( O
Mann O
and O
Thompson O
, O
1988 O
) O
or O
Discourse- O
LTAG O
( O
Webber O
, O
2004 O
) O
. O

Many O
of O
these O
formalisms O
have O
a O
constituency O
structure O
, O
where O
textual O
units O
( O
e.g. O
, O
phrases O
, O
sentences O
) O
are O
organized O
into O
nested O
constituents O
. O

For O
example O
, O
Figure O
1 O
shows O
exam- O
ples O
of O
a O
phrase O
structure O
tree O
and O
a O
sentence O
- O
level O
discourse O
tree O
( O
RST O
) O
that O
respectively O
represent O
how O
the O
phrases O
and O
clauses O
are O
hierarchically O
or O
- O
ganized O
into O
a O
constituency O
structure O
. O

Developing O
efﬁcient O
and O
effective O
parsing O
solutions O
has O
always O
been O
a O
key O
focus O
in O
NLP O
. O

In O
this O
work O
, O
we O
consider O
both O
phrasal O
( O
syntactic O
) O
and O
discourse O
parsing O
. O

In O
recent O
years O
, O
neural O
end O
- O
to O
- O
end O
parsing O
meth- O
ods O
have O
outperformed O
traditional O
methods O
that O
use O
grammar O
, O
lexicon O
and O
hand O
- O
crafted O
features O
. O

These O
methods O
can O
be O
broadly O
categorized O
based O
on O
whether O
they O
employ O
a O
greedy O
transition O
- O
based O
, O
a O
globally O
optimized O
chart O
parsing O
or O
a O
greedy O
top- O
down O
algorithm O
. O

Transition O
- O
based O
parsers O
( O
Dyer O
et O
al O
. O
, O
2016 O
; O
Cross O
and O
Huang O
, O
2016 O
; O
Liu O
and O
Zhang O
, O
2017 O
; O
Wang O
et O
al O
. O
, O
2017 O
) O
generate O
trees O
auto O
- O
regressively O
as O
a O
form O
of O
shift O
- O
reduce O
deci- O
sions O
. O

Though O
computationally O
attractive O
, O
the O
local O
decisions O
made O
at O
each O
step O
may O
propagate O
errors O
to O
subsequent O
steps O
due O
to O
exposure O
bias O
( O
Bengio O
et O
al O
. O
, O
2015 O
) O
. O

Moreover O
, O
there O
may O
be O
mismatches O
in O
shift O
and O
reduce O
steps O
, O
resulting O
in O
invalid O
trees O
. O

Chart O
based O
methods O
, O
on O
the O
other O
hand O
, O
train O
neural O
scoring O
functions O
to O
model O
the O
tree O
structure O
globally O
( O
Durrett O
and O
Klein O
, O
2015 O
; O
Gaddy O
et O
al O
. O
, O
2018 O
; O
Kitaev O
and O
Klein O
, O
2018 O
; O
Zhang O
et O
al O
. O
, O
2020b O
; O
Joty O
et O
al O
. O
, O
2012 O
, O
2013 O
) O
. O

By O
utilizing O
dynamic O
pro- O
gramming O
, O
these O
methods O
can O
perform O
exact O
in- O

ference O
to O
combine O
these O
constituent O
scores O
into O
ﬁnding O
the O
highest O
probable O
tree O
. O

However O
, O
they O
are O
generally O
slow O
with O
at O
least O
O(n3)time O
com- O
plexity O
. O

Greedy O
top O
- O
down O
parsers O
ﬁnd O
the O
split O
points O
recursively O
and O
have O
received O
much O
atten- O
tion O
lately O
due O
to O
their O
efﬁciency O
, O
which O
is O
usually O
O(n2)(Stern O
et O
al O
. O
, O
2017a O
; O
Shen O
et O
al O
. O
, O
2018 O
; O
Lin O
et O
al O
. O
, O
2019 O
; O
Nguyen O
et O
al O
. O
, O
2020 O
) O
. O

However O
, O
they O
still O
suffer O
from O
exposure O
bias O
, O
where O
one O
incorrect O
splitting O
step O
may O
affect O
subsequent O
steps O
. O

Discourse O
parsing O
in O
RST O
requires O
an O
addi- O
tional O
step O
– O
discourse O
segmentation O
which O
in- O
volves O
breaking O
the O
text O
into O
contiguous O
clause O
- O
like O
units O
called O
Elementary O
Discourse O
Units O
or O
EDUs O
( O
Figure O
1 O
) O
. O

Traditionally O
, O
segmentation O
has O
been O

5796considered O
separately O
and O
as O
a O
prerequisite O
step O
for O
the O
parsing O
task O
which O
links O
the O
EDUs O
( O
and O
larger O
spans O
) O
into O
a O
discourse O
tree O
( O
Soricut O
and O
Marcu O
, O
2003 O
; O
Joty O
et O
al O
. O
, O
2012 O
; O
Wang O
et O
al O
. O
, O
2017 O
) O
. O

In O
this O
way O
, O
the O
errors O
in O
discourse O
segmentation O
can O
prop- O
agate O
to O
discourse O
parsing O
( O
Lin O
et O
al O
. O
, O
2019 O
) O
. O

In O
this O
paper O
, O
we O
propose O
a O
generic O
top O
- O
down O
neural O
framework O
for O
constituency O
parsing O
that O
we O
validate O
on O
both O
syntactic O
and O
sentence O
- O
level O
dis- O
course O
parsing O
. O

Our O
main O
contributions O
are O
: O
•We O
cast O
the O
constituency O
parsing O
task O
into O
a O
se- O
ries O
of O
conditional O
splitting O
decisions O
and O
use O
a O
seq2seq O
architecture O
to O
model O
the O
splitting O
decision O
at O
each O
decoding O
step O
. O

Our O
parsing O
model O
, O
which O
is O
an O
instance O
of O
a O
Pointer O
Network O
( O
Vinyals O
et O
al O
. O
, O
2015a O
) O
, O
estimates O
the O
pointing O
score O
from O
a O
span O
to O
a O
splitting O
boundary O
point O
, O
representing O
the O
likelihood O
that O
the O
span O
will O
be O
split O
at O
that O
point O
and O
create O
two O
child O
spans O
. O

•The O
conditional O
probabilities O
of O
the O
splitting O
deci- O
sions O
are O
optimized O
using O
a O
cross O
entropy O
loss O
and O
structural O
consistency O
is O
maintained O
through O
a O
global O
pointing O
mechanism O
. O

The O
training O
process O
can O
be O
fully O
parallelized O
without O
requiring O
struc- O
tured O
inference O
as O
in O
( O
Shen O
et O
al O
. O
, O
2018 O
; O
Gómez O
and O
Vilares O
, O
2018 O
; O
Nguyen O
et O
al O
. O
, O
2020 O
) O
. O

•Our O
model O
enables O
efﬁcient O
top O
- O
down O
decoding O
withO(n)running O
time O
like O
transition O
- O
based O
parsers O
, O
while O
also O
supporting O
a O
customized O
beam O
search O
to O
get O
the O
best O
tree O
by O
searching O
through O
a O
reasonable O
search O
space O
of O
high O
scoring O
trees O
. O

The O
beam O
- O
search O
inference O
along O
with O
the O
struc- O
tural O
consistency O
from O
the O
modeling O
makes O
our O
approach O
competitive O
with O
existing O
structured O
chart O
methods O
for O
syntactic O
( O
Kitaev O
and O
Klein O
, O
2018 O
) O
and O
discourse O
parsing O
( O
Zhang O
et O
al O
. O
, O
2020b O
) O
. O

Moreover O
, O
our O
parser O
does O
not O
rely O
on O
any O
hand- O
crafted O
features O
( O
not O
even O
part O
- O
of O
- O
speech O
tags O
) O
, O
which O
makes O
it O
more O
efﬁcient O
and O
be O
ﬂexible O
to O
different O
domains O
or O
languages O
. O

•For O
discourse O
analysis O
, O
we O
demonstrate O
that O
our O
method O
can O
effectively O
ﬁnd O
the O
segments O
( O
EDUs O
) O
by O
simply O
performing O
one O
additional O
step O
in O
the O
top O
- O
down O
parsing O
process O
. O

In O
other O
words O
, O
our O
method O
can O
parse O
a O
text O
into O
the O
discourse O
tree O
without O
needing O
discourse O
segmentation O
as O
a O
pre- O
requisite O
; O
instead O
, O
it O
produces O
the O
segments O
as O
a O
by O
- O
product O
. O

To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
ﬁrst O
model O
that O
can O
perform O
segmentation O
and O
parsing O
in O
a O
single O
embedded O
framework O
. O

In O
the O
experiments O
with O
English O
Penn O
Tree- O
bank O
, O
our O
model O
without O
pre O
- O
trained O
representa- O
tions O
achieves O
93.8 O
F1 O
, O
outperforming O
all O
exist- O
ing O
methods O
with O
similar O
time O
complexity O
. O

With O
pre O
- O
training O
, O
our O
model O
pushes O
the O
F1 O
score O
to O
95.7 O
, O
which O
is O
on O
par O
with O
the O
SoTA O
while O
sup- O
porting O
faster O
decoding O
with O
a O
speed O
of O
over O
1,100 O
sentences O
per O
second O
( O
fastest O
so O
far O
) O
. O

Our O
model O
also O
performs O
competitively O
with O
SoTA O
methods O
on O
the O
multilingual O
parsing O
tasks O
in O
the O
SPMRL O
2013/2014 O
shared O
tasks O
. O

In O
discourse O
parsing O
, O
our O
method O
establishes O
a O
new O
SoTA O
in O
end O
- O
to O
- O
end O
sentence O
- O
level O
parsing O
performance O
on O
the O
RST O
Discourse O
Treebank O
with O
an O
F1 O
score O
of O
78.82 O
. O

We O
make O
our O
code O
available O
at O
https://ntunlpsg.github.io/project/condition- O
constituency O
- O
style O
- O
parser/ O
2 O
Parsing O
as O
a O
Splitting O
Problem O
Constituency O
parsing O
( O
both O
syntactic O
and O
discourse O
) O
can O
be O
considered O
as O
the O
problem O
of O
ﬁnding O
a O
set O
of O
labeled O
spans O
over O
the O
input O
text O
( O
Stern O
et O
al O
. O
, O
2017a O
) O
. O

Let O
S(T)denote O
the O
set O
of O
labeled O
spans O
for O
a O
parse O
tree O
T O
, O
which O
can O
formally O
be O
expressed O
as O
( O
excluding O
the O
trivial O
singleton O
span O
layer O
): O
S(T O
) O
: O

= O
f((it;jt);lt)gjS(T)j O
t=1 O
forit O
< O
jt O
( O
1 O
) O
whereltis O
the O
label O
of O
the O
text O
span O
( O
it;jt)encom- O
passing O
tokens O
from O
index O
itto O
indexjt O
. O

Previous O
approaches O
to O
syntactic O
parsing O
( O
Stern O
et O
al O
. O
, O
2017a O
; O
Kitaev O
and O
Klein O
, O
2018 O
; O
Nguyen O
et O
al O
. O
, O
2020 O
) O
train O
a O
neural O
model O
to O
score O
each O
possible O
span O
and O
then O
apply O
a O
greedy O
or O
dynamic O
program- O
ming O
algorithm O
to O
ﬁnd O
the O
parse O
tree O
. O

In O
other O
words O
, O
these O
methods O
are O
span O
- O
based O
formulation O
. O

In O
contrary O
, O
we O
formulate O
constituency O
parsing O
as O
the O
problem O
of O
ﬁnding O
the O
splitting O
points O
in O
a O
recursive O
, O
top O
- O
down O
manner O
. O

For O
each O
parent O
node O
in O
a O
tree O
that O
spans O
over O
( O
i;j O
) O
, O
our O
parsing O
model O
is O
trained O
to O
point O
to O
the O
boundary O
between O
the O
tokens O
atkandk+1positions O
to O
split O
the O
parent O
span O
into O
two O
child O
spans O
( O
i;k)and(k+ O
1;j O
) O
. O

This O
is O
done O
through O
the O
Pointing O
mechanism O
( O
Vinyals O
et O
al O
. O
, O
2015a O
) O
, O
where O
each O
splitting O
decision O
is O
modeled O
as O
a O
multinomial O
distribution O
over O
the O
input O
elements O
, O
which O
in O
our O
case O
are O
the O
token O
boundaries O
. O

The O
correspondence O
between O
token- O
and O
boundary O
- O
based O
representations O
of O
a O
tree O
is O
straight- O
forward O
. O

After O
including O
the O
start O
( O
< O
sos O
> O
) O
and O
end O
( O
< O
eos O
> O
) O
tokens O
, O
the O
token O
- O
based O
span O
( O
i;j O
) O
is O
equivalent O
to O
the O
boundary O
- O
based O
span O
( O
i 1;j O
) O

5797 O
Labeled O
span O
representation O
S(T O
) O
= O
{ O
( O
( O
1 O
, O
5 O
) O
, O
S O
) O
, O
( O
( O
2 O
, O
5 O
) O
, O
? O
) O
, O
( O
( O
2 O
, O
4 O
) O
, O
VP O
) O
, O
( O
( O
3 O
, O
4 O
) O
, O
S O
- O
VP O
) O
} O
Boundary O
- O
based O
splitting O
representation O
C(T O
) O
= O
{ O
( O
0;5))1,(1;5))4,(1;4))2,(2;4))3 O
} O
Labeled O
span O
representation O
S(DT O
) O
= O
{ O
( O
( O
1 O
, O
8 O
, O
11 O
) O
, O
Same O
- O
Unit O
NN O
) O
, O
( O
( O
1 O
, O
5 O
, O
8) O
, O
Elaboration O
NS O
) O
} O
Boundary O
- O
based O
splitting O
representation O
C(DT O
) O
= O
{ O
( O
0;11 O
) O
) O
8,(0;8))5,(0;5))5,(5;8))8,(8;11))11 O
} O
Figure O
1 O
: O
A O
syntactic O
tree O
at O
the O
left O
and O
a O
discourse O
tree O
( O
DT O
) O
at O
the O
right O
; O
both O
have O
a O
constituency O
structure O
. O

The O
internal O
nodes O
in O
the O
discourse O
tree O
( O
Elaboration O
, O
Same O
- O
Unit O
) O
represent O
coherence O
relations O
and O
the O
edge O
labels O
indicate O
the O
nuclearity O
statuses O
( O
‘ O
N O
’ O
for O
Nucleus O
and O
‘S O
’ O
for O
Satellite O
) O
of O
the O
child O
spans O
. O

Below O
the O
tree O
, O
we O
show O
the O
labeled O
span O
and O
splitting O
representations O
. O

The O
bold O
splits O
in O
the O
DT O
representation O
( O
C(DT O
) O
) O
indicate O
the O
end O
of O
further O
splitting O
into O
smaller O
spans O
( O
i.e. O
,they O
are O
EDUs O
) O
. O

and O
the O
boundary O
between O
i O
- O
th O
and O
( O
i+1)-th O
tokens O
is O
indexed O
as O
i. O

For O
example O
, O
the O
( O
boundary O
- O
based O
) O
span O
“ O
enjoys O
playing O
tennis O
” O
in O
Figure O
1 O
is O
deﬁned O
as(1;4 O
) O
. O

Similarly O
, O
the O
boundary O
between O
the O
to- O
kens O
“ O
enjoys O
” O
and O
“ O
playing O
” O
is O
indexed O
with O
2.1 O
Following O
the O
common O
practice O
in O
syntactic O
pars- O
ing O
, O
we O
binarize O
the O
n O
- O
ary O
tree O
by O
introducing O
a O
dummy O
label O
? O
. O

We O
also O
collapsed O
the O
nested O
la- O
beled O
spans O
in O
the O
unary O
chains O
into O
unique O
atomic O
labels O
, O
such O
as O
S O
- O
VP O
in O
Figure O
1 O
. O

Every O
span O
repre- O
sents O
an O
internal O
node O
in O
the O
tree O
, O
which O
has O
a O
left O
and O
a O
right O
child O
. O

Therefore O
, O
we O
can O
represent O
each O
internal O
node O
by O
its O
split O
into O
left O
and O
right O
chil- O
dren O
. O

Based O
on O
this O
, O
we O
deﬁne O
the O
set O
of O
splitting O
decisions O
C(T)for O
a O
syntactic O
tree O
Tas O
follows O
. O

Proposition O
1 O

A O
binary O
syntactic O
tree O
Tof O
a O
sen- O
tence O
containing O
ntokens O
can O
be O
transformed O
into O
a O
set O
of O
splitting O
decisions O
C(T O
) O

= O
f(i;j))k O
: O
i O
< O

k O
< O
jgsuch O
that O
the O
parent O
span O
( O
i;j)is O
split O
into O
two O
child O
spans O
( O
i;k)and(k;j O
) O
. O

An O
example O
of O
the O
splitting O
representation O
of O
a O
tree O
is O
shown O
in O
Figure O
1 O
( O
without O
the O
node O
labels O
) O
. O

Note O
that O
our O
transformed O
representation O
has O
a O
one O
- O
to- O
one O
mapping O
with O
the O
tree O
since O
each O
splitting O
de- O
cision O
corresponds O
to O
one O
and O
only O
one O
internal O
node O
in O
the O
tree O
. O

We O
follow O
a O
depth-ﬁrst O
order O
of O
the O
decision O
sequence O
, O
which O
in O
our O
preliminary O
experiments O
showed O
more O
consistent O
performance O
than O
other O
alternatives O
like O
breadth-ﬁrst O
order O
. O

Extension O
to O
End O
- O
to O
- O
End O
Discourse O
Parsing O
Note O
that O
in O
syntactic O
parsing O
, O
the O
split O
position O
1We O
use O
the O
same O
example O
from O
( O
Stern O
et O
al O
. O
, O
2017a O
; O
Shen O
et O
al O
. O
, O
2018 O
; O
Nguyen O
et O
al O
. O
, O
2020 O
) O
to O
distinguish O
the O
differences O
between O
the O
methods.must O
be O
within O
the O
span O
but O
not O
at O
its O
edge O
, O

that O
is O
, O
kmust O
satisfy O
i O
< O

k O
< O
j O
for O
each O
boundary O
span O
( O
i;j O
) O
. O

Otherwise O
, O
it O
will O
not O
produce O
valid O
sub O
- O
trees O
. O

In O
this O
case O
, O
we O
keep O
splitting O
until O
each O
span O
contains O
a O
single O
leaf O
token O
. O

However O
, O
for O
discourse O
trees O
, O
each O
leaf O
is O
an O
EDU O
– O
a O
clause O
- O
like O
unit O
that O
can O
contain O
one O
or O
multiple O
tokens O
. O

Unlike O
previous O
studies O
which O
assume O
discourse O
segmentation O
as O
a O
pre O
- O
processing O
step O
, O
we O
propose O
a O
uniﬁed O
formulation O
that O
treats O
segmentation O
as O
one O
additional O
step O
in O
the O
top O
- O
down O
parsing O
process O
. O

To O
accommodate O
this O
, O
we O
relax O
Proposition O
1 O
as O
: O
Proposition O
2 O
A O
binary O
discourse O
treeDT O
of O
a O
text O
containing O
ntokens O
can O
be O
transformed O
into O
a O
set O
of O
splitting O
decisions O
C(DT O
) O

= O
f(i;j))k O
: O
i O
< O
kjgsuch O
that O
the O
parent O
span O
( O
i;j)gets O
split O
into O
two O
child O
spans O
( O
i;k)and(k;j)fork O
< O
j O
or O
a O
terminal O
span O
or O
EDU O
for O
k O
= O
j(end O
of O
splitting O
the O
span O
further O
) O
. O

We O
illustrate O
it O
with O
the O
DT O
example O
in O
Figure O
1 O
. O

Each O
splitting O
decision O
in O
C(DT)represents O
ei- O
ther O

the O
splitting O
of O
the O
parent O
span O
into O
two O
child O
spans O
( O
when O
the O
splitting O
point O
is O
strictly O
within O
the O
span O
) O
or O
the O
end O
of O
any O
further O
splitting O
( O
when O
the O
splitting O
point O
is O
the O
right O
endpoint O
of O
the O
span O
) O
. O

By O
making O
this O
simple O
relaxation O
, O
our O
formulation O
can O
not O
only O
generate O
the O
discourse O
tree O
( O
in O
the O
for- O
mer O
case O
) O
but O
can O
also O
ﬁnd O
the O
discourse O
segments O
( O
EDUs O
) O
as O
a O
by O
- O
product O
( O
in O
the O
latter O
case O
) O
. O

3 O
Seq2Seq O
Parsing O
Framework O
LetC(T)andL(T)respectively O
denote O
the O
struc- O
ture O
( O
in O
split O
representation O
) O
and O
labels O
of O
a O
tree O
T O
( O
syntactic O
or O
discourse O
) O
for O
a O
given O
text O
x. O

We O
can O
express O
the O
probability O
of O
the O
tree O
as O
: O

5798 O
Figure O
2 O
: O
Our O
syntatic O
parser O
along O
with O
the O
decoding O
process O
for O
a O
given O
sentence O
. O

The O
input O
to O
the O
decoder O
at O
each O
step O
is O
the O
representation O
of O
the O
span O
to O
be O
split O
. O

We O
predict O
the O
splitting O
point O
using O
a O
biafﬁne O
function O
between O
the O
corresponding O
decoder O
state O
and O
the O
boundary O
- O
based O
encoder O
representations O
. O

A O
label O
classiﬁer O
is O
used O
to O
assign O
labels O
to O
the O
left O
and O
right O
spans O
. O

P(Tjx O
) O

= O
P(L(T);C(T)jx O
) O

= O
P(L(T)jC(T);x)P(C(T)jx)(2 O
) O

This O
factorization O
allows O
us O
to O
ﬁrst O
infer O
the O
tree O
structure O
from O
the O
input O
text O
, O
and O
then O
ﬁnd O
the O
cor- O
responding O
labels O
. O

As O
discussed O
in O
the O
previous O
section O
, O
we O
consider O
the O
structure O
prediction O
as O
a O
sequence O
of O
splitting O
decisions O
to O
generate O
the O
tree O
in O
a O
top O
- O
down O
manner O
. O

Speciﬁcally O
, O
at O
each O
de- O
coding O
stept O
, O
the O
outputytrepresents O
the O
splitting O
decision O
( O
it;jt))ktandy O
< O
trepresents O
the O
previ- O
ous O
splitting O
decisions O
. O

Thus O
, O
we O
can O
express O
the O
probability O
of O
the O
tree O
structure O
as O
follows O
: O
P(C(T)jx O
) O
= O
Y O
yt2C(T)P(ytjy O
< O
t;x O
) O
= O
jC(T)jY O
t=1P((it;jt))ktj((i;j))k)<t;x)(3 O
) O

This O
can O
effectively O
be O
modeled O
within O
a O
Seq2Seq O
pointing O
framework O
as O
shown O
in O
Figure O
2 O
. O

At O
each O
stept O
, O
the O
decoder O
autoregressively O
predicts O
the O
split O
pointktin O
the O
input O
by O
conditioning O
on O
the O
current O
input O
span O
( O
it;jt)and O
previous O
splitting O
decisions O
( O
i;j))k)<t O
. O

This O
conditional O
splitting O
formulation O
( O
decision O
at O
step O
tdepends O
on O
previous O
steps O
) O
can O
help O
our O
model O
to O
ﬁnd O
better O
trees O
com- O
pared O
to O
non O
- O
conditional O
top O
- O
down O
parsers O
( O
Stern O
et O
al O
. O
, O
2017a O
; O

Shen O
et O
al O
. O
, O
2018 O
; O
Nguyen O
et O
al O
. O
, O
2020 O
) O
, O
thus O
bridging O
the O
gap O
between O
the O
global O
( O
but O
ex- O
pensive O
) O
and O
the O
local O
( O
but O
efﬁcient O
) O
models O
. O

The O
labelsL(T)can O
be O
modeled O
by O
using O
a O
label O
clas- O
siﬁer O
, O
as O
described O
later O
in O
the O
next O
section.3.1 O
Model O
Architecture O

We O
now O
describe O
the O
components O
of O
our O
parsing O
model O
: O
the O
sentence O
encoder O
, O
the O
span O
representa- O
tion O
, O
the O
pointing O
model O
and O
the O
labeling O
model O
. O

Sentence O
Encoder O
Given O
an O
input O
sequence O
of O
n O
tokens O
x= O

( O
x1;:::;x O
n O
) O
, O
we O
ﬁrst O
add O
< O
sos O
> O
and O
< O
eos O
> O
markers O
to O
the O
sequence O
. O

After O
that O
, O
each O
tokentin O
the O
sequence O
is O
mapped O
into O
its O
dense O
vector O
representation O
etas O
et= O
[ O
echar O
t;eword O
t O
] O
( O
4 O
) O
where O
echar O
t O
, O
eword O
t O
are O
respectively O
the O
character O
and O
word O
embeddings O
of O
token O
t. O
Similar O
to O
( O
Ki- O
taev O
and O
Klein O
, O
2018 O
; O
Nguyen O
et O
al O
. O
, O
2020 O
) O
, O
we O
use O
a O
character O
LSTM O
to O
compute O
the O
character O
embed- O
ding O
of O
a O
token O
. O

We O
experiment O
with O
both O
randomly O
initialized O
and O
pretrained O
token O
embeddings O
. O

When O
pretrained O
embedding O
is O
used O
, O
the O
character O
embed- O
ding O
is O
replaced O
by O
the O
pretrained O
token O
embedding O
. O

The O
token O
representations O
are O
then O
passed O
to O
a O
3- O
layer O
Bi O
- O
LSTM O
encoder O
to O
obtain O
their O
contextual O
representations O
. O

In O
the O
experiments O
, O
we O
ﬁnd O
that O
even O
without O
the O
POS O
- O
tags O
, O
our O
model O
performs O
competitively O
with O
other O
baselines O
that O
use O
them O
. O

Boundary O
and O
Span O
Representations O
To O
repre- O
sent O
each O
boundary O
between O
positions O
kandk+ O
1 O
, O
we O
use O
the O
fencepost O
representation O
( O
Cross O
and O
Huang O
, O
2016 O
; O
Stern O
et O
al O
. O
, O
2017a O
): O

hk= O

[ O
fk;bk+1 O
] O
( O
5 O
) O
where O
fkandbk+1are O
the O
forward O
and O
backward O
LSTM O
hidden O
vectors O
at O
positions O
kandk+ O
1 O
, O
re- O

5799 O
Figure O
3 O
: O
Illustration O
of O
our O
boundary O
- O
based O
span O
encoder O
. O

Here O
we O
have O
shown O
the O
representation O
for O
the O
boundary O
at O
1 O
and O
the O
representation O
of O
the O
boundary O
- O
based O
span O
( O
0;5)that O
corresponds O
to O
the O
sentence O
“ O
She O
enjoys O
playing O
tennis O
. O
” O
. O

spectively O
. O

To O
represent O
the O
span O
( O
i;j O
) O
, O
we O
compute O
a O
linear O
combination O
of O
the O
two O
endpoints O
hi;j O
= O
W1hi+W2hj O
( O
6 O
) O
This O
span O
representation O
will O
be O
used O
as O
input O
to O
the O
decoder O
. O

Figure O
3 O
shows O
the O
boundary O
- O
based O
span O
representations O
for O
our O
example O
. O

The O
Decoder O
Our O
model O
uses O
a O
unidirectional O
LSTM O
as O
the O
decoder O
. O

At O
each O
decoding O
step O
t O
, O
the O
decoder O
takes O
as O
input O
the O
corresponding O
span O
( O
i;j)(speciﬁcally O
, O
hi;j O
) O
and O
its O
previous O
state O
dt 1 O
to O
generate O
the O
current O
state O
dtand O
then O
apply O
a O
biafﬁne O
function O
( O
Dozat O
and O
Manning O
, O
2017 O
) O
be- O
tween O
dtandallof O
the O
encoded O
boundary O
represen- O
tations O
( O
h0;h1;:::;hn)as O
follows O
: O
d0 O
t O
= O
MLPd(dt)h0 O
i O
= O
MLPh(hi O
) O
( O
7 O
) O
st;i O
= O
d0 O
tTWdhh0 O
i+h0 O
iTwh O
( O
8) O
at;i O
= O
exp(st;i)Pn O
i=1exp(st;i)(9 O
) O
where O
each O
MLP O
operation O
includes O
a O
linear O
trans- O
formation O
with O
LeakyReLU O
activation O
to O
transform O
dandhinto O
equal O
- O
sized O
vectors O
, O
and O
Wdh2 O

I O
Rddandwh2I O
Rdare O
respectively O
the O
weight O
matrix O
and O
weight O
vector O
for O
the O
biafﬁne O
func- O
tion O
. O

The O
biafﬁne O
scores O
are O
then O
passed O
through O
a O
softmax O
layer O
to O
acquire O
the O
pointing O
distribution O
at2[0;1]nfor O
the O
splitting O
decision O
. O

When O
decoding O
the O
tree O
during O
inference O
, O
at O
each O
step O
we O
only O
examine O
the O
‘ O
valid O
’ O
splitting O
points O
betweeniandj O
– O
for O
syntactic O
parsing O
, O
it O
is O
i O
< O
k O
< O
j O
and O
for O
discourse O
parsing O
, O
it O
is O
i O
< O
kj O
. O

Label O
Classiﬁer O
For O
syntactic O
parsing O
, O
we O
per- O
form O
the O
label O
assignments O
for O
a O
span O
( O
i;j)as O
: O

hl O
i O
= O
MLPl(hi);hr O
j O
= O
MLPr(hj O
) O
( O
10 O
) O
P(lji;j O
) O
= O
softmax O
( O
( O
hl O
i)TWlrhr O

j O
+ O
( O
hl O
i)TWl+ O
( O
hr O
j)TWr+b O
) O
( O
11 O
) O
li;j= O
arg O
max O
l2LP(lji;j O
) O

( O
12 O
) O
where O
each O
of O
MLP O
land O
MLP O
rincludes O
a O
lin- O
ear O
transformation O
with O
LeakyReLU O
activations O
to O
transform O
the O
left O
and O
right O
spans O
into O
equal O
- O
sized O
vectors O
, O
and O
Wlr2I O
RdLd;Wl2I O
RdL;Wr2 O

I O
RdLare O
the O
weights O
and O
bis O
a O
bias O
vector O
with O
Lbeing O
the O
number O
of O
phrasal O
labels O
. O

For O
discourse O
parsing O
, O
we O
perform O
label O
assign- O
ment O
after O
every O
split O
decision O
since O
the O
label O
here O
represents O
the O
relation O
between O
the O
child O
spans O
. O

Speciﬁcally O
, O
as O
we O
split O
a O
span O
( O
i;j)into O
two O
child O
spans O
( O
i;k)and(k;j O
) O
, O
we O
determine O
the O
relation O
label O
as O
the O
following O
. O

hl O
ik O
= O
MLPl([hi;hk]);hr O
kj O
= O
MLPr([hk;hj])(13 O
) O
P(lj(i;k);(k;j O
) O
) O

= O
softmax O
( O
( O
hl O
ik)TWlrhr O
kj O
+ O
( O
hl O
ik)TWl+ O
( O
hr O
kj)TWr+b)(14 O
) O
l(i;k);(k;j)= O
arg O
max O
l2LP(lj(i;k);(k;j O
) O
) O
( O
15 O
) O
where O
MLP O
l;MLP O
r O
, O
Wlr;Wl;Wr;bare O
similarly O
deﬁned O
. O

Training O
Objective O
The O
total O
loss O
is O
simply O
the O
sum O
of O
the O
cross O
entropy O
losses O
for O
predicting O
the O
structure O
( O
split O
decisions O
) O
and O
the O
labels O
: O
Ltotal( O
) O
= O
Lsplit(e;d O
) O
+ O
Llabel(e;label O
) O
( O
16 O
) O
where=fe;d;labelgdenotes O
the O
overall O
model O
parameters O
, O
which O
includes O
the O
encoder O
pa- O
rameterseshared O
by O
all O
components O
, O
parameters O
for O
splitting O
dand O
parameters O
for O
labeling O
label O
. O

3.2 O
Top O
- O
Down O
Beam O
- O
Search O
Inference O
As O
mentioned O
, O
existing O
top O
- O
down O
syntactic O
parsers O
do O
not O
consider O
the O
decoding O
history O
. O

They O
also O
per- O
form O
greedy O
inference O
. O

With O
our O
conditional O
split- O
ting O
formulation O
, O
our O
method O
can O
not O
only O
model O
the O
splitting O
history O
but O
also O
enhance O
the O
search O
space O
of O
high O
scoring O
trees O
through O
beam O
search O
. O

At O
each O
step O
, O
our O
decoder O
points O
to O
allthe O
en- O
coded O
boundary O
representations O
which O
ensures O
that O
the O
pointing O
scores O
are O
in O
the O
same O
scale O
, O
allow- O
ing O
a O
fair O
comparison O
between O
the O
total O
scores O
of O
all O
candidate O
subtrees O
. O

With O
these O
uniform O
scores O
, O
we O
could O
apply O
a O
beam O
search O
to O
infer O
the O
most O

5800probable O
tree O
using O
our O
model O
. O

Speciﬁcally O
, O
the O
method O
generates O
the O
tree O
in O
depth-ﬁrst O
order O
while O
maintaining O
top- O
B(beam O
size O
) O
partial O
trees O
at O
each O
step O
. O

It O
terminates O
exactly O
after O
n 1steps O
, O
which O
matches O
the O
number O
of O
internal O
nodes O
in O
the O
tree O
. O

Because O
beam O
size O
Bis O
constant O
with O
regards O
to O
the O
sequence O
length O
, O
we O
can O
omit O
it O
in O
the O
Big O
O O
notation O
. O

Therefore O
, O
each O
decoding O
step O
with O
beam O
search O
can O
be O
parallelized O
( O
O(1)complexity O
) O
using O
GPUs O
. O

This O
makes O
our O
algorithm O
run O
at O
O(n)time O
complexity O
, O
which O
is O
faster O
than O
most O
top O
- O
down O
methods O
. O

If O
we O
strictly O
use O
CPU O
, O
our O
method O
runs O
atO(n2 O
) O
, O
while O
chart O
- O
based O
parsers O
run O
at O
O(n3 O
) O
. O

Algorithm O
1 O
illustrate O
the O
syntactic O
tree O
inference O
procedure O
. O

We O
also O
propose O
a O
similar O
version O
of O
the O
inference O
algorithm O
for O
discourse O
parsing O
in O
the O
Appendix O
. O

Algorithm O
1 O
Syntactic O
Tree O
Inference O
with O
Beam O
Search O
Input O
: O
Sentence O
length O
n O
; O
beam O
width O
B O
; O
boundary O
- O
based O
encoder O
states O
: O
( O
h0;h1;:::;hn O
) O
; O
label O
scores O
: O
P(lji;j O
) O
, O
0i O
< O
jn;l2f1;:::;Lg O
, O
initial O
decoder O
state O
s. O
Output O
: O
Parse O
treeT O
1 O
: O
Ld O
= O
n 1 O
// O

Decoding O
length O
2 O
: O
beam O
= O
array O
of O
Lditems O
// O
List O
of O
empty O
beam O
items O
3 O
: O
init_tree O
= O
[ O
( O
0;n);(0;0 O
) O
; O
: O
: O
: O
; O
( O
0;0 O
) O
] O
//n 2paddings O
( O
0,0 O
) O
4 O
: O
beam[0 O
] O
= O
( O
0;s;init_tree O
) O
// O

Init O
1st O
item(log O
- O
prob O
, O
state O
, O
tree O
) O
5 O
: O
fort= O
1toLddo O
6 O
: O
for(logp;s;tree)2beam O

[ O
t 1]do O
7 O
: O
( O
i;j O
) O

= O
tree[t 1 O
] O
// O

Current O
span O
to O
split O
8 O
: O
a;s0 O
= O
decoder O
- O
step O
( O
s;hi;j)//a O
: O
split O
prob O
. O

dist O
. O

9 O
: O
for(k;pk)2top O
- O
B(a)andi O
< O
k O
< O
j O
do O
10 O
: O
curr O
- O
tree O
= O
tree O
11 O
: O
ifk O
> O
i O
+ O
1then O
12 O
: O
curr O
- O
tree O
[ O
t O
] O
= O
( O
i;k O
) O
13 O
: O
end O
if O
14 O
: O
ifj O
> O
k O

+ O
1then O
15 O
: O
curr O
- O
tree O
[ O
t+j k 1 O
] O
= O
( O
k;j O
) O
16 O
: O
end O
if O
17 O
: O
push O
( O
logp O
+ O
log(pk);s0;curr O
- O
tree O
) O
to O
beam[t O
] O
18 O
: O
end O
for O
19 O
: O
end O
for O
20 O
: O
prune O
beam[t O
] O
// O

Keep O
top- O

Bhighest O
score O
trees O
21 O
: O
end O
for O
22 O
: O
logp*;s;S= O
arg O
maxlogpbeam O

[ O
Ld O
] O
//S O
: O
best O
structure O
23 O
: O
labeled O
- O
spans O
= O
[ O
( O
i;j;arg O
maxlP(lji;j))8(i;j)2 O
S O
] O
24 O
: O
labeled O
- O
singletons O
= O
[ O
( O
i;i+ O
1;arg O
maxlP(lji;i+ O
1))fori O

= O
f0;:::;n 1 O
g O
] O
25 O
: O
T O
= O
labeled O
- O
spans[labeled O
- O
singletons O
By O
enabling O
beam O
search O
, O
our O
method O
can O
ﬁnd O
the O
best O
tree O
by O
comparing O
high O
scoring O
trees O
within O
a O
reasonable O
search O
space O
, O
making O
our O
model O
com- O
petitive O
with O
existing O
structured O
( O
globally O
) O
infer- O
ence O
methods O
that O
use O
more O
expensive O
algorithmslike O
CKY O
and/or O
larger O
models O
( O
Kitaev O
and O
Klein O
, O
2018 O
; O
Zhang O
et O
al O
. O
, O
2020b O
) O
. O

4 O
Experiment O
Datasets O
and O
Metrics O
To O
show O
the O
effectiveness O
of O
our O
approach O
, O
we O
conduct O
experiments O
on O
both O
syntactic O
and O
sentence O
- O
level O
RST O
parsing O
tasks.2 O
We O
use O
the O
standard O
Wall O
Street O
Journal O
( O
WSJ O
) O
part O
of O
the O
Penn O
Treebank O
( O
PTB O
) O
( O
Marcus O
et O
al O
. O
, O
1993 O
) O
for O
syntactic O
parsing O
and O
RST O
Discourse O
Treebank O
( O
RST O
- O
DT O
) O
( O
Lynn O
et O
al O
. O
, O
2002 O
) O
for O
discourse O
parsing O
. O

For O
syntactic O
parsing O
, O
we O
also O
experiment O
with O
the O
multilingual O
parsing O
tasks O
on O
seven O
different O
lan- O
guages O
from O
the O
SPMRL O
2013 O
- O
2014 O
shared O
task O
( O
Seddah O
et O
al O
. O
, O
2013 O
): O
Basque O
, O
French O
, O
German O
, O
Hungarian O
, O
Korean O
, O
Polish O
and O
Swedish O
. O

For O
evaluation O
on O
syntactic O
parsing O
, O
we O
report O
the O
standard O
labeled O
precision O
( O
LP O
) O
, O
labeled O
recall O
( O
LR O
) O
, O
and O
labelled O
F1 O
computed O
by O
evalb3 O
. O

For O
evaluation O
on O
RST O
- O
DT O
, O
we O
report O
the O
standard O
span O
, O
nuclearity O
label O
, O
relation O
label O
F1 O
scores O
, O
computed O
using O
the O
implementation O
of O
( O
Lin O
et O
al O
. O
, O
2019).4 O
4.1 O
English O
( O
PTB O
) O
Syntactic O
Parsing O
Setup O
We O
follow O
the O
standard O
train O
/ O
valid O
/ O
test O
split O
, O
which O
uses O
Sections O
2 O
- O
21 O
for O
training O
, O
Section O
22 O
for O
development O
and O
Section O
23 O
for O
evaluation O
. O

This O
results O
in O
39,832 O
sentences O
for O
training O
, O
1,700 O
for O
development O
, O
and O
2,416 O
for O
testing O
. O

For O
our O
model O
, O
we O
use O
an O
LSTM O
encoder O
- O
decoder O
frame- O
work O
with O
a O
3 O
- O
layer O
bidirectional O
encoder O
and O
3- O
layer O
unidirectional O
decoder O
. O

The O
word O
embedding O
size O
is O
100 O
while O
the O
character O
embedding O
size O
is O
50 O
; O
the O
LSTM O
hidden O
size O
is O
400 O
. O

The O
hidden O
di- O
mension O
in O
MLP O
modules O
and O
biafﬁne O
function O
for O
split O
point O
prediction O
is O
500 O
. O

The O
beam O
width O
Bis O
set O
to O
20 O
. O

We O
use O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
a O
batch O
size O
of O
5000 O
tokens O
, O
and O
an O
initial O
learning O
rate O
of O
0:002which O
decays O
at O
the O
rate0:75exponentially O
at O
every O
5k O
steps O
. O

Model O
selection O
for O
ﬁnal O
evaluation O
is O
performed O
based O
on O
the O
labeled O
F1 O
score O
on O
the O
development O
set O
. O

Results O
without O
Pre O
- O
training O
From O
the O
results O
shown O
in O
Table O
1 O
, O
we O
see O
that O
our O
model O
achieves O
an O
F1 O
of O
93:77 O
, O
the O
highest O
among O
models O
that O
use O
2Extending O
the O
discourse O
parser O
to O
the O
document O
level O
may O
require O
handling O
of O
intra- O
and O
multi O
- O
sentential O
constituents O
differently O
, O
which O
we O
leave O
for O
future O
work O
. O

3http://nlp.cs.nyu.edu/evalb/ O
4https://github.com/ntunlpsg/ O
UnifiedParser_RST O

5801Model O
LR O
LP O
F1 O
Top O
- O
Down O
Inference O
Stern O
et O
al O
. O
( O
2017a O
) O
93.20 O
90.30 O
91.80 O
Shen O
et O
al O
. O

( O
2018 O
) O
92.00 O
91.70 O
91.80 O

Nguyen O
et O
al O
. O

( O
2020 O
) O
92.91 O
92.75 O
92.78 O
Our O
Model O
93.90 O
93.63 O
93.77 O
CKY O
/ O
Chart O
Inference O
Gaddy O
et O
al O
. O

( O
2018 O
) O
91.76 O
92.41 O
92.08 O
Kitaev O
and O
Klein O
( O
2018 O
) O

93.20 O
93.90 O
93.55 O
Wei O
et O
al O
. O
( O
2020 O
) O
93.3 O
94.1 O
93.7 O
Zhang O
et O
al O
. O
( O
2020b O
) O
93.84 O
93.58 O
93.71 O
Other O
Approaches O
Gómez O
and O
Vilares O
( O
2018 O
) O
- O
- O
90.7 O
Liu O
and O
Zhang O
( O
2017 O
) O
- O
- O
91.8 O
Stern O
et O
al O
. O
( O
2017b O
) O
92.57 O
92.56 O
92.56 O
Zhou O
and O
Zhao O
( O
2019 O
) O
93.64 O
93.92 O
93.78 O
Table O
1 O
: O
Results O
for O
single O
models O
( O
no O
pre O
- O
training O
) O
on O
the O
PTB O
WSJ O
test O
set O
, O
Section O
23 O
. O

Model O
F1 O

Nguyen O
et O
al O
. O

( O
2020 O
) O
95.5 O

Our O
model O
95.7 O
Kitaev O
et O
al O
. O

( O
2019 O
) O
95.6 O
Zhang O
et O
al O
. O

( O
2020b O
) O
95.7 O
Wei O
et O
al O
. O

( O
2020 O
) O
95.8 O
Zhou O
and O
Zhao O
( O
2019 O
) O
95.8 O
Table O
2 O
: O
Results O
on O
PTB O
WSJ O
test O
set O
with O
pretraining O
. O

top O
- O
down O
methods O
. O

Speciﬁcally O
, O
our O
parser O
outper- O
forms O
Stern O
et O
al O
. O
( O
2017a O
) O
; O
Shen O
et O
al O
. O

( O
2018 O
) O
by O
about O
2points O
in O
F1 O
- O
score O
and O
Nguyen O

et O
al O
. O

( O
2020 O
) O
by1point O
. O

Notably O
, O
without O
beam O
search O
( O
beam O
width O
1 O
or O
greedy O
decoding O
) O
, O
our O
model O
achieves O
an O
F1 O
of O
93:40 O
, O
which O
is O
still O
better O
than O
other O
top- O
down O
methods O
. O

Our O
model O
also O
performs O
compet- O
itively O
with O
CKY O
- O
based O
methods O
like O
( O
Kitaev O
and O
Klein O
, O
2018 O
; O
Zhang O
et O

al O
. O
, O
2020b O
; O
Wei O
et O
al O
. O
, O
2020 O
; O
Zhou O
and O
Zhao O
, O
2019 O
) O
, O
while O
these O
methods O
run O
slower O
than O
ours O
. O

Plus O
, O
Zhou O
and O
Zhao O
( O
2019 O
) O
uses O
external O
su- O
pervision O
( O
head O
information O
) O
from O
the O
dependency O
parsing O
task O
. O

Dependency O
parsing O
models O
, O
in O
fact O
, O
have O
a O
strong O
resemblance O
to O
the O
pointing O
mecha- O
nism O
that O
our O
model O
employs O
( O
Ma O
et O
al O
. O
, O
2018 O
) O
. O

As O
such O
, O
integrating O
dependency O
parsing O
information O
into O
our O
model O
may O
also O
be O
beneﬁcial O
. O

We O
leave O
this O
for O
future O
work O
. O

Results O
with O
Pre O
- O
training O
Similar O
to O
( O
Kitaev O
and O
Klein O
, O
2018 O
; O
Kitaev O
et O
al O
. O
, O
2019 O
) O
, O
we O
also O
eval O
- O
uate O
our O
parser O
with O
BERT O
embeddings O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

They O
ﬁne O
- O
tuned O
Bert O
- O
large O
- O
cased O
on O
the O
task O
, O
while O
in O
our O
work O
keeping O
it O
frozen O
was O
already O
good O
enough O
( O
gives O
training O
efﬁciency O
) O
. O

As O
shown O
in O
Table O
2 O
, O
our O
model O
achieves O
an O
F1 O
of O
95:7 O
, O
which O
is O
on O
par O
with O
SoTA O
models O
. O

However O
, O
our O
parser O
runs O
faster O
than O
other O
methods O
. O

Speciﬁcally O
, O
our O
model O
runs O
at O
O(n)time O
complexity O
, O
while O
CKY O
needsO(n3 O
) O
. O

Comprehensive O
comparisons O
on O
parsing O
speed O
are O
presented O
later O
. O

4.2 O
SPMRL O
Multilingual O
Syntactic O

Parsing O
We O
use O
the O
identical O
hyper O
- O
parameters O
and O
opti- O
mizer O
setups O
as O
in O
English O
PTB O
. O

We O
follow O
the O
stan- O
dard O
train O
/ O
valid O
/ O
test O
split O
provided O
in O
the O
SPMRL O
datasets O
; O
details O
are O
reported O
in O
the O
Table O
3 O
. O

Language O
Train O
Valid O
Test O
Basque O
7,577 O
948 O
946 O
French O
14,759 O
1,235 O
2,541 O
German O
40,472 O
5,000 O
5,000 O
Hungarian O
8,146 O
1,051 O
1,009 O
Korean O
23,010 O
2,066 O
2,287 O
Polish O
6,578 O
821 O
822 O
Swedish O
5,000 O
494 O
666 O
Table O
3 O
: O
SPMRL O
Multilingual O
dataset O
split O
. O

From O
the O
results O
in O
Table O
4 O
, O
we O
see O
that O
our O
model O
achieves O
the O
highest O
F1 O
in O
French O
, O
Hungar- O
ian O
and O
Korean O
and O
higher O
than O
the O
best O
baseline O
by0:06,0:15and0:13 O
, O
respectively O
. O

Our O
method O
also O
rivals O
existing O
SoTA O
methods O
on O
other O
lan- O
guages O
even O
though O
some O
of O
them O
use O
predicted O
POS O
tags O
( O
Nguyen O
et O
al O
. O
, O
2020 O
) O
or O
bigger O
models O
( O
75 O
M O
parameters O
) O
( O
Kitaev O
and O
Klein O
, O
2018 O
) O
. O

Mean- O
while O
, O
our O
model O
is O
smaller O
( O
31 O
M O
) O
, O
uses O
no O
extra O
information O
and O
runs O
40 O
% O
faster O
. O

4.3 O
Discourse O
Parsing O
Setup O
For O
discourse O
parsing O
, O
we O
follow O
the O
stan- O
dard O
split O
from O
( O
Lin O
et O
al O
. O
, O
2019 O
) O
, O
which O
has O
7321 O
sentence O
- O
level O
discourse O
trees O
for O
training O
and O
951 O
for O
testing O
. O

We O
also O
randomly O
select O
10 O
% O
of O
the O
training O
for O
validation O
. O

Model O
selection O
for O
test- O
ing O
is O
performed O
based O
on O
the O
F1 O
of O
relation O
labels O
on O
the O
validation O
set O
. O

We O
use O
the O
same O
model O
set- O
tings O
as O
the O
constituency O
parsing O
experiments O
, O
with O
BERT O
as O
pretrained O
embeddings.5 O
5Lin O
et O
al O
. O

( O
2019 O
) O
used O
ELMo O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
as O
pre- O
trained O
embeddings O
. O

With O
BERT O
, O
their O
model O
performs O
worse O
which O
we O
have O
conﬁrmed O
with O
the O
authors O
. O

5802Model O
Basque O
French O
German O
Hungarian O
Korean O
Polish O
Swedish O
Bjorkelund O
et O
al O
. O

( O
2014)+88.24 O
82.53 O
81.66 O
91.72 O
83.81 O
90.50 O
85.50 O
Coavoux O
and O
Crabbé O
( O
2017)+88.81 O
82.49 O
85.34 O
92.34 O
86.04 O
93.64 O
84.0 O
Kitaev O
and O
Klein O
( O
2018 O
) O
89.71 O
84.06 O
87.69 O
92.69 O
86.59 O
93.69 O
84.45 O
Nguyen O
et O
al O
. O

( O
2020)+90.23 O
82.20 O
84.91 O
91.07 O
85.36 O
93.99 O
86.87 O
Our O
Model O
89.74 O
84.12 O
85.21 O
92.84 O
86.72 O
92.10 O
85.81 O
Table O
4 O
: O
Results O
on O
SPMRL O
test O
sets O
without O
pre O
- O
training O
. O

The O
sign+denotes O
that O
systems O
use O
predicted O
POS O
tags O
. O

Approach O
Span O
Nuclearity O
Relation O
Parsing O
with O
gold O
EDU O
segmentation O
Human O
Agreement O
95.7 O
90.4 O
83.0 O

Baselines O
Wang O
et O
al O
. O

( O
2017 O
) O
95.6 O
87.8 O
77.6 O
Lin O
et O
al O
. O

( O
2019 O
) O
( O
single O
) O
96.94 O
90.89 O
81.28 O
Lin O
et O
al O
. O

( O
2019 O
) O
( O
joint O
) O
97.44 O
91.34 O
81.70 O
Our O
Model O
97.37 O
91.95 O
82.10 O
End O
- O
to O
- O
End O
parsing O
Baselines O
Soricut O
and O
Marcu O
( O
2003 O
) O
76.7 O
70.2 O
58.0 O
Joty O
et O
al O
. O

( O
2012 O
) O
82.4 O
76.6 O
67.5 O
Lin O
et O
al O
. O

( O
2019 O
) O
( O
pipeline O
) O
91.14 O
85.80 O
76.94 O
Lin O
et O
al O
. O

( O
2019 O
) O
( O
joint O
) O
91.75 O
86.38 O
77.52 O
Our O
Model O
92.02 O
87.05 O
78.82 O
Table O
5 O
: O
Results O
on O
discourse O
parsing O
tasks O
on O
the O
RST- O
DT O
test O
set O
with O
and O
without O
gold O
segmentation O
. O

Results O
Table O
5 O
compares O
the O
results O
on O
the O
dis- O
course O
parsing O
tasks O
in O
two O
settings O
: O
( O
i O
) O
when O
the O
EDUs O
are O
given O
( O
gold O
segmentation O
) O
and O
( O
ii O
) O
end- O
to O
- O
end O
parsing O
. O

We O
see O
that O
our O
model O
outperforms O
the O
baselines O
in O
both O
parsing O
conditions O
achieving O
SoTA O
. O

When O
gold O
segmentation O
is O
provided O
, O
our O
model O
outperforms O
the O
single O
- O
task O
training O
model O
of O
( O
Lin O
et O
al O
. O
, O
2019 O
) O
by O
0.43 O
% O
, O
1.06 O
% O
and O
0.82 O
% O
absolute O
in O
Span O
, O
Nuclearity O
and O
Relation O
, O
respec- O
tively O
. O

Our O
parser O
also O
surpasses O
their O
joint O
training O
model O
, O
which O
uses O
multi O
- O
task O
training O
( O
segmenta- O
tion O
and O
parsing O
) O
, O
with O
0.61 O
% O
and O
0.4 O
% O
absolute O
improvements O
in O
Nuclearity O
and O
Relation O
, O
respec- O
tively O
. O

For O
end O
- O
to O
- O
end O
parsing O
, O
compared O
to O
the O
best O
baseline O
( O
Lin O
et O
al O
. O
, O
2019 O
) O
, O
our O
model O
yields O
0.27 O
% O
, O
0.67 O
% O
, O
and O
1.30 O
% O
absolute O
improvements O
in O
Span O
, O
Nuclearity O
, O
Relation O
, O
respectively O
. O

This O
demonstrates O
the O
effectiveness O
of O
our O
conditional O
splitting O
approach O
and O
end O
- O
to O
- O
end O
formulation O
of O
the O
discourse O
analysis O
task O
. O

The O
fact O
that O
our O
model O
improves O
on O
span O
identiﬁcation O
indicates O
that O
our O
method O
also O
yields O
better O
EDU O
segmentation O
. O

4.4 O

Parsing O
Speed O
Comparison O
We O
compare O
parsing O
speed O
of O
different O
models O
in O
Table O
6 O
. O

We O
ran O
our O
models O
on O
both O
CPU O
( O
IntelXeon O
W-2133 O
) O
and O
GPU O
( O
Nvidia O
GTX O
1080 O
Ti O
) O
. O

Syntactic O
Parsing O
The O
Berkeley O
Parser O
and O
ZPar O
are O
two O
representative O
non O
- O
neural O
parsers O
without O
access O
to O
GPUs O
. O

Stern O
et O
al O
. O

( O
2017a O
) O
employ O
max- O
margin O
training O
and O
perform O
top O
- O
down O
greedy O
de- O
coding O
on O
CPUs O
. O

Meanwhile O
, O
Kitaev O
and O
Klein O
( O
2018 O
) O

; O
Zhou O
and O
Zhao O
( O
2019 O
) O
; O

Wei O
et O
al O
. O
( O
2020 O
) O
use O
a O
self O
- O
attention O
encoder O
and O
perform O
decoding O
using O
Cython O
for O
acceleration O
. O

Zhang O
et O
al O
. O

( O
2020b O
) O
perform O
CKY O
decoding O
on O
GPU O
. O

The O
parser O
pro- O
posed O
by O
Gómez O
and O
Vilares O
( O
2018 O
) O
is O
also O
efﬁ- O
cient O
as O
it O
treats O
parsing O
as O
a O
sequence O
labeling O
task O
. O

However O
, O
its O
parsing O
accuracy O
is O
much O
lower O
compared O
to O
others O
( O
90.7 O
F1 O
in O
Table O
1 O
) O
. O

We O
see O
that O
our O
parser O
is O
much O
more O
efﬁcient O
than O
existing O
ones O
. O

It O
utilizes O
neural O
modules O
to O
perform O
splitting O
, O
which O
is O
optimized O
and O
paral- O
lelized O
with O
efﬁcient O
GPU O
implementation O
. O

It O
can O
parse O
1;127sentences O
/ O
second O
, O
which O
is O
faster O
than O
existing O
parsers O
. O

In O
fact O
, O
there O
is O
still O
room O
to O
im- O
prove O
our O
speed O
by O
choosing O
better O
architectures O
, O
like O
the O
Transformer O
which O
has O
O(1)running O
time O
in O
encoding O
a O
sentence O
compared O
to O
O(n)of O
the O
bi O
- O
LSTM O
encoder O
. O

Moreover O
, O
allowing O
tree O
gener- O
ation O
by O
splitting O
the O
spans O
/ O
nodes O
at O
the O
same O
tree O
level O
in O
parallel O
at O
each O
step O
can O
boost O
the O
speed O
further O
. O

We O
leave O
these O
extensions O
to O
future O
work O
. O

Discourse O
Parsing O
For O
measuring O
discourse O
parsing O
speed O
, O
we O
follow O
the O
same O
set O
up O
as O
Lin O
et O
al O
. O

( O
2019 O
) O
, O
and O
evaluate O
the O
models O
with O
the O
same O
100 O
sentences O
randomly O
selected O
from O
the O
test O
set O
. O

We O
include O
the O
model O
loading O
time O
for O
all O
the O
systems O
. O

Since O
SPADE O
and O
CODRA O
need O
to O
extract O
a O
handful O
of O
features O
, O
they O
are O
typically O
slower O
than O
the O
neural O
models O
which O
use O
pretrained O
embeddings O
. O

In O
addition O
, O
CODRA O
’s O
DCRF O
parser O
has O
aO(n3)inference O
time O
complexity O
. O

As O
shown O
, O
our O
parser O
is O
4.7x O
faster O
than O
the O
fastest O
end O
- O
to O
- O
end O
parser O
of O
Lin O
et O
al O
. O

( O
2019 O
) O

, O
making O
it O
not O
only O
ef- O
fective O
but O
also O
highly O
efﬁcient O
. O

Even O
when O
tested O
only O
on O
the O
CPU O
, O
our O
model O
is O
faster O
than O
all O
the O
other O
models O
which O
run O
on O
GPU O
or O
CPU O
, O
thanks O

5803System O
Speed O
( O
Sents O
/ O
s O
) O
Speedup O
Syntactic O
Parser O
Petrov O
and O
Klein O
( O
2007 O
) O
( O
Berkeley O
) O
6 O
1.0x O
Zhu O
et O
al O
. O

( O
2013)(ZPar O
) O
90 O
15.0x O
Stern O
et O
al O
. O
( O
2017a O
) O
76 O
12.7x O
Shen O
et O
al O
. O

( O
2018 O
) O
111 O
18.5x O

Nguyen O
et O
al O
. O

( O
2020 O
) O
130 O
21.7x O
Zhou O
and O
Zhao O
( O
2019 O
) O

159 O
26.5x O
Wei O
et O
al O
. O

( O
2020 O
) O
220 O
36.7x O
Gómez O
and O
Vilares O
( O
2018 O
) O
780 O
130x O
Kitaev O
and O
Klein O
( O
2018 O
) O
( O
GPU O
) O
830 O
138.3x O
Zhang O
et O
al O
. O

( O
2020b O
) O
924 O
154x O
Our O
model O
( O
GPU O
) O
1127 O
187.3x O
End O
- O
to O
- O
End O
Discourse O
parsing O
( O
Segmenter O
+ O
Parser O
) O
CODRA O
( O
Joty O
et O
al O
. O
, O
2015 O
) O
3.05 O
1.0x O
SPADE O
( O
Soricut O
and O
Marcu O
, O
2003 O
) O
4.90 O
1.6x O
( O
Lin O
et O
al O
. O
, O
2019 O
) O
28.96 O
9.5x O
Our O
end O
- O
to O
- O
end O
parser O
( O
CPU O
) O
59.03 O
19.4x O

Our O
end O
- O
to O
- O
end O
parser O
( O
GPU O
) O
135.85 O
44.5x O
Table O
6 O
: O
Speed O
comparison O
of O
our O
parser O
with O
existing O
syntactic O
and O
discourse O
parsers O
. O

to O
the O
end O
- O
to O
- O
end O
formulation O
that O
does O
not O
need O
EDU O
segmentation O
beforehand O
. O

5 O
Related O
Work O
With O
the O
recent O
popularity O
of O
neural O
architectures O
, O
such O
as O
LSTMs O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
and O
Transformers O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
var- O
ious O
neural O
models O
have O
been O
proposed O
to O
encode O
the O
input O
sentences O
and O
infer O
their O
constituency O
trees O
. O

To O
enforce O
structural O
consistency O
, O
such O
meth- O
ods O
employ O
either O
a O
greedy O
transition O
- O
based O
( O
Dyer O
et O
al O
. O
, O
2016 O
; O
Liu O
and O
Zhang O
, O
2017 O
) O
, O
a O
globally O
op- O
timized O
chart O
parsing O
( O
Gaddy O
et O
al O
. O
, O
2018 O
; O
Kitaev O
and O
Klein O
, O
2018 O
) O
, O
or O
a O
greedy O
top O
- O
down O
algorithm O
( O
Stern O
et O
al O
. O
, O
2017a O
; O
Shen O
et O
al O
. O
, O
2018 O
) O
. O

Meanwhile O
, O
researchers O
also O
tried O
to O
cast O
the O
parsing O
problem O
into O
tasks O
that O
can O
be O
solved O
differently O
. O

For O
exam- O
ple O
, O
Gómez O
and O
Vilares O
( O
2018 O
) O
; O
Shen O
et O
al O
. O

( O
2018 O
) O
proposed O
to O
map O
the O
syntactic O
tree O
of O
a O
sentence O
containingntokens O
into O
a O
sequence O
of O
n 1la- O
bels O
or O
scalars O
. O

However O
, O
parsers O
of O
this O
type O
suffer O
from O
the O
exposure O
bias O
during O
inference O
. O

Beside O
these O
methods O
, O
Seq2Seq O
models O
have O
been O
used O
to O
generate O
a O
linearized O
form O
of O
the O
tree O
( O
Vinyals O
et O
al O
. O
, O
2015b O
; O
Kamigaito O
et O
al O
. O
, O
2017 O
; O
Suzuki O
et O
al O
. O
, O
2018 O
; O
Fernández O
- O
González O
and O
Gómez O
- O
Rodríguez O
, O
2020a O
) O
. O

However O
, O
these O
methods O
may O
generate O
in- O
valid O
trees O
when O
the O
open O
and O
end O
brackets O
do O
not O
match O
. O

In O
discourse O
parsing O
, O
existing O
parsers O
receive O
the O
EDUs O
from O
a O
segmenter O
to O
build O
the O
discourse O
tree O
, O
which O
makes O
them O
susceptible O
to O
errors O
when O
the O
segmenter O
produces O
incorrect O
EDUs O
( O
Joty O
et O
al O
. O
,2012 O
, O
2015 O
; O
Lin O
et O
al O
. O
, O
2019 O
; O
Zhang O
et O
al O
. O
, O
2020a O
; O
Liu O
et O
al O
. O
, O
2020 O
) O
. O

There O
are O
also O
attempts O
which O
model O
constituency O
and O
discourse O
parsing O
jointly O
( O
Zhao O
and O
Huang O
, O
2017 O
) O
and O
do O
not O
need O
to O
perform O
EDU O
preprocessing O
. O

It O
is O
based O
on O
the O
ﬁnding O
that O
each O
EDU O
generally O
corresponds O
to O
a O
constituent O
in O
constituency O
tree O
, O
i.e. O
,discourse O
structure O
usually O
aligns O
with O
constituency O
structure O
. O

However O
, O
it O
has O
the O
drawback O
that O
it O
needs O
to O
build O
joint O
syntacto- O
discourse O
data O
set O
for O
training O
which O
is O
not O
easily O
adaptable O
to O
new O
languages O
and O
domains O
. O

Our O
approach O
differs O
from O
previous O
methods O
in O
that O
it O
represents O
the O
constituency O
structure O
as O
a O
se- O
ries O
of O
splitting O
representations O
, O
and O
uses O
a O
Seq2Seq O
framework O
to O
model O
the O
splitting O
decision O
at O
each O
step O
. O

By O
enabling O
beam O
search O
, O
our O
model O
can O
ﬁnd O
the O
best O
trees O
without O
the O
need O
to O
perform O
an O
expensive O
global O
search O
. O

We O
also O
unify O
discourse O
segmentation O
and O
parsing O
into O
one O
system O
by O
gen- O
eralizing O
our O
model O
, O
which O
has O
been O
done O
for O
the O
ﬁrst O
time O
to O
the O
best O
of O
our O
knowledge O
. O

Our O
splitting O
mechanism O
shares O
some O
similari- O
ties O
with O
Pointer O
Network O
( O
Vinyals O
et O
al O
. O
, O
2015a O
; O
Ma O
et O
al O
. O
, O
2018 O
; O
Fernández O
- O
González O
and O
Gómez- O
Rodríguez O
, O
2019 O
, O
2020b O
) O
or O
head O
- O
selection O
ap- O
proaches O
( O
Zhang O
et O
al O
. O
, O
2017 O
; O
Kurita O
and O
Søgaard O
, O
2019 O
) O
, O
but O
is O
distinct O
from O
them O
that O
in O
each O
decod- O
ing O
step O
, O
our O
method O
identiﬁes O
the O
splitting O
point O
of O
a O
span O
and O
generates O
a O
new O
input O
for O
future O
steps O
instead O
of O
pointing O
to O
generate O
the O
next O
decoder O
input O
. O

6 O
Conclusion O
We O
have O
presented O
a O
novel O
, O
generic O
parsing O
method O
for O
constituency O
parsing O
based O
on O
a O
Seq2Seq O
frame- O
work O
. O

Our O
method O
supports O
an O
efﬁcient O
top O
- O
down O
decoding O
algorithm O
that O
uses O
a O
pointing O
function O
for O
scoring O
possible O
splitting O
points O
. O

The O
pointing O
mechanism O
captures O
global O
structural O
properties O
of O
a O
tree O
and O
allows O
efﬁcient O
training O
with O
a O
cross O
entropy O
loss O
. O

Our O
formulation O
, O
when O
applied O
to O
discourse O
parsing O
, O
can O
bypass O
discourse O
segmenta- O
tion O
as O
a O
pre O
- O
requisite O
step O
. O

Through O
experiments O
we O
have O
shown O
that O
our O
method O
outperforms O
all O
existing O
top O
- O
down O
methods O
on O
English O
Penn O
Tree- O
bank O
and O
RST O
Discourse O
Treebank O
sentence O
- O
level O
parsing O
tasks O
. O

With O
pre O
- O
trained O
representations O
, O
our O
method O
rivals O
state O
- O
of O
- O
the O
- O
art O
methods O
, O
while O
being O
faster O
. O

Our O
model O
also O
establishes O
a O
new O
state O
- O
of- O
the O
- O
art O
for O
sentence O
- O
level O
RST O
parsing O
. O

5804References O
Yoav O
Artzi O
, O
Nicholas O
FitzGerald O
, O
and O
Luke O
Zettle- O
moyer O
. O

2013 O
. O

Semantic O
parsing O
with O
Combinatory O
Categorial O
Grammars O
. O

In O
Proceedings O
of O
the O
51st O
Annual O
Meeting O
of O
the O
Association O
for O
Computa- O
tional O
Linguistics O
( O
Tutorials O
) O
, O
page O
2 O
, O
Soﬁa O
, O
Bul- O
garia O
. O

Association O
for O
Computational O
Linguistics O
. O

Laura O
Banarescu O
, O
Claire O
Bonial O
, O
Shu O
Cai O
, O
Madalina O
Georgescu O
, O
Kira O
Grifﬁtt O
, O
Ulf O
Hermjakob O
, O
Kevin O
Knight O
, O
Philipp O
Koehn O
, O
Martha O
Palmer O
, O
and O
Nathan O
Schneider O
. O

2013 O
. O

Abstract O
Meaning O
Representation O
for O
sembanking O
. O

In O
Proceedings O
of O
the O
7th O
Linguis- O

tic O
Annotation O
Workshop O
and O
Interoperability O
with O
Discourse O
, O
pages O
178–186 O
, O
Soﬁa O
, O
Bulgaria O
. O

Associa- O
tion O
for O
Computational O
Linguistics O
. O

Samy O
Bengio O
, O
Oriol O
Vinyals O
, O
Navdeep O
Jaitly O
, O
and O
Noam O
Shazeer O
. O

2015 O
. O

Scheduled O
sampling O
for O
se- O
quence O
prediction O
with O
recurrent O
neural O
networks O
. O

InAdvances O
in O
Neural O
Information O
Processing O
Sys- O
tems O
, O
volume O
28 O
, O
pages O
1171–1179 O
. O

Curran O
Asso- O
ciates O
, O

Inc. O
Anders O
Bjorkelund O
, O
Ozlem O
Cetinoglu O
, O
Agnieszka O
Falenska O
, O
Richard O
Farkas O
, O
Thomas O
Mueller O
, O
Wolf- O
gang O
Seeker O
, O
and O
Zsolt O
Szanto O
. O

2014 O
. O

The O
ims- O
wrocław O
- O
szeged O
- O
cis O
entry O
at O
the O
spmrl O
2014 O
shared O
task O
: O
Reranking O
and O
morphosyntax O
meet O
unlabeled O
data O
. O

In O
Proceedings O
of O
the O
First O
Joint O
Workshop O
on O
Statistical O
Parsing O
of O
Morphologically O
Rich O
Lan- O
guages O
and O
Syntactic O
Analysis O
of O
NonCanonical O
Languages O
, O
pages O
97–102 O
. O

Maximin O
Coavoux O
and O
Benoît O
Crabbé O
. O
2017 O
. O

Multi- O

lingual O
lexicalized O
constituency O
parsing O
with O
word- O
level O
auxiliary O
tasks O
. O

In O
Proceedings O
of O
the O
15th O
Con- O
ference O
of O
the O
European O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Volume O
2 O
, O
Short O
Pa- O
pers O
, O
pages O
331–336 O
, O
Valencia O
, O
Spain O
. O

Association O
for O
Computational O
Linguistics O
. O

James O
Cross O
and O
Liang O
Huang O
. O

2016 O
. O

Span O
- O
based O
con- O
stituency O
parsing O
with O
a O
structure O
- O
label O
system O
and O
provably O
optimal O
dynamic O
oracles O
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1–11 O
, O
Austin O
, O
Texas O
. O

Association O
for O
Computational O
Linguistics O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
under- O
standing O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
, O
Minneapolis O
, O
Minnesota O
. O

Associ- O
ation O
for O
Computational O
Linguistics O
. O

Timothy O
Dozat O
and O
Christopher O
D. O
Manning O
. O

2017 O
. O

Deep O
biafﬁne O
attention O
for O
neural O
dependency O
pars- O
ing O
. O

In O
5th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2017 O
, O
Toulon O
, O
France O
, O
April O
24 O
- O
26 O
, O
2017 O
, O
Conference O
Track O
Proceedings O
.Greg O

Durrett O
and O
Dan O
Klein O
. O

2015 O
. O

Neural O
CRF O
pars- O
ing O
. O

In O
Proceedings O
of O
the O
53rd O
Annual O
Meet- O
ing O
of O
the O
Association O
for O
Computational O
Linguis- O
tics O
and O
the O
7th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
Volume O
1 O
: O
Long O
Pa- O
pers O
) O
, O
pages O
302–312 O
, O
Beijing O
, O
China O
. O

Association O
for O
Computational O
Linguistics O
. O

Chris O
Dyer O
, O
Adhiguna O
Kuncoro O
, O
Miguel O
Ballesteros O
, O
and O
Noah O
A. O
Smith O
. O

2016 O
. O

Recurrent O
neural O
network O
grammars O
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
199–209 O
, O
San O
Diego O
, O
California O
. O
Association O
for O
Computational O
Linguistics O
. O

Daniel O
Fernández O
- O
González O
and O
Carlos O
Gómez- O
Rodríguez O
. O

2019 O
. O

Left O
- O
to O
- O
right O
dependency O
parsing O
with O
pointer O
networks O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
710–716 O
, O
Minneapolis O
, O
Minnesota O
. O

Association O
for O
Computational O
Linguistics O
. O

Daniel O
Fernández O
- O
González O
and O
Carlos O
Gómez- O
Rodríguez O
. O

2020a O
. O

Enriched O
in O
- O
order O
linearization O
for O
faster O
sequence O
- O
to O
- O
sequence O
constituent O
parsing O
. O

InProceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
4092–4099 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Daniel O
Fernández O
- O
González O
and O
Carlos O
Gómez- O
Rodríguez O
. O

2020b O
. O

Transition O
- O
based O
semantic O
dependency O
parsing O
with O
pointer O
networks O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
7035–7046 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

David O
Gaddy O
, O
Mitchell O
Stern O
, O
and O
Dan O
Klein O
. O

2018 O
. O

What O
’s O
going O
on O
in O
neural O
constituency O
parsers O
? O

an O
analysis O
. O

In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Tech- O
nologies O
, O
Volume O
1 O
( O
Long O
Papers O
) O
, O
pages O
999–1010 O
, O
New O
Orleans O
, O
Louisiana O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Carlos O
Gómez O
, O
Rodríguez O
and O
David O
Vilares O
. O

2018 O
. O

Constituent O
parsing O
as O
sequence O
labeling O
. O

In O
Pro- O
ceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Meth- O
ods O
in O
Natural O
Language O
Processing O
, O
pages O
1314 O
– O
1324 O
, O
Brussels O
, O
Belgium O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Sepp O
Hochreiter O
and O
Jürgen O
Schmidhuber O
. O

1997 O
. O

Long O
short O
- O
term O
memory O
. O

Neural O
computation O
, O
9(8):1735–1780 O
. O

Shaﬁq O
Joty O
, O
Giuseppe O
Carenini O
, O
and O
Raymond O
Ng O
. O
2012 O
. O

A O
novel O
discriminative O
framework O
for O
sentence O
- O
level O
discourse O
analysis O
. O

In O
Proceedings O
of O
the O
2012 O
Joint O
Conference O
on O
Empirical O
Methods O

5805 O
in O
Natural O
Language O
Processing O
and O
Computational O
Natural O
Language O
Learning O
, O
pages O
904–915 O
, O
Jeju O
Is- O
land O
, O
Korea O
. O

Association O
for O
Computational O
Linguis- O
tics O
. O

Shaﬁq O
Joty O
, O
Giuseppe O
Carenini O
, O
Raymond O
Ng O
, O
and O
Yashar O
Mehdad O
. O
2013 O
. O

Combining O
intra- O
and O
multi- O
sentential O
rhetorical O
parsing O
for O
document O
- O
level O
dis- O
course O
analysis O
. O

In O
Proceedings O
of O
the O
51st O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
486–496 O
, O
Soﬁa O
, O
Bulgaria O
. O

Association O
for O
Computational O
Lin- O
guistics O
. O

Shaﬁq O
Joty O
, O
Giuseppe O
Carenini O
, O
and O
Raymond O
T. O
Ng O
. O
2015 O
. O

CODRA O
: O

A O
novel O
discriminative O
framework O
for O
rhetorical O
analysis O
. O

Computational O
Linguistics O
, O
41(3):385–435 O
. O

Hidetaka O
Kamigaito O
, O
Katsuhiko O
Hayashi O
, O
Tsutomu O
Hirao O
, O
Hiroya O
Takamura O
, O
Manabu O
Okumura O
, O
and O
Masaaki O
Nagata O
. O
2017 O
. O

Supervised O
attention O
for O
sequence O
- O
to O
- O
sequence O
constituency O
parsing O
. O

In O
Pro- O
ceedings O
of O
the O
Eighth O
International O
Joint O
Confer- O
ence O
on O
Natural O
Language O
Processing O
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
7–12 O
, O
Taipei O
, O
Taiwan O
. O

Asian O
Federation O
of O
Natural O
Language O
Processing O
. O

Diederik O
P. O
Kingma O
and O
Jimmy O
Ba O
. O
2015 O
. O

Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O

In O
3rd O
Inter- O
national O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2015 O
, O
San O
Diego O
, O
CA O
, O
USA O
, O
May O
7 O
- O
9 O
, O
2015 O
, O
Conference O
Track O
Proceedings O
. O

Nikita O
Kitaev O
, O
Steven O
Cao O
, O
and O
Dan O
Klein O
. O

2019 O
. O

Multi- O

lingual O
constituency O
parsing O
with O
self O
- O
attention O
and O
pre O
- O
training O
. O

In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
, O
pages O
3499–3505 O
, O
Florence O
, O
Italy O
. O

Associa- O
tion O
for O
Computational O
Linguistics O
. O

Nikita O
Kitaev O
and O
Dan O
Klein O
. O

2018 O
. O

Constituency O
pars- O
ing O
with O
a O
self O
- O
attentive O
encoder O
. O

In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
2676–2686 O
, O
Melbourne O
, O
Australia O
. O

Associa- O
tion O
for O
Computational O
Linguistics O
. O

Shuhei O
Kurita O
and O
Anders O
Søgaard O
. O

2019 O
. O

Multi O
- O
task O
semantic O
dependency O
parsing O
with O
policy O
gradient O
for O
learning O
easy-ﬁrst O
strategies O
. O

In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Com- O
putational O
Linguistics O
, O
pages O
2420–2430 O
, O
Florence O
, O
Italy O
. O

Association O
for O
Computational O
Linguistics O
. O

Xiang O
Lin O
, O
Shaﬁq O
Joty O
, O
Prathyusha O
Jwalapuram O
, O
and O
M O
Saiful O
Bari O
. O
2019 O
. O

A O
uniﬁed O
linear O
- O
time O
frame- O
work O
for O
sentence O
- O
level O
discourse O
parsing O
. O

In O
Pro- O
ceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Asso- O
ciation O
for O
Computational O
Linguistics O
, O
pages O
4190 O
– O
4200 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O

Jiangming O
Liu O
and O
Yue O
Zhang O
. O

2017 O
. O

Shift O
- O
reduce O
constituent O
parsing O
with O
neural O
lookahead O
features O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
5:45–58 O
. O

Zhengyuan O
Liu O
, O
Ke O
Shi O
, O
and O
Nancy O
Chen O
. O
2020 O
. O

Mul- O
tilingual O
neural O
RST O
discourse O
parsing O
. O

In O
Proceed- O
ings O
of O
the O
28th O
International O
Conference O
on O
Com- O
putational O
Linguistics O
, O
pages O
6730–6738 O
, O
Barcelona O
, O
Spain O
( O
Online O
) O
. O

International O
Committee O
on O
Compu- O
tational O
Linguistics O
. O

Carlson O
Lynn O
, O
Daniel O
Marcu O
, O
and O
Mary O
Ellen O
Okurowski O
. O

2002 O
. O

Rst O
discourse O
treebank O
( O
rst O
– O
dt O
) O
ldc2002t07 O
. O

Linguistic O
Data O
Consortium O
. O

Xuezhe O
Ma O
, O
Zecong O
Hu O
, O
Jingzhou O
Liu O
, O
Nanyun O
Peng O
, O
Graham O
Neubig O
, O
and O
Eduard O
Hovy O
. O

2018 O
. O

Stack- O
pointer O
networks O
for O
dependency O
parsing O
. O

In O
Pro- O
ceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Associa- O
tion O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1403–1414 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O

William O
Mann O
and O
Sandra O
Thompson O
. O

1988 O
. O

Rhetori- O
cal O
Structure O
Theory O
: O
Toward O
a O
Functional O
Theory O
of O
Text O
Organization O
. O

Text O
, O
8(3):243–281 O
. O

Mitchell O
P. O
Marcus O
, O
Mary O
Ann O
Marcinkiewicz O
, O
and O
Beatrice O
Santorini O
. O

1993 O
. O

Building O
a O
large O
annotated O
corpus O
of O
english O
: O
The O
penn O
treebank O
. O

Comput O
. O

Lin- O
guist O
. O
, O
19(2):313–330 O
. O

Thanh O
- O
Tung O
Nguyen O
, O
Xuan O
- O
Phi O
Nguyen O
, O
Shaﬁq O
Joty O
, O
and O
Xiaoli O
Li O
. O

2020 O
. O

Efﬁcient O
constituency O
pars- O
ing O
by O
pointing O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
, O
pages O
3284–3294 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Matthew O
E. O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O

2018 O
. O

Deep O
contextualized O
word O
repre- O
sentations O
. O

In O
Proc O
. O

of O
NAACL O

. O

Slav O
Petrov O
and O
Dan O
Klein O
. O

2007 O
. O

Improved O
inference O
for O
unlexicalized O
parsing O
. O

In O
Human O
Language O
Tech- O
nologies O
2007 O
: O
The O
Conference O
of O
the O
North O
Amer- O
ican O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
; O
Proceedings O
of O
the O
Main O
Conference O
, O
pages O
404–411 O
, O
Rochester O
, O
New O
York O
. O

Association O
for O
Computational O
Linguistics O
. O

Djamé O
Seddah O
, O
Reut O
Tsarfaty O
, O
Sandra O
Kübler O
, O
Marie O
Candito O
, O
Jinho O
D. O
Choi O
, O
Richárd O
Farkas O
, O
Jen- O
nifer O
Foster O
, O
Iakes O
Goenaga O
, O
Koldo O
Gojenola O
Gal- O
letebeitia O
, O
Yoav O
Goldberg O
, O
Spence O
Green O
, O
Nizar O
Habash O
, O
Marco O
Kuhlmann O
, O
Wolfgang O
Maier O
, O
Joakim O
Nivre O
, O
Adam O
Przepiórkowski O
, O
Ryan O
Roth O
, O
Wolfgang O
Seeker O
, O
Yannick O
Versley O
, O
Veronika O
Vincze O
, O
Marcin O
Woli O
´ O
nski O
, O
Alina O
Wróblewska O
, O
and O
Eric O
Villemonte O
de O
la O
Clergerie O
. O

2013 O
. O

Overview O
of O
the O
SPMRL O
2013 O
shared O
task O
: O
A O
cross O
- O
framework O
evaluation O
of O
parsing O
morphologically O
rich O
languages O
. O

In O
Proceed- O
ings O
of O
the O
Fourth O
Workshop O
on O
Statistical O
Parsing O
of O
Morphologically O
- O
Rich O
Languages O
, O
pages O
146–182 O
, O
Seattle O
, O
Washington O
, O
USA O
. O
Association O
for O
Compu- O
tational O
Linguistics O
. O

5806Yikang O
Shen O
, O
Zhouhan O
Lin O
, O
Athul O
Paul O
Jacob O
, O
Alessan- O
dro O
Sordoni O
, O
Aaron O
Courville O
, O
and O
Yoshua O
Bengio O
. O

2018 O
. O

Straight O
to O
the O
tree O
: O
Constituency O
parsing O
with O
neural O
syntactic O
distance O
. O

In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Compu- O
tational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1171–1180 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O

Radu O
Soricut O
and O
Daniel O
Marcu O
. O

2003 O
. O

Sentence O
level O
discourse O
parsing O
using O
syntactic O
and O
lexical O
infor- O
mation O
. O

In O
Proceedings O
of O
the O
2003 O
Human O
Lan- O
guage O
Technology O
Conference O
of O
the O
North O
Ameri- O
can O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
228–235 O
. O

Mitchell O
Stern O
, O
Jacob O
Andreas O
, O
and O
Dan O
Klein O
. O
2017a O
. O

A O
minimal O
span O
- O
based O
neural O
constituency O
parser O
. O

In O
Proceedings O
of O
the O
55th O
Annual O
Meeting O
of O
the O
As- O
sociation O
for O
Computational O
Linguistics O
, O
ACL O
2017 O
, O
Vancouver O
, O
Canada O
, O
July O
30 O
- O
August O
4 O
, O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
818–827 O
. O

Mitchell O
Stern O
, O
Daniel O
Fried O
, O
and O
Dan O
Klein O
. O
2017b O
. O

Effective O
inference O
for O
generative O
neural O
parsing O
. O

In O
Proceedings O
of O
the O
2017 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1695–1700 O
, O
Copenhagen O
, O
Denmark O
. O

Association O
for O
Computational O
Linguistics O
. O

Jun O
Suzuki O
, O
Sho O
Takase O
, O
Hidetaka O
Kamigaito O
, O
Makoto O
Morishita O
, O
and O
Masaaki O
Nagata O
. O

2018 O
. O

An O
empirical O
study O
of O
building O
a O
strong O
baseline O
for O
constituency O
parsing O
. O

In O
Proceedings O
of O
the O
56th O
Annual O
Meet- O
ing O
of O
the O
Association O
for O
Computational O
Linguis- O
tics O
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
612–618 O
, O
Mel- O
bourne O
, O
Australia O
. O

Association O
for O
Computational O
Linguistics O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
Ł O
ukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
I. O
Guyon O
, O
U. O
V O
. O

Luxburg O
, O
S. O
Bengio O
, O
H. O
Wallach O
, O
R. O
Fergus O
, O
S. O
Vishwanathan O
, O
and O
R. O
Gar- O
nett O
, O
editors O
, O
Advances O
in O
Neural O
Information O
Pro- O
cessing O
Systems O
30 O
, O
pages O
5998–6008 O
. O

Curran O
Asso- O
ciates O
, O
Inc. O
Oriol O
Vinyals O
, O
Meire O
Fortunato O
, O
and O
Navdeep O
Jaitly O
. O

2015a O
. O

Pointer O
networks O
. O

In O
C. O
Cortes O
, O
N. O
D. O
Lawrence O
, O
D. O
D. O
Lee O
, O
M. O
Sugiyama O
, O
and O
R. O
Gar- O
nett O
, O
editors O
, O
Advances O
in O
Neural O
Information O
Pro- O
cessing O
Systems O
28 O
, O
pages O
2692–2700 O
. O

Curran O
Asso- O
ciates O
, O
Inc. O
Oriol O
Vinyals O
, O
Ł O
ukasz O
Kaiser O
, O
Terry O
Koo O
, O
Slav O
Petrov O
, O
Ilya O
Sutskever O
, O
and O
Geoffrey O
Hinton O
. O

2015b O
. O

Gram- O
mar O
as O
a O
foreign O
language O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
volume O
28 O
, O
pages O
2773–2781 O
. O

Curran O
Associates O
, O
Inc. O

Yizhong O
Wang O
, O
Sujian O
Li O
, O
and O
Houfeng O
Wang O
. O

2017 O
. O

A O
two O
- O
stage O
parsing O
method O
for O
text O
- O
level O
discourse O
analysis O
. O

In O
Proceedings O
of O
the O
55th O
Annual O
Meet- O
ing O
of O
the O
Association O
for O
Computational O
Linguistics(Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
184–188 O
. O

Associa- O
tion O
for O
Computational O
Linguistics O
. O

B. O
Webber O
. O

2004 O
. O

D O
- O
LTAG O
: O

Extending O
Lexicalized O
TAG O
to O
Discourse O
. O

Cognitive O
Science O
, O
28(5):751 O
– O
779 O
. O

Yang O
Wei O
, O
Yuanbin O
Wu O
, O
and O
Man O
Lan O
. O

2020 O
. O

A O
span- O
based O
linearization O
for O
constituent O
trees O
. O

In O
Pro- O
ceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Asso- O
ciation O
for O
Computational O
Linguistics O
, O
pages O
3267 O
– O
3277 O
, O
Online O
. O

Association O
for O
Computational O
Lin- O
guistics O
. O

Longyin O
Zhang O
, O
Yuqing O
Xing O
, O
Fang O
Kong O
, O
Peifeng O
Li O
, O
and O
Guodong O
Zhou O
. O

2020a O
. O

A O
top O
- O
down O
neural O
architecture O
towards O
text O
- O
level O
parsing O
of O
discourse O
rhetorical O
structure O
. O

In O
Proceedings O
of O
the O
58th O
An- O
nual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
6386–6395 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Xingxing O
Zhang O
, O
Jianpeng O
Cheng O
, O
and O
Mirella O
Lapata O
. O

2017 O
. O

Dependency O
parsing O
as O
head O
selection O
. O

In O
Proceedings O
of O
the O
15th O
Conference O
of O
the O
European O
Chapter O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
: O
Volume O
1 O
, O
Long O
Papers O
, O
pages O
665–676 O
, O
Valencia O
, O
Spain O
. O

Association O
for O
Computational O
Lin- O
guistics O
. O

Yu O
Zhang O
, O
Houquan O
Zhou O
, O
and O
Zhenghua O
Li O
. O

2020b O
. O

Fast O
and O
accurate O
neural O
crf O
constituency O
parsing O
. O

InProceedings O
of O
the O
Twenty O
- O
Ninth O
International O
Joint O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
IJCAI- O
20 O
, O
pages O
4046–4053 O
. O

International O
Joint O
Confer- O
ences O
on O
Artiﬁcial O
Intelligence O
Organization O
. O

Main O
track O
. O

Kai O
Zhao O
and O
Liang O
Huang O
. O
2017 O
. O

Joint O
syntacto- O
discourse O
parsing O
and O
the O
syntacto O
- O
discourse O
tree- O
bank O
. O

In O
Proceedings O
of O
the O
2017 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2117–2123 O
, O
Copenhagen O
, O
Denmark O
. O

Associa- O
tion O
for O
Computational O
Linguistics O
. O

Junru O
Zhou O
and O
Hai O
Zhao O
. O

2019 O
. O

Head O
- O
driven O
phrase O
structure O
grammar O
parsing O
on O
penn O
treebank O
. O

In O
Pro- O
ceedings O
of O
the O
57th O
Conference O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
ACL O
2019 O
, O
Florence O
, O
Italy O
, O
July O
28- O
August O
2 O
, O
2019 O
, O
Volume O
1 O
: O
Long O
Pa- O
pers O
, O
pages O
2396–2408 O
. O

Muhua O
Zhu O
, O
Yue O
Zhang O
, O
Wenliang O
Chen O
, O
Min O
Zhang O
, O
and O
Jingbo O
Zhu O
. O
2013 O
. O

Fast O
and O
accurate O
shift- O
reduce O
constituent O
parsing O
. O

In O
Proceedings O
of O
the O
51st O
Annual O
Meeting O
of O
the O
Association O
for O
Compu- O
tational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
434–443 O
, O
Soﬁa O
, O
Bulgaria O
. O
Association O
for O
Computa- O
tional O
Linguistics O
. O

Appendix O
6.1 O
Discourse O
Parsing O
Architecture O
Figure O
4 O
illustrates O
our O
end O
- O
to O
- O
end O
model O
architec- O
ture O
for O
discourse O
parsing O
. O

5807 O
Figure O
4 O
: O
Our O
discourse O
parser O
a O
long O
with O
the O
decoding O
process O
for O
a O
given O
sentence O
. O

The O
input O
to O
the O
decoder O
at O
each O
step O
is O
the O
representation O
of O
the O
span O
to O
be O
split O
. O

We O
predict O
splitting O
point O
using O
the O
biafﬁne O
function O
between O
the O
corresponding O
decoder O
state O
and O
the O
boundary O
representations O
. O

The O
relationship O
between O
left O
and O
right O
spans O
are O
assigned O
with O
the O
label O
using O
the O
label O
classiﬁer O
. O

6.2 O
Discourse O
Parsing O
Inference O
Algorithms O
Algorithm O
2 O
shows O
the O
end O
- O
to O
- O
end O
discourse O
pars- O
ing O
inference O
process O
. O

Algorithm O
2 O
Discourse O
Inference O
] O

Input O
: O
Sentence O
length O
n O
; O
boundary O
encoder O
states O
: O
( O
h0;h1;:::;hn O
) O
; O
label O
scores O
: O
P(lj(i;k);(k;j)),0i O
< O
kjn;l2L O
, O
initial O
decoder O
state O
st O
. O
Output O
: O

Parse O
treeT O
ST= O

[ O
( O
1;n O
) O
] O
// O
stack O
of O
spans O
S= O

[ O
] O
whileST O
6=?do O
( O
i;j O
) O
= O
pop(ST O
) O
prob;st O
= O
dec(st;(i;j O
) O
) O

k= O
arg O
maxi O
< O
kjprob O

curr_partial_tree O

= O
partial O
_ O
tree O
ifj 1 O
> O
k O
> O
i O
+ O
1then O
push(ST;(k;j O
) O
) O
push(ST;(i;k O
) O
) O
else O
ifj 1 O
> O
k O
= O
i+ O
1then O
push(ST;(k;j O
) O
) O
else O
ifk O
= O
j 1 O
> O
i+ O
1then O
push(ST;(i;k O
) O
) O
end O
if O
ifk6 O
= O
jthen O
push(S((i;k;j O
) O
) O

end O
if O
end O
while O
T= O

[ O
( O
( O
i;k;j O
) O
; O
argmaxlP(lj(i;k)(k;j))8(i;k;j O
) O
2S O
] O

