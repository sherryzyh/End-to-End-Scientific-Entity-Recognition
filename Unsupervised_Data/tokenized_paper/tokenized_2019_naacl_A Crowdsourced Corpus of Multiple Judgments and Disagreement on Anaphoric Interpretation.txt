Proceedings of NAACL - HLT 2019 , pages 1778–1789 Minneapolis , Minnesota , June 2 - June 7 , 2019 . 
c 
2019 Association for Computational Linguistics1778A Crowdsourced Corpus of Multiple Judgments and Disagreement on Anaphoric Interpretation Massimo Poesio Queen Mary University m.poesio@qmul.ac.ukJon Chamberlain University of Essex jchamb@essex.ac.ukUdo Kruschwitz University of Essex udo@essex.ac.uk Silviu Paun Queen Mary University 
s.paun@qmul.ac.ukAlexandra Uma Queen Mary University a.n.uma@qmul.ac.ukJuntao Yu Queen Mary University juntao.cn@gmail.com Abstract We present a corpus of anaphoric information ( coreference ) crowdsourced through a game- with - a - purpose . 
The corpus , containing anno- tations for about 108,000 markables , is one of the largest corpora for coreference for English , and one of the largest crowdsourced NLP cor- pora , but its main feature is the large num- ber of judgments per markable : 20 on aver- age , and over 2.2 M in total . 
This character- 
istic makes the corpus a unique resource for the study of disagreements on anaphoric in- terpretation . 
A second distinctive feature is its rich annotation scheme , covering single- tons , expletives , and split - antecedent plurals . 
Finally , the corpus also comes with labels in- ferred using a recently proposed probabilistic model of annotation for coreference . 
The la- bels are of high quality and make it possible to successfully train a state of the art corefer- ence resolver , including training on singletons and non - referring expressions . 
The annotation model can also result in more than one label , or no label , being proposed for a markable , thus serving as a baseline method for automatically identifying ambiguous markables . 
A prelimi- nary analysis of the results is presented . 
1 Introduction A number of datasets for anaphora resolution / coreference now exist ( Poesio et al . , 2016 ) , includ- ing ONTONOTES that has been the de facto stan- dard since the CONLL shared tasks in 2011 and 2012 ( Pradhan et al . , 2012 ) , and the just intro- duced and very substantial P RECOcorpus ( Chen et al . , 2018 ) . 
None of these datasets however take into account the research challenging the idea that a ‘ gold standard ’ interpretation can be obtained through adjudication , in particular for anaphora ( Poesio and Artstein , 2005b ; Wong and Lee , 2013 ; Aroyo and Welty , 2015 ) . 
Virtually every project devoted to large - scale annotation of discourse orsemantic phenomena has reached the conclusion that genuine disagreements are widespread . 
This has long been known for anaphora ( Poesio and Artstein , 2005b ; Versley , 2008 ; Recasens et al . , 2011 ) ( see also the analysis of disagreements in ONTONOTES in ( Pradhan et al . , 2012 ) ) and word- senses ( Passonneau et al . , 2012 ) , but more recent work has provided evidence that disagreements are frequent for virtually every aspect of language interpretation , not just in subjective tasks such as sentiment analysis ( Kenyon - Dean et al . , 2018 ) , but even in the case of tasks such as part - of - speech tagging ( Plank et al . , 2014 ) . 
In fact , researchers in the CrowdTruth project view disagreement as positive , arguing that “ disagreement is signal , not noise ” ( Aroyo and Welty , 2015 ) . 
In this paper we present what to our knowledge is the largest cor- pus containing alternative anaphoric judgments : 20.6 judgments per markable on average ( up to 90 judgments in some cases ) for about 108,000 markables . 
We are not aware of any comparable resource for studying disagreement and ambiguity in anaphora or indeed any other area of NLP . 
We present some preliminary analysis in the paper . 
The corpus presented in this paper is also the largest corpus for anaphora / coreference entirely created through crowdsourcing , and one of the largest corpus of coreference information for En- glish in terms of markables . 
So far , only fairly small coreference corpora have been created using crowdsourcing ( Chamberlain et al . ; 
Guha et al . , 2015 ) . 
The corpus presented here provides anno- tations for about 108,000 markables , 55 % of the number of markables in ONTONOTES . 
Another novelty is that the corpus was created through a ‘ quasi ’ Game - With - A - Purpose ( GWAP ) ( von Ahn , 2006 ; 
Lafourcade et al . , 2015 ) , Phrase Detectives ( Poesio et al . , 2013 ) , and is , to our knowledge , the largest GWAP -created corpus for NLP . 
( So far , the success of GWAP s in other areas of science ( Clery , 
17792011 ; Cooper et al . , 2010 ) has not been replicated inNLP . ) 
Finally , the corpus is notable for a richer annotation scheme than the other large corefer- ence corpora . 
Singletons were marked as well as mentions participating in coreference chains ( the omission of singletons being one of the main prob- lems with ONTONOTES ) . 
Non - referring expres- sions were also annotated : both expletives ( not annotated either in ONTONOTES or P RECO ) and predicative NPs . 
Finally , all types of plurals were annotated , including also split - antecedent plu- rals as in John met with Mary , and they went to dinner , which again are not annotated either in ONTONOTES or P RECO . 
Turning a crowdsourced corpus into a high- quality dataset suitable to train and evaluate NLP systems requires , however , an aggregation method appropriate to the data and capable of achieving sufﬁcient quality , something that simple major- ity voting typically can not guarantee ( Dawid and Skene , 1979 ; Hovy et al . , 2013 ) . 
What made it possible to extract such a dataset from the col- lected judgments was the recent development of a probabilistic method for aggregating coreference annotations called MPA ( Paun et al . , 2018b ) . 
MPA extracts silver labels from a coreference annota- tion and associates them with a probability , allow- ing for multiple labels in cases of ambiguity . 
As far as we know , ours is the ﬁrst use of MPA to cre- ate a large - scale dataset . 
We show in the paper that MPA can be used to extract from the judgments a high quality coreference dataset that can be used to develop standard coreference resolvers , as well as to investigate disagreements on anaphora . 
2 Background 2.1 Datasets for Anaphora / Coreference Since the two CONLL shared tasks ( Pradhan et al . , 2012 ) , ONTONOTES has become the dominant re- source for anaphora resolution research ( Fernan- des et al . , 2014 ; Bj ¨orkelund and Kuhn , 2014 ; Martschat and Strube , 2015 ; Clark and Man- ning , 2015 , 2016a , b ; Lee et al . , 2017 , 2018 ) . 
ONTONOTES contains documents in three lan- guages , Arabic ( 300 K tokens ) , Chinese ( 950 K ) and English ( 1.6 M ) , from several genres but pre- dominantly news . 
One frequently discussed lim- itation of ONTONOTES is the absence of single- tons ( De Marneffe et 
al . , 2015 ; Chen et al . , 2018 ) , which makes it harder to train models for mention detection ( Poesio et al . , 2018 ) . 
Another limitationis that expletives are not annotated . 
As a conse- quence , downstream applications such as machine translation ( Guillou and Hardmeier , 2016 ) that re- quire pronoun interpretation have to adopt various workarounds . 
Because of these two restrictions , ONTONOTES only has 195 K markables , and a low markable density ( 0.12 markable / token ) . 
A number of smaller corpora provide linguisti- cally richer information ( Poesio et al . , 2016 ) . 
Ex- amples include ANCORA for Spanish ( Recasens and Mart ´ ı , 2010 ) , TUBA -D / Zfor German ( Hin- richs et al . , 2005 ) , the Prague Dependency Tree- bank for Czech and English ( Nedoluzhko et al . , 2009 ) , and ARRAU for English ( Uryupina et al . , To Appear ) . 
In ARRAU , for example , single- tons and expletives are annotated as well , as are split antecedent plurals , generic coreference , dis- course deixis , and bridging references . 
The AR- RAU corpus is relatively small in terms of to- kens ( 350 K ) , but has a higher markable density than ONTONOTES ( 0.29 markable / token ) , so it has around 100 K markables , half the number of ONTONOTES .ARRAU was recently used in the CRAC 2018 shared task ( Poesio et 
al . , 2018 ) to evaluate a number of anaphora resolution tasks . 
The recently introduced P RECOcorpus ( Chen et al . , 2018 ) is the largest existing coreference cor- pus , consisting of 35,000 documents for a total of 12.5 M tokens and 3.8 M markables , half of which are singletons . 
However , the corpus is not in- tended as a general purpose dataset as only the 3000 most common English words appear in the documents ( the majority - 2/3 - of the documents are from Chinese high - school English tests ) . 
The corpus ’s annotation scheme mainly follows the ONTONOTES guidelines , with a few important dif- ferences : singleton mentions and generic coref- erence are annotated , event anaphora is not , and predicative NPs are annotated as co - referring with their argument , as previously done in the MUC ( Grishman and Sundheim , 1995 ; Chinchor , 1998 ) and ACE ( Doddington et al . , 2004 ) 
corpora.1As one could expect , the corpus is relatively easy for coreference systems . 
The Peters et al . 
( 2018 ) sys- tem trained and tested on P RECOachieves an av- 1An example of predicative NPis24 degrees 
inThe tem- perature is 24 degrees . 
As discussed by van Deemter and Kibble ( 2000 ) , annotating The temperature and24 degrees as coreferent would result in nonsensical coreference chains for sentences like The temperature was 24 degrees 
but it is 27 degrees now . 
As a result , such markables were annotated as predicative in recent corpora . 
It ’s not clear why we ﬁnd a return to the old practice in P RECO . 
1780erage CONLL score of 81.5 % , whereas the same system trained and tested on ONTONOTES only achieves a score of 70.4 % . 
2.2 Crowdsourcing and GWAPs for NLP A revolution in the way language annotation tasks are carried out was achieved by crowdsourcing ( Howe , 2008 ; Snow et al . , 2008 ) . 
Crowdsourcing comes in many forms , including citizen science andmicroworking . 
A third approach is to use a game - with - a - purpose ( GWAP ) to aggregate data from non - expert players for collective decisions similar to those from an expert ( von Ahn , 2006 ) . 
The game - based approach to collecting language data is initially costly , but once a game is deployed it can continue to collect data with very little ﬁnan- cial support , especially if there is an active com- munity . 
GWAP s such as Phrase Detectives ( Poesio et 
al . , 2013 ) , JeuxDesMots ( Joubert and Lafour- cade , 2008 ) and Zombie Lingo ( Fort et al . , 2014 ) have been used in NLP to collect data on speciﬁc linguistic features ; broader platforms such as Wor- drobe ( Venhuizen et al . , 2013 ) to gamify the entire text annotation pipeline . 
Crowdsourcing is the most realistic approach to collect a large number of judgments about phe- nomena such as anaphora . 
Games in particular are the one type of crowdsourcing scalable to the goal of , for example , a 100 M word corpus . 
So far , however , only small and medium scale resources forNLP have been created via crowdsourcing . 
For coreference we are only aware of two , both around 50 K tokens in size ( Chamberlain et al . ; 
Guha et al . , 2015 ) . 
The Groningen Meaning Bank being col- lected through the Wordrobe platform ( Bos et al . , 2017 ) includes many more documents , but so far only very few interpretations have been obtained through the games ( e.g. , only around 4 K judg- ments have been collected for anaphora ) . 
2.3 Collecting Multiple Judgments In most of the best known efforts at creating anaphoric corpora for English and other lan- guages substantial disagreements between the coders were observed , but none of the result- ing resources contains multiple anaphoric inter- pretations . 
Systematic analyses of the disagree- ments among coders observed in such annotation efforts were provided for ANCORA by Recasens et al . 
( 2011 ) and for TUBA -D / Zby Versley ( 2008 ) . 
The entire ONTONOTES corpus was double anno- tated , ﬁnding disagreements on around 20 % of themarkables , i.e. , around 40,000 cases . 
An analysis of such disagreements can be found in ( Pradhan et al . , 2012 ) , but ultimately only the result of ad- judication was included in the corpus . 
Most of the PRECOcorpus was doubly annotated and the re- sults adjudicated , but only the result of adjudica- tion is released . 
We are aware of only two corpus annotation schemes which explicitly allowed the annotation of anaphoric ambiguity : ARRAU and the Pots- dam Commentary Corpus ( Krasavina and Chiar- cos , 2007 ) . 
Most of the ARRAU corpus was single- annotated by a highly experienced annotator , who was allowed to mark a variety of cases of ambi- guity ( Poesio and Artstein , 2005b ) . 
It is known , however , that such explicit marking of ambiguity is difﬁcult ( Poesio and Artstein , 2005b ; Recasens et al . , 2012 ) , and indeed not many cases of ambi- guity were marked in this way in ARRAU . 
3 Collecting the Judgments In this Section we discuss what type of judgments were collected , and how . 
3.1 A gamiﬁed approach The gamiﬁed online platform Phrase Detectives2 ( Chamberlain et al . , 2008 ; Poesio et 
al . , 2013 ) was used to collect the judgments about anaphoric ref- erence included in the corpus . 
Phrase Detectives is articulated around a number of tasks centered around the detective metaphor and uses scoring , progression and a variety of other mechanisms to make the activity enjoyable . 
In annotation mode ( Name the Culprit ) , the participant provides an anaphoric judgment about a highlighted markable ( the possible judgments according to the annota- tion scheme are discussed next ) . 
If different par- ticipants enter different interpretations for a mark- 
able then each interpretation is presented to other participants in validation mode ( Detectives Con- ference ) , in which the participants have to agree or disagree with the interpretation . 
One of the key differences between Phrase De- tectives and GWAP s such as those developed by von Ahn and his lab ( von Ahn , 2006 ) is the much greater complexity of judgments required . 
Yet clearly we can not expect participants to be experts about anaphora , or to be willing to read a manual explaining the annotation scheme , so all the train- ing still has to be done while playing the game . 
2http://www.phrasedetectives.com 
1781Therefore , we developed a number of mechanisms that could help in this respect : giving suggestions and tips ( global , contextual and FAQ ) , compar- ing decisions with the gold standard , and showing agreement with other players in Validation Mode . 
When participants begin to play they are shown training texts ( in which the answer is known from a gold standard ) and get feedback as to whether their decisions agree with the gold standard . 
Once the player has completed all training tasks they are given a user rating ( the percentage of correct deci- sions out of the total number of training tasks ) . 
As of 17th of March 2019 , 60,822 individuals have participated in Phrase Detectives over ten years and using different platforms , providing over 4.26 million judgments , about half of which are included in the present release . 
3.2 Types of Judgments The judgments asked to the participants to Phrase Detectives follow a simpliﬁed version of the AR- RAU annotation scheme , which is the result of ex- tensive tests for intercoder agreement ( Uryupina et al . , To Appear ) . 
The participants are asked to make two basic distinctions : whether a markable is referring or not , and if referring , whether it is Discourse - Old ( DO ) , i.e. , it refers to an entity al- 
ready mentioned ( in which case the players were asked to indicate the latest mention of that en- tity ) , or Discourse - New ( DN ) , i.e. , it introduces a new entity in the discourse . 
Anaphoric refer- ence marked include split antecedent anaphora , as in John met Mary , and they went out for drinks , where the antecedent for they is the set consisting of the separately introduced John andMary . 
Two types of non - referring expressions were marked : expletives , as in It ’s ﬁve o’clock orThere was a ﬁreplace in the room ; and predicative NPs , as in The temperature is 24 degrees . 
In the case of pred- icative NPs , players were asked to mark the near- est mention of the entity that the predication ap- plied to , following in this case the ONTONOTES approach instead of ARRAU ’ s. 
The key difference between this corpus and any other existing corpus for anaphora / coreference with the exception of ARRAU is that the corpus was designed to collect information about dis- agreement . 
The main difference from ARRAU is that no attempt was made to ask players to iden- tify ambiguity , as that has proven hard or impos- sible to do ( Poesio and Artstein , 2005b ) . 
Insteadofexplicit ( marking of ) ambiguity , the develop- ers relied on implicit ambiguity : that genuine am- biguity would emerge if enough players supplied judgments . 
All the judgments produced by the players were therefore stored , without attempting to choose among them at collection . 
The differences between the four corpora being compared are summarized in Table 1 , modelled on a similar table in ( Chen et al . , 2018 ) . 
In the Phrase Detectives corpus predication and corefer- ence are clearly distinguished , as in ONTONOTES and ARRAU but unlike in P RECO . 
Singletons are considered markables . 
Expletives and split antecedent plurals are marked , unlike in either ONTONOTES or P RECO . 
Most importantly , ambi- guity of anaphoric interpretation ( as in the exam- ple from the TRAINS corpus ( Poesio and Artstein , 2005b ) ) is marked , but implicitly , i.e. , by asking the judgment of at least 8 players per markable , as opposed to explicitly , as attempted in ARRAU ( with little success ) . 
3.3 Markable identiﬁcation Following standard practice in anaphoric annota- tion and GWAP s , the markables to be annotated were not identiﬁed by the participants themselves ; instead , markable identiﬁcation was carried out semi - automatically . 
Each document would ﬁrst be processed by a pipeline combining off - the - shelf tools ( sentence splitting and tokenization using the OpenNLP pipeline3and parsing using the Berke- ley Parser ( Petrov and Klein , 2007 ) ) and custom preprocessing and post - processing heuristic steps to correct the output . 
( See ( Poesio et al . , 2013 ) for more details about the pipeline and its perfor- mance . ) 
Then one of the administrators would carry out a quick check of the document remov- ing the most obvious mistakes before uploading it . 
After the document was uploaded , participants could report markable errors , which would then be corrected by hand.4 4 The corpus 4.1 Basic statistics 
This second release of the Phrase Detectives cor- pus consists of a total of 542 documents contain- 
3http://opennlp.apache.org 4As participants report over 10,000 errors per year , it be- came quickly apparent that carrying out the corrections our- selves was unfeasible . 
In subsequent work , we developed a gamiﬁed approach to markable identiﬁcation and correction centered around the TileAttack ! 
GWAP ( Madge et al . , 2017 ) . 
1782Type Example ONTONOTES PRECO 
ARRAU Present corpus predicative NPs 
[ John ] is a teacher 
Pred Coref Pred Pred [ John , a teacher ] singletons 
No 
Yes 
Yes Yes expletives It ’s ﬁve o’clock 
No 
No Yes Yes split antecedent plurals 
[ John ] met [ Mary ] 
No 
No 
Yes Yes and they ... 
generic mentions 
[ Parents ] are usually busy . 
Only with Yes 
Yes Yes Parents should get involved pronouns event 
anaphora Sales [ grew ] 10 % . 
Yes No 
Yes No This growth is exciting ambiguity Hook up [ the engine ] 
No No Explicit Implicit to [ the boxcar ] and send it to Avon Table 1 : Comparison between the annotation schemes in ONTONOTES , PRECO , ARRAU and the present corpus Docs Tokens Markables PDgoldGutenberg 5 7536 1947 ( 1392 ) Wikipedia 35 15287 3957 ( 1355 ) GNOME 5 989 274 ( 96 ) Subtotal 45 23812 6178 ( 2843 ) 
PDsilverGutenberg 145 158739 41989 ( 26364 ) Wikipedia 350 218308 57678 ( 19444 ) Other 2 7294 2126 ( 1339 ) 
Subtotal 497 384341 101793 ( 47147 ) 
All Total 542 408153 107971 ( 49990 ) Table 2 : Summary of the contents of the current re- lease . 
The numbers in parentheses indicate the total number of markables that are non - singletons . 
ing 408 K tokens and 108 K markables from two main genres : Wikipedia articles and ﬁction from the Gutenberg collection . 
This corpus is divided in two subsets . 
The subset we refer to as PDsilver consists of 497 documents , for a total of 384 K to- kens and 101 K markables , whose annotation was completed – i.e. 8 judgments per markable were collected , and 4 validations per interpretation – as of 12th of October 2018 . 
In these documents , an aggregated ( ‘ silver ’ ) label obtained through MPA ( see next Section ) is also provided . 
45 additional documents were also gold - annotated by two ex- perts annotators . 
We refer to the subset of the cor- pus for which both gold and silver annotations are available as PDgold , as it is intended to be used as test set.5The gold subset consists of a total of 23 K tokens and 6 K markables . 
The contents of the cor- pus are summarized in Table 2 . 
By comparison , the English corpus used for the CONLL 2011 and 2012 shared tasks consists of 3493 documents , for a total of 1.6 M tokens and 5PDgoldis the dataset released in 2016 as Phrase Detectives corpus , Release 1 ( Chamberlain et al.).1 2 3 4 5 6 7 PDgold 38.8 30.6 18.5 7.3 2.5 1.0 0.6 PDsilver 36.0 30.0 19.0 8.8 3.8 1.8 0.8 Table 3 : Percentage of markables with X distinct inter- pretations 194480 markables . 
In other words , although the current release of the corpus is only about 25 % of the CONLL corpus in terms of tokens , it is 55.5 % of its size in terms of annotated markables , i.e. , actual training / testing items . 
4.2 Number of Judgments In total , 2,235,664 judgments from 1958 play- ers are included in the current release , of which 1,358,559 annotations and 867,844 validations . 
On average , 20.6 judgments were collected per markable : 12.6 annotations and 8 validations . 
In addition , around 10 K expert judgments were col- lected for the gold portion of the corpus from two expert annotators . 
This compares with 600 K esti- mated judgments for the entire ONTONOTES cor- pus , about 3 per markable ( total number of annota- tors not known ) , and around 10 M for P RECO , also 3 per markable , from about 80 annotators . 
4.3 Disagreement : a preliminary analysis The ‘ raw ’ statistics about disagreement in the cor- pus are shown in Table 3 . 
In total , only 35.7 % of the markables in the corpus ( 38,579 ) were as- signed only one interpretation by the participants , whereas 64.3 % received more than one interpre- tation . 
This ﬁgure would seem to suggest mas- sive ambiguity , but we are not saying that 64.3 % of markables in the corpus are ambiguous . 
As al- ready pointed out e.g. in ( Pradhan et al . , 2012 ) , 
1783there are a number of reasons for disagreements among coders / players apart from ambiguity . 
In the case of ONTONOTES , the causes for the 20,000 observed disagreements include : Ambiguity proper , i.e. , unclear interpreta- tion ( ’ Genuine Ambiguity ’ in ( Pradhan et al . , 2012 ) ) and/or disagreement on reference ( 31 % of the disagreements in ONTONOTES , around 7 % of all markables ) ; 
Annotator error ( another 25 % of the cases of disagreement in ONTONOTES ) ; Various limitations of the coding scheme : unclarity in the guidelines , inability to mark certain types of coreference e.g. , between generics , etc . ( 36.5 % of the cases of dis- agreement in ONTONOTES ) . 
Interface limitations ( around 7.5 % of the dis- agreements in ONTONOTES ) . 
Some of the disagreements due to other causes – and in particular annotation errors – can be ﬁltered through validation , i.e. , by excluding those inter- pretations of a markable for which the validation score ( annotations + agreements - disagreements ) falls below a threshold . 
For example , if only inter- pretations with a validation score > 0are consid- ered , we ﬁnd that 51,075 / 107,971 markables have at least two such interpretations , i.e. , 47.3 % of the total , which is considerably less than the 64.3 % of markables with more than one interpretation , but it ’s still a large number . 
We will discuss a more sophisticated method for automatically identifying plausible interpreta- tions , as well as the results of a preliminary hand- analysis of the disagreements in a few documents in our corpus , in Section 7 . 5 Aggregation 5.1 Probabilistic Aggregation Methods 
The data collected via Phrase Detectives require an aggregation method to help choose between the different interpretations provided by the play- ers . 
Simple heuristics such as majority voting are known to underperform compared to probabilis- tic models of annotation ( Whitehill et al . , 2009 ; Raykar et al . , 2010 ; 
Quoc Viet Hung et al . , 2013 ; Sheshadri and Lease , 2013 ; Hovy et al . , 2013 ; Pas- sonneau and Carpenter , 2014 ; Paun et al . , 2018a).The models offer a rich framework of interpreta- tion and can employ distinct prior and likelihood structures ( pooled , unpooled , and partially pooled ) and a diverse set of effects ( annotator ability , item difﬁculty , or a subtractive relationship between the two ) . 
However , most work on models of anno- tation assumes that the set of classes the anno- tators can choose from is ﬁxed across the anno- tated items , which is not the case for anaphoric annotation . 
More speciﬁcally , in Phrase Detec- tives the participants can classify a markable as non - referring ( expletive or predicative ) ; as intro- ducing a new discourse entity ; or as discourse- old , in which case they link it to the most recent mention of its antecedent – and coreference chains are document - speciﬁc and not ﬁxed in number ( see Section 3.2 for more details on the annotation scheme ) . 
Recently , however , Paun et al . 
( 2018b ) developed a probabilistic model ( MPA ) able to ag- gregate such crowdsourced anaphoric annotations . 
5.2 MPA InMPA , the term label is used to refer to a speciﬁc interpretation provided by a player , and the term class to refer to general interpretation categories such as discourse old , discourse new , expletive , or predicative NP . 
Please note that under this formal- ism each label belongs to a class : the antecedents belong to the discourse old category , while the other possible labels ( e.g. , discourse new ) coin- cide with the classes they belong to . 
The model as- sumes a preprocessing step in which the markable- level annotations are transformed into a series of binary decisions with respect to each candidate label . 
MPA then models these ( label - level ) deci- sions as the result of the sensitivity ( the true pos- itive rate ) and speciﬁcity ( the true negative rate ) of the annotators which it assumes are class de- pendent . 
This latter assumption allows inferring different levels of annotator ability for each class ( thus capturing , for instance , the fact that whereas most participants are generally able to recognize discourse - new mentions , they are much less good at identifying correct antecedents ) . 
5.3 Aggregating the game data We use the MPA model as a component in a stan- dard mention - pair framework to extract corefer- ence clusters : 1 ) link each markable with the most likely label as identiﬁed by the model , and 2 ) follow the link structure to build the coreference 
1784MethodDiscouse old class Discourse new class Predicative NPs class Expletives class Avg . F1Accuracy P R F1 P R F1 P R F1 P R F1 MAJVOTE 94.5 62.8 75.4 79.1 99.0 87.9 53.9 9.7 16.4 97.2 71.4 82.4 65.5 82.9 MPA 90.4 87.3 88.8 94.5 96.0 95.3 64.0 72.4 68.0 94.1 98.0 96.0 87.0 92.2 Table 4 : A per class evaluation of aggregated interpretations against expert annotations . 
MethodMUC BCUB CEAFE Avg . F1P R F1 P R F1 P R F1 Singletons includedMAJVOTE 96.0 63.9 76.7 95.7 78.7 86.4 77.1 94.9 85.1 82.7 MPA 91.6 82.4 86.8 94.8 87.8 91.2 92.4 93.8 93.1 90.3 STANFORD 65.4 62.4 63.8 78.9 76.1 77.5 78.4 85.2 81.7 74.3 Singletons excludedMAJVOTE 96.1 64.8 77.4 93.8 45.0 60.8 66.3 48.5 56.1 64.8 MPA 92.2 89.2 90.7 88.1 77.8 82.6 79.5 80.2 79.8 84.4 STANFORD 65.7 62.1 63.9 50.3 42.5 46.1 42.7 49.8 46.0 52.0 Table 5 : The quality of the coreference chains for the PDgoldsubset . 
chains . 
We next evaluate both of these compo- nents against expert annotations . 
Table 4 shows a per class evaluation of the aggregated interpretations from the PDgoldsub- set . 
The results indicate an overall better agree- ment with the expert annotations of MPA com- pared with a simple majority voting ( MAJVOTE ) baseline . 
This is because MAJVOTE makes the an implicit assumption that the annotators have equal expertise , which is not true in general even with data crowdsourced on microworking platforms , and even more so with data collected through GWAP s ( Paun et al . , 2018a ) . 
After inferring the mention pairs , coreference chains can be extracted and their quality as- sessed using standard coreference metrics . 
Table 5 presents the evaluation against gold chains in PDgold . 
We compare the chains produced from the mention pairs inferred by MPA and by MAJVOTE , and the chains produced by the STANFORD deter- ministic coreference system ( Lee et al . , 2011 ) ( for which we switched off post - processing to output singleton clusters ) . 
The results indicate a far bet- ter quality of the chains produced using MPA over the alternative methods . 
Another interesting re- sult is that even a simple MAJVOTE baseline based on crowdsourced annotations performed far better than the STANFORD system , underlining the ad- vantage of crowdsourced annotations for corefer- ence over automatically produced annotations.6 Using the corpus for coreference resolution Some NLP researchers may question the useful- ness of the information about disagreements for coreference resolution ( or other NLPtasks ) . 
In this Section , we demonstrate that even those purely interested in CONLL -style coreference resolution can use the Phrase Detectives corpus aggregated with MPA as a dataset . 
We use PDsilver to train a coreference system able to simultaneously iden- tify non - referring expression and build corefer- ence chains ( including singletons ) . 
As no other system of this type exists at the moment , we de- veloped one ourselves . 
6.1 Our system The system trained and tested on the corpus is a cluster ranking system that does mention detec- tion and corefence resolution jointly . 
The system uses the mention representation from the state- of - the - art ( Lee et al . , 2018 ) system , but replaces their mention - ranking model with a cluster rank- ing model . 
Our cluster ranking model forms clus- ters by going through the candidate mentions in their text order and adding them to the clusters , which take into consideration the relative impor- tance of the mentions . 
An attention mechanism is used to assign mentions within the clusters salience scores , and the clusters are represented as the weighted sums of the mention representa- tions . 
Separate classiﬁers are used to identify non- referring markables and singletons . 
1785Singletons MethodMUC BCUB 
CEAFE Avg . F1P R F1 P R F1 P R F1 Included Our Model 79.3 72.5 75.7 72.1 69.3 70.7 70.5 73.2 71.8 72.7 ExcludedOur Model 79.3 72.5 75.7 58.3 52.4 55.2 58.3 49.5 53.5 61.5 Our Model * 77.8 71.8 74.6 55.4 53.7 54.6 56.2 49.0 52.4 60.5 Lee et al . 
( 2018 ) * 80.8 66.1 72.7 63.3 45.1 52.7 56.7 44.7 50.0 58.5 Table 6 : The CoNLL scores for our systems trained on PDsilverand tested on PDgold . 
* indicates the models trained on the simpliﬁed corpus . 
P R F1 
Non - referring 55.2 54.0 54.6 Expletives 62.3 86.0 72.3 Predicative NPs 49.7 47.7 48.7 Table 7 : Non - referring scores for our model 6.2 Evaluation Methodology We randomly chose 1/20 of PDsilver as a develop- ment set and use the rest as the training set ; PDgold was used as test set . 
To get a baseline , we compare the results of our system on a simpliﬁed version of the corpus without singletons and expletives with those ob- tained by the current state - of - the - art system on ONTONOTES , Lee et al . 
( 2018 ) trained and tested on the same data . 
6.3 Results Table 6 shows the results of both systems on the simpliﬁed corpus . 
Our cluster ranking system achieved an average CONLL score of 60.5 % , out- performing the Lee et al . 
( 2018 ) system by 2 per- centage points . 
Note that the Lee et al . 
( 2018 ) sys- tem achieved a higher score on the CONLL data , which suggests that the present corpus is different from that dataset . 
In the same Table , we also report the results ob- tained by training our system on the full corpus including both non - referring expressions and sin- gletons . 
This version of system achieves an aver- age CONLL score of 72.7%.6We will note that although this score is on system mentions , it is very close to the score ( 74.3 % ) achieved by the Stanford deterministic system evaluated with gold mentions ( see Table 5 in Section 5 ) . 
Also , this model trained on the full corpus including single- 6The Extended Coreference Scorer developed for the 2018 CRAC shared task ( Poesio et al . , 2018 ) was used to eval- uate coreference chains on a corpus using singletons and to assess non - referring expressions identiﬁcation.tons achieves a gain of 1 percentage point when compared with the model trained on the simpli- ﬁed corpus even when evaluated in a singleton ex- cluded setting . 
This indicates that the availability of the singletons is also helpful for resolving non- singleton clusters . 
In total , this model achieved a CONLL score on singletons excluded of 3 percent- age points 3 % better than our baseline . 
Regarding the task of identifying non - referring mentions , our model achieved a F1 score of 54.6 % ( see Table 7 ) . 
The scores of the system on distinct types of non - referring expressions is presented in the following two rows of Table 7 . 
Our model achieved a higher F1 score of 72.3 % on expletives , and a lower score ( 48.7 % ) on predicative NPs . 
Overall , these results – the ﬁrst results on system mentions for PDgold – suggest that the silver corpus is sufﬁcient to train a state - of - the - art system and achieve a reasonably good performance . 
Also , that training a model on a corpus enhanced by single- tons and non - referring markables results in a better CONLL score when compared with a model trained on the simpliﬁed corpus . 
7 Disagreements , revisited In the previous Section we showed that MPA can be used to extract a silver standard out of the an- notations that is suitable to train a CONLL -style coreference resolver or an extended coreference resolver also attempting identiﬁcation of single- tons and non - referring expressions . 
The key prop- erty of the corpus however is the information it provides about disagreements . 
The second use- ful contribution of MPA is that it can be used to get an assessment of the ambiguity of markables which is more reﬁned than that discussed in Sec- tion 4.3 . 
For each markable , MPA assigns a proba- bility to each interpretation . 
Given that the model does not assume the existence of a ‘ gold ’ , there are three possible cases for each markable : either only one interpretation has a probability above a 
1786None One Two Zero or more PDgold 2.3 % 93.4 % 4.3 % 6.6 % PDsilver 3.5 % 94 % 2.4 % 5.9 % Table 8 : Ambiguity in the corpus according to MPA certain threshold – say , 0.5 ; or more than one inter- pretation is above that threshold ; or none is . 
This assessment of ambiguity according to MPA is sum- marized in Table 8 . 
This assessment appears to suggest a similar prevalence of ambiguity in our corpus than found inONTONOTES in the already mentioned analysis by Pradhan et al . 
( 2012 ) . 
In order to verify this , two experts hand - analyzed 2 documents in PDgold containing a total of 900 markables : Little Red Cap ( LRC ) andRobber Bridegroom ( RG ) . 
Given that each markable has on average 20 interpreta- tions , and that player errors are frequent ( there is at least one player error for almost every markable ) it was n’t possible to use the same categories as Pradhan et al . 
Instead , we simply attempted to as- sign markables to one of the categories : Genuine ambiguity ( GA ) , Interface or Coding Scheme Problem ( ICP ) , Other ( O ) . 
The results are sum- marized in Table 9 . 
The Table has one row per document . 
The ﬁrst column lists the total num- ber of markables in a document ; the second ( Dis ) the percentage of markables on which there is dis- agreement ; the third ( GA ) the percentage of the total number of markables which are cases of gen- uine ambiguity ; and the fourth ( ICP ) the percent- age which are cases of Interface or Coding Scheme Problem . 
As we can see from the Table , 9 % of the total number of markables in these documents ( 80 out of 865 ) are genuinely ambiguous , i.e. , that 12.6 % of the disagreements ( 80 out of 633 ) are cases of genuine ambiguity . 
These are only pre- liminary ﬁgures , and we suspect that the ultimate ﬁgures on the prevalence of ambiguity are go- ing to be much higher , given that Recasens et al . 
( 2012 ) report that 12 - 15 % of coreference relations in their corpus are cases of quasi - coreference , and that Poesio and Artstein ( 2005a ) report a ﬁgure of 42.6 % once ambiguity on discourse deictic refer- ence are taken into account . 
We next checked the extent to which MPA can correctly predict genuine ambiguity . 
The results suggest that MPA is good at removing spurious am- biguity , but as a predictor of ambiguity it only has a recall of around 20 % and a precision of slightlyTotal Dis GA ICP LRC 401 79.1 % 7 % ( 28 ) 7.7 % ( 31 ) RG 464 68.3 % 11.2 % ( 52 ) 12.9 % ( 60 ) Average 73.7 % 9.1 % 10.3 % Table 9 : Analysis of disagreements in two corpus doc- uments under 50 % . 
Improving these results is one of the objectives of our current research . 
8 Conclusions We presented a novel resource for anaphora that , because of its annotation scheme and size , at the very least should be useful to those in the commu- nity interested in developing systems able to per- form a more comprehensive form of anaphora res- olution , including for instance expletive detection and split antecedent resolution . 
The key property of this new resource however is that it provides a large number of judgments about each anaphoric expression , thus enabling the development of sys- tems that do not make the assumption that a ‘ gold standard ’ exists , an assumption questioned by all studies associated with the creation of the current resources for the task . 
The dataset is also to our knowledge the ﬁrst solid evidence that the games- with - a - purpose approach can be successfully de- ployed to obtain substantial resources for NLP . 
The corpus is freely available from the Lin- guistic Data Consortium and from http:// www.dali-ambiguity.org . 
It is distributed in three formats : an XML format including all the judgments , suitable for analysis of disagree- ments and/or the development of systems taking disagreement into account ; and in CONLL and CRAC 18 format , with only the gold annotation or the silver label extracted , for those interested in using the corpus as an alternative resource for de- veloping coreference systems only . 
Acknowledgements This research was supported by the DALI project , funded by the European Research Council ( ERC ) , Grant agreement ID : 695662 . 
1787References Luis von Ahn . 2006 . 
Games with a purpose . 
Com- puter , 39(6):92–94 . 
Lora Aroyo and Chris Welty . 2015 . 
Truth is a lie : Crowd truth and the seven myths of human anno- tation . 
AI Magazine , 36(1):15–24 . 
Anders 
Bj ¨orkelund and Jonas Kuhn . 2014 . 
Learn- ing structured perceptrons for coreference resolution with latent antecedents and non - local features . 
In Proceedings of the 52nd ACL , volume 1 , pages 47 – 57 . 
Johan Bos , Valerio Basile , Kilian Evang , Noortje J. Venhuizen , and Johannes Bjerva . 
2017 . 
The gronin- gen meaning bank . 
In N. Ide and J. Pustejovsky , ed- itors , The Handbook of Linguistic Annotation , chap- ter 18 , pages 463–496 . 
Springer . 
Jon Chamberlain , Massimo Poesio , and Udo Kr- uschwitz . 
phrase detectives corpus . 
Jon Chamberlain , Massimo Poesio , and Udo Kr- uschwitz . 2008 . 
Phrase Detectives : A web - based collaborative annotation game . 
In Proceedings of I- Semantics 2008 . 
Hong Chen , Zhenhua Fan , Hao Lu , Alan Yuille , and Shu Rong . 
2018 . 
PreCo : 
A large - scale dataset in preschool vocabulary for coreference resolution . 
In Proceedings of EMNLP , pages 172–181 , Brussels , Belgium . 
Nancy Chinchor . 
1998 . 
Overview of MUC-7 . 
In Proceedings of the Seventh Message Understanding Conference ( MUC-7 ) . 
Kevin Clark and Christopher D. Manning . 
2015 . 
Entity - centric coreference resolution with model stacking . 
In Proceedings of the ACL . 
Kevin Clark and Christopher D. Manning . 
2016a . 
Deep reinforcement learning for mention - ranking coreference models . 
In Proceedings of EMNLP . 
Kevin Clark and Christopher D. Manning . 
2016b . 
Im- proving coreference resolution by learning entity- level distributed representations . 
In Proceedings of the ACL . 
Daniel Clery . 2011 . 
Galaxy evolution . 
Galaxy Zoo vol- unteers share pain and glory of research . 
Science , 333(6039):173–5 . 
Seth Cooper , Firsas Khatib , Adrien Treuille , Janos Barbero , Jeehyung Lee , Michael Beenen , Andrew Leaver - Fay , David Baker , Zoran Popovic , and the Foldit Players . 
2010 . 
Predicting protein structures with a multiplayer online game . 
Nature , 466:756 – 760 . 
Alexander P. Dawid and Allan M. Skene . 
1979 . 
Maximum likelihood estimation of observer error- rates using the EM algorithm . 
Applied Statistics , 28(1):20–28.Marie - Catherine De Marneffe , Marta Recasens , and Christopher Potts . 2015 . 
Modeling the lifespan of discourse entities with application to coreference resolution . 
Journal of Artiﬁcial Intelligence Re- search , 52(1):445–475 . 
Kees van Deemter and Rodger Kibble . 
2000 . 
On core- ferring : Coreference in MUC and related annotation schemes . 
Computational Linguistics , 26(4):629 – 637 . Squib . 
George R Doddington , Alexis Mitchell , Mark A Przy- bocki , Lance A Ramshaw , Stephanie Strassel , and Ralph M Weischedel . 
2004 . 
The automatic content extraction ( ace ) program - tasks , data , and evaluation . 
InProceedings of LREC , volume 2 , page 1 . Eraldo R. Fernandes , C ´ ıcero N. dos Santos , and Ruy L. Milidi ´ u. 2014 . 
Latent trees for coreference resolu- tion . 
Computational Linguistics , 40(4):801–835 . 
Karen Fort , Bruno Guillaume , and H. Chastant . 
2014 . 
Creating Zombilingo , a game with a purpose for de- pendency syntax annotation . 
In Proceedings of the 1st International Workshop on Gamiﬁcation for In- formation Retrieval ( GamifIR’14 ) , pages 2–6 . ACM . 
Ralph Grishman and Beth Sundheim . 
1995 . 
Design of the muc-6 evaluation . 
In Proceedings of the 6th Conference on Message Understanding , pages 1–11 . 
Association for Computational Linguistics . 
Anupam Guha , Mohit Iyyer , Danny Bouman , and Jor- dan Boyd - Graber . 2015 . 
Removing the training wheels : A coreference dataset that entertains hu- mans and challenges computers . 
In Proceedings of NAACL . 
Liane Guillou and Christian Hardmeier . 2016 . 
Protest : A test suite for evaluating pronouns in machine translation . 
In Proceedings of LREC , Paris , France . 
Erhard W. Hinrichs , Sandra ¨ubler , and Karin Naumann . 
2005 . 
A uniﬁed representation for morphological , syntactic , semantic and referential annotations . 
In Proc . 
of the ACL Workshop on Frontiers in Corpus Annotation II : Pie in the Sky , Ann Arbor , Michigan . 
Dirk Hovy , Taylor Berg - Kirkpatrick , Ashish Vaswani , and Eduard Hovy . 
2013 . 
Learning whom to trust with MACE . 
In Proceedings of NAACL , pages 1120–1130 . 
Jeff Howe . 2008 . 
Crowdsourcing : Why the power of the crowd is driving the future of business . 
Crown Publishing Group . 
Alan Joubert and Mathieu Lafourcade . 2008 . 
Jeuxde- mots : Un prototype ludique pour l ’ ´ emergence de relations entre termes . 
In Proceedings of JADT . 
Kian Kenyon - Dean , Eisha Ahmed , Jeremy Georges- Filteau , Christopher Glasz , Barleen Kaur , 
Au- guste Lalande , Shruti Bhanderi , Robert Belfer , Nir- mal Kanagasabai , Roman Sarrazingendron , Rohit Verma , and Derek Ruths . 
2018 . 
Sentiment analysis : 
1788it ’s complicated ! 
In Proceedings of NAACL , pages 1886–1895 . 
ACL . 
Olga Krasavina and Christian Chiarcos . 
2007 . 
The Potsdam coreference scheme . 
In Proceedings of the 1st Linguistic Annotation Workshop , pages 156 – 163 . 
Mathieu Lafourcade , Alain Joubert , and Nathalie Le Brun . 2015 . 
Games with a Purpose ( GWAPs ) . 
Wiley . 
Heeyoung Lee , Yves Peirsman , Angel Chang , Nathanael Chambers , Mihai Surdeanu , and Dan Ju- rafsky . 
2011 . 
Stanford ’s multi - pass sieve corefer- ence resolution system at the CoNLL-2011 shared task . 
In Proceedings of CONLL : Shared Task , pages 28–34 , Stroudsburg , PA , USA . 
Kenton Lee , Luheng He , Mike Lewis , and Luke Zettle- moyer . 
2017 . 
End - to - end neural coreference resolu- tion . 
In Proceedings of EMNLP . 
Kenton Lee , Luheng He , and Luke S. Zettlemoyer . 
2018 . 
Higher - order coreference resolution with coarse - to-ﬁne inference . 
In Proceedings of ACL . 
Chris Madge , Jon Chamberlain , Udo Kruschwitz , and Massimo Poesio . 
2017 . 
Experiment - driven devel- opment of a gwap for marking segments in text . 
In Proceedings of CHI PLAY , Amsterdam . 
Sebastian Martschat and Michael Strube . 
2015 . 
La- tent structures for coreference resolution . 
Transac- tions of the Association for Computational Linguis- tics , 3:405–418 . 
A. Nedoluzhko , J. Mirokvsk ´ y , and P. Pajas . 2009 . 
The coding scheme for annotating extended nominal coreference and bridging anaphora in the prague de- pendency treebank . 
In Proceedings of the Linguistic Annotation Workshop , pages 108–111 . 
Rebecca J. Passonneau , Vikas Bhardwaj , Ansaf Salleb- Aouissi , and Nancy Ide . 2012 . 
Multiplicity and word sense : evaluating and learning from multi- ply labeled word sense annotations . 
Language Re- sources and Evaluation , 46(2):219–252 . 
Rebecca J. Passonneau and Bob Carpenter . 
2014 . 
The beneﬁts of a model of annotation . 
Transactions of the Association for Computational Linguistics , 2:311–326 . 
Silviu Paun , Bob Carpenter , Jon Chamberlain , Dirk Hovy , Udo Kruschwitz , and Massimo Poesio . 
2018a . 
Comparing bayesian models of annotation . 
Transactions of the Association for Computational Linguistics . 
Silviu Paun , Jon Chamberlain , Udo Kruschwitz , Jun- tao Yu , and Massimo Poesio . 
2018b . 
A probabilis- tic annotation model for crowdsourcing coreference . 
InProceedings of EMNLP , pages 1926–1937 , Brus- sels , Belgium . 
Association for Computational Lin- guistics . 
Matthew E. Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke S. Zettlemoyer . 
2018 . 
Deep contextualized word representations . 
In Proceedings of NAACL . 
Slav Petrov and Dan Klein . 
2007 . 
Improved inference for unlexicalized parsing . 
In Proceedings of HLT- NAACL . 
Barbara Plank , Dirk Hovy , and Anders Sogaard . 
2014 . 
Linguistically debatable or just plain wrong ? 
In Proceedings of EACL . 
Massimo Poesio and Ron Artstein . 
2005a . 
Annotating ( anaphoric ) ambiguity . 
In Proc . 
of the Corpus Lin- guistics Conference , Birmingham . 
Massimo Poesio and Ron Artstein . 
2005b . 
The relia- bility of anaphoric annotation , reconsidered : Taking ambiguity into account . 
In Proceedings of the ACL Workshop on Frontiers in Corpus Annotation , pages 76–83 . 
Massimo Poesio , Jon Chamberlain , Udo Kruschwitz , Livio Robaldo , and Luca Ducceschi . 
2013 . 
Phrase Detectives : Utilizing collective intelligence for internet - scale language resource creation . 
ACM Transactions on Interactive Intelligent Systems , 3(1):1–44 . 
Massimo Poesio , Yulia Grishina , Varada Kolhatkar , Naﬁse Moosavi , Ina Roesiger , Adam Roussel , Fabian Simonjetz , Alexandra Uma , Olga Uryupina , Juntao Yu , and Heike Zinsmeister . 
2018 . 
Anaphora resolution with the arrau corpus . 
In Proc . 
of the NAACL Worskhop on Computational Models of Ref- erence , Anaphora and Coreference ( CRAC ) , pages 11–22 , New Orleans . 
Massimo Poesio , Sameer Pradhan , Marta Recasens , Kepa Rodriguez , and Yannick Versley . 
2016 . 
An- notated corpora and annotation tools . 
In M. Poesio , R. Stuckardt , and Y . 
Versley , editors , Anaphora Res- olution : Algorithms , Resources and Applications , chapter 4 . Springer . 
Sameer Pradhan , Alessandro Moschitti , Nianwen Xue , Olga Uryupina , and Yuchen Zhang . 
2012 . 
CoNLL- 2012 shared task : Modeling multilingual unre- stricted coreference in OntoNotes . 
In Proceedings of the Sixteenth Conference on Computational Natu- ral Language Learning ( CoNLL 2012 ) , Jeju , Korea . 
Nguyen Quoc Viet Hung , Nguyen Thanh Tam , Lam Ngoc Tran , and Karl Aberer . 2013 . 
An evalua- tion of aggregation techniques in crowdsourcing . 
In Web Information Systems Engineering – WISE 2013 , pages 1–15 , Berlin , Heidelberg . 
Springer Berlin Heidelberg . 
Vikas C. Raykar , Shipeng Yu , Linda H. Zhao , Ger- ardo Hermosillo Valadez , Charles Florin , Luca Bo- goni , and Linda Moy . 2010 . 
Learning from crowds . 
Journal of Machine Learning Research , 11:1297 – 1322 . 
1789Marta Recasens , Ed Hovy , and M. Ant ` onia Mart ´ ı . 2011 . 
Identity , non - identity , and near - identity : Ad- dressing the complexity of coreference . 
Lingua , 121(6):1138–1152 . Marta Recasens and M. Ant ` onia Mart ´ ı . 2010 . 
AnCora- CO : Coreferentially annotated corpora for Spanish and Catalan . 
Language Resources and Evaluation , 44(4):315–345 . 
Marta Recasens , M. Antonia Mart ´ ı , and Constantin Orasan . 
2012 . 
Annotating near - identity from coref- erence disagreements . 
In Proceedings of LREC . 
Aashish Sheshadri and Matthew Lease . 
2013 . 
SQUARE : 
A benchmark for research on computing crowd consensus . 
In Proceedings of the 1st AAAI Conference on Human Computation ( HCOMP ) , pages 156–164 . 
Rion Snow , Brendan O’Connor , Daniel Jurafsky , and Andrew Y . 
Ng . 2008 . 
Cheap and fast - but is it good ? 
Evaluating non - expert annotations for natural language tasks . 
In Proceedings of EMNLP , pages 254–263 . 
Olga Uryupina , Ron Artstein , Antonella Bristot , Fed- erica Cavicchio , Francesca Delogu , Kepa J. Ro- driguez , and Massimo Poesio . 
To Appear . 
Anno- tating a broad range of anaphoric phenomena , in a variety of genres : the ARRAU corpus . 
Journal of Natural Language Engineering . 
Noortje Venhuizen , Valerio Basile , Kilian Evang , and Johan Bos . 
2013 . 
Gamiﬁcation for word sense label- ing . 
In Proceedings of the 10th International Con- ference on Computational Semantics ( IWCS’13 ) . 
Yannick Versley . 
2008 . 
Vagueness and referential am- biguity in a large - scale annotated corpus . 
Research on Language and Computation , 6:333–353 . 
Jacob Whitehill , Ting - fan Wu , Jacob Bergsma , Javier R. Movellan , and Paul L. Ruvolo . 2009 . 
Whose vote should count more : Optimal integra- tion of labels from labelers of unknown expertise . 
In Advances in Neural Information Processing Systems 22 , pages 2035–2043 . 
Curran Associates , Inc. 
Billy T. M. Wong and Sophia Y . 
M. Lee . 
2013 . 
An- notating legitimate disagreement in corpus construc- tion . 
In Proceedings of IJCNLP , pages 51–57 , Nagoya , Japan . 