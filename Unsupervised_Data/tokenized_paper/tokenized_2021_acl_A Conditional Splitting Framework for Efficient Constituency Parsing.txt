Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing , pages 5795–5807 August 1–6 , 2021 . 
© 2021 Association for Computational Linguistics5795A Conditional Splitting Framework for Efﬁcient Constituency Parsing Thanh - Tung Nguyeny { , Xuan - Phi Nguyeny { , Shaﬁq Joty{x , Xiaoli Liy { { Nanyang Technological University xSalesforce Research Asia yInstitute for Infocomm Research , A - STAR Singapore { ng0155ng@e.;nguyenxu002@e.;srjoty@}ntu.edu.sg 
xlli@i2r.a-star.edu.sg Abstract We introduce a generic seq2seq parsing frame- work that casts constituency parsing problems ( syntactic and discourse parsing ) into a series of conditional splitting decisions . 
Our pars- ing model estimates the conditional probabil- ity distribution of possible splitting points in a given text span and supports efﬁcient top- down decoding , which is linear in number of nodes . 
The conditional splitting formulation together with efﬁcient beam search inference facilitate structural consistency without rely- ing on expensive structured inference . 
Cru- cially , for discourse analysis we show that in our formulation , discourse segmentation can be framed as a special case of parsing which allows us to perform discourse parsing without requiring segmentation as a pre - requisite . 
Ex- periments show that our model achieves good results on the standard syntactic parsing tasks under settings with / without pre - trained repre- sentations and rivals state - of - the - art ( SoTA ) methods that are more computationally ex- pensive than ours . 
In discourse parsing , our method outperforms SoTA by a good margin . 
1 Introduction A number of formalisms have been introduced to analyze natural language at different linguistic lev- els . 
This includes syntactic structures in the form of phrasal and dependency trees , semantic struc- tures in the form of meaning representations ( Ba- narescu et al . , 2013 ; Artzi et 
al . , 2013 ) , and dis- course structures with Rhetorical Structure Theory ( RST ) ( Mann and Thompson , 1988 ) or Discourse- LTAG ( Webber , 2004 ) . 
Many of these formalisms have a constituency structure , where textual units ( e.g. , phrases , sentences ) are organized into nested constituents . 
For example , Figure 1 shows exam- ples of a phrase structure tree and a sentence - level discourse tree ( RST ) that respectively represent how the phrases and clauses are hierarchically or - ganized into a constituency structure . 
Developing efﬁcient and effective parsing solutions has always been a key focus in NLP . 
In this work , we consider both phrasal ( syntactic ) and discourse parsing . 
In recent years , neural end - to - end parsing meth- ods have outperformed traditional methods that use grammar , lexicon and hand - crafted features . 
These methods can be broadly categorized based on whether they employ a greedy transition - based , a globally optimized chart parsing or a greedy top- down algorithm . 
Transition - based parsers ( Dyer et al . , 2016 ; Cross and Huang , 2016 ; Liu and Zhang , 2017 ; Wang et al . , 2017 ) generate trees auto - regressively as a form of shift - reduce deci- sions . 
Though computationally attractive , the local decisions made at each step may propagate errors to subsequent steps due to exposure bias ( Bengio et al . , 2015 ) . 
Moreover , there may be mismatches in shift and reduce steps , resulting in invalid trees . 
Chart based methods , on the other hand , train neural scoring functions to model the tree structure globally ( Durrett and Klein , 2015 ; Gaddy et al . , 2018 ; Kitaev and Klein , 2018 ; Zhang et al . , 2020b ; Joty et al . , 2012 , 2013 ) . 
By utilizing dynamic pro- gramming , these methods can perform exact in- 
ference to combine these constituent scores into ﬁnding the highest probable tree . 
However , they are generally slow with at least O(n3)time com- plexity . 
Greedy top - down parsers ﬁnd the split points recursively and have received much atten- tion lately due to their efﬁciency , which is usually O(n2)(Stern et al . , 2017a ; Shen et al . , 2018 ; Lin et al . , 2019 ; Nguyen et al . , 2020 ) . 
However , they still suffer from exposure bias , where one incorrect splitting step may affect subsequent steps . 
Discourse parsing in RST requires an addi- tional step – discourse segmentation which in- volves breaking the text into contiguous clause - like units called Elementary Discourse Units or EDUs ( Figure 1 ) . 
Traditionally , segmentation has been 
5796considered separately and as a prerequisite step for the parsing task which links the EDUs ( and larger spans ) into a discourse tree ( Soricut and Marcu , 2003 ; Joty et al . , 2012 ; Wang et al . , 2017 ) . 
In this way , the errors in discourse segmentation can prop- agate to discourse parsing ( Lin et al . , 2019 ) . 
In this paper , we propose a generic top - down neural framework for constituency parsing that we validate on both syntactic and sentence - level dis- course parsing . 
Our main contributions are : •We cast the constituency parsing task into a se- ries of conditional splitting decisions and use a seq2seq architecture to model the splitting decision at each decoding step . 
Our parsing model , which is an instance of a Pointer Network ( Vinyals et al . , 2015a ) , estimates the pointing score from a span to a splitting boundary point , representing the likelihood that the span will be split at that point and create two child spans . 
•The conditional probabilities of the splitting deci- sions are optimized using a cross entropy loss and structural consistency is maintained through a global pointing mechanism . 
The training process can be fully parallelized without requiring struc- tured inference as in ( Shen et al . , 2018 ; Gómez and Vilares , 2018 ; Nguyen et al . , 2020 ) . 
•Our model enables efﬁcient top - down decoding withO(n)running time like transition - based parsers , while also supporting a customized beam search to get the best tree by searching through a reasonable search space of high scoring trees . 
The beam - search inference along with the struc- tural consistency from the modeling makes our approach competitive with existing structured chart methods for syntactic ( Kitaev and Klein , 2018 ) and discourse parsing ( Zhang et al . , 2020b ) . 
Moreover , our parser does not rely on any hand- crafted features ( not even part - of - speech tags ) , which makes it more efﬁcient and be ﬂexible to different domains or languages . 
•For discourse analysis , we demonstrate that our method can effectively ﬁnd the segments ( EDUs ) by simply performing one additional step in the top - down parsing process . 
In other words , our method can parse a text into the discourse tree without needing discourse segmentation as a pre- requisite ; instead , it produces the segments as a by - product . 
To the best of our knowledge , this is the ﬁrst model that can perform segmentation and parsing in a single embedded framework . 
In the experiments with English Penn Tree- bank , our model without pre - trained representa- tions achieves 93.8 F1 , outperforming all exist- ing methods with similar time complexity . 
With pre - training , our model pushes the F1 score to 95.7 , which is on par with the SoTA while sup- porting faster decoding with a speed of over 1,100 sentences per second ( fastest so far ) . 
Our model also performs competitively with SoTA methods on the multilingual parsing tasks in the SPMRL 2013/2014 shared tasks . 
In discourse parsing , our method establishes a new SoTA in end - to - end sentence - level parsing performance on the RST Discourse Treebank with an F1 score of 78.82 . 
We make our code available at https://ntunlpsg.github.io/project/condition- constituency - style - parser/ 2 Parsing as a Splitting Problem Constituency parsing ( both syntactic and discourse ) can be considered as the problem of ﬁnding a set of labeled spans over the input text ( Stern et al . , 2017a ) . 
Let S(T)denote the set of labeled spans for a parse tree T , which can formally be expressed as ( excluding the trivial singleton span layer ): S(T ) : 
= f((it;jt);lt)gjS(T)j t=1 forit < jt ( 1 ) whereltis the label of the text span ( it;jt)encom- passing tokens from index itto indexjt . 
Previous approaches to syntactic parsing ( Stern et al . , 2017a ; Kitaev and Klein , 2018 ; Nguyen et al . , 2020 ) train a neural model to score each possible span and then apply a greedy or dynamic program- ming algorithm to ﬁnd the parse tree . 
In other words , these methods are span - based formulation . 
In contrary , we formulate constituency parsing as the problem of ﬁnding the splitting points in a recursive , top - down manner . 
For each parent node in a tree that spans over ( i;j ) , our parsing model is trained to point to the boundary between the tokens atkandk+1positions to split the parent span into two child spans ( i;k)and(k+ 1;j ) . 
This is done through the Pointing mechanism ( Vinyals et al . , 2015a ) , where each splitting decision is modeled as a multinomial distribution over the input elements , which in our case are the token boundaries . 
The correspondence between token- and boundary - based representations of a tree is straight- forward . 
After including the start ( < sos > ) and end ( < eos > ) tokens , the token - based span ( i;j ) is equivalent to the boundary - based span ( i 1;j ) 
5797 Labeled span representation S(T ) = { ( ( 1 , 5 ) , S ) , ( ( 2 , 5 ) , ? ) , ( ( 2 , 4 ) , VP ) , ( ( 3 , 4 ) , S - VP ) } Boundary - based splitting representation C(T ) = { ( 0;5))1,(1;5))4,(1;4))2,(2;4))3 } Labeled span representation S(DT ) = { ( ( 1 , 8 , 11 ) , Same - Unit NN ) , ( ( 1 , 5 , 8) , Elaboration NS ) } Boundary - based splitting representation C(DT ) = { ( 0;11 ) ) 8,(0;8))5,(0;5))5,(5;8))8,(8;11))11 } Figure 1 : A syntactic tree at the left and a discourse tree ( DT ) at the right ; both have a constituency structure . 
The internal nodes in the discourse tree ( Elaboration , Same - Unit ) represent coherence relations and the edge labels indicate the nuclearity statuses ( ‘ N ’ for Nucleus and ‘S ’ for Satellite ) of the child spans . 
Below the tree , we show the labeled span and splitting representations . 
The bold splits in the DT representation ( C(DT ) ) indicate the end of further splitting into smaller spans ( i.e. ,they are EDUs ) . 
and the boundary between i - th and ( i+1)-th tokens is indexed as i. 
For example , the ( boundary - based ) span “ enjoys playing tennis ” in Figure 1 is deﬁned as(1;4 ) . 
Similarly , the boundary between the to- kens “ enjoys ” and “ playing ” is indexed with 2.1 Following the common practice in syntactic pars- ing , we binarize the n - ary tree by introducing a dummy label ? . 
We also collapsed the nested la- beled spans in the unary chains into unique atomic labels , such as S - VP in Figure 1 . 
Every span repre- sents an internal node in the tree , which has a left and a right child . 
Therefore , we can represent each internal node by its split into left and right chil- dren . 
Based on this , we deﬁne the set of splitting decisions C(T)for a syntactic tree Tas follows . 
Proposition 1 
A binary syntactic tree Tof a sen- tence containing ntokens can be transformed into a set of splitting decisions C(T ) 
= f(i;j))k : i < 
k < jgsuch that the parent span ( i;j)is split into two child spans ( i;k)and(k;j ) . 
An example of the splitting representation of a tree is shown in Figure 1 ( without the node labels ) . 
Note that our transformed representation has a one - to- one mapping with the tree since each splitting de- cision corresponds to one and only one internal node in the tree . 
We follow a depth-ﬁrst order of the decision sequence , which in our preliminary experiments showed more consistent performance than other alternatives like breadth-ﬁrst order . 
Extension to End - to - End Discourse Parsing Note that in syntactic parsing , the split position 1We use the same example from ( Stern et al . , 2017a ; Shen et al . , 2018 ; Nguyen et al . , 2020 ) to distinguish the differences between the methods.must be within the span but not at its edge , 
that is , kmust satisfy i < 
k < j for each boundary span ( i;j ) . 
Otherwise , it will not produce valid sub - trees . 
In this case , we keep splitting until each span contains a single leaf token . 
However , for discourse trees , each leaf is an EDU – a clause - like unit that can contain one or multiple tokens . 
Unlike previous studies which assume discourse segmentation as a pre - processing step , we propose a uniﬁed formulation that treats segmentation as one additional step in the top - down parsing process . 
To accommodate this , we relax Proposition 1 as : Proposition 2 A binary discourse treeDT of a text containing ntokens can be transformed into a set of splitting decisions C(DT ) 
= f(i;j))k : i < kjgsuch that the parent span ( i;j)gets split into two child spans ( i;k)and(k;j)fork < j or a terminal span or EDU for k = j(end of splitting the span further ) . 
We illustrate it with the DT example in Figure 1 . 
Each splitting decision in C(DT)represents ei- ther 
the splitting of the parent span into two child spans ( when the splitting point is strictly within the span ) or the end of any further splitting ( when the splitting point is the right endpoint of the span ) . 
By making this simple relaxation , our formulation can not only generate the discourse tree ( in the for- mer case ) but can also ﬁnd the discourse segments ( EDUs ) as a by - product ( in the latter case ) . 
3 Seq2Seq Parsing Framework LetC(T)andL(T)respectively denote the struc- ture ( in split representation ) and labels of a tree T ( syntactic or discourse ) for a given text x. 
We can express the probability of the tree as : 
5798 Figure 2 : Our syntatic parser along with the decoding process for a given sentence . 
The input to the decoder at each step is the representation of the span to be split . 
We predict the splitting point using a biafﬁne function between the corresponding decoder state and the boundary - based encoder representations . 
A label classiﬁer is used to assign labels to the left and right spans . 
P(Tjx ) 
= P(L(T);C(T)jx ) 
= P(L(T)jC(T);x)P(C(T)jx)(2 ) 
This factorization allows us to ﬁrst infer the tree structure from the input text , and then ﬁnd the cor- responding labels . 
As discussed in the previous section , we consider the structure prediction as a sequence of splitting decisions to generate the tree in a top - down manner . 
Speciﬁcally , at each de- coding stept , the outputytrepresents the splitting decision ( it;jt))ktandy < trepresents the previ- ous splitting decisions . 
Thus , we can express the probability of the tree structure as follows : P(C(T)jx ) = Y yt2C(T)P(ytjy < t;x ) = jC(T)jY t=1P((it;jt))ktj((i;j))k)<t;x)(3 ) 
This can effectively be modeled within a Seq2Seq pointing framework as shown in Figure 2 . 
At each stept , the decoder autoregressively predicts the split pointktin the input by conditioning on the current input span ( it;jt)and previous splitting decisions ( i;j))k)<t . 
This conditional splitting formulation ( decision at step tdepends on previous steps ) can help our model to ﬁnd better trees com- pared to non - conditional top - down parsers ( Stern et al . , 2017a ; 
Shen et al . , 2018 ; Nguyen et al . , 2020 ) , thus bridging the gap between the global ( but ex- pensive ) and the local ( but efﬁcient ) models . 
The labelsL(T)can be modeled by using a label clas- siﬁer , as described later in the next section.3.1 Model Architecture 
We now describe the components of our parsing model : the sentence encoder , the span representa- tion , the pointing model and the labeling model . 
Sentence Encoder Given an input sequence of n tokens x= 
( x1;:::;x n ) , we ﬁrst add < sos > and < eos > markers to the sequence . 
After that , each tokentin the sequence is mapped into its dense vector representation etas et= [ echar t;eword t ] ( 4 ) where echar t , eword t are respectively the character and word embeddings of token t. Similar to ( Ki- taev and Klein , 2018 ; Nguyen et al . , 2020 ) , we use a character LSTM to compute the character embed- ding of a token . 
We experiment with both randomly initialized and pretrained token embeddings . 
When pretrained embedding is used , the character embed- ding is replaced by the pretrained token embedding . 
The token representations are then passed to a 3- layer Bi - LSTM encoder to obtain their contextual representations . 
In the experiments , we ﬁnd that even without the POS - tags , our model performs competitively with other baselines that use them . 
Boundary and Span Representations To repre- sent each boundary between positions kandk+ 1 , we use the fencepost representation ( Cross and Huang , 2016 ; Stern et al . , 2017a ): 
hk= 
[ fk;bk+1 ] ( 5 ) where fkandbk+1are the forward and backward LSTM hidden vectors at positions kandk+ 1 , re- 
5799 Figure 3 : Illustration of our boundary - based span encoder . 
Here we have shown the representation for the boundary at 1 and the representation of the boundary - based span ( 0;5)that corresponds to the sentence “ She enjoys playing tennis . ” . 
spectively . 
To represent the span ( i;j ) , we compute a linear combination of the two endpoints hi;j = W1hi+W2hj ( 6 ) This span representation will be used as input to the decoder . 
Figure 3 shows the boundary - based span representations for our example . 
The Decoder Our model uses a unidirectional LSTM as the decoder . 
At each decoding step t , the decoder takes as input the corresponding span ( i;j)(speciﬁcally , hi;j ) and its previous state dt 1 to generate the current state dtand then apply a biafﬁne function ( Dozat and Manning , 2017 ) be- tween dtandallof the encoded boundary represen- tations ( h0;h1;:::;hn)as follows : d0 t = MLPd(dt)h0 i = MLPh(hi ) ( 7 ) st;i = d0 tTWdhh0 i+h0 iTwh ( 8) at;i = exp(st;i)Pn i=1exp(st;i)(9 ) where each MLP operation includes a linear trans- formation with LeakyReLU activation to transform dandhinto equal - sized vectors , and Wdh2 
I Rddandwh2I Rdare respectively the weight matrix and weight vector for the biafﬁne func- tion . 
The biafﬁne scores are then passed through a softmax layer to acquire the pointing distribution at2[0;1]nfor the splitting decision . 
When decoding the tree during inference , at each step we only examine the ‘ valid ’ splitting points betweeniandj – for syntactic parsing , it is i < k < j and for discourse parsing , it is i < kj . 
Label Classiﬁer For syntactic parsing , we per- form the label assignments for a span ( i;j)as : 
hl i = MLPl(hi);hr j = MLPr(hj ) ( 10 ) P(lji;j ) = softmax ( ( hl i)TWlrhr 
j + ( hl i)TWl+ ( hr j)TWr+b ) ( 11 ) li;j= arg max l2LP(lji;j ) 
( 12 ) where each of MLP land MLP rincludes a lin- ear transformation with LeakyReLU activations to transform the left and right spans into equal - sized vectors , and Wlr2I RdLd;Wl2I RdL;Wr2 
I RdLare the weights and bis a bias vector with Lbeing the number of phrasal labels . 
For discourse parsing , we perform label assign- ment after every split decision since the label here represents the relation between the child spans . 
Speciﬁcally , as we split a span ( i;j)into two child spans ( i;k)and(k;j ) , we determine the relation label as the following . 
hl ik = MLPl([hi;hk]);hr kj = MLPr([hk;hj])(13 ) P(lj(i;k);(k;j ) ) 
= softmax ( ( hl ik)TWlrhr kj + ( hl ik)TWl+ ( hr kj)TWr+b)(14 ) l(i;k);(k;j)= arg max l2LP(lj(i;k);(k;j ) ) ( 15 ) where MLP l;MLP r , Wlr;Wl;Wr;bare similarly deﬁned . 
Training Objective The total loss is simply the sum of the cross entropy losses for predicting the structure ( split decisions ) and the labels : Ltotal( ) = Lsplit(e;d ) + Llabel(e;label ) ( 16 ) where=fe;d;labelgdenotes the overall model parameters , which includes the encoder pa- rameterseshared by all components , parameters for splitting dand parameters for labeling label . 
3.2 Top - Down Beam - Search Inference As mentioned , existing top - down syntactic parsers do not consider the decoding history . 
They also per- form greedy inference . 
With our conditional split- ting formulation , our method can not only model the splitting history but also enhance the search space of high scoring trees through beam search . 
At each step , our decoder points to allthe en- coded boundary representations which ensures that the pointing scores are in the same scale , allow- ing a fair comparison between the total scores of all candidate subtrees . 
With these uniform scores , we could apply a beam search to infer the most 
5800probable tree using our model . 
Speciﬁcally , the method generates the tree in depth-ﬁrst order while maintaining top- B(beam size ) partial trees at each step . 
It terminates exactly after n 1steps , which matches the number of internal nodes in the tree . 
Because beam size Bis constant with regards to the sequence length , we can omit it in the Big O notation . 
Therefore , each decoding step with beam search can be parallelized ( O(1)complexity ) using GPUs . 
This makes our algorithm run at O(n)time complexity , which is faster than most top - down methods . 
If we strictly use CPU , our method runs atO(n2 ) , while chart - based parsers run at O(n3 ) . 
Algorithm 1 illustrate the syntactic tree inference procedure . 
We also propose a similar version of the inference algorithm for discourse parsing in the Appendix . 
Algorithm 1 Syntactic Tree Inference with Beam Search Input : Sentence length n ; beam width B ; boundary - based encoder states : ( h0;h1;:::;hn ) ; label scores : P(lji;j ) , 0i < jn;l2f1;:::;Lg , initial decoder state s. Output : Parse treeT 1 : Ld = n 1 // 
Decoding length 2 : beam = array of Lditems // List of empty beam items 3 : init_tree = [ ( 0;n);(0;0 ) ; : : : ; ( 0;0 ) ] //n 2paddings ( 0,0 ) 4 : beam[0 ] = ( 0;s;init_tree ) // 
Init 1st item(log - prob , state , tree ) 5 : fort= 1toLddo 6 : for(logp;s;tree)2beam 
[ t 1]do 7 : ( i;j ) 
= tree[t 1 ] // 
Current span to split 8 : a;s0 = decoder - step ( s;hi;j)//a : split prob . 
dist . 
9 : for(k;pk)2top - B(a)andi < k < j do 10 : curr - tree = tree 11 : ifk > i + 1then 12 : curr - tree [ t ] = ( i;k ) 13 : end if 14 : ifj > k 
+ 1then 15 : curr - tree [ t+j k 1 ] = ( k;j ) 16 : end if 17 : push ( logp + log(pk);s0;curr - tree ) to beam[t ] 18 : end for 19 : end for 20 : prune beam[t ] // 
Keep top- 
Bhighest score trees 21 : end for 22 : logp*;s;S= arg maxlogpbeam 
[ Ld ] //S : best structure 23 : labeled - spans = [ ( i;j;arg maxlP(lji;j))8(i;j)2 S ] 24 : labeled - singletons = [ ( i;i+ 1;arg maxlP(lji;i+ 1))fori 
= f0;:::;n 1 g ] 25 : T = labeled - spans[labeled - singletons By enabling beam search , our method can ﬁnd the best tree by comparing high scoring trees within a reasonable search space , making our model com- petitive with existing structured ( globally ) infer- ence methods that use more expensive algorithmslike CKY and/or larger models ( Kitaev and Klein , 2018 ; Zhang et al . , 2020b ) . 
4 Experiment Datasets and Metrics To show the effectiveness of our approach , we conduct experiments on both syntactic and sentence - level RST parsing tasks.2 We use the standard Wall Street Journal ( WSJ ) part of the Penn Treebank ( PTB ) ( Marcus et al . , 1993 ) for syntactic parsing and RST Discourse Treebank ( RST - DT ) ( Lynn et al . , 2002 ) for discourse parsing . 
For syntactic parsing , we also experiment with the multilingual parsing tasks on seven different lan- guages from the SPMRL 2013 - 2014 shared task ( Seddah et al . , 2013 ): Basque , French , German , Hungarian , Korean , Polish and Swedish . 
For evaluation on syntactic parsing , we report the standard labeled precision ( LP ) , labeled recall ( LR ) , and labelled F1 computed by evalb3 . 
For evaluation on RST - DT , we report the standard span , nuclearity label , relation label F1 scores , computed using the implementation of ( Lin et al . , 2019).4 4.1 English ( PTB ) Syntactic Parsing Setup We follow the standard train / valid / test split , which uses Sections 2 - 21 for training , Section 22 for development and Section 23 for evaluation . 
This results in 39,832 sentences for training , 1,700 for development , and 2,416 for testing . 
For our model , we use an LSTM encoder - decoder frame- work with a 3 - layer bidirectional encoder and 3- layer unidirectional decoder . 
The word embedding size is 100 while the character embedding size is 50 ; the LSTM hidden size is 400 . 
The hidden di- mension in MLP modules and biafﬁne function for split point prediction is 500 . 
The beam width Bis set to 20 . 
We use the Adam optimizer ( Kingma and Ba , 2015 ) with a batch size of 5000 tokens , and an initial learning rate of 0:002which decays at the rate0:75exponentially at every 5k steps . 
Model selection for ﬁnal evaluation is performed based on the labeled F1 score on the development set . 
Results without Pre - training From the results shown in Table 1 , we see that our model achieves an F1 of 93:77 , the highest among models that use 2Extending the discourse parser to the document level may require handling of intra- and multi - sentential constituents differently , which we leave for future work . 
3http://nlp.cs.nyu.edu/evalb/ 4https://github.com/ntunlpsg/ UnifiedParser_RST 
5801Model LR LP F1 Top - Down Inference Stern et al . ( 2017a ) 93.20 90.30 91.80 Shen et al . 
( 2018 ) 92.00 91.70 91.80 
Nguyen et al . 
( 2020 ) 92.91 92.75 92.78 Our Model 93.90 93.63 93.77 CKY / Chart Inference Gaddy et al . 
( 2018 ) 91.76 92.41 92.08 Kitaev and Klein ( 2018 ) 
93.20 93.90 93.55 Wei et al . ( 2020 ) 93.3 94.1 93.7 Zhang et al . ( 2020b ) 93.84 93.58 93.71 Other Approaches Gómez and Vilares ( 2018 ) - - 90.7 Liu and Zhang ( 2017 ) - - 91.8 Stern et al . ( 2017b ) 92.57 92.56 92.56 Zhou and Zhao ( 2019 ) 93.64 93.92 93.78 Table 1 : Results for single models ( no pre - training ) on the PTB WSJ test set , Section 23 . 
Model F1 
Nguyen et al . 
( 2020 ) 95.5 
Our model 95.7 Kitaev et al . 
( 2019 ) 95.6 Zhang et al . 
( 2020b ) 95.7 Wei et al . 
( 2020 ) 95.8 Zhou and Zhao ( 2019 ) 95.8 Table 2 : Results on PTB WSJ test set with pretraining . 
top - down methods . 
Speciﬁcally , our parser outper- forms Stern et al . ( 2017a ) ; Shen et al . 
( 2018 ) by about 2points in F1 - score and Nguyen 
et al . 
( 2020 ) by1point . 
Notably , without beam search ( beam width 1 or greedy decoding ) , our model achieves an F1 of 93:40 , which is still better than other top- down methods . 
Our model also performs compet- itively with CKY - based methods like ( Kitaev and Klein , 2018 ; Zhang et 
al . , 2020b ; Wei et al . , 2020 ; Zhou and Zhao , 2019 ) , while these methods run slower than ours . 
Plus , Zhou and Zhao ( 2019 ) uses external su- pervision ( head information ) from the dependency parsing task . 
Dependency parsing models , in fact , have a strong resemblance to the pointing mecha- nism that our model employs ( Ma et al . , 2018 ) . 
As such , integrating dependency parsing information into our model may also be beneﬁcial . 
We leave this for future work . 
Results with Pre - training Similar to ( Kitaev and Klein , 2018 ; Kitaev et al . , 2019 ) , we also eval - uate our parser with BERT embeddings ( Devlin et al . , 2019 ) . 
They ﬁne - tuned Bert - large - cased on the task , while in our work keeping it frozen was already good enough ( gives training efﬁciency ) . 
As shown in Table 2 , our model achieves an F1 of 95:7 , which is on par with SoTA models . 
However , our parser runs faster than other methods . 
Speciﬁcally , our model runs at O(n)time complexity , while CKY needsO(n3 ) . 
Comprehensive comparisons on parsing speed are presented later . 
4.2 SPMRL Multilingual Syntactic 
Parsing We use the identical hyper - parameters and opti- mizer setups as in English PTB . 
We follow the stan- dard train / valid / test split provided in the SPMRL datasets ; details are reported in the Table 3 . 
Language Train Valid Test Basque 7,577 948 946 French 14,759 1,235 2,541 German 40,472 5,000 5,000 Hungarian 8,146 1,051 1,009 Korean 23,010 2,066 2,287 Polish 6,578 821 822 Swedish 5,000 494 666 Table 3 : SPMRL Multilingual dataset split . 
From the results in Table 4 , we see that our model achieves the highest F1 in French , Hungar- ian and Korean and higher than the best baseline by0:06,0:15and0:13 , respectively . 
Our method also rivals existing SoTA methods on other lan- guages even though some of them use predicted POS tags ( Nguyen et al . , 2020 ) or bigger models ( 75 M parameters ) ( Kitaev and Klein , 2018 ) . 
Mean- while , our model is smaller ( 31 M ) , uses no extra information and runs 40 % faster . 
4.3 Discourse Parsing Setup For discourse parsing , we follow the stan- dard split from ( Lin et al . , 2019 ) , which has 7321 sentence - level discourse trees for training and 951 for testing . 
We also randomly select 10 % of the training for validation . 
Model selection for test- ing is performed based on the F1 of relation labels on the validation set . 
We use the same model set- tings as the constituency parsing experiments , with BERT as pretrained embeddings.5 5Lin et al . 
( 2019 ) used ELMo ( Peters et al . , 2018 ) as pre- trained embeddings . 
With BERT , their model performs worse which we have conﬁrmed with the authors . 
5802Model Basque French German Hungarian Korean Polish Swedish Bjorkelund et al . 
( 2014)+88.24 82.53 81.66 91.72 83.81 90.50 85.50 Coavoux and Crabbé ( 2017)+88.81 82.49 85.34 92.34 86.04 93.64 84.0 Kitaev and Klein ( 2018 ) 89.71 84.06 87.69 92.69 86.59 93.69 84.45 Nguyen et al . 
( 2020)+90.23 82.20 84.91 91.07 85.36 93.99 86.87 Our Model 89.74 84.12 85.21 92.84 86.72 92.10 85.81 Table 4 : Results on SPMRL test sets without pre - training . 
The sign+denotes that systems use predicted POS tags . 
Approach Span Nuclearity Relation Parsing with gold EDU segmentation Human Agreement 95.7 90.4 83.0 
Baselines Wang et al . 
( 2017 ) 95.6 87.8 77.6 Lin et al . 
( 2019 ) ( single ) 96.94 90.89 81.28 Lin et al . 
( 2019 ) ( joint ) 97.44 91.34 81.70 Our Model 97.37 91.95 82.10 End - to - End parsing Baselines Soricut and Marcu ( 2003 ) 76.7 70.2 58.0 Joty et al . 
( 2012 ) 82.4 76.6 67.5 Lin et al . 
( 2019 ) ( pipeline ) 91.14 85.80 76.94 Lin et al . 
( 2019 ) ( joint ) 91.75 86.38 77.52 Our Model 92.02 87.05 78.82 Table 5 : Results on discourse parsing tasks on the RST- DT test set with and without gold segmentation . 
Results Table 5 compares the results on the dis- course parsing tasks in two settings : ( i ) when the EDUs are given ( gold segmentation ) and ( ii ) end- to - end parsing . 
We see that our model outperforms the baselines in both parsing conditions achieving SoTA . 
When gold segmentation is provided , our model outperforms the single - task training model of ( Lin et al . , 2019 ) by 0.43 % , 1.06 % and 0.82 % absolute in Span , Nuclearity and Relation , respec- tively . 
Our parser also surpasses their joint training model , which uses multi - task training ( segmenta- tion and parsing ) , with 0.61 % and 0.4 % absolute improvements in Nuclearity and Relation , respec- tively . 
For end - to - end parsing , compared to the best baseline ( Lin et al . , 2019 ) , our model yields 0.27 % , 0.67 % , and 1.30 % absolute improvements in Span , Nuclearity , Relation , respectively . 
This demonstrates the effectiveness of our conditional splitting approach and end - to - end formulation of the discourse analysis task . 
The fact that our model improves on span identiﬁcation indicates that our method also yields better EDU segmentation . 
4.4 
Parsing Speed Comparison We compare parsing speed of different models in Table 6 . 
We ran our models on both CPU ( IntelXeon W-2133 ) and GPU ( Nvidia GTX 1080 Ti ) . 
Syntactic Parsing The Berkeley Parser and ZPar are two representative non - neural parsers without access to GPUs . 
Stern et al . 
( 2017a ) employ max- margin training and perform top - down greedy de- coding on CPUs . 
Meanwhile , Kitaev and Klein ( 2018 ) 
; Zhou and Zhao ( 2019 ) ; 
Wei et al . ( 2020 ) use a self - attention encoder and perform decoding using Cython for acceleration . 
Zhang et al . 
( 2020b ) perform CKY decoding on GPU . 
The parser pro- posed by Gómez and Vilares ( 2018 ) is also efﬁ- cient as it treats parsing as a sequence labeling task . 
However , its parsing accuracy is much lower compared to others ( 90.7 F1 in Table 1 ) . 
We see that our parser is much more efﬁcient than existing ones . 
It utilizes neural modules to perform splitting , which is optimized and paral- lelized with efﬁcient GPU implementation . 
It can parse 1;127sentences / second , which is faster than existing parsers . 
In fact , there is still room to im- prove our speed by choosing better architectures , like the Transformer which has O(1)running time in encoding a sentence compared to O(n)of the bi - LSTM encoder . 
Moreover , allowing tree gener- ation by splitting the spans / nodes at the same tree level in parallel at each step can boost the speed further . 
We leave these extensions to future work . 
Discourse Parsing For measuring discourse parsing speed , we follow the same set up as Lin et al . 
( 2019 ) , and evaluate the models with the same 100 sentences randomly selected from the test set . 
We include the model loading time for all the systems . 
Since SPADE and CODRA need to extract a handful of features , they are typically slower than the neural models which use pretrained embeddings . 
In addition , CODRA ’s DCRF parser has aO(n3)inference time complexity . 
As shown , our parser is 4.7x faster than the fastest end - to - end parser of Lin et al . 
( 2019 ) 
, making it not only ef- fective but also highly efﬁcient . 
Even when tested only on the CPU , our model is faster than all the other models which run on GPU or CPU , thanks 
5803System Speed ( Sents / s ) Speedup Syntactic Parser Petrov and Klein ( 2007 ) ( Berkeley ) 6 1.0x Zhu et al . 
( 2013)(ZPar ) 90 15.0x Stern et al . ( 2017a ) 76 12.7x Shen et al . 
( 2018 ) 111 18.5x 
Nguyen et al . 
( 2020 ) 130 21.7x Zhou and Zhao ( 2019 ) 
159 26.5x Wei et al . 
( 2020 ) 220 36.7x Gómez and Vilares ( 2018 ) 780 130x Kitaev and Klein ( 2018 ) ( GPU ) 830 138.3x Zhang et al . 
( 2020b ) 924 154x Our model ( GPU ) 1127 187.3x End - to - End Discourse parsing ( Segmenter + Parser ) CODRA ( Joty et al . , 2015 ) 3.05 1.0x SPADE ( Soricut and Marcu , 2003 ) 4.90 1.6x ( Lin et al . , 2019 ) 28.96 9.5x Our end - to - end parser ( CPU ) 59.03 19.4x 
Our end - to - end parser ( GPU ) 135.85 44.5x Table 6 : Speed comparison of our parser with existing syntactic and discourse parsers . 
to the end - to - end formulation that does not need EDU segmentation beforehand . 
5 Related Work With the recent popularity of neural architectures , such as LSTMs ( Hochreiter and Schmidhuber , 1997 ) and Transformers ( Vaswani et al . , 2017 ) , var- ious neural models have been proposed to encode the input sentences and infer their constituency trees . 
To enforce structural consistency , such meth- ods employ either a greedy transition - based ( Dyer et al . , 2016 ; Liu and Zhang , 2017 ) , a globally op- timized chart parsing ( Gaddy et al . , 2018 ; Kitaev and Klein , 2018 ) , or a greedy top - down algorithm ( Stern et al . , 2017a ; Shen et al . , 2018 ) . 
Meanwhile , researchers also tried to cast the parsing problem into tasks that can be solved differently . 
For exam- ple , Gómez and Vilares ( 2018 ) ; Shen et al . 
( 2018 ) proposed to map the syntactic tree of a sentence containingntokens into a sequence of n 1la- bels or scalars . 
However , parsers of this type suffer from the exposure bias during inference . 
Beside these methods , Seq2Seq models have been used to generate a linearized form of the tree ( Vinyals et al . , 2015b ; Kamigaito et al . , 2017 ; Suzuki et al . , 2018 ; Fernández - González and Gómez - Rodríguez , 2020a ) . 
However , these methods may generate in- valid trees when the open and end brackets do not match . 
In discourse parsing , existing parsers receive the EDUs from a segmenter to build the discourse tree , which makes them susceptible to errors when the segmenter produces incorrect EDUs ( Joty et al . ,2012 , 2015 ; Lin et al . , 2019 ; Zhang et al . , 2020a ; Liu et al . , 2020 ) . 
There are also attempts which model constituency and discourse parsing jointly ( Zhao and Huang , 2017 ) and do not need to perform EDU preprocessing . 
It is based on the ﬁnding that each EDU generally corresponds to a constituent in constituency tree , i.e. ,discourse structure usually aligns with constituency structure . 
However , it has the drawback that it needs to build joint syntacto- discourse data set for training which is not easily adaptable to new languages and domains . 
Our approach differs from previous methods in that it represents the constituency structure as a se- ries of splitting representations , and uses a Seq2Seq framework to model the splitting decision at each step . 
By enabling beam search , our model can ﬁnd the best trees without the need to perform an expensive global search . 
We also unify discourse segmentation and parsing into one system by gen- eralizing our model , which has been done for the ﬁrst time to the best of our knowledge . 
Our splitting mechanism shares some similari- ties with Pointer Network ( Vinyals et al . , 2015a ; Ma et al . , 2018 ; Fernández - González and Gómez- Rodríguez , 2019 , 2020b ) or head - selection ap- proaches ( Zhang et al . , 2017 ; Kurita and Søgaard , 2019 ) , but is distinct from them that in each decod- ing step , our method identiﬁes the splitting point of a span and generates a new input for future steps instead of pointing to generate the next decoder input . 
6 Conclusion We have presented a novel , generic parsing method for constituency parsing based on a Seq2Seq frame- work . 
Our method supports an efﬁcient top - down decoding algorithm that uses a pointing function for scoring possible splitting points . 
The pointing mechanism captures global structural properties of a tree and allows efﬁcient training with a cross entropy loss . 
Our formulation , when applied to discourse parsing , can bypass discourse segmenta- tion as a pre - requisite step . 
Through experiments we have shown that our method outperforms all existing top - down methods on English Penn Tree- bank and RST Discourse Treebank sentence - level parsing tasks . 
With pre - trained representations , our method rivals state - of - the - art methods , while being faster . 
Our model also establishes a new state - of- the - art for sentence - level RST parsing . 
5804References Yoav Artzi , Nicholas FitzGerald , and Luke Zettle- moyer . 
2013 . 
Semantic parsing with Combinatory Categorial Grammars . 
In Proceedings of the 51st Annual Meeting of the Association for Computa- tional Linguistics ( Tutorials ) , page 2 , Soﬁa , Bul- garia . 
Association for Computational Linguistics . 
Laura Banarescu , Claire Bonial , Shu Cai , Madalina Georgescu , Kira Grifﬁtt , Ulf Hermjakob , Kevin Knight , Philipp Koehn , Martha Palmer , and Nathan Schneider . 
2013 . 
Abstract Meaning Representation for sembanking . 
In Proceedings of the 7th Linguis- 
tic Annotation Workshop and Interoperability with Discourse , pages 178–186 , Soﬁa , Bulgaria . 
Associa- tion for Computational Linguistics . 
Samy Bengio , Oriol Vinyals , Navdeep Jaitly , and Noam Shazeer . 
2015 . 
Scheduled sampling for se- quence prediction with recurrent neural networks . 
InAdvances in Neural Information Processing Sys- tems , volume 28 , pages 1171–1179 . 
Curran Asso- ciates , 
Inc. Anders Bjorkelund , Ozlem Cetinoglu , Agnieszka Falenska , Richard Farkas , Thomas Mueller , Wolf- gang Seeker , and Zsolt Szanto . 
2014 . 
The ims- wrocław - szeged - cis entry at the spmrl 2014 shared task : Reranking and morphosyntax meet unlabeled data . 
In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Lan- guages and Syntactic Analysis of NonCanonical Languages , pages 97–102 . 
Maximin Coavoux and Benoît Crabbé . 2017 . 
Multi- 
lingual lexicalized constituency parsing with word- level auxiliary tasks . 
In Proceedings of the 15th Con- ference of the European Chapter of the Association for Computational Linguistics : Volume 2 , Short Pa- pers , pages 331–336 , Valencia , Spain . 
Association for Computational Linguistics . 
James Cross and Liang Huang . 
2016 . 
Span - based con- stituency parsing with a structure - label system and provably optimal dynamic oracles . 
In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1–11 , Austin , Texas . 
Association for Computational Linguistics . 
Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 . 
BERT : Pre - training of deep bidirectional transformers for language under- standing . 
In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171–4186 , Minneapolis , Minnesota . 
Associ- ation for Computational Linguistics . 
Timothy Dozat and Christopher D. Manning . 
2017 . 
Deep biafﬁne attention for neural dependency pars- ing . 
In 5th International Conference on Learning Representations , ICLR 2017 , Toulon , France , April 24 - 26 , 2017 , Conference Track Proceedings .Greg 
Durrett and Dan Klein . 
2015 . 
Neural CRF pars- ing . 
In Proceedings of the 53rd Annual Meet- ing of the Association for Computational Linguis- tics and the 7th International Joint Conference on Natural Language Processing ( Volume 1 : Long Pa- pers ) , pages 302–312 , Beijing , China . 
Association for Computational Linguistics . 
Chris Dyer , Adhiguna Kuncoro , Miguel Ballesteros , and Noah A. Smith . 
2016 . 
Recurrent neural network grammars . 
In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 199–209 , San Diego , California . Association for Computational Linguistics . 
Daniel Fernández - González and Carlos Gómez- Rodríguez . 
2019 . 
Left - to - right dependency parsing with pointer networks . 
In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 710–716 , Minneapolis , Minnesota . 
Association for Computational Linguistics . 
Daniel Fernández - González and Carlos Gómez- Rodríguez . 
2020a . 
Enriched in - order linearization for faster sequence - to - sequence constituent parsing . 
InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4092–4099 , Online . 
Association for Computational Linguistics . 
Daniel Fernández - González and Carlos Gómez- Rodríguez . 
2020b . 
Transition - based semantic dependency parsing with pointer networks . 
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7035–7046 , Online . 
Association for Computational Linguistics . 
David Gaddy , Mitchell Stern , and Dan Klein . 
2018 . 
What ’s going on in neural constituency parsers ? 
an analysis . 
In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Tech- nologies , Volume 1 ( Long Papers ) , pages 999–1010 , New Orleans , Louisiana . 
Association for Computa- tional Linguistics . 
Carlos Gómez , Rodríguez and David Vilares . 
2018 . 
Constituent parsing as sequence labeling . 
In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing , pages 1314 – 1324 , Brussels , Belgium . 
Association for Computa- tional Linguistics . 
Sepp Hochreiter and Jürgen Schmidhuber . 
1997 . 
Long short - term memory . 
Neural computation , 9(8):1735–1780 . 
Shaﬁq Joty , Giuseppe Carenini , and Raymond Ng . 2012 . 
A novel discriminative framework for sentence - level discourse analysis . 
In Proceedings of the 2012 Joint Conference on Empirical Methods 
5805 in Natural Language Processing and Computational Natural Language Learning , pages 904–915 , Jeju Is- land , Korea . 
Association for Computational Linguis- tics . 
Shaﬁq Joty , Giuseppe Carenini , Raymond Ng , and Yashar Mehdad . 2013 . 
Combining intra- and multi- sentential rhetorical parsing for document - level dis- course analysis . 
In Proceedings of the 51st Annual Meeting of the Association for Computational Lin- guistics ( Volume 1 : Long Papers ) , pages 486–496 , Soﬁa , Bulgaria . 
Association for Computational Lin- guistics . 
Shaﬁq Joty , Giuseppe Carenini , and Raymond T. Ng . 2015 . 
CODRA : 
A novel discriminative framework for rhetorical analysis . 
Computational Linguistics , 41(3):385–435 . 
Hidetaka Kamigaito , Katsuhiko Hayashi , Tsutomu Hirao , Hiroya Takamura , Manabu Okumura , and Masaaki Nagata . 2017 . 
Supervised attention for sequence - to - sequence constituency parsing . 
In Pro- ceedings of the Eighth International Joint Confer- ence on Natural Language Processing ( Volume 2 : Short Papers ) , pages 7–12 , Taipei , Taiwan . 
Asian Federation of Natural Language Processing . 
Diederik P. Kingma and Jimmy Ba . 2015 . 
Adam : A method for stochastic optimization . 
In 3rd Inter- national Conference on Learning Representations , ICLR 2015 , San Diego , CA , USA , May 7 - 9 , 2015 , Conference Track Proceedings . 
Nikita Kitaev , Steven Cao , and Dan Klein . 
2019 . 
Multi- 
lingual constituency parsing with self - attention and pre - training . 
In Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics , pages 3499–3505 , Florence , Italy . 
Associa- tion for Computational Linguistics . 
Nikita Kitaev and Dan Klein . 
2018 . 
Constituency pars- ing with a self - attentive encoder . 
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 2676–2686 , Melbourne , Australia . 
Associa- tion for Computational Linguistics . 
Shuhei Kurita and Anders Søgaard . 
2019 . 
Multi - task semantic dependency parsing with policy gradient for learning easy-ﬁrst strategies . 
In Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics , pages 2420–2430 , Florence , Italy . 
Association for Computational Linguistics . 
Xiang Lin , Shaﬁq Joty , Prathyusha Jwalapuram , and M Saiful Bari . 2019 . 
A uniﬁed linear - time frame- work for sentence - level discourse parsing . 
In Pro- ceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 4190 – 4200 , Florence , Italy . Association for Computational Linguistics . 
Jiangming Liu and Yue Zhang . 
2017 . 
Shift - reduce constituent parsing with neural lookahead features . 
Transactions of the Association for Computational Linguistics , 5:45–58 . 
Zhengyuan Liu , Ke Shi , and Nancy Chen . 2020 . 
Mul- tilingual neural RST discourse parsing . 
In Proceed- ings of the 28th International Conference on Com- putational Linguistics , pages 6730–6738 , Barcelona , Spain ( Online ) . 
International Committee on Compu- tational Linguistics . 
Carlson Lynn , Daniel Marcu , and Mary Ellen Okurowski . 
2002 . 
Rst discourse treebank ( rst – dt ) ldc2002t07 . 
Linguistic Data Consortium . 
Xuezhe Ma , Zecong Hu , Jingzhou Liu , Nanyun Peng , Graham Neubig , and Eduard Hovy . 
2018 . 
Stack- pointer networks for dependency parsing . 
In Pro- ceedings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1403–1414 , Melbourne , Australia . Association for Computational Linguistics . 
William Mann and Sandra Thompson . 
1988 . 
Rhetori- cal Structure Theory : Toward a Functional Theory of Text Organization . 
Text , 8(3):243–281 . 
Mitchell P. Marcus , Mary Ann Marcinkiewicz , and Beatrice Santorini . 
1993 . 
Building a large annotated corpus of english : The penn treebank . 
Comput . 
Lin- guist . , 19(2):313–330 . 
Thanh - Tung Nguyen , Xuan - Phi Nguyen , Shaﬁq Joty , and Xiaoli Li . 
2020 . 
Efﬁcient constituency pars- ing by pointing . 
In Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics , pages 3284–3294 , Online . 
Association for Computational Linguistics . 
Matthew E. Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer . 
2018 . 
Deep contextualized word repre- sentations . 
In Proc . 
of NAACL 
. 
Slav Petrov and Dan Klein . 
2007 . 
Improved inference for unlexicalized parsing . 
In Human Language Tech- nologies 2007 : The Conference of the North Amer- ican Chapter of the Association for Computational Linguistics ; Proceedings of the Main Conference , pages 404–411 , Rochester , New York . 
Association for Computational Linguistics . 
Djamé Seddah , Reut Tsarfaty , Sandra Kübler , Marie Candito , Jinho D. Choi , Richárd Farkas , Jen- nifer Foster , Iakes Goenaga , Koldo Gojenola Gal- letebeitia , Yoav Goldberg , Spence Green , Nizar Habash , Marco Kuhlmann , Wolfgang Maier , Joakim Nivre , Adam Przepiórkowski , Ryan Roth , Wolfgang Seeker , Yannick Versley , Veronika Vincze , Marcin Woli ´ nski , Alina Wróblewska , and Eric Villemonte de la Clergerie . 
2013 . 
Overview of the SPMRL 2013 shared task : A cross - framework evaluation of parsing morphologically rich languages . 
In Proceed- ings of the Fourth Workshop on Statistical Parsing of Morphologically - Rich Languages , pages 146–182 , Seattle , Washington , USA . Association for Compu- tational Linguistics . 
5806Yikang Shen , Zhouhan Lin , Athul Paul Jacob , Alessan- dro Sordoni , Aaron Courville , and Yoshua Bengio . 
2018 . 
Straight to the tree : Constituency parsing with neural syntactic distance . 
In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics ( Volume 1 : Long Papers ) , pages 1171–1180 , Melbourne , Australia . Association for Computational Linguistics . 
Radu Soricut and Daniel Marcu . 
2003 . 
Sentence level discourse parsing using syntactic and lexical infor- mation . 
In Proceedings of the 2003 Human Lan- guage Technology Conference of the North Ameri- can Chapter of the Association for Computational Linguistics , pages 228–235 . 
Mitchell Stern , Jacob Andreas , and Dan Klein . 2017a . 
A minimal span - based neural constituency parser . 
In Proceedings of the 55th Annual Meeting of the As- sociation for Computational Linguistics , ACL 2017 , Vancouver , Canada , July 30 - August 4 , Volume 1 : Long Papers , pages 818–827 . 
Mitchell Stern , Daniel Fried , and Dan Klein . 2017b . 
Effective inference for generative neural parsing . 
In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 1695–1700 , Copenhagen , Denmark . 
Association for Computational Linguistics . 
Jun Suzuki , Sho Takase , Hidetaka Kamigaito , Makoto Morishita , and Masaaki Nagata . 
2018 . 
An empirical study of building a strong baseline for constituency parsing . 
In Proceedings of the 56th Annual Meet- ing of the Association for Computational Linguis- tics ( Volume 2 : Short Papers ) , pages 612–618 , Mel- bourne , Australia . 
Association for Computational Linguistics . 
Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Ł ukasz Kaiser , and Illia Polosukhin . 2017 . 
Attention is all you need . 
In I. Guyon , U. V . 
Luxburg , S. Bengio , H. Wallach , R. Fergus , S. Vishwanathan , and R. Gar- nett , editors , Advances in Neural Information Pro- cessing Systems 30 , pages 5998–6008 . 
Curran Asso- ciates , Inc. Oriol Vinyals , Meire Fortunato , and Navdeep Jaitly . 
2015a . 
Pointer networks . 
In C. Cortes , N. D. Lawrence , D. D. Lee , M. Sugiyama , and R. Gar- nett , editors , Advances in Neural Information Pro- cessing Systems 28 , pages 2692–2700 . 
Curran Asso- ciates , Inc. Oriol Vinyals , Ł ukasz Kaiser , Terry Koo , Slav Petrov , Ilya Sutskever , and Geoffrey Hinton . 
2015b . 
Gram- mar as a foreign language . 
In Advances in Neural Information Processing Systems , volume 28 , pages 2773–2781 . 
Curran Associates , Inc. 
Yizhong Wang , Sujian Li , and Houfeng Wang . 
2017 . 
A two - stage parsing method for text - level discourse analysis . 
In Proceedings of the 55th Annual Meet- ing of the Association for Computational Linguistics(Volume 2 : Short Papers ) , pages 184–188 . 
Associa- tion for Computational Linguistics . 
B. Webber . 
2004 . 
D - LTAG : 
Extending Lexicalized TAG to Discourse . 
Cognitive Science , 28(5):751 – 779 . 
Yang Wei , Yuanbin Wu , and Man Lan . 
2020 . 
A span- based linearization for constituent trees . 
In Pro- ceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 3267 – 3277 , Online . 
Association for Computational Lin- guistics . 
Longyin Zhang , Yuqing Xing , Fang Kong , Peifeng Li , and Guodong Zhou . 
2020a . 
A top - down neural architecture towards text - level parsing of discourse rhetorical structure . 
In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics , pages 6386–6395 , Online . 
Association for Computational Linguistics . 
Xingxing Zhang , Jianpeng Cheng , and Mirella Lapata . 
2017 . 
Dependency parsing as head selection . 
In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Lin- guistics : Volume 1 , Long Papers , pages 665–676 , Valencia , Spain . 
Association for Computational Lin- guistics . 
Yu Zhang , Houquan Zhou , and Zhenghua Li . 
2020b . 
Fast and accurate neural crf constituency parsing . 
InProceedings of the Twenty - Ninth International Joint Conference on Artiﬁcial Intelligence , IJCAI- 20 , pages 4046–4053 . 
International Joint Confer- ences on Artiﬁcial Intelligence Organization . 
Main track . 
Kai Zhao and Liang Huang . 2017 . 
Joint syntacto- discourse parsing and the syntacto - discourse tree- bank . 
In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2117–2123 , Copenhagen , Denmark . 
Associa- tion for Computational Linguistics . 
Junru Zhou and Hai Zhao . 
2019 . 
Head - driven phrase structure grammar parsing on penn treebank . 
In Pro- ceedings of the 57th Conference of the Association for Computational Linguistics , ACL 2019 , Florence , Italy , July 28- August 2 , 2019 , Volume 1 : Long Pa- pers , pages 2396–2408 . 
Muhua Zhu , Yue Zhang , Wenliang Chen , Min Zhang , and Jingbo Zhu . 2013 . 
Fast and accurate shift- reduce constituent parsing . 
In Proceedings of the 51st Annual Meeting of the Association for Compu- tational Linguistics ( Volume 1 : Long Papers ) , pages 434–443 , Soﬁa , Bulgaria . Association for Computa- tional Linguistics . 
Appendix 6.1 Discourse Parsing Architecture Figure 4 illustrates our end - to - end model architec- ture for discourse parsing . 
5807 Figure 4 : Our discourse parser a long with the decoding process for a given sentence . 
The input to the decoder at each step is the representation of the span to be split . 
We predict splitting point using the biafﬁne function between the corresponding decoder state and the boundary representations . 
The relationship between left and right spans are assigned with the label using the label classiﬁer . 
6.2 Discourse Parsing Inference Algorithms Algorithm 2 shows the end - to - end discourse pars- ing inference process . 
Algorithm 2 Discourse Inference ] 
Input : Sentence length n ; boundary encoder states : ( h0;h1;:::;hn ) ; label scores : P(lj(i;k);(k;j)),0i < kjn;l2L , initial decoder state st . Output : 
Parse treeT ST= 
[ ( 1;n ) ] // stack of spans S= 
[ ] whileST 6=?do ( i;j ) = pop(ST ) prob;st = dec(st;(i;j ) ) 
k= arg maxi < kjprob 
curr_partial_tree 
= partial _ tree ifj 1 > k > i + 1then push(ST;(k;j ) ) push(ST;(i;k ) ) else ifj 1 > k = i+ 1then push(ST;(k;j ) ) else ifk = j 1 > i+ 1then push(ST;(i;k ) ) end if ifk6 = jthen push(S((i;k;j ) ) 
end if end while T= 
[ ( ( i;k;j ) ; argmaxlP(lj(i;k)(k;j))8(i;k;j ) 2S ] 