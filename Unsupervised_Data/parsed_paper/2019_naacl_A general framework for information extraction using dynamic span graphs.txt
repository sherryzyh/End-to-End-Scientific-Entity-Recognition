Proceedings of NAACL-HLT 2019 , pages 3036–3046
Minneapolis, Minnesota, June 2 - June 7, 2019.
c2019 Association for Computational Linguistics3036A General Framework for Information Extraction using Dynamic Span Graphs Yi LuanyDave WaddenyLuheng
HezAmy Shahy Mari OstendorfyHannaneh Hajishirziy
yUniversity of Washington Allen Institute for Artiﬁcial Intelligence zGoogle AI Language fluanyi, dwadden, amyshah, ostendor, hannaneh g@uw.edu
luheng@google.com
Abstract We introduce a general framework for sev- eral information extraction tasks that share span representations using dynamically con- structed span graphs.
The graphs are con- structed by selecting the most conﬁdent entity spans and linking these nodes with conﬁdence- weighted relation types and coreferences.
The dynamic span graph allows coreference and re- lation type conﬁdences to propagate through the graph to iteratively reﬁne the span rep- resentations.
This is unlike previous multi- task frameworks for information extraction in which the only interaction between tasks is in the shared ﬁrst-layer LSTM.
Our framework signiﬁcantly outperforms the state-of-the-art on multiple information extraction tasks across multiple datasets reﬂecting different domains.
We further observe that the span enumeration approach is good at detecting nested span enti- ties, with signiﬁcant F1 score improvement on the ACE dataset.1 1 Introduction Most Information Extraction (IE) tasks require identifying and categorizing phrase spans, some of which might be nested.
For example, entity recognition involves assigning an entity label to a phrase span.
Relation Extraction (RE) involves assigning a relation type between pairs of spans.
Coreference resolution groups spans referring to the same entity into one cluster.
Thus, we might expect that knowledge learned from one task might beneﬁt another.
Most previous work in IE (e.g., (Nadeau and Sekine, 2007; Chan and Roth, 2011)) employs a pipeline approach, ﬁrst detecting entities and then using the detected entity spans for relation extrac- tion and coreference resolution.
To avoid cascading 1Code and pre-trained models are publicly available at https://github.com/luanyi/DyGIE .
COREFTom’s car broke down as he arrived at Starbucks to meet Mike.
“This thing’s useless!”
Tom exclaimed as it gave off smoke.
PER-SOCPHYSVEHCOREFPERLOCPERPERVEHPERVEHPHYSFigure 1: A text passage illustrating interactions be- tween entities, relations and coreference links.
Some relation and coreference links are omitted.
errors introduced by pipeline-style systems, recent work has focused on coupling different IE tasks as in joint modeling of entities and relations (Miwa and Bansal, 2016; Zhang et al., 2017), entities and coreferences (Hajishirzi et al., 2013; Durrett and Klein, 2014), joint inference (Singh et al., 2013) or multi-task (entity/relation/coreference) learn- ing (Luan et al., 2018a).
These models mostly rely on the ﬁrst layer LSTM to share span repre-
sentations between different tasks and are usually designed for speciﬁc domains.
In this paper, we introduce a general framework Dynamic Graph IE ( DYGIE) for coupling multiple information extraction tasks through shared span representations which are reﬁned leveraging con- textualized information from relations and coref- erences.
Our framework is effective in several do- mains, demonstrating a beneﬁt from incorporating broader context learned from relation and corefer- ence annotations.
Figure 1 shows an example illustrating the po- tential beneﬁts of entity, relation, and coreference contexts.
It is impossible to predict the entity la- bels for This thing anditfrom within-sentence con- text alone.
However, the antecedent carstrongly suggests that these two entities have a VEH type.
Similarly, the fact that Tom is located at Starbucks andMike has a relation to Tom provides support for
3037the fact that Mike is located at Starbucks .
DYGIE uses multi-task learning to identify en- tities, relations, and coreferences through shared span representations using dynamically constructed span graphs.
The nodes in the graph are dynam- ically selected from a beam of highly-conﬁdent mentions, and the edges are weighted according to the conﬁdence scores of relation types or coref- erences.
Unlike the multi-task method that only shares span representations from the local con- text (Luan et al., 2018a), our framework leverages rich contextual span representations by propagat- ing information through coreference and relation links.
Unlike previous BIO-based entity recogni- tion systems (Collobert and Weston, 2008; Lample et al., 2016; Ma and Hovy, 2016) that assign a text span to at most one entity, our framework enumer- ates and represents all possible spans to recognize arbitrarily overlapping entities.
We evaluate DYGIE on several datasets span- ning many domains (including news, scientiﬁc arti- cles, and wet lab experimental protocols), achiev-
ing state-of-the-art performance across all tasks and domains and demonstrating the value of coupling related tasks to learn richer span representations.
For example, DYGIE achieves relative improve-
ments of 5.7% and 9.9% over state of the art on the ACE05 entity and relation extraction tasks, and an 11.3% relative improvement on the ACE05 over- lapping entity extraction task.
The contributions of this paper are threefold.
1) We introduce the dynamic span graph frame- work as a method to propagate global contextual information, making the code publicly available.
2) We demonstrate that our framework signiﬁcantly outperforms the state-of-the-art on joint entity and relation detection tasks across four datasets: ACE 2004, ACE 2005, SciERC and the Wet Lab Proto- col Corpus.
3) We further show that our approach excels at detecting entities with overlapping spans, achieving an improvement of up to 8 F1 points on three benchmarks annotated with overlapped spans: ACE 2004, ACE 2005 and GENIA.
2 Related Work Previous studies have explored joint model- ing (Miwa and Bansal, 2016; Zhang et al., 2017; Singh et
al., 2013; Yang and Mitchell, 2016)) and multi-task learning (Peng and Dredze, 2015; Peng et al., 2017; Luan et al., 2018a, 2017a) as methods to share representational strength across related in-formation extraction tasks.
The most similar to ours is the work in Luan et
al. (2018a) that takes a multi-task learning approach to entity, relation, and coreference extraction.
In this model, the dif- ferent tasks share span representations that only incorporate broader context indirectly via the gra- dients passed back to the LSTM layer.
In contrast, DYGIE uses dynamic graph propagation to explic- itly incorporate rich contextual information into the span representations.
Entity recognition has commonly been cast as a sequence labeling problem, and has beneﬁted substantially from the use of neural architectures (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Luan et
al., 2017b, 2018b).
However, most systems based on sequence labeling suffer from an inability to extract entities with overlap- ping spans.
Recently Katiyar and Cardie (2018) and Wang and Lu (2018) have presented methods enabling neural models to extract overlapping enti- ties, applying hypergraph-based representations on top of sequence labeling systems.
Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possi- ble spans as candidate entities.
Neural graph-based models have achieved sig-
niﬁcant improvements over traditional feature- based approaches on several graph modeling tasks.
Knowledge graph completion (Yang et al., 2015; Bordes et al., 2013) is one prominent example.
For relation extraction tasks, graphs have been used primarily as a means to incorporate pipelined features such as syntactic or discourse relations (Peng et al., 2017; Song et al., 2018; Zhang et al., 2018).
Christopoulou et
al.
(2018) models all pos- sible paths between entities as a graph, and reﬁnes pair-wise embeddings by performing a walk on the graph structure.
All these previous works assume that the nodes of the graph (i.e. the entity candi- dates to be considered during relation extraction) are predeﬁned and ﬁxed throughout the learning process.
On the other hand, our framework does not require a ﬁxed set of entity boundaries as an input for graph construction.
Motivated by state-of- the-art span-based approaches to coreference res-
olution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018), the model uses a beam pruning strategy to dynamically select high-quality spans, and constructs a graph using the selected spans as nodes.
Many state-of-the-art RE models rely upon
3038domain-speciﬁc external syntactic tools to con- struct dependency paths between the entities in a sentence (Li and Ji, 2014; Xu et
al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017).
These sys- tems suffer from cascading errors from these tools and are hard to generalize to different domains.
To make the model more general, we combine the multitask learning framework with ELMo em- beddings (Peters et al., 2018) without relying on external syntactic tools and risking the cascading errors that accompany them, and improve the inter- action between tasks through dynamic graph prop- agation.
While the performance of DyGIE beneﬁts from ELMo, it advances over some systems (Luan et al., 2018a; Sanh et al., 2019) that also incorporate ELMo.
The analyses presented here give insights into the beneﬁts of joint modeling.
3 Model Problem Deﬁnition
The input is a document rep- resented as a sequence of words
D, from which we deriveS=fs1;:::;s Tg, the set of all possible within-sentence word sequence spans (up to length L) in the document.
The output contains three structures: the entity types Efor all spans S, the relationsRfor all span pairs SSwithin the same sentence, and the coreference links Cfor all spans inSacross sentences.
We consider two primary tasks.
First, Entity Recognition is the task of pre- dicting the best entity type labels eifor each span si.
Second, Relation Extraction involves predicting the best relation type rijfor all span pairs (si;sj).
We provide additional supervision by also training our model to perform a third, auxiliary task: Coref- erence resolution .
For this task we predict the best antecedentcifor each span si.
Our Model We develop a general information extraction framework ( DYGIE) to identify and classify entities, relations, and coreference in a multi-task setup.
DYGIE ﬁrst enumerates all text spans in each sentence, and computes a locally- contextualized vector space representation of each span.
The model then employs a dynamic span graph to incorporate global information into its span representations, as follows.
At each training step, the model identiﬁes the text spans that are most likely to represent entities, and treats these spans as nodes in a graph structure.
It constructs conﬁdence-weighted arcs for each node according to its predicted coreference and relation links with the other nodes in the graph.
Then, the span repre-sentations are reﬁned using broader context from gated updates propagated from neighboring rela- tion types and co-referred entities.
These reﬁned span representations are used in a multi-task frame- work to predict entity types, relation types, and coreference links.
3.1 Model Architecture In this section, we give an overview of the main components and layers of the DYGIE framework, as illustrated in Figure 2.
Details of the graph con- struction and reﬁnement process will be presented in the next section.
Token Representation Layer We apply a bidi- rectional LSTM over the input tokens.
The input for each token is a concatenation of the character reprensetation, GLoVe (Pennington et al., 2014) word embeddings, and ELMo embeddings (Peters et al., 2018).
The output token representations are obtained by stacking the forward and backward LSTM hidden states.
Span Representation Layer For each span si, its initial vector representation g0 iis obtained by concatenating BiLSTM outputs at the left and right end points of si, an attention-based soft “head- word,” and an embedded span width feature, fol- lowing Lee et al.
(2017).
Coreference Propagation Layer The propaga- tion process starts from the span representations g0 i.
At each iteration t, we ﬁrst compute an update vector ut Cfor each span si.
Then we use ut Cto update the current representation gt i, producing the next span representation gt+1 i.
By repeating this processNtimes, the ﬁnal span representations gN
i share contextual information across spans that are likely to be antecedents in the coreference graph, similar to the process in (Lee et al., 2018).
Relation Propagation Layer The outputs gN i from the coreference propagation layer are passed as inputs to the relation propagation layer.
Similar to the coreference propagation process, at each it- erationt, we ﬁrst compute the update vectors ut R for each span si, then use it to compute gt+1 i. In- formation can be integrated from multiple relation paths by repeating this process Mtimes.
Final Prediction Layer We use the outputs of
the relation graph layer gN+M ito predict the entity labelsEand relation labels R.
For entities, we passgN+M i to a feed-forward network (FFNN) to
3039 arrive atStarbuckscarTom Input documentSpan enumerationFinal prediction  of entities and relations Coref.
Tom’s car broke down as he arrived at Starbucks to meet Mike.
“This thing’s useless!”
Tom exclaimed as it gave off smoke.
Sentence-level BiLSTMSentence-level BiLSTMTomcararrive atStarbucksMikeTomthis thingitToken  representations…Coref.carthis thingitTomMikeStarbucksPER-SOCPHYSMikeTomthis
thingit…PERVEHNULLPHYSVEHCoref.carthis thingitFinal prediction of coreferenceIterative inference and propagation for relationsIterative inference and propagation for coreferencePERPERPHYSVEHPER-SOCPHYSCoref.
LOCM times……N times…Figure 2: Overview of our D YGIE model.
Dotted arcs indicate conﬁdence weighted graph edges.
Solid lines indicate the ﬁnal predictions.
produce per-class scores PE(i)for spansi.
For relations, we pass the concatenation of gN+M i and gN+M j to a FFNN to produce per-class relation scores PR(i;j)between spans siandsj.
Entity and relation scores are normalized across the label space, similar to Luan et al. (2018a).
For coref- erence, the scores between span pairs ( si;sj) are computed from the coreference graph layer outputs (gN i;gN j), and then normalized across all possible antecedents, similar to Lee et al.
(2018).
3.2 Dynamic Graph Construction and Span Reﬁnement The dynamic span graph facilitates propagating broader contexts through soft coreference and rela- tion links to reﬁne span representations.
The nodes in the graph are spans siwith vector representa- tionsgt i2Rdfor thet-th iteration.
The edges are weighted by the coreference and relation scores, which are trained according to the neural archi- tecture explained in Section 3.1.
In this section, we explain how coreference and relation links can update span representations.
Coreference Propagation Similar to (Luan et al., 2018a), we deﬁne a beam BCconsisting ofbcspans that are most likely to be in a corefer- ence chain.
We consider Pt Cto be a matrix of real values that indicate coreference conﬁdence scores between these spans at the t-th iteration.
Pt Cis of sizebcK, whereKis the maximum num- ber of antecedents considered.
For the coreferencegraph, an edge in the graph is single directional, connecting the current span siwith all its poten- tial antecedents sjin the coreference beam, where j <i .
The edge between siandsjis weighted by coreference conﬁdence score at the current itera- tionPt C(i;j).
The span update vector ut C(i)2Rd is computed by aggregating the neighboring span representations gt j, weighted by their coreference scoresPt C(i;j): ut C(i) =X j2BC(i)Pt C(i;j)gt
j (1) whereBC(i)is the set of Kspans that are an- tecedents of si, Pt C(i;j)
=exp(Vt C(i;j))P j02BC(i)exp(Vt C(i;j))(2)
Vt C(i;j)is a scalar score computed by concate- nating the span representations
[
gt i;gt j;gt
igt j], whereis element-wise multiplication.
The con- catenated vector is then fed as input to a FFNN, similar to (Lee et al., 2018).
Relation Propagation For each sentence, we deﬁne a beam BRconsisting of brentity spans that are mostly likely to be involved in a rela- tion.
Unlike the coreference graph, the weights of relation edges capture different relation types.
Therefore, for the t-th iteration, we use a tensor Vt R2RbRbRLRto capture scores of each of the LRrelation types.
In other words, each edge in the
3040relation graph connects two entity spans siandsj in the relation beam BR.Vt R(i;j)is aLR-length vector of relation scores, computed with a FFNN with[gt i;gt j]as the input.
The relation update vec- torut R(i)2Rdis computed by aggregating neigh- boring span representations on the relation graph: ut R(i) =X j2BRf(Vt R(i;j))ARgt j;(3) where AR2RLRdis a trainable linear projection matrix,fis a non-linear function to select the most important relations.
Because only a small number of entities in the relation beam are actually linked to the target span, propagation among all possi- ble span pairs would introduce too much noise to the new representation.
Therefore, we choose f to be the ReLU function to remove the effect of unlikely relations by setting the all negative rela- tion scores to 0.
Unlike coreference connections, two spans linked via a relation are not expected to have similar representations, so the matrix AR helps to transform the embedding gt jaccording to each relation type.
Updating Span Representations with Gating To compute the span representations for the next iterationt2f1;:::;N
+Mg, we deﬁne a gating vector ft x(i)2Rd, wherex2fC;Rg, to deter- mine whether to keep the previous span represen- tation gt ior to integrate new information from the coreference or relation update vectors ut x(i).
For- mally, ft x(i) =g(Wf x[gt i;ut x(i)]) (4) gt+1
i =ft x(i)gt i+ (1 ft x(i))ut x(i); where Wf x2Rd2dare trainable parameters, and gis an element-wise sigmoid function.
3.3 Training The loss function is deﬁned as a weighted sum of the log-likelihood of all three tasks: X (D;R;E;C)2Dn ElogP(EjC;R;D ) (5) +RlogP(RjC;D) +ClogP(CjD)o whereE,RandCare gold structures of the entity types, relations and coreference, respec- tively.
Dis the collection of all training documents D.
The task weights E,R, andCare hyper- parameters to control the importance of each task.
Domain Docs Ent Rel Coref ACE04 News 348 7 7 3 ACE05 News 511 7 6 7 SciERC AI 500 6 7 3 WLP Bio lab 622 18 13 7 Table 1: Datasets for joint entity and relation extraction and their statistics.
Ent: Number of entity categories.
Rel: Number of relation categories.
We use a 1 layer BiLSTM with 200-dimensional hidden layers.
All the feed-forward functions have 2 hidden layers of 150 dimensions each.
We use 0.4 variational dropout (Gal and Ghahramani, 2016) for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.
The hidden layer dimensions and dropout rates are chosen based on the development set performance in multiple do- mains.
The task weights, learning rate, maximum span length, number of propagation iterations and beam size are tuned speciﬁcally for each dataset using development data.
4 Experiments DYGIE is a general IE framework that can be ap- plied to multiple tasks.
We evaluate the perfor- mance of DYGIE against models from two lines of work: combined entity and relation extraction, and overlapping entity extraction.
4.1 Entity and relation extraction For the entity and relation extraction task, we test the performance of DYGIE on four different datasets: ACE2004, ACE2005, SciERC and the Wet Lab Protocol Corpus.
We include the rela- tion graph propagation layer in our models for all datasets.
We include the coreference graph propa- gation layer on the data sets that have coreference annotations available.
Data All four data sets are annotated with entity and relation labels.
Only a small fraction of entities (<3%of total) in these data sets have a text span that overlaps the span of another entity.
Statistics on all four data sets are displayed in Table 1.
The ACE2004 andACE2005 corpora provide entity and relation labels for a collection of docu- ments from a variety of domains, such as newswire and online forums.
We use the same entity and relation types, data splits, and preprocessing as Miwa and Bansal (2016) and Li and Ji (2014).
Fol- lowing the convention established in this line of work, an entity prediction is considered correct
3041Dataset System Entity Relation ACE04Bekoulis et al.
(2018) 81.6 47.5 Miwa and Bansal (2016) 81.8 48.4 DYGIE 87.4 59.7 ACE05Miwa and Bansal (2016) 83.4 55.6 Zhang et al.
(2017) 83.6 57.5 Sanh et al.
(2019) 87.5 62.7 DYGIE 88.4 63.2 SciERCLuan et al.
(2018a) 64.2 39.3 DYGIE 65.2 41.6 WLPCKulkarni et al.
(2018)
78.0 *54.9 DYGIE 79.5 64.1 Table 2: F1 scores on the joint entity and relation
ex- traction task on each test set, compared against the pre- vious best systems.
* indicates relation extraction sys- tem that takes gold entity boundary as input.
if its type label and head region match those of a gold entity.
We will refer to this version of the ACE2004 and ACE2005 data as ACE04 and ACE05.
Since the domain and mention span an- notations in the ACE datasets are very similar to those of OntoNotes (Pradhan et al., 2012), and OntoNotes contains signiﬁcantly more documents with coreference annotations, we use OntoNotes to train the parameters for the auxiliary corefer- ence task.
The OntoNotes corpus contains 3493 documents, averaging roughly 450 words in length.
The SciERC corpus (Luan et al., 2018a) pro- vides entity, coreference and relation annotations for a collection of documents from 500 AI paper abstracts.
The dataset deﬁnes scientiﬁc term types and relation types specially designed for AI domain knowledge graph construction.
An entity predic- tion is considered correct if its label and span match with a gold entity.
The Wet Lab Protocol Corpus (WLPC) pro- vides entity, relation, and event annotations for 622 wet lab protocols (Kulkarni et al., 2018).
A wet lab protocol is a series of instructions specifying how to perform a biological experiment.
Following the procedure in Kulkarni et al.
(2018), we perform entity recognition on the union of entity tags and event trigger tags, and relation extraction on the union of entity-entity relations and entity-trigger event roles.
Coreference annotations are not avail- able for this dataset.
Baselines We compare DYGIE with current state of the art methods in different datasets.
Miwa and Bansal (2016) provide the current state of the art on ACE04.
They construct a Tree LSTM using dependency parse information, and use the repre-sentations learned by the tree structure as features for relation classiﬁcation.
Bekoulis et
al.
(2018) use adversarial training as regularization for a neu- ral model.
Zhang et al.
(2017) cast joint entity and relation extraction as a table ﬁlling problem and build a globally optimized neural model incorpo- rating syntactic representations from a dependency parser.
Similar to DYGIE, Sanh et al.
(2019) and Luan et
al.
(2018a) use a multi-task learning frame- work for extracting entity, relation and coreference labels.
Sanh et al.
(2019) improved the state of the art on ACE05 using multi-task, hierarchical supervised training with a set of low level tasks at the bottom layers of the model and more com- plex tasks at the top layers of the model.
Luan et
al.
(2018a) previously achieved the state of the art on SciERC and use a span-based neural model like our DYGIE.
Kulkarni et al.
(2018) provide a baseline for the WLPC data set.
They employ an LSTM-CRF for entity recognition, following Lample
et al.
(2016)
.
For relation extraction, they assume the presence of gold entities and train a maximum-entropy classiﬁer using features from the labeled entities.
Results Table 2 shows test set F1 on the joint entity and relation extraction task.
We observe that DYGIE achieves substantial improvements on both entity recognition and relation extraction across the four data sets and three domains, all in the realistic setting where no “gold” entity labels are supplied at test time.
DYGIE achieves 7.1% and 7.0% rela- tive improvements over the state of the art on NER for ACE04 and ACE05, respectively.
For the rela- tion extraction task, DYGIE attains 25.8% relative improvement over SOTA on ACE04 and 13.7% rel-
ative improvement on ACE05.
For ACE05, the best entity extraction performance is obtained by switch- ing the order between CorefProp andRelProp (RelProp ﬁrst then CorefProp ).
On SciERC, DYGIE advances the state of the art by 5.9% and 1.9% for relation extraction and NER, respectively.
The improvement of DYGIE over the previous SciERC model underscores the ability of coreference and relation propagation to construct rich contextualized representations.
The results from Kulkarni et al.
(2018) estab- lish a baseline for IE on the WLPC.
In that work, relation extraction is performed using gold entity boundaries as input.
Without using any gold entity information, DYGIE improves on the baselines by 16.8% for relation extraction and 2.2% for NER.
3042Domain Docs Ent Overlap Coref ACE04-O News 443 7 42% 3 ACE05-O News 437 7 32% 7 GENIA Biomed 1999 5 24% 3 Table 3: Datasets for overlapping entity extraction and their statistics.
Ent: Number of entity categories.
Over- lap: Percentage of sentences that contain overlapping entities.
On the OntoNotes data set used for the auxiliary coreference task with ACE05, our model achieves coreference test set performance of 70.4 F1, which is competitive with the state-of-the-art performance reported in Lee et al.
(2017).
4.2 Overlapping Entity Extraction There are many applications where the correct iden- tiﬁcation of overlapping entities is crucial for cor- rect document understanding.
For instance, in the biomedical domain, a BRCA1 mutation carrier could refer to a patient taking part in a clinical trial, while BRCA1 is the name of a gene.
We evaluate the performance of DYGIE on overlapping entity extraction in three datasets: ACE2004, ACE2005 and GENIA.
Since relation annotations are not available for these datasets, we include the coreference propagation layer in our models but not the relation layer.2 Data Statistics on our three datasets are listed in Table 3.
All three have a substantial number (>20% of total) of overlapping entities, making them appropriate for this task.
As in the joint case, we evaluate our model on ACE2004 andACE2005 , but here we follow the same data preprocessing and evaluation scheme as Wang and Lu (2018).
We refer to these data sets as ACE04-O and ACE05-O. Unlike the joint en- tity and relation task in Sec.
4.1, where only the entity head span need be predicted, an entity pre- diction is considered correct in these experiments if both its entity label and its full text span match a gold prediction.
This is a more stringent evalua- tion criterion than the one used in Section 4.1.
As before, we use the OntoNotes annotations to train the parameters of the coreference layer.
TheGENIA corpus (Kim et al., 2003) provides entity tags and coreferences for 1999 abstracts from the biomedical research literature.
We only use the IDENT label to extract coreference clusters.
2We use the pre-processed ACE dataset from previous work and relation annotation is not available.
Dataset System Entity F1 ACE04-OKatiyar and Cardie (2018) 72.7 Wang and Lu (2018)
75.1 DYGIE 84.7
ACE05-OKatiyar and Cardie (2018) 70.5 Wang and Lu (2018) 74.5 DYGIE 82.9 GENIAKatiyar and Cardie (2018) 73.8 Wang and Lu (2018) 75.1 DYGIE 76.2 Table 4: Performance on the overlapping entity extrac- tion task, compared to previous best systems.
We re- port F1 of extracted entities on the test sets.
Entity Relation Model P R F1 P R F1 DYGIE 87.4 86.7 87.1 56.2 60.9 58.4  CorefProp 86.2 85.2 85.7 64.3 56.7 60.2  RelProp 87.0 86.7 86.9 60.4 55.8 58.0 Base 86.1 85.7 85.9 59.5 55.7 57.6 Table 5: Ablations on the ACE05 development set with different graph propagation setups.
 CorefProp ablates the coreference propagation layers, while  RelProp ablates the relation propagation layers.
Base is the system without any propagation.
We use the same data set split and preprocessing procedure as Wang and Lu (2018) for overlapping entity recognition.
Baselines The current state-of-the-art approach on all three data sets is Wang and Lu (2018), which uses a segmental hypergraph coupled with neural networks for feature learning.
Katiyar and Cardie (2018) also propose a hypergraph approach using a recurrent neural network as a feature extractor.
Results Table 4 presents the results of our over- lapping entity extraction experiments on the differ- ent datsets.
DYGIE improves 11.6% on the state of the art for ACE04-O and 11.3% for ACE05-O. DY- GIE also advances the state of the art on GENIA, albeit by a more modest 1.5%.
Together these re- sults suggest that DYGIE can be utilized fruitfully for information extraction across different domains with overlapped entities, such as bio-medicine.
5 Analysis of Graph Propagation We use the dev sets of ACE2005 and SciERC to analyze the effect of different model components.
5.1 Coreference and Relation Graph Layers Tables 5 and 6 show the effects of graph propa- gation on entity and relation prediction accuracy,
3043Entity Relation Model P R F1 P R F1 DYGIE 68.6 67.8 68.2 46.2
38.5 42.0  CorefProp 69.2 66.9 68.0 42.0 40.5 41.2  RelProp 69.1 66.0 67.5 43.6 37.6 40.4 Base 70.0 66.3 68.1 45.4 34.9 39.5 Table 6: Ablations on the SciERC development set on different graph progation setups
.
CorefProp has a much smaller effect on entity F1 compared to ACE05. 0 1 2 3808284868890 Num. iterations NEntity F1 (a) Entity F1 with different number of CorefProp it- erations N.0 1 2 3545658606264 Num. iterations MRelation F1 (b) Relation F1 with differ- ent number of RelProp it- erations M. Figure 3: F1 score of each layer on ACE development set for different number of iterations.
N= 0orM= 0 indicates no propagation is made for the layer.
where CorefProp and RelProp denote ab- lating the propagation process by setting N= 0
orM= 0, respectively.
Base is the base model without any propagation.
For ACE05, we observe that coreference propagation is mainly helpful for entities; it appears to hurt relation extraction.
On SciIE, coreference propagation gives a small ben- eﬁt on both tasks.
Relation propagation signiﬁ- cantly beneﬁts both entity and relation extraction in both domains.
In particular, there are a large por- tion of sentences with multiple relation instances across different entities in both ACE05 and Sci- ERC, which is the scenario in which we expect relation propagation to help.
Since coreference propagation has more effect on entity extraction and relation propagation has more effect on relation extraction, we mainly focus on ablating the effect of coreference propagation on entity extraction and relation propagation on relation extraction in the following subsections.
5.2 Coreference Propagation and Entities A major challenge of ACE05 is to disambiguate the entity class for pronominal mentions, which requires reasoning with cross-sentence contexts.
For example, in a sentence from ACE05 dataset, “One of [them] PER, from a very close friend of [ours] ORG.”
It is impossible to identity whether them andours is a person ( PER) or organization (ORG ) unless we have read previous sentences.
WeEntity Perf.
on Pronouns P R F1 DYGIE 79.0 77.1 78.0 DYGIE CorefProp 73.8 72.6 73.2 Table 7: Entity extraction performance on pronouns in ACE05.
CorefProp signiﬁcantly increases entity ex- traction F1 on hard-to-disambiguate pronouns by allow- ing the model to leverage cross-sentence contexts.
hypothesize that this is a context where coreference propagation can help.
Table 7 shows the effect of the coreference layer for entity categorization of pronouns.3DYGIE has 6.6% improvement on pronoun performance, conﬁrming our hypothesis.
Looking further, Table 8 shows the impact on all entity categories, giving the difference between the confusion matrix entries with and without CorefProp .
The frequent confusions associated with pronouns ( GPE/PER andPER/ORG , where GPE is a geopolitical entity) greatly improve, but the beneﬁt of CorefProp extends to most cate- gories.
Of course, there are a few instances where CorefProp causes errors in entity extraction.
For example, in the sentence “[They]ORG PER might have been using Northshore...”, DYGIE predicted They to be of ORG type because the most conﬁdent an- tecedent is those companies in the previous sen- tence: “The money was invested in those compa- nies.”
However, They is actually referring to these fund managers earlier in the document, which be- longs to PER category.
In the SciERC dataset, the pronouns are uni- formly assigned with a Generic label, which ex- plains why CorefProp does not have much ef- fect on entity extraction performance.
The Figure 3a shows the effect of number of iterations for coreference propagation in the entity extraction task.
The ﬁgure shows that coreference layer obtains the best performance on the second iteration (N= 2).
5.3 Relation Propagation Impact Figure 4 shows relation scores as a function of num- ber of entities in sentence for DYGIE andDYGIE without relation propagation on ACE05.
The ﬁgure indicates that relation propagation achieves signiﬁ- cant improvement in sentences with more entities, where one might expect that using broader context 3Pronouns included: anyone, everyone, it, itself, one, our, ours, their, theirs, them, themselves, they, us, we,
who
3044LOC WEA GPE PER FAC ORG VEH LOC 5 0 -2 -1 2 -1 0 WEA 0 3 0 0 1 -3 -1 GPE -3 0 31 -26 3 -7 0 PER 0 -2 -3 18 -1 -26 4 FAC 4 -1 2 -3 2 -5 1 ORG 0 0 0 -8 -1 6 0 VEH 0 -2 -1 2 5 -1 1 Table 8: Difference in the confusion matrix counts for ACE05 entity extraction associated with adding CorefProp .
2 3 4-5 6-11 12-max506070 Num. entities in sentenceRelation F1DYGIE DYGIE RelProp Figure 4: Relation F1 broken down by number of enti- ties in each sentence.
The performance of relation ex-
traction degrades on sentences containing more entities.
Adding relation propagation alleviates this problem.
could have more impact.
Figure 3b shows the effect of number of itera- tions for relation propagation in the relation extrac- tion task.
Our model achieves the best performance on the second iteration ( M= 2).
6 Conclusion We have introduced DYGIE as a general informa- tion extraction framework, and have demonstrated that our system achieves state-of-the art results on entity recognition and relation extraction tasks across a diverse range of domains.
The key con- tribution of our model is the dynamic span graph approach, which enhance interaction across tasks that allows the model to learn useful information from broader context.
Unlike many IE frameworks, our model does not require any preprocessing using syntactic tools, and has signiﬁcant improvement across different IE tasks including entity, relation extraction and overlapping entity extraction.
The addition of co-reference and relation propagation across sentences adds only a small computation cost to inference; the memory cost is controlled by beam search.
These added costs are small relative to those of the baseline span-based model.
We wel- come the community to test our model on different information extraction tasks.
Future directions in- clude extending the framework to encompass more structural IE tasks such as event extraction.
Acknowledgments This research was supported by the Ofﬁce of Naval Research under the MURI grant N00014-18-1- 2670, NSF (IIS 1616112, III 1703166), Allen Dis- tinguished Investigator Award, Samsung GRO and gifts from Allen Institute for AI, Google, Amazon, and Bloomberg.
We also thank the anonymous re- viewers and the UW-NLP group for their helpful comments.
References Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder.
2018.
Adversarial training for multi-context joint entity and relation extraction.
In Proc.
Conf.
Empirical Methods Natural Language Process.
(EMNLP) , pages 2830–2836.
Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko.
2013.
Translating embeddings for modeling multi- relational data.
In Advances in neural information processing systems .
Yee Seng Chan and Dan Roth.
2011.
Exploiting syntactico-semantic structures for relation extrac- tion.
In Proc.
Annu.
Meeting Assoc.
for Computa- tional Linguistics (ACL)
.
Fenia Christopoulou, Makoto Miwa, and Sophia Ana- niadou.
2018.
A walk-based model on entity graphs for relation extraction.
In Proc.
Annu.
Meeting As- soc.
for Computational Linguistics (ACL) , volume 2, pages 81–88.
Ronan Collobert and Jason Weston.
2008.
A uniﬁed architecture for natural language processing:
Deep neural networks with multitask learning.
In Proc.
Int.
Conf.
Machine Learning (ICML) , pages 160– 167.
Ronan Collobert, Jason Weston, L ´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
J. Machine Learning Research , 12(Aug):2493– 2537.
Greg Durrett and Dan Klein.
2014.
A joint model for entity analysis: Coreference, typing, and linking.
Trans.
Assoc.
for Computational Linguistics (TACL) , 2:477–490.
Yarin Gal and Zoubin Ghahramani. 2016.
A theoret- ically grounded application of dropout in recurrent neural networks.
In Proc.
Annu.
Conf.
Neural In- form.
Process.
Syst.
(NIPS) .
Hannaneh Hajishirzi, Leila Zilles, Daniel S Weld, and Luke Zettlemoyer.
2013.
Joint coreference res- olution and named-entity linking with multi-pass sieves.
In Proc.
Conf.
Empirical Methods Natural Language Process.
(EMNLP) , pages 289–299.
3045Luheng He, Kenton Lee, Omer Levy, and Luke Zettle- moyer.
2018.
Jointly predicting predicates and argu- ments in neural semantic role labeling.
In ACL.
Arzoo Katiyar and Claire Cardie.
2018.
Nested named entity recognition revisited.
In Proc.
Conf.
North American Assoc.
for Computational Linguis- tics (NAACL) .
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun’ichi Tsujii.
2003.
Genia corpus - a semantically annotated corpus for bio-textmining. Bioinformat-
ics, 19 Suppl 1:i180–2.
Chaitanya Kulkarni, Wei Xu, Alan Ritter, and Raghu Machiraju. 2018.
An annotated corpus for machine reading of instructions in wet lab protocols.
In NAACL-HLT .
Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
InProc.
Conf.
North American Assoc.
for Compu- tational Linguistics (NAACL) .
Kenton Lee, Luheng He, Mike Lewis, and Luke S. Zettlemoyer.
2017.
End-to-end neural coreference resolution.
In EMNLP .
Kenton Lee, Luheng He, and Luke Zettlemoyer.
2018.
Higher-order coreference resolution with coarse-to- ﬁne inference.
In NAACL .
Qi Li and Heng Ji. 2014.
Incremental joint extrac- tion of entity mentions and relations.
In Proc.
Annu.
Meeting Assoc.
for Computational Linguistics (ACL) , volume 1, pages 402–412.
Yi Luan, Chris Brockett, Bill Dolan, Jianfeng Gao, and Michel Galley.
2017a.
Multi-task learning for speaker-role adaptation in neural conversation mod- els.
In Proc.
IJCNLP .
Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018a.
Multi-task identiﬁcation of enti- ties, relations, and coreference for scientiﬁc knowl- edge graph construction.
In Proc.
Conf.
Empirical Methods Natural Language Process.
(EMNLP) .
Yi Luan, Mari Ostendorf, and Hannaneh Hajishirzi.
2017b.
Scientiﬁc information extraction with semi- supervised neural tagging.
In Proc.
Conf.
Empirical Methods Natural Language Process.
(EMNLP) .
Yi Luan, Mari Ostendorf, and Hannaneh Hajishirzi. 2018b.
The uwnlp system at semeval-2018 task 7: Neural relation extraction model with selectively in- corporated concept embeddings.
In Proc.
Int.
Work- shop on Semantic Evaluation (SemEval) , pages 788– 792.
Xuezhe Ma and Eduard Hovy. 2016.
End-to-end sequence labeling via bi-directional LSTM-CNNs- CRF.
In Proc.
Annu.
Meeting Assoc.
for Computa- tional Linguistics (ACL) .Makoto
Miwa and Mohit Bansal. 2016.
End-to-end re- lation extraction using lstms on sequences and tree structures.
In Proc.
Annu.
Meeting Assoc.
for Com- putational Linguistics (ACL) , pages 1105–1116.
David Nadeau and Satoshi Sekine. 2007.
A survey of named entity recognition and classiﬁcation.
Lingvis- ticae Investigationes , 30(1):3–26.
Nanyun Peng and Mark Dredze. 2015.
Named en- tity recognition for chinese social media with jointly trained embeddings.
In Proc.
Conf.
Empirical Meth- ods Natural Language Process.
(EMNLP) , pages 548–554.
Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. 2017.
Cross-sentence n-ary relation extraction with graph lstms.
Trans.
As- soc.
for Computational Linguistics (TACL) , 5:101– 115.
Jeffrey Pennington, Richard Socher, and Christopher D Manning.
2014.
Glove: Global vectors for word rep-
resentation.
In Proc.
Conf.
Empirical Methods Natu- ral Language Process.
(EMNLP) , volume 14, pages 1532–1543.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.
2018.
Deep contextualized word repre- sentations.
In NAACL .
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang.
2012.
Conll- 2012 shared task: Modeling multilingual unre- stricted coreference in ontonotes.
In Joint Confer- ence on EMNLP and CoNLL-Shared Task , pages 1– 40.
Association for Computational Linguistics.
Victor Sanh, Thomas Wolf, and Sebastian Ruder.
2019.
A hierarchical multi-task approach for learning em- beddings from semantic tasks.
AAAI .
Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping Zheng, and Andrew McCallum.
2013.
Joint infer- ence of entities, relations, and coreference.
In Proc.
of the 2013 workshop on Automated knowledge base construction , pages 1–6.
ACM.
Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea.
2018.
N-ary relation extraction using graph- state lstm.
In Proc.
Conf.
Empirical Methods Natu- ral Language Process.
(EMNLP) , pages 2226–2235.
Bailin Wang and Wei Lu.
2018.
Neural segmental hy- pergraphs for overlapping mention recognition.
In EMNLP .
Kun Xu, Yansong Feng, Songfang Huang, and Dongyan Zhao. 2015.
Semantic relation classiﬁca- tion via convolutional neural networks with simple negative sampling.
In Proc.
Conf.
Empirical Meth- ods Natural Language Process.
(EMNLP) , pages 536–540.
3046Bishan Yang and Tom M Mitchell.
2016.
Joint extrac- tion of events and entities within a document context.
InProceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies , pages 289–299.
Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015.
Embedding entities and relations for learning and inference in knowledge bases.
In Proc.
Int.
Conf.
Learning Representations (ICLR) .
Meishan Zhang, Yue Zhang, and Guohong Fu. 2017.
End-to-end neural relation extraction with global op- timization.
In Proc.
Conf.
Empirical Methods Natu- ral Language Process.
(EMNLP) , pages 1730–1740.
Yuhao Zhang, Peng Qi, and Christopher D Man- ning.
2018.
Graph convolution over pruned depen- dency trees improves relation extraction.
In Proc.
Conf.
Empirical Methods Natural Language Pro- cess.
(EMNLP) .
