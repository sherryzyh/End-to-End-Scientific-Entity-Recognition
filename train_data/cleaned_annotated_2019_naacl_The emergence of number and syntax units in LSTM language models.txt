The O
Emergence O
of O
Number O
and O
Syntax O
Units O
in O
LSTM B-TaskName
Language O
Models O
Yair O
Lakretz O
Cognitive O
Neuroimaging O
Unit O
NeuroSpin O
center O
91191 O
, O
Gif O
- O
sur O
- O
Yvette O
, O
France O
yair.lakretz@gmail.comGerman O
Kruszewski O
Facebook O
AI O
Research O
Paris O
, O
France O

germank@gmail.com O

Theo O
Desbordes O
Facebook O
AI O
Research O
Paris O
, O
France O
tdesbordes@fb.comDieuwke O
Hupkes O
ILLC O
, O
University O
of O
Amsterdam O
Amsterdam O
, O
Netherlands O
d.hupkes@uva.nl O
Stanislas O
Dehaene O
Cognitive O
Neuroimaging O
Unit O
NeuroSpin O
center O
91191 O
, O
Gif O
- O
sur O
- O
Yvette O
, O
France O
stanislas.dehaene@gmail.comMarco O
Baroni O
Facebook O
AI O
Research O
Paris O
, O

France O
mbaroni@fb.com O

Abstract O
Recent O
work O
has O
shown O
that O
LSTMs B-TaskName
trained O
on O
a O
generic O
language O
modeling O
objective O
capture O
syntax O
- O
sensitive O
generalizations O
such O
as O
long- O
distance O
number O
agreement O
. O

We O
have O
however O
no O
mechanistic O
understanding O
of O
how O
they O
ac- O
complish O
this O
remarkable O
feat O
. O

Some O
have O
conjectured O
it O
depends O
on O
heuristics O
that O
do O
not O
truly O
take O
hierarchical O
structure O
into O
account O
. O

We O
present O
here O
a O
detailed O
study O
of O
the O
inner O
mechanics O
of O
number O
tracking O
in O
LSTMs B-TaskName
at O
the O
single O
neuron O
level O
. O

We O
discover O
that O
long- O
distance O
number O
information O
is O
largely O
man- O
aged O
by O
two O
“ O
number O
units O
” O
. O

Importantly O
, O
the O
behaviour O
of O
these O
units O
is O
partially O
controlled O
by O
other O
units O
independently O
shown O
to O
track O
syntactic O
structure O
. O

We O
conclude O
that O
LSTMs B-TaskName
are O
, O
to O
some O
extent O
, O
implementing O
genuinely O
syntactic O
processing O
mechanisms O
, O
paving O
the O
way O
to O
a O
more O
general O
understanding O
of O
gram- O
matical O
encoding O
in O
LSTMs B-TaskName
. O

1 O
Introduction O
In O
the O
last O
years O
, O
recurrent O
neural O
networks O
( O
RNNs B-TaskName
) O
, O
and O
particularly O
long O
- O
short O
- O
term O
- O
memory O
( O
LSTM B-TaskName
) O
architectures O
( O
Hochreiter O
and O
Schmidhu O
- O
ber O
, O
1997 O
) O
, O
have O
been O
successfully O
applied O
to O
a O
variety O
of O
NLP B-TaskName
tasks O
. O

This O
has O
spurred O
interest O
in O
whether O
these O
generic O
sequence O
- O
processing O
de- O
vices O
are O
discovering O
genuine O
structural O
properties O
of O
language O
in O
their O
training O
data O
, O
or O
whether O
their O
success O
can O
be O
explained O
by O
opportunistic O
surface- O
pattern O
- O
based O
heuristics O
. O

Until O
now O
, O
this O
debate O
has O
mostly O
relied O
on O
“ O
behavioural O
” O
evidence O
: O
The O
LSTM B-TaskName
had O
been O
treated O
as O
a O
black O
box O
, O
and O
its O
capacities O
had O
been O
indirectly O
inferred O
by O
its O
performance O
on O
linguistic O
tasks O
. O

In O
this O
study O
, O
we O
took O
a O
com- O
plementary O
approach O
inspired O
by O
neuroscience O
: O
We O
thoroughly O
investigated O
the O
inner O
dynamics O
of O
an O
LSTM B-TaskName
language O
model O
performing O
a O
number O
agreement O
task O
, O
striving O
to O
achieve O
a O
mechanis- O
tic O
understanding O
of O
how O
it O
accomplishes O
it O
. O

We O
found O
that O
the O
LSTM B-TaskName
had O
specialized O
two O
“ O
grand- O
mother O
” O
cells O
( O
Bowers O
, O
2009 O
) O
to O
carry O
number O
fea- O
tures O
from O
the O
subject O
to O
the O
verb O
across O
the O
in- O
tervening O
material.1Interestingly O
, O
the O
LSTM B-TaskName
also O
1In O
the O
neuroscientiﬁc O
literature O
, O
“ O
grandmother O
” O
cells O
are O
( O
sets O
of O
) O
neurons O
coding O
for O
speciﬁc O
information O
, O
e.g. O
, O
about O
your O
grandmother O
, O
in O
a O
non O
- O
distributed O
manner O
. O

12possesses O
a O
more O
distributed O
mechanism O
to O
predict O
number O
when O
subject O
and O
verb O
are O
close O
, O
with O
the O
grandmother O
number O
cells O
only O
playing O
a O
crucial O
role O
in O
more O
difﬁcult O
long O
- O
distance O
cases O
. O

Cru- O
cially O
, O
we O
independently O
identiﬁed O
a O
set O
of O
cells O
tracking O
syntactic O
structure O
, O
and O
found O
that O
one O
of O
them O
encodes O
the O
presence O
of O
an O
embedded O
phrase O
separating O
the O
main O
subject O
- O
verb O
depen- O
dency O
, O
and O
has O
strong O
efferent O
connections O
to O
the O
long O
- O
distance O
number O
cells O
, O
suggesting O
that O
the O
network O
relies O
on O
genuine O
syntactic O
information O
to O
regulate O
agreement O
- O
feature O
percolation O
. O

Our O
analysis O
thus O
provides O
direct O
evidence O
for O
the O
claim O
that O
LSTMs B-TaskName
trained O
on O
unannotated O
cor- O
pus O
data O
, O
despite O
lacking O
signiﬁcant O
linguistic O
pri- O
ors O
, O
learn O
to O
perform O
structure O
- O
dependent O
linguis- O
tic O
operations O
. O

In O
turn O
, O
this O
suggests O
that O
raw O
lin- O
guistic O
input O
and O
generic O
memory O
mechanisms O
, O
such O
as O
those O
implemented O
in O
LSTMs B-TaskName
, O
may O
sufﬁce O
to O
trigger O
the O
induction O
of O
non O
- O
trivial O
grammatical O
rules O
. O

2 O
Related O
Work O
Starting O
with O
the O
seminal O
work O
of O
Linzen O
et O
al O
. O

( O
2016 O
) O

, O
a O
long O
- O
distance O
number O
agreement O
task O
has O
emerged O
as O
a O
standard O
way O
to O
probe O
the O
syn- O
tactic O
capabilities O
of O
neural O
language O
models O
. O

In O
the O
number O
agreement O
task O
, O
a O
model O
is O
asked O
to O
predict O
the O
verb O
in O
a O
sentence O
where O
the O
subject O
and O
main O
verb O
are O
separated O
by O
one O
or O
more O
inter- O
vening O
nouns O
( O
“ O
the O
boynear O
the O
carsgreets O
. O
. O
. O
” O
) O
and O
evaluated O
based O
on O
how O
often O
it O
predicts O
the O
right O
verb O
form O
. O

Following O
mixed O
initial O
results O
by O
Linzen O
and O
colleagues O
and O
Bernardy O
and O
Lappin O
( O
2017 O
) O
, O
Gu- O
lordava O
et O
al O
. O

( O
2018 O
) O
and O
Kuncoro O
et O

al O
. O
( O
2018b O
) O
have O
robustly O
established O
that O
LSTM B-TaskName
language O
models O
achieve O
near O
- O
human O
performance O
on O
the O
agreement O
task O
. O

While O
Gulordava O
and O
colleagues O
provided O
some O
evidence O
that O
the O
LSTMs B-TaskName
are O
re- O
lying O
on O
genuine O
syntactic O
generalizations O
, O
Kun- O
coro O
et O
al O
. O
( O
2018a O
) O
and O
Linzen O
and O
Leonard O
( O
2018 O
) O
suggested O
that O
the O
LSTM B-TaskName
achievements O
can O
, O
at O
least O
in O
part O
, O
be O
accounted O
by O
superﬁcial O
heuristics O
( O
e.g. O
, O
“ O
percolate O
the O
number O
of O
the O
ﬁrst O
noun O
in O
a O
sentence O
” O
) O
. O

Other O
recent O
work O
has O
extended O
syn- O
tax O
probing O
to O
other O
phenomena O
such O
as O
negative O
polarity O
items O
and O
island O
constraints O
( O
Chowdhury O
and O
Zamparelli O
, O
2018 O
; O
Jumelet O
and O
Hupkes O
, O
2018 O
; O
Marvin O
and O
Linzen O
, O
2018 O
; O
Wilcox O
et O
al O
. O
, O
2018 O
) O
. O

While O
Linzen O
et O
al O
. O

( O
2016 O
) O
presented O
intrigu O
- O
ing O
qualitative O
data O
showing O
cells O
that O
track O
gram- O
matical O
number O
in O
a O
network O
directly O
trained O
on O
the O
agreement O
task O
, O
most O
of O
the O
following O
work O
focused O
on O
testing O
the O
network O
output O
behaviour O
, O
rather O
than O
on O
understanding O
how O
the O
latter O
fol- O
lows O
from O
the O
inner O
representations O
of O
the O
net- O
work O
. O

Another O
research O
line O
studied O
linguistic O
processing O
in O
neural O
networks O
through O
‘ O
diagnos- O
tic O
classiﬁers O
’ O
, O
that O
is O
, O
classiﬁers O
trained O
to O
predict O
a O
certain O
property O
from O
network O
activations O
( O
e.g. O
, O
Gelderloos O
and O
Chrupała O
, O
2016 O
; O

Adi O
et O
al O
. O
, O
2017 O
; O
Alain O
and O
Bengio O
, O
2017 O
; O
Hupkes O
et O
al O
. O
, O
2018 O
) O
. O

This O
approach O
may O
give O
insight O
into O
which O
infor- O
mation O
is O
encoded O
by O
the O
network O
in O
different O
lay- O
ers O
or O
at O
different O
time O
points O
, O
but O
it O
only O
provides O
indirect O
evidence O
about O
the O
speciﬁc O
mechanics O
of O
linguistic O
processing O
in O
the O
network O
. O

Other O
studies O
are O
closer O
to O
our O
approach O
in O
that O
they O
attempt O
to O
attribute O
function O
to O
spe- O
ciﬁc O
network O
cells O
, O
often O
by O
means O
of O
visual- O
ization O
( O
Karpathy O
et O
al O
. O
, O
2016 O
; O
Li O
et O

al O
. O
, O
2016 O
; O
Tang O
et O
al O
. O
, O
2017 O
) O
. O

Radford O
et O

al O
. O
( O
2017 O
) O
, O
for O
example O
, O
detected O
a O
“ O
sentiment O
” O
grandmother O
cell O
in O
a O
language O
- O
model O
- O
trained O
network O
. O

Ke- O
mentchedjhieva O
and O
Lopez O
( O
2018 O
) O
recently O
found O
a O
character O
- O
level O
RNN B-TaskName
to O
track O
morpheme O
bound- O
aries O
in O
a O
single O
cell O
. O

We O
are O
however O
not O
aware O
of O
others O
studies O
systematically O
characterizing O
the O
processing O
of O
a O
linguistic O
phenomenon O
at O
the O
level O
of O
RNN B-TaskName
cell O
dynamics O
, O
as O
is O
the O
attempt O
in O
the O
study O
hereby O
presented O
. O

3 O
Setup O
Language O
Model O
We O
study O
the O
pretrained O
LSTM B-TaskName
language O
model O
made O
available O
by O
Gu- O
lordava O
et O

al O
. O
( O
2018 O
) O
. O

This O
model O
is O
composed O
of O
a O
650 B-HyperparameterValue
- I-HyperparameterValue
dimensional I-HyperparameterValue
embedding B-HyperparameterName
layer I-HyperparameterName
, O
two O
650 B-HyperparameterValue
- I-HyperparameterValue
dimensional I-HyperparameterValue
hidden B-HyperparameterName
layers I-HyperparameterName
, O
and O
an O
output O
layer O
with O
vocabulary B-HyperparameterName
size I-HyperparameterName
50,000 B-HyperparameterValue
. O

The O
model O
was O
trained O
on O
Wikipedia B-DatasetName
data I-DatasetName
, O
without O
ﬁne O
- O
tuning O
for O
number O
agreement O
, O
and O
obtained O
perplexity O
close O
to O
state O
of O
the O
art O
in O
the O
experiments O
of O
Gulordava O
et O
al.2 O
Number O
- O
Agreement O
Tasks O
We O
complement O
analysis O
of O
the O
naturalistic O
, O
corpus B-DatasetName
- O
derived O
number O
- O
agreement O
test O
set O
of O
Linzen O

et O

al O
. O
( O
2016 O
) O
, O
in O
the O
version O
made O
available O
by O
Gulordava O
et O
al O
. O
( O
2018 O
) O
, O
with O
synthetically O
generated O
data O
- O
sets O
. O

2Key O
ﬁndings O
reported O
below O
were O
also O
replicated O
with O
the O
same O
model O
trained O
with O
different O
initialization O
seeds O
and O
variations O
with O
different O
hyper O
- O
parameters O
. O

13Simple O
theboy O
greets O
the O
guy O
Adv O
theboyprobably O
greets O
the O
guy O
2Adv O
theboymost O
probably O
greets O
the O
guy O
CoAdv B-TaskName
theboyopenly O
and O
deliberately O
greets O
the O
guy O
NamePP O
theboynear O
Pat O
greets O
the O
guy O
NounPP O
theboynear O
the O
car O
greets O
the O
guy O
NounPPAdv O
theboynear O
the O
car O
kindly O
greets O
the O
guy O
Table O
1 O
: O
NA B-TaskName
tasks O
illustrated O
by O
representative O
singular O
sentences O
. O

Each O
synthetic O
number O
- O
agreement O
task O
( O
NA B-TaskName
- O
task O
) O
instantiates O
a O
ﬁxed O
syntactic O
structure O
with O
varied O
lexical O
material O
, O
in O
order O
to O
probe O
subject O
- O
verb O
number O
agreement O
in O
controlled O
and O
increasingly O
challenging O
setups.3The O
different O
structures O
are O
illustrated O
in O
Table O
1 O
, O
where O
all O
forms O
are O
in O
the O
singular O
. O

Distinct O
sentences O
were O
randomly O
generated O
by O
selecting O
words O
from O
pools O
of O
20 O
subject O
/ O
object O
nouns O
, O
15 O
verbs O
, O
10 O
adverbs O
, O
5 O
prepositions O
, O
10 O
proper O
nouns O
and O
10 O
location O
nouns O
. O

The O
items O
were O
selected O
so O
that O
their O
combination O
would O
not O
lead O
to O
semantic O
anoma- O
lies O
. O

For O
each O
NA B-TaskName
- O
task O
, O
we O
generated O
singular O
and O
plural O
versions O
of O
each O
sentence O
. O

We O
refer O
to O
each O
such O
version O
as O
a O
condition O
. O

For O
NA- O
tasks O
that O
have O
other O
nouns O
occurring O
between O
subject O
and O
main O
verb O
, O
we O
also O
systematically O
vary O
their O
number O
, O
resulting O
in O
two O
congruent O
and O
two O
incongruent O
conditions O
. O

For O
example O
, O
the O
NounPP O
sentence O
in O
the O
table O
illustrates O
the O
congruent O
SS O
( O
singular O
- O
singular O
) O
condition O
and O
the O
corresponding O
sentence O
in O
the O
incongruent O
PS O
( O
plural O
- O
singular O
) O
condition O
is O
: O
“ O
the O
boys O
near O
thecar O
greet O
the O
guy O
” O
. O

For O
all O
NA B-TaskName
- O
tasks O
, O
each O
condition O
consisted O
of O
600 O
sentences O
Syntactic O
Depth O
Data O
- O
Set O
We O
probed O
the O
im- O
plicit O
syntax O
- O
parsing O
abilities O
of O
the O
model O
by O
test- O

ing O
whether O
its O
representations O
predict O
the O
syn- O
tactic O
depth O
of O
the O
words O
they O
process O
. O

Follow- O
ing O
Nelson O
et O
al O
. O

( O
2017 O
) O
, O
this O
was O
operational- O
ized O
as O
predicting O
the O
number O
of O
open O
syntactic O
nodes O
at O
each O
word O
, O
given O
the O
canonical O
syntac- O
tic O
parse O
of O
a O
sentence O
. O

We O
generated O
a O
data O
- O
set O
of O
sentences O
with O
unambiguous O
but O
varied O
syntac- O
tic O
structures O
and O
annotated O
them O
with O
the O
number O
of O
open O
nodes O
at O
each O
word O
. O

For O
example O
: O
“ O
Ten O
1 O
really O
2ecstatic O
3cousins O
3of4four O
5teachers O
6are2 O
quickly O
3laughing O
4 O
” O
, O
where O
indexes O
show O
the O
cor- O
3We O
exclude O
, O
for O
the O
time O
being O
, O
agreement O
across O
a O
rel- O
ative O
clause O
, O
as O
it O
comes O
with O
the O
further O
complication O
of O
ac- O
counting O
for O
the O
extra O
agreement O
process O
taking O
place O
inside O
the O
relative O
clause.responding O
number O
of O
open O
nodes O
. O

Since O
syntactic O
depth O
is O
naturally O
correlated O
with O
the O
position O
of O
a O
word O
in O
a O
sentence O
, O
we O
used O
a O
data O
- O
point O
sampling O
strategy O
to O
de O
- O
correlate O
these O
factors O
. O

For O
each O
length O
between O
2 O
and O
25 O
words O
, O
we O
randomly O
gen- O
erated O
300 O
sentences O
. O

From O
this O
set O
, O
we O
randomly O
picked O
examples O
uniformly O
covering O
all O
possible O
position O
- O
depth O
combinations O
within O
the O
7 O
- O
12 O
posi- O
tion O
and O
3 O
- O
8 O
depth O
ranges O
. O

The O
ﬁnal O
data O
- O
set O
con- O
tains O
4,033 O
positions O
from O
1,303 O
sentences.4 O
4 O
Experiments O
To O
successfully O
perform O
the O
NA B-TaskName
- O
task O
, O
the O
LSTM B-TaskName
should O
: O
( O
1 O
) O
encode O
and O
store O
the O
grammatical O
number O
of O
the O
subject O
; O
and O
( O
2 O
) O
track O
the O
main O
subject O
- O
verb O
syntactic O
dependency O
. O

The O
latter O
in- O
formation O
is O
important O
for O
identifying O
the O
time O
period O
during O
which O
subject O
number O
should O
be O
stored O
, O
output O
and O
then O
updated O
by O
the O
network O
. O

This O
section O
describes O
the O
‘ O
neural O
circuit O
’ O
that O
en- O
codes O
and O
processes O
this O
information O
in O
the O
LSTM B-TaskName
. O

4.1 O
Long O
- O
Range O
Number O
Units O
We O
ﬁrst O
tested O
the O
performance O
of O
the O
LSTM B-TaskName
on O
the O
Linzen O
’s O
data O
and O
on O
the O
NA B-TaskName
- O
tasks O
in O
Table O
1 O
. O

Following O
Linzen O
et O
al O
. O

( O
2016 O
) O
and O
later O
work O
, O
we O
computed O
the O
likelihood O
that O
the O
LSTM B-TaskName
assigns O
to O
the O
main O
verb O
of O
each O
sentence O
given O
the O
preced- O
ing O
context O
and O
compared O
it O
to O
the O
likelihood O
it O
as- O
signs O
to O
the O
wrong O
verb O
inﬂection O
. O

Accuracy O
in O
a O
given O
condition O
was O
measured O
as O
the O
proportion O
of O
sentences O
in O
this O
condition O
for O
which O
the O
model O
as- O
signed O
a O
higher O
likelihood O
to O
the O
correct O
verb O
form O
than O
to O
the O
wrong O
one O
. O

Network O
performance O
is O
reported O
in O
Table O
2 O
( O
right O
column O
– O
‘ O
Full O
’ O
) O
. O

We O
ﬁrst O
note O
that O
our O
results O
on O
the O
Linzen O
NA B-TaskName
- O
task O
conﬁrm O
those O
re- O
ported O
in O
Gulordava O
et O
al O
. O
( O
2018 O
) O
. O

For O
the O
other O
NA B-TaskName
- O
tasks O
, O
results O
show O
that O
some O
tasks O
and O
condi- O
tions O
are O
more O
difﬁcult O
than O
others O
. O

For O
example O
, O
performance O
on O
the O
Simple O
( O
0 O
- O
distance O
) O

NA B-TaskName
- O
task O
is O
better O
than O
that O
on O
the O
Co O
- O
Adv O
NA B-TaskName
- O
task O
, O
which O
in O
turn O
is O
better O
than O
that O
of O
the O
nounPP B-TaskName
tasks O
. O

Second O
, O
as O
expected O
, O
incongruent O
conditions O
( O
the O
number O
- O
mismatch O
conditions O
of O
namePP B-TaskName
, O
nounPP B-TaskName
and O
nounPPAdv B-TaskName
) O
reduce O
network O
performance O
. O

4All O
our O
data O
- O
sets O
are O
available O
at O
: O
https O
: O
//github.com O
/ O
FAIRNS O
/ O
Number_and_syntax O
_ O
units_in_LSTM_LMs O
. O

14NA B-TaskName
task O
CAblatedFull776 O
988 O
Simple O
S O
- O
- O
100 O
Adv O
S O
- O
- O
100 O
2Adv O
S O
- O
- O
99.9 O
CoAdv B-TaskName
S O
- O
82 O
98.7 O
namePP B-TaskName
SS O
- O
- O
99.3 O
nounPP B-TaskName
SS O
- O
- O
99.2 O
nounPP B-TaskName
SP O
- O
54.2 O
87.2 O
nounPPAdv B-TaskName
SS O
- O
- O
99.5 O
nounPPAdv B-TaskName
SP O
- O
54.0 O
91.2 O
Simple O
P O
- O
- O
100 O
Adv O
P O
- O
- O
99.6 O
2Adv O
P O
- O
- O
99.3 O
CoAdv B-TaskName
P O
79.2 O
- O
99.3 O
namePP B-TaskName
PS O
39.9 O
- O
68.9 O
nounPP B-TaskName
PS O
48.0 O
- O
92.0 O
nounPP B-TaskName
PP O
78.3 O
- O
99.0 O
nounPPAdv B-TaskName
PS O
63.7 O
- O
99.2 O
nounPPAdv B-TaskName
PP O
- O
- O
99.8 O
Linzen O
- O
75.3 O
- O
93.9 O
Table O
2 O
: O
Ablation O
- O
experiments O
results O
: O

Percentage O
ac- O
curacy O
in O
all O
NA B-TaskName
- O
tasks O
. O

Full O
: O
non O
- O
ablated O
model O
, O
C O
: O
condition O
, O
S O
: O
singular O
, O
P O
: O
plural O
. O

Pink O
( O
dark O
lines O
in O
B&W O
printing O
): O
plural O
subject O
, O
Light O
blue O
: O
singular O
subject O
. O

Performance O
reduction O
less O
than O
10 O
% O
is O
de- O
noted O
by O
‘ O
- O
’ O
. O

Third O
, O
for O
long O
- O
range O
dependencies O
, O
reliably O
en- O
coding O
singular O
subject O
across O
an O
interfering O
noun O
is O
more O
difﬁcult O
than O
a O
plural O
subject O
: O
for O
both O
nounPP B-TaskName
and O
nounPPAdv B-TaskName
, O
PS O
is O
easier O
than O
SP O
. O

A O
possible O
explanation O
for O
this O
ﬁnding O
is O
that O
in O
En- O
glish O
the O
plural O
form O
is O
almost O
always O
more O
fre- O
quent O
than O
the O
singular O
one O
, O
as O
the O
latter O
only O
marks O
third O
person O
singular O
, O
whereas O
the O
former O
is O
identical O
to O
the O
inﬁnitive O
and O
other O
forms O
. O

Thus O
, O
if O
the O
network O
reverts O
to O
unigram O
probabilities O
, O
it O
will O
tend O
to O
prefer O
the O
plural O
. O

Looking O
for O
Number O
Units O
Through O
Ablation O
Number O
information O
may O
be O
stored O
in O
the O
network O
in O
either O
a O
local O
, O
sparse O
, O
or O
a O
distributed O
way O
, O
de- O
pending O
on O
the O
fraction O
of O
active O
units O
that O
carry O
it O
. O

We O
hypothesized O
that O
if O
the O
network O
uses O
a O
local O
or O
sparse O
coding O
, O
meaning O
that O
there O
’s O
a O
small O
set O
of O
units O
that O
encode O
number O
information O
, O

then O
ablat- O
ing O
these O
units O
would O
lead O
to O
a O
drastic O
decrease O
in O
performance O
in O
the O
NA B-TaskName
- O
tasks O
. O

To O
test O
this O
, O
we O
ab- O
lated O
each O
unit O
of O
the O
network O
, O
one O
at O
a O
time O
, O
by O
ﬁxing O
its O
activation O
to O
zero O
, O
and O
tested O
on O
the O
NA B-TaskName
- O
tasks O
. O

Two O
units O
were O
found O
to O
have O
exceptional O
ef- O
fect O
on O
network O
performance O
( O
Table O
2 O
, O
776 O
and O
988 O
columns).5Ablating O
them O
reduced O
network O
performance O
by O
more O
than O
10 B-MetricValue
% I-MetricValue
across O
various O
conditions O
, O
and O
, O
importantly O
, O
they O
were O
the O
only O
units O
whose O
ablation O
consistently O
brought O
network O
performance O
to O
around O
chance O
level O
in O
the O
more O
difﬁcult O
incongruent O
conditions O
of O
the O
namePP B-TaskName
, O
nounPP B-TaskName
and O
nounPPAdv B-TaskName
tasks O
. O

Moreover O
, O
the O
ablation O
effect O
depended O
on O
the O
grammatical O
number O
of O
the O
subject O
: O
ablating O
776 O
signiﬁcantly O
reduced O
network O
performance O
only O
if O
the O
subject O
was O
plural O
( O
P O
, O
PS O
or O
PP O
condi- O
tions O
) O
and O
988 O
only O
if O
the O
subject O
was O
singular O
( O
S O
, O
SP O
or O
SS O
conditions O
) O
. O

In O
what O
follows O
, O
we O
will O
therefore O
refer O
to O
these O
units O
as O
the O
‘ O
plural O
’ O
and O
‘ O
singular O
’ O
units O
, O
respectively O
, O
or O
long O
- O
range O
( O
LR O
) O
number O
units O
when O
referring O
to O
both O
. O

Finally O
, O
we O
note O
that O
although O
the O
Linzen O
NA B-TaskName
- O
task O
contained O
mixed O
stimuli O
from O
many O
types O
of O
conditions O
, O
the O
plural O
unit O
was O
found O
to O
have O
a O
substantial O
effect O
on O
average O
on O
network O
performance O
. O

The O
singu- O
lar O
unit O
did O
n’t O
show O
a O
similar O
effect O
in O
this O
case O
, O
which O
highlights O
the O
importance O
of O
using O
carefully O
crafted O
stimuli O
, O
as O
in O
the O
nounPP B-TaskName
and O
nounPPAdv B-TaskName
tasks O
, O
for O
understanding O
network O
dynamics O
. O

Taken O
together O
, O
these O
results O
suggest O
a O
highly O
local O
cod- O
ing O
scheme O
of O
grammatical O
number O
when O
process- O
ing O
long O
- O
range O
dependencies O
. O

Visualizing O
Gate O
and O
Cell O
- O
State O
Dynamics O
To O
understand O
the O
functioning O
of O
the O
number O
units O
, O
we O
now O
look O
into O
their O
gate O
and O
state O
dynam- O
ics O
during O
sentence O
processing O
. O

We O
focus O
on O
the O
nounPP B-TaskName
NA B-TaskName
- O
task O
, O
which O
is O
the O
simplest O
NA B-TaskName
- O
task O
that O
includes O
a O
long O
- O
range O
dependency O
with O
an O
in- O
terfering O
noun O
, O
in O
both O
SP O
and O
PS O
conditions O
. O

Recall O
the O
standard O
LSTM B-TaskName
memory O
update O
and O
output O
rules O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
): O

Ct O
= O
ftCt 1+iteCt O
( O
1 O
) O
ht O
= O
ottanh O
( O
Ct O
) O
; O
( O
2 O
) O
where O
ft O
; O
it O
; O
ot2(0;1)are O
gating O
scalars O
com- O
puted O
by O
the O
network O
, O
and O
eCt2( 1;1)is O
an O
up- O
date O
candidate O
for O
cell O
value O
. O

Consider O
now O
how O
a O
number O
unit O
may O
reliably O
encode O
and O
store O
subject O
number O
across O
interfer- O
ing O
nouns O
. O

Figure O
1c O
exempliﬁes O
this O
for O
a O
singular O
5Units O
1 O
- O
650 O
belong O
to O
the O
ﬁrst O
layer O
, O
651 O
- O
1300 O
to O
the O
sec- O
ond O
. O

All O
units O
detected O
by O
our O
analyses O
come O
from O
the O
latter O
. O

15 O
−1.51.5 O
˜Ct O
01 O
it O
01 O
ft O
−1.51.5 O
Ct O
The O
boy(s O
) O
near O
the O
car(s O
) O
greet(s O
) O
the01 O
ot(a O
) O
988 O
( O
singular O
) O
−1.51.5 O
˜Ct O
01 O
it O
01 O

ft O
−1.51.5 O
Ct O
The O
boy(s O
) O
near O
the O
car(s O
) O
greet(s O
) O

the01 O
ot O
( O
b O
) O
776 O
( O
plural O
) O
−1.51.5˜Ct O
01it O
01 O
ft O
−1.51.5Ct O

The O
boy(s O
) O
near O
the O
car(s O
) O
greet(s O
) O
the01ot O
( O
c O
) O
Prediction O
( O
singular O
) O
( O
d O
) O
Efferent O
weights O
of O
the O
LR O
- O
units O
( O
776 O
and O
988 O
) O
, O
the O
syntax O
unit O
( O
1150 O
; O
section O
4.3 O
) O
and O
two O
arbitrary O
units O
( O
651 O
and O
1300 O
) O
. O

Figure O
1 O
: O
( O
a O
) O
to O
( O
c O
) O
– O
Cell O
and O
gate O
activations O
during O
processing O
of O
sentences O
with O
a O
prepositional O
phrase O
between O
subject O
and O
verb O
. O

Values O
in O
( O
a O
) O
and O
( O
b O
) O
are O
averaged O
across O
all O
condition O
sentences O
, O
with O
error O
bars O
showing O
standard O
deviations O
. O

( O
d O
) O
– O
Efferent O
weights O
of O
speciﬁc O
units O
at O
the O
output O
layer O
to O
singular O
and O
plural O
verb O
forms O
. O

unit O
, O
showing O
the O
desired O
gate O
and O
cell O
dynamics O
. O

The O
four O
conditions O
are O
represented O
with O
separated O
curves O
- O
pink O
for O
plural O
subject O
, O
light O
blue O
for O
sin- O
gular O
, O
and O
dashed O
lines O
for O
incongruent O
conditions O
. O

Gate O
and O
cell O
activity O
at O
time O
points O
unrelated O
to O
solving O
the O
NA B-TaskName
- O
task O
are O
masked O
with O
white O
, O
as O
we O
do O
not O
make O
precise O
predictions O
for O
them O
. O

The O
update O
rule O
of O
the O
LSTM B-TaskName
cell O
has O
two O
terms O
( O
Eq O
. O
1).6In O
the O
ﬁrst O
, O
ftCt 1 O
, O
the O
forget O
gate O
controls O
whether O
to O
keep O
the O
previous O
cell O
content O
( O
ft= O
1 O
: O
perfect O
remembering O
) O
or O
forget O
it O
( O
ft= O
0 O
: O
complete O
forgetting O
) O
. O

In O
the O
second O
, O
it~Ct O
, O
the O
6We O
abuse O
notation O
here O
, O
using O
the O
symbols O
denoting O
whole O
layers O
in O
equations O
( O
1 O
) O
and O
( O
2 O
) O
to O
denote O
the O
compo- O
nents O
of O
single O
cells.input O
gate O
controls O
whether O
the O
information O
cur- O
rently O
presented O
to O
the O
network O
, O
as O
encoded O
by O
~Ct O
, O
should O
be O
written O
onto O
the O
cell O
( O
it= O
1 O
: O
full O
ac- O
cess O
) O
or O
not O
( O
it= O
0 O
) O
. O

The O
singular O
unit O
can O
thus O
use O
these O
gates O
to O
reliably O
store O
number O
informa- O
tion O
across O
long O
- O
range O
dependencies O
. O

Speciﬁcally O
, O
the O
unit O
can O
( O
enumeration O
follows O
the O
same O
or- O
der O
as O
the O
panels O
in O
Figure O
1c O
): O
( O
1 O
) O
encode O
sub- O
ject O
number O
via O
~Ctsubject O
with O
different O
values O
for O
singular O
and O
plural O
; O
( O
2 O
) O
open O
the O
input O
gate O
only O
when O
a O
singular O
subject O
is O
presented O
( O
itsubject O
= O
1 O
in O
light O
- O
blue O
curves O
only O
) O
and O
protect O
it O
from O
in- O
terfering O
nouns O
( O
it= O
0 O
; O
tsubject O
< O
t O
< O
t O
verb O
) O
; O
( O
3 O
) O
at O
the O
same O
time O
, O
clear O
the O
cell O
from O
previ- O
ously O
stored O
information O
( O
ftsubject O
= O
0 O
) O
and O
then O

16store O
subject O
number O
across O
the O
entire O
dependency O
( O
ft= O
1 O
; O
tsubject O
< O
t O
< O
t O
verb O
) O
; O
( O
4 O
) O
this O
will O
result O
in O
stable O
encoding O
of O
subject O
number O
in O
the O
cell O
Ct O
throughout O
the O
dependency O
; O
( O
5 O
) O
ﬁnally O
, O
output O
sub- O
ject O
number O
at O
the O
right O
moment O
, O
when O
predicting O
the O
verb O
form O
( O
otverb 1= O
1 O
) O
( O
Eq O
. O
2 O
) O
. O

Figures O
1a O
and O
1b O
present O
the O
actual O
gate O
and O
cell O
dynamics O
of O
the O
singular O
and O
plural O
units O
. O

Both O
units O
follow O
the O
general O
solution O
for O
reliable O
number O
storage O
described O
above O
. O

Note O
that O
for O
~Ctandit O
, O
and O
as O
a O
result O
also O
for O
Ct O
, O
the O
plural O
unit O
‘ O
mirrors O
’ O
the O
singular O
unit O
with O
respect O
to O
sub- O
ject O
number O
( O
pink O
curves O
of O
PP O
and O
PS O
vs. O
Light O
blue O
curves O
of O
SS O
and O
SP O
) O
. O

This O
is O
in O
accordance O
with O
the O
results O
of O
the O
ablation O
experiments O
, O
which O
showed O
that O
ablating O
these O
units O
had O
an O
effect O
that O
depended O
on O
the O
grammatical O
number O
of O
the O
sub- O
ject O
( O
Table O
2 O
) O
. O

This O
provides O
complementary O
sup- O
port O
for O
the O
identiﬁcation O
of O
these O
units O
as O
‘ O
singu- O
lar O
’ O
and O
‘ O
plural O
’ O
. O

A O
single O
divergence O
between O
the O
solution O
de- O
picted O
in O
Figure O
1c O
and O
the O
actual O
dynamics O
of O
the O
number O
units O
is O
that O
input O
gate O
activity O
is O
smaller O
, O
but O
not O
zero O
, O
at O
the O
time O
step O
immediately O
fol- O
lowing O
the O
subject O
. O

One O
speculative O
explanation O
is O
that O
this O
might O
be O
useful O
to O
process O
compound O
nouns O
. O

In O
these O
cases O
, O
subject O
number O
information O
is O
stored O
with O
the O
second O
noun O
, O
whereas O
in O
the O
case O
of O
simple O
nouns O
there O
is O
no O
‘ O
risk O
’ O
of O
encountering O
an O
interfering O
noun O
immediately O
after O
the O
subject O
, O
making O
the O
delay O
in O
closing O
the O
gate O
safe O
. O

The O
singular O
and O
plural O
units O
had O
emerged O
at O
the O
second O
layer O
of O
the O
network O
. O

This O
seems O
appropri- O
ate O
since O
number O
information O
needs O
to O
be O
directly O
projected O
to O
the O
output O
layer O
for O
correct O
verb O
- O
form O
prediction O
. O

Moreover O
, O
number O
- O
unit O
output O
should O
be O
projected O
differently O
to O
singular O
and O
plural O
verb O
forms O
in O
the O
output O
layer O
, O
only O
increasing O
activ- O
ity O
in O
output O
units O
representing O
the O
suitable O
form O
. O

For O
example O
, O
for O
the O
singular O
unit O
, O
since O
singu- O
lar O
subjects O
are O
encoded O
with O
a O
negative O
value O
( O
Ctverb 1< 1 O
in O
ﬁgure O
1a O
) O
, O
the O
more O
negative O
its O
efferent O
weights O
to O
singular O
verb O
forms O
in O
the O
output O
layer O
, O
the O
higher O
the O
probabilities O
of O
these O
verb O
forms O
would O
be O
. O

Figure O
1d O
shows O
the O
effer- O
ent O
weights O
of O
the O
LR O
- O
number O
units O
to O
all O
verbs O
in O
our O
data O
- O
sets O
. O

We O
found O
that O
, O
indeed O
, O
the O
effer- O
ent O
weights O
to O
the O
singular O
and O
plural O
verb O
forms O
are O
segregated O
from O
each O
other O
, O
with O
weight O
signs O
that O
correspond O
to O
the O
negative O
encoding O
of O
sub- O
ject O
number O
used O
by O
both O
singular O
and O
plural O
units O
. O

Figure O
2 O
: O
Generalization O
across O
time O
of O
subject O
- O
number O
prediction O
. O

Error O
bars O
represent O
standard O
deviations O
across O
cross O
- O
validation O
splits O
. O

Two O
other O
arbitrary O
units O
, O
651 O
and O
1300 O
, O
and O
the O
syntax O
unit O
1150 O
to O
be O
described O
below O
( O
Section O
4.3 O
) O
do O
not O
have O
segregated O
efferent O
weights O
to O
verb O
forms O
, O
as O
expected O
. O

4.2 O
Short O
- O
Range O
Number O
Information O
Performance O
on O
the O
easier O
NA B-TaskName
- O
tasks O
( O
Simple O
, O
Adv O
, O
2Adv O
) O
was O
not O
impaired O
by O
single O
- O
unit O
ablations O
. O

This O
suggests O
that O
number O
may O
be O
encoded O
also O
elsewhere O
in O
the O
network O
, O
perhaps O
via O
a O
more O
dis- O
tributed O
code O
. O

To O
verify O
this O
, O
we O
tested O
whether O
subject O
number O
can O
be O
decoded O
from O
the O
whole O
pattern O
of O
activities O
in O
the O
network O
( O
excluding O
the O
two O
LR O
- O
number O
units O
) O
and O
whether O
this O
decoding O
is O
stable O
across O
time O
( O
see O
Giulianelli O
et O
al O
. O
, O
2018 O
, O
for O
similar O
observations O
and O
related O
methods O
) O
. O

We O
expected O
this O
distributed O
activity O
to O
track O
number O
in O
a O
small O
time O
window O
after O
the O
subject O
, O
but O
, O
un- O
like O
the O
LR O
- O
number O
units O
, O
to O
be O
affected O
by O
incon- O
gruent O
intervening O
nouns O
. O

We O
trained O
a O
linear O
model O
to O
predict O
the O
gram- O
matical O
number O
of O
the O
subject O
from O
network O
activ- O
ity O
in O
response O
to O
the O
presentation O
of O
the O
subject O
, O
and O
tested O
its O
prediction O
on O
test O
sets O
from O
all O
time O
points O
( O
King O
and O
Dehaene O
, O
2014 O
) O
, O
in O
incongruent O
conditions O
only O
of O
the O
nounPP B-TaskName
task O
. O

We O
used O
Area B-MetricName
under I-MetricName
of I-MetricName
Curve I-MetricName
( O
AUC B-MetricName
) O
to O
evaluate O
model O
perfor- O
mance O
. O

Figure O
2 O
shows O
decoding O
across O
time O
of O
subject O
number O
from O
cell O
activity O
of O
each O
number O
unit O
separately O
and O
from O
cell O
activity O
of O
the O
entire O
network O
without O
these O
two O
units O
( O
‘ O
Full O
model O
mi- O
nus O
LR O
- O
units O
’ O
) O
. O

Results O
show O
that O
number O
infor- O
mation O
can O
be O
efﬁciently O
decoded O
from O
other O
units O
in O
the O
network O
, O
and O
that O
this O
information O
can O
be O
carried O
for O
several O
time O
steps O
( O
relatively O
high O
AUC B-MetricName
up O
to O
the O
second O
determiner O
) O
. O

However O
, O
the O
way O
in O
which O
these O
units O
encode O
number O
is O
sensitive O
to O
the O
last O
encountered O
noun O
, O
with O
AUC B-MetricName
decreasing O

17 O
( O
a O
) O
2Adv O
  O
( O
b O
) O
nounPP B-TaskName
  O
( O
c O
) O
subject O
relative O
( O
d O
) O
Two O
embeddings O
with O
subject O
relatives O
Figure O
3 O
: O
Cell O
activity O
of O
syntax O
unit O
1150 O
while O
processing O
various O
syntactic O
structures O
. O

Values O
averaged O
across O
all O
stimuli O
in O
an O
NA B-TaskName
- O
task O
, O
with O
error O
bars O
representing O
standard O
deviations O
. O

Relative O
clause O
NA B-TaskName
- O
task O
stimuli O
were O
speciﬁcally O
generated O
for O
this O
visualization O
. O

to O
zero O
around O
the O
second O
noun O
( O
‘ O
cars O
’ O
) O
, O
whereas O
test O
performance O
of O
the O
models O
trained O
on O
cell O
ac- O
tivity O
of O
the O
LR O
- O
number O
units O
is O
consistently O
high O
. O

This O
conﬁrms O
that O
number O
prediction O
is O
supported O
both O
by O
the O
LR O
- O
number O
units O
, O
and O
by O
distributed O
activation O
patterns O
of O
other O
short O
- O
range O
( O
SR O
) O
num- O
ber O
units O
. O

The O
latter O
, O
however O
, O
are O
not O
syntax- O
sensitive O
, O
and O
simply O
encode O
the O
number O
of O
the O
last O
noun O
encountered O
. O

A O
full O
description O
of O
the O
SR O
- O
number O
units O
is O
be- O
yond O
our O
scope O
. O

However O
, O
we O
note O
that O
10 O
SR- O
number O
units O
in O
the O
second O
layer O
of O
the O
network O
were O
identiﬁed O
, O
which O
had O
efferent O
weights O
with O
a O
similar O
segregated O
structure O
as O
that O
of O
the O
LR O
units O
( O
Figure O
1d O
) O
. O

These O
units O
were O
indeed O
sen- O
sitive O
to O
the O
last O
encountered O
noun O
: O
subject O
num- O
ber O
could O
be O
decoded O
from O
single O
- O
unit O
cell O
activ- O
ity O
during O
its O
presentation O
( O
AUC B-MetricName
> O
0:9 O
) O
, O
but O
ac- O
tivity O
‘ O
swaps O
’ O
once O
an O
interfering O
noun O
appears O
( O
i.e. O
, O
AUC B-MetricName
decreases O
to O
zero O
in O
a O
generalization- O
across O
- O
time O
analysis O
) O
. O

Finally O
, O
to O
validate O
the O
role O
of O
SR O
- O
number O
units O
in O
encoding O
number O
for O
eas- O
ier O
NA B-TaskName
- O
tasks O
, O
we O
ablated O
both O
SR O
and O
LR O
number O
units O
( O
12 O
in O
total O
) O
or O
SR O
units O
only O
( O
10 O
in O
total O
) O
and O
evaluated O
network O
performance O
on O
these O
NA- O
tasks O
. O

Both O
experiments O
resulted O
in O
a O
signiﬁcant O
reduction O
in O
task O
performance O
compared O
to O
1,000 O
random O
equi O
- O
size O
ablations O
( O
p O
< O
0:01 O
in O
all O
‘ O
eas- O
ier O
’ O
tasks O
) O
. O

Intriguingly O
, O
we O
observed O
qualitatively O
that O
LR O
units O
are O
almost O
always O
making O
the O
right O
predic- O
tion O
, O
even O
when O
the O
network O
predicts O
the O
wrong O
number O
. O

The O
wrong O
outcome O
, O
in O
such O
cases O
, O
might O
be O
due O
to O
interference O
from O
the O
syntax O
- O
insensitive O
SR O
units O
. O

We O
leave O
the O
study O
of O
LR O
- O
SR O
unit O
inter- O
play O
to O
future O
work.4.3 O
Syntax O
Units O
We O
saw O
how O
the O
input O
and O
forget O
gates O
of O
the O
LR- O
number O
units O
control O
the O
ﬂow O
of O
subject O
- O
number O
information O
. O

It O
remains O
unclear O
, O
however O
, O
how O
the O
dynamics O
of O
these O
gates O
are O
controlled O
by O
the O
net- O
work O
. O

We O
hypothesized O
that O
other O
units O
in O
the O
net- O
work O
may O
encode O
information O
about O
the O
syntac- O
tic O
structure O
of O
the O
sentence O
, O
and O
thus O
about O
the O
subject O
- O
verb O
dependency O
. O

These O
units O
could O
then O
control O
and O
coordinate O
the O
opening O
and O
closing O
of O
the O
input O
and O
forget O
gates O
of O
the O
number O
units O
. O

To O
identify O
such O
’ O
syntax O
’ O
units O
, O
we O
tested O
from O
which O
units O
syntactic O
information O
can O
be O
efﬁ- O
ciently O
decoded O
. O

We O
used O
depth O
of O
the O
syntac- O
tic O
tree O
as O
a O
proxy O
for O
syntactic O
structure O
( O
Nel- O
son O
et O
al O
. O
, O
2017 O
) O
and O
trained O
an O
L2 O
- O
regularized O
regression O
model O
to O
predict O
syntactic O
tree O
- O
depth O
from O
the O
hidden O
- O
state O
activity O
of O
all O
units O
. O

In O
all O
experiments O
, O
we O
used O
the O
data O
presented O
in O
Sec- O
tion O
3 O
above O
and O
performed O
a O
nested O
5 O
- O
fold O
cross- O
validation O
procedure O
. O

Word O
frequency O
, O
which O
was O
added O
as O
a O
covariate O
to O
the O
model O
, O
had O
a O
negligi- O
ble O
effect O
on O
the O
results O
. O

Syntactic O
tree O
- O
depth O
was O
found O
to O
be O
efﬁciently O
decodable O
from O
network O
activity O
( O
R2 O
test set= O
0:850:009 O
; O
covariate- O
corrected O
) O
. O

A O
small O
subset O
of O
‘ O
syntax O
’ O
units O
had O
relatively O
high O
weights O
in O
the O
regression O
model O
( O
mean O
weight O
= O
7:610 4 O
, O
SD= O
7:8610 2 O
; O
cut- O
off O
for O
outlier O
weights O
was O
set O
to O
three O
SDs O
) O
. O

Since O
the O
interpretation O
of O
the O
regression O
weights O
may O
depend O
on O
possible O
correlations O
among O
the O
fea- O
tures O
, O
we O
also O
tested O
the O
causal O
effect O
of O
these O
units O
on O
NA B-TaskName
- O
task O
performance O
. O

Ablating O
the O
syntax O
units O
together O
resulted O
in O
signiﬁcant O
performance O
reduction O
in O
NA B-TaskName
- O
tasks O
that O
have O
an O
interfering O
noun O
: O

Linzen O
NA B-TaskName
- O
task O
: O
p= O
0:024 O
, O
nounPPAdv- O

18 O
( O
a O
) O
Input O
gate O
  O
( O
b O
) O
Forget O
gate O
Figure O
4 O
: O
Connectivity O
among O
the O
syntax O
unit O
1150 O
and O
LR O
- O
number O
units O
776 O
and O
988 O
. O

Projecting O
units O
are O
on O
the O
table O
rows O
. O

Blue O
background O
highlights O
outlier O
values O
( O
jz scorej>3 O
) O
. O

Weights O
from O
the O
syntax O
unit O
are O
marked O
with O
large O
diamond O
markers O
and O
are O
explicitly O
labeled O
in O
the O
plots O
. O

SP O
: O
p= O
0:011 O
, O
nounPPAdv B-TaskName
- O
PS O
: O
p= O
0:034 O
, O
nounPP B-TaskName
- O
SP O
: O
p O
< O
0:001and O
marginally O
signiﬁcant O
in O
nounPP B-TaskName
- O
PS O
: O
p= O
0:052(compared O
to O
1000 O
ran- O
dom O
ablations O
of O
subsets O
of O
units O
of O
the O
same O
size O
) O
. O

To O
gain O
further O
insight O
regarding O
the O
functioning O
of O
the O
syntax O
units O
, O
we O
next O
visualized O
their O
gate O
and O
cell O
dynamics O
during O
sentence O
processing O
. O

We O
found O
that O
cell O
activity O
of O
unit O
1150 O
, O
which O
also O
had O
one O
of O
the O
highest O
weights O
in O
the O
regression O
model O
, O
was O
remarkably O
structured O
. O

The O
activity O
of O
this O
unit O
increases O
across O
the O
entire O
subject- O
verb O
dependency O
and O
drops O
abruptly O
right O
after O
. O

Figures O
3a O
and O
3b O
show O
cell O
activity O
of O
this O
unit O
during O
the O
processing O
of O
stimuli O
from O
the O
2Adv O
and O
nounPP B-TaskName
tasks O
. O

We O
found O
the O
same O
dynamics O
in O
cases O
where O
another O
verb O
occurs O
between O
sub- O
ject O
and O
main O
verb O
, O
as O
in O
subject O
relatives O
( O
Figure O
3c O
) O
, O
and O
in O
exceptionally O
long O
- O
distance O
dependen- O
cies O
with O
two O
interfering O
nouns O
and O
verbs O
( O
Figure O
3d O
) O
. O

Taken O
together O
, O
these O
results O
suggest O
that O
unit O
1150 O
consistently O
encodes O
subject O
- O
verb O
dependen- O
cies O
in O
a O
syntax O
- O
sensitive O
manner O
. O

Other O
syntax O
units O
did O
not O
show O
an O
easily O
interpretable O
dynam- O
ics O
and O
had O
no O
clear O
interactions O
with O
the O
number O
units O
in O
the O
analysis O
discussed O
next O
. O

This O
suggests O
that O
they O
perform O
different O
syntactic O
, O
or O
possibly O
other O
, O
functions O
. O

4.4 O
Syntax O
- O
Number O
Units O
Connections O
We O
ﬁnally O
look O
at O
the O
connections O
that O
were O
learned O
by O
the O
LSTM B-TaskName
between O
syntax O
unit O
1150 O
, O
which O
appears O
to O
be O
more O
closely O
involved O
in O
tracking O
subject O
- O
verb O
agreement O
, O
and O
the O
LR O
num- O
ber O
units O
, O
as O
well O
as O
at O
the O
connections O
between O
the O
LR O
- O
number O
units O
themselves O
. O

For O
each O
unit O
pair O
, O
there O
are O
4 O
connection O
types O
, O
one O
for O
each O
com- O
ponent O
of O
the O
target O
cell O
( O
to O
the O
3 O
gates O
and O
to O
the O
update O
candidate O
) O
. O

We O
focus O
on O
input O
and O
forget O
gates O
, O
as O
they O
control O
the O
ﬂow O
and O
storage O
of O
num O
- O
ber O
information O
. O

Figures O
4a O
and O
4b O
show O
the O
distributions O
of O
all O
afferent O
recurrent O
weights O
to O
the O
input O
and O
forget O
gates O
of O
the O
LR O
- O
number O
units O
, O
scaled O
by O
the O
maxi- O
mal O
activity O
htof O
the O
pre O
- O
synaptic O
units O
during O
the O
nounPP B-TaskName
task O
( O
this O
scaling O
evaluates O
the O
effective O
in- O
put O
to O
the O
units O
and O
did O
not O
change O
the O
conclusions O
described O
below O
) O
. O

We O
found O
that O
the O
weights O
from O
the O
syntax O
unit O
to O
the O
forget O
gate O
of O
both O
776 O
and O
988 O
are O
exceptionally O
high O
in O
the O
positive O
direc- O
tion O
compared O
to O
all O
other O
afferent O
connections O
in O
the O
network O
( O
z score O
= O
8:1;11:2 O
, O
respec- O
tively O
) O
and O
those O
to O
their O
input O
gates O
exception- O
ally O
negative O
( O
z score O
= O
 16:2; 7:2 O
) O
. O

Since O
the O
cell O
activity O
of O
syntax O
unit O
1150 O
is O
positive O
across O
the O
entire O
subject O
- O
verb O
dependency O
( O
e.g. O
, O
Figure O
3d O
) O
, O
the O
connectivity O
from O
the O
syntax O
unit O
drives O
the O
number O
unit O
forget O
gates O
towards O
one O
( O
Wf O
776;1150h1150 O

0andWf O
988;1150h1150 O

0 O
; O
tsubject O
< O
t O
< O
t O
verb O
) O
and O
their O
input O
gates O
towards O
zero O
( O
Wi O
776;1150h1150 O

0andWi O
988;1150h1150 O

  O
0 O
) O
. O

Looking O
at O
the O
right O
- O
hand O
- O
side O
of O
Eq O
. O

( O
1 O
) O
, O
this O
means O
that O
the O
ﬁrst O
term O
becomes O
dominant O
and O
the O
second O
vanishes O
, O
suggesting O
that O
, O
across O
the O
entire O
dependency O
, O
the O
syntax O
unit O
conveys O
a O
‘ O
re- O
member O
ﬂag O
’ O
to O
the O
number O
units O
. O

Similarly O
, O
when O
the O
activity O
of O
the O
syntax O
unit O
becomes O
negative O
at O
the O
end O
of O
the O
dependency O
, O
it O
conveys O
an O
‘ O
update O
ﬂag O
’ O
. O

Last O
, O
we O
note O
that O
the O
reciprocal O
connectivity O
between O
the O
two O
LR O
- O
number O
units O
is O
always O
pos- O
itive O
, O
to O
both O
input O
and O
forget O
gates O
( O
with O
jz  O
scorej>3for O
the O
776 O
-to-988 O
direction O
) O
. O

Since O
their O
activity O
is O
negative O
throughout O
the O
subject- O
verb O
dependency O
( O
Figures O
1a O
and O
1b O
) O
, O
this O
means O
that O
they O
are O
mutually O
inhibiting O
, O
thus O
steering O
to- O
wards O
an O
unequivocal O
signal O
about O
the O
grammati- O
cal O
number O
of O
the O
subject O
to O
the O
output O
layer O
. O

195 O
Summary O
and O
Discussion O
We O
provided O
the O
ﬁrst O
detailed O
description O
of O
the O
underlying O
mechanism O
by O
which O
an O
LSTM B-TaskName
language O
- O
model O
performs O
long O
- O
distance O
number O
agreement O
. O

Strikingly O
, O
simply O
training O
an O
LSTM B-TaskName
on O
a O
language O
- O
model O
objective O
on O
raw O
corpus B-DatasetName
data I-DatasetName
brought O
about O
single O
units O
carrying O
exceptionally O
speciﬁc O
linguistic O
information O
. O

Three O
of O
these O
units O
were O
found O
to O
form O
a O
highly O
interactive O
lo- O
cal O
network O
, O
which O
makes O
up O
the O
central O
part O
of O
a O
‘ O
neural O
’ O
circuit O
performing O
long O
- O
distance O
number O
agreement O
. O

One O
of O
these O
units O
encodes O
and O
stores O
gram- O
matical O
number O
information O
when O
the O
main O
sub- O
ject O
of O
a O
sentence O
is O
singular O
, O
and O
it O
successfully O
carries O
this O
information O
across O
long O
- O
range O
depen- O
dencies O
. O

Another O
unit O
similarly O
encodes O
plurality O
. O

These O
number O
units O
show O
that O
a O
highly O
local O
en- O
coding O
of O
linguistic O
features O
can O
emerge O
in O
LSTMs B-TaskName
during O
language O
- O
model O
training O
, O
as O
was O
previously O
suggested O
by O
theoretical O
studies O
of O
artiﬁcial O
neural O
networks O
( O
e.g. O
, O
Bowers O
, O
2009 O
) O
and O
in O
neuroscience O
( O
e.g. O
, O
Kutter O
et O
al O
. O
, O
2018 O
) O
. O

Our O
analysis O
also O
identiﬁed O
units O
whose O
activity O
correlates O
with O
syntactic O
complexity O
. O

These O
units O
, O
as O
a O
whole O
, O
affect O
performance O
on O
the O
agreement O
tasks O
. O

We O
further O
found O
that O
one O
of O
them O
encodes O
the O
main O
subject O
- O
verb O
dependency O
across O
various O
syntactic O
constructions O
. O

Moreover O
, O
the O
highest O
af- O
ferent O
weights O
to O
the O
forget O
and O
input O
gates O
of O
both O
LR O
- O
number O
units O
were O
from O
this O
unit O
. O

A O
natural O
interpretation O
is O
that O
this O
unit O
propagates O
syntax O
- O
based O
remember O
and O
update O
ﬂags O
that O
con- O
trol O
when O
the O
number O
units O
store O
and O
release O
infor- O
mation O
. O

Finally O
, O
number O
is O
also O
redundantly O
encoded O
in O
a O
more O
distributed O
way O
, O
but O
the O
latter O
mechanism O
is O
unable O
to O
carry O
information O
across O
embedded O
syntactic O
structures O
. O

The O
computational O
burden O
of O
tracking O
number O
information O
thus O
gave O
rise O
to O
two O
types O
of O
units O
in O
the O
network O
, O
encoding O
similar O
in- O
formation O
with O
distinct O
properties O
and O
dynamics O
. O

The O
relationship O
we O
uncovered O
and O
character- O
ized O
between O
syntax O
and O
number O
units O
suggests O
that O
agreement O
in O
an O
LSTM B-TaskName
language O
- O
model O
can- O
not O
be O
entirely O
explained O
away O
by O
superﬁcial O
heuristics O
, O
and O
the O
networks O
have O
, O
to O
some O
extent O
, O
learned O
to O
build O
and O
exploit O
structure O
- O
based O
syn- O
tactic O
representations O
, O
akin O
to O
those O
conjectured O
to O
support O
human O
- O
sentence O
processing O
. O

In O
future O
work O
, O
we O
intend O
to O
explore O
how O
the O
en O
- O
coding O
pattern O
we O
found O
varies O
across O
network O
ar- O
chitectures O
and O
hyperparameters O
, O
as O
well O
as O
across O
languages O
and O
domains O
. O

We O
also O
would O
like O
to O
investigate O
the O
timecourse O
of O
emergence O
of O
the O
found O
behaviour O
over O
training O
time O
. O

More O
generally O
, O
we O
hope O
that O
our O
study O
will O
inspire O
more O
analyses O
of O
the O
inner O
dynamics O
of O
LSTMs B-TaskName
and O
other O
sequence O
- O
processing O
networks O
, O
complementing O
the O
currently O
popular O
“ O
black O
- O
box O
probing O
” O
approach O
. O

Besides O
bringing O
about O
a O
mechanistic O
understanding O
of O
language O
process- O
ing O
in O
artiﬁcial O
models O
, O
this O
could O
inform O
work O
on O
human O
- O
sentence O
processing O
. O

Indeed O
, O
our O
study O
yields O
particular O
testable O
predictions O
on O
brain O
dy- O
namics O
, O
given O
that O
the O
computational O
burden O
of O
long O
- O
distance O
agreement O
remains O
the O
same O
for O
ar- O
tiﬁcial O
and O
biological O
neural O
network O
, O
despite O
im- O
plementation O
differences O
and O
different O
data O
sizes O
required O
for O
language O
acquisition O
. O

We O
conjecture O
a O
similar O
distinction O
between O
SR O
and O
LR O
units O
to O
be O
found O
in O
the O
human O
brain O
, O
as O
well O
as O
an O
in- O
teraction O
between O
syntax O
- O
processing O
and O
feature- O
carrying O
units O
such O
as O
the O
LR O
units O
, O
and O
plan O
to O
test O
these O
in O
future O
work O
. O

Acknowledgments O
We O
would O
like O
to O
thank O
Kristina O
Gulordava O
, O
Jean- O
Remi O
King O
, O
Tal O
Linzen O
, O
Gabriella O
Vigliocco O
and O
Christophe O
Pallier O
for O
helpful O
feedback O
and O
com- O
ments O
on O
the O
work O
. O

References O
Yossi O
Adi O
, O
Einat O
Kermany O
, O
Yonatan O
Belinkov O
, O
Ofer O
Lavi O
, O
and O
Yoav O
Goldberg O
. O
2017 O
. O

Fine O
- O
grained O
analysis O
of O
sentence O
embeddings O
using O
auxil- O
iary O
prediction O
tasks O
. O

In O
Proceedings O
of O
ICLR O
Conference O
Track O
, O
Toulon O
, O
France O
. O

Published O
online O
: O
https://openreview.net/group O
? O

id O
= O
ICLR.cc/2017 O
/ O
conference O
. O

Guillaume O
Alain O
and O
Yoshua O
Bengio O
. O

2017 O
. O

Under- O
standing O
intermediate O
layers O
using O
linear O
classiﬁer O
probes O
. O

In O
Proceedings O
of O
ICLR O
Conference O
Track O
, O
Toulon O
, O
France O
. O

Jean O
- O
Philippe O
Bernardy O
and O
Shalom O
Lappin O
. O
2017 O
. O

Us- O
ing O
deep O
neural O
networks O
to O
learn O
syntactic O
agree- O
ment O
. O

Linguistic O
Issues O
in O
Language O
Technology O
, O
15(2):1–15 O
. O

Jeffrey O
Bowers O
. O
2009 O
. O

On O
the O
biological O
plausibility O
of O
grandmother O
cells O
: O
Implications O
for O
neural O
network O
theories O
in O
psychology O
and O
neuroscience O
. O

Psycho- O

logical O
Review O
, O
116(1):220–251 O
. O

20Shammur O
Chowdhury O
and O
Roberto O
Zamparelli O
. O

2018 O
. O

RNN B-TaskName
simulations O
of O
grammaticality O
judgments O
on O
long O
- O
distance O
dependencies O
. O

In O
Proceedings O
of O
COLING O
, O
pages O
133–144 O
, O
Santa O
Fe O
, O
NM O
. O

Lieke O
Gelderloos O
and O
Grzegorz O
Chrupała O
. O

2016 O
. O

From O
phonemes O
to O
images O
: O
levels O
of O
representation O
in O
a O
recurrent O
neural O
model O
of O
visually O
- O
grounded O
lan- O
guage O
learning O
. O

In O
Proceedings O
of O
COLING O
2016 O
, O
the O
26th O
International O
Conference O
on O
Computational O
Linguistics O
: O
Technical O
Papers O
, O
pages O
1309–1319 O
. O

Mario O
Giulianelli O
, O
Jack O
Harding O
, O
Florian O
Mohnert O
, O
Dieuwke O
Hupkes O
, O
and O
Willem O
Zuidema O
. O

2018 O
. O

Un- O
der O
the O
hood O
: O
Using O
diagnostic O
classiﬁers O
to O
investi- O
gate O
and O
improve O
how O
language O
models O
track O
agree- O
ment O
information O
. O

In O
Proceedings O
of O
the O
EMNLP B-TaskName
BlackboxNLP B-TaskName
Workshop O
, O
pages O
240–248 O
, O
Brussels O
, O
Belgium O
. O

Kristina O
Gulordava O
, O
Piotr O
Bojanowski O
, O
Edouard O
Grave O
, O
Tal O
Linzen O
, O
and O
Marco O
Baroni O
. O

2018 O
. O

Colorless O
green O
recurrent O
networks O
dream O
hierarchically O
. O

In O
Proceedings O
of O
NAACL O
, O
pages O
1195–1205 O
, O
New O
Or- O
leans O
, O
LA O
. O

Sepp O
Hochreiter O
and O
J O
¨urgen O
Schmidhuber O
. O

1997 O
. O

Long O
short O
- O
term O
memory O
. O

Neural O
Computation O
, O
9(8):1735–1780 O
. O

Dieuwke O
Hupkes O
, O
Sara O
Veldhoen O
, O
and O
Willem O
Zuidema O
. O

2018 O
. O

Visualisation O
and O
’ O
diagnostic O
classi- O
ﬁers O
’ O
reveal O
how O
recurrent O
and O
recursive O
neural O
net- O
works O
process O
hierarchical O
structure O
. O

Journal O
of O
Ar- O
tiﬁcial O
Intelligence O
Research O
, O
61:907–926 O
. O

Jaap O
Jumelet O
and O
Dieuwke O
Hupkes O
. O

2018 O
. O

Do O
lan- O
guage O
models O
understand O
anything O
? O

on O
the O
ability O
of O
lstms O
to O
understand O
negative O
polarity O
items O
. O

In O
Proceedings O
of O
the O
2018 O
EMNLP B-TaskName
Workshop O
Black- O

boxNLP B-TaskName
: O

Analyzing O
and O
Interpreting O
Neural O
Net- O
works O
for O
NLP B-TaskName
, O
pages O
222–231 O
. O

Andrej O
Karpathy O
, O
Justin O
Johnson O
, O
and O
Fei O
- O
Fei O
Li O
. O
2016 O
. O

Visualizing O
and O
understanding O
recur- O
rent O
networks O
. O

In O
Proceedings O
of O
ICLR O
Work- O
shop O
Track O
, O
San O
Juan O
, O
Puerto O
Rico O
. O

Published O
online O
: O
https://openreview.net/group O
? O

id O
= O
ICLR.cc/2016 O
/ O
workshop O
. O

Yova O
Kementchedjhieva O
and O
Adam O
Lopez O
. O
2018 O
. O

‘ O
In- O
dicatements O
’ O
that O
character O
language O
models O
learn O
English O
morpho O
- O
syntactic O
units O
and O
regularities O
. O

In O
Proceedings O
of O
the O
EMNLP B-TaskName
Workshop O
on O
analyzing O
and O
interpreting O
neural O
networks O
for O
NLP B-TaskName
, O
Brussels O
, O
Belgium O
. O

In O
press O
. O

Jean O
- O
R O
´ O
emi O
King O
and O
Stanislas O
Dehaene O
. O

2014 O
. O

Charac- O
terizing O
the O
dynamics O
of O
mental O
representations O
: O
The O
temporal O
generalization O
method O
. O

Trends O
in O
Cogni- O
tive O
Sciences O
, O
18(4):203–210 O
. O

Adhiguna O
Kuncoro O
, O
Chris O
Dyer O
, O
John O
Hale O
, O
and O
Phil O
Blunsom O
. O

2018a O
. O

The O
perils O
of O
natural O
behavioral O
tests O
for O
unnatural O
models O
: O
The O
case O
of O
numberagreement O
. O

Poster O
presented O
at O
the O
Learning O
Lan- O
guage O
in O
Humans O
and O
in O
Machines O
conference O
, O
on- O
line O
at O
: O
https://osf.io/view/L2HM/ O
. O

Adhiguna O
Kuncoro O
, O
Chris O
Dyer O
, O
John O
Hale O
, O
Dani O
Yo- O
gatama O
, O
Stephen O
Clark O
, O
and O
Phil O
Blunsom O
. O

2018b O
. O

LSTMs B-TaskName
can O
learn O
syntax O
- O
sensitive O
dependencies O
well O
, O
but O
modeling O
structure O
makes O
them O
better O
. O

In O
Proceedings O
of O
ACL O
, O
pages O
1426–1436 O
, O
Melbourne O
, O
Australia O
. O

Esther O
Kutter O
, O
Jan O
Bostroem O
, O
Christian O
Elger O
, O
Florian O
Mormann O
, O
and O
Andreas O
Nieder O
. O

2018 O
. O

Single O
neu- O
rons O
in O
the O
human O
brain O
encode O
numbers O
. O

Neuron O
, O
100(3):753–761 O
. O

Jiwei O
Li O
, O
Xinlei O
Chen O
, O
Eduard O
Hovy O
, O
and O
Dan O
Jurafsky O
. O
2016 O
. O

Visualizing O
and O
understanding O
neural O
mod- O
els O
in O
NLP B-TaskName
. O

In O
Proceedings O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
: O
Human O
Language O
Technologies O
( O
NAACL- O
HLT O
) O
, O
pages O
681–691 O
. O

Tal O
Linzen O
, O
Emmanuel O
Dupoux O
, O
and O
Yoav O
Goldberg O
. O
2016 O
. O

Assessing O
the O
ability O
of O
LSTMs B-TaskName
to O
learn O
syntax O
- O
sensitive O
dependencies O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
4:521 O
– O
535 O
. O

Tal O
Linzen O
and O
Brian O
Leonard O
. O

2018 O
. O

Distinct O
patterns O
of O
syntactic O
agreement O
errors O
in O
recurrent O
networks O
and O
humans O
. O

In O
Proceedings O
of O
CogSci O
, O
pages O
692 O
– O
697 O
, O
Austin O
, O
TX O
. O

Rebecca O
Marvin O
and O
Tal O
Linzen O
. O

2018 O
. O

Targeted O
syn- O
tactic O
evaluation O
of O
language O
models O
. O

In O
Proceed- O
ings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1192–1202 O
. O

Matthew O
Nelson O
, O
Imen O
El O
Karoui O
, O
Kristof O
Giber O
, O
Xi- O
aofang O
Yang O
, O
Laurent O
Cohen O
, O
Hilda O
Koopman O
, O
Syd- O
ney O
Cash O
, O
Lionel O
Naccache O
, O
John O
Hale O
, O
Christophe O
Pallier O
, O
and O
Stanislas O
Dehaene O
. O

2017 O
. O

Neurophysio- O
logical O
dynamics O
of O
phrase O
- O
structure O
building O
during O
sentence O
processing O
. O

Proceedings O
of O
the O
National O
Academy O
of O
Sciences O
, O
114(18):E3669 O
– O
E3678 O
. O

Alec O
Radford O
, O
Rafal O
Jozefowicz O
, O
and O
Ilya O
Sutskever O
. O
2017 O
. O

Learning O
to O
generate O
reviews O
and O
discovering O
sentiment O
. O

https://arxiv.org/abs/1704 O
. O
01444 O
. O

Zhiyuan O
Tang O
, O
Ying O
Shi O
, O
Dong O
Wang O
, O
Yang O
Feng O
, O
and O
Shiyue O
Zhang O
. O

2017 O
. O

Memory O
visualization O
for O
gated O
recurrent O
neural O
networks O
in O
speech O
recogni- O
tion O
. O

In O
Acoustics O
, O
Speech O
and O
Signal O
Processing O
( O
ICASSP O
) O
, O
2017 O
IEEE O
International O
Conference O
on O
, O
pages O
2736–2740 O
. O

IEEE O
. O

Ethan O
Wilcox O
, O
Roger O
Levy O
, O
Takashi O
Morita O
, O
and O
Richard O
Futrell O
. O

2018 O
. O

What O
do O
RNN B-TaskName
language O
models O
learn O
about O
ﬁller O
– O
gap O
dependencies O
? O

In O
Proceedings O
of O
the O
2018 O
EMNLP B-TaskName
Workshop O
Black- O

boxNLP B-TaskName
: O

Analyzing O
and O
Interpreting O
Neural O
Net- O
works O
for O
NLP B-TaskName
, O
pages O
211–221 O
. O

