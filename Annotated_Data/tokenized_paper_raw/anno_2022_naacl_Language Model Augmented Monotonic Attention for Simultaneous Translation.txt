Proceedings O
of O
the O
2022 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
38 O
- O
45 O
July O
10 O
- O
15 O
, O
2022 O
© O
2022 O
Association O
for O
Computational O
Linguistics O
Language O
Model O
Augmented O
Monotonic O
Attention O
for O
Simultaneous O
Translation O
Sathish O
Indurthi§∗Mohd O
Abbas O
Zaidi‡ O
Beomseok O
Lee‡†Nikhil O
Kumar O
Lakumarapu‡†Sangha O
Kim‡ O
‡Samsung O
Research O
, O
South O
Korea§Zoom O
AI O
Lab O
, O
Singapore O
sathishreddy.indurthi@zoom.us O
, O
{ O
abbas.zaidi O
, O
bsgunn.lee O
, O
n07.kumar O
, O
sangha01.kim}@samsung.com O

Abstract O
The O
state O
- O
of O
- O
the O
- O
art O
adaptive O
policies O
for O
simul- O
taneous O
neural O
machine O
translation O
( O
SNMT O
) O
use O
monotonic O
attention O
to O
perform O
read O
/ O
write O
decisions O
based O
on O
the O
partial O
source O
and O
target O
sequences O
. O

The O
lack O
of O
sufficient O
information O
might O
cause O
the O
monotonic O
attention O
to O
take O
poor O
read O
/ O
write O
decisions O
, O
which O
in O
turn O
neg- O
atively O
affects O
the O
performance O
of O
the O
SNMT O
model O
. O

On O
the O
other O
hand O
, O
human O
translators O
make O
better O
read O
/ O
write O
decisions O
since O
they O
can O
anticipate O
the O
immediate O
future O
words O
using O
linguistic O
information O
and O
domain O
knowledge O
. O

In O
this O
work O
, O
we O
propose O
a O
framework O
to O
aid O
monotonic O
attention O
with O
an O
external O
language O
model O
to O
improve O
its O
decisions O
. O

Experiments O
on O
MuST O
- O
C O
English O
- O
German O
and O
English O
- O
French O
speech O
- O
to O
- O
text O
translation O
tasks O
show O
the O
future O
information O
from O
language O
model O
improves O
the O
state O
- O
of O
- O
the O
- O
art O
monotonic O
multi O
- O
head O
attention O
model O
further O
. O

1 O
Introduction O
A O
typical O
application O
of O
simultaneous O
neural O
ma- O
chine O
translation O
( O
SNMT O
) O
is O
conversational O
speech O
or O
live O
video O
caption O
translation O
. O

In O
order O
to O
achieve O
live O
translation O
, O
an O
SNMT O
model O
alternates O
be- O
tween O
performing O
read O
from O
source O
sequence O
and O
write O
to O
target O
sequence O
. O

For O
a O
model O
to O
decide O
whether O
to O
read O
orwrite O
at O
certain O
moment O
, O
either O
a O
fixed O
or O
an O
adaptive O
read O
/ O
write O
policy O
can O
be O
used O
. O

Earlier O
approaches O
in O
simultaneous O
translation O
such O
as O
Ma O
et O
al O
. O
( O
2019a O
) O
and O
Dalvi O
et O
al O
. O

( O
2018 O
) O
employ O
a O
fixed O
policy O
that O
alternate O
between O
read O
andwrite O
after O
the O
waiting O
period O
of O
ktokens O
. O

To O
alleviate O
possible O
long O
delay O
of O
fixed O
polices O
, O
re- O
cent O
works O
such O
as O
monotonic O
infinite O
lookback O
attention O
( O
MILk O
) O
( O
Arivazhagan O
et O
al O
. O
, O
2019 O
) O
, O
and O
monotonic O
multihead O
attention O
( O
MMA O
) O
( O
Ma O
et O
al O
. O
, O
2019c O
) O
developed O
flexible O
policies O
using O
monotonic O
attention O
( O
Raffel O
et O
al O
. O
, O
2017 O
) O
. O

∗⋆Work O
done O
while O
at O
Samsung O
Research O
†Equal O
contribution O
Figure O
1 O
: O
The O
finetuned O
XLM O
- O
RoBERTa O
language O
model O
predicts O
German O
words O
using O
the O
prefix O
as O
in- O
put.(Green O
: O
Correct O
, O
Red O
: O
Incorrect O
, O
Black O
: O
Neutral O
) O
. O

While O
these O
monotonic O
attention O
anticipates O
tar- O
get O
words O
using O
only O
available O
prefix O
source O
and O
target O
sequence O
, O
human O
translators O
anticipate O
the O
target O
words O
using O
their O
language O
expertise O
( O
linguis- O
tic O
anticipation O
) O
as O
well O
as O
contextual O
information O
( O
extra O
- O
linguistic O
anticipation O
) O
( O
Vandepitte O
, O
2001 O
) O
. O

Inspired O
by O
human O
translation O
experts O
, O
we O
aim O
to O
augment O
monotonic O
attention O
with O
future O
informa- O
tion O
using O
language O
models O
( O
LM O
) O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Conneau O
et O
al O
. O
, O
2019 O
) O
. O

Integrating O
the O
external O
information O
effectively O
into O
text O
- O
to O
- O
text O
machine O
translation O
( O
MT O
) O
systems O
has O
been O
explored O
by O
several O
works O
( O
Khandelwal O
et O
al O
. O
, O
2020 O
; O
Gulcehre O
et O
al O
. O
, O
2015 O
, O
2017 O
; O
Stahlberg O
et O
al O
. O
, O
2018 O
) O
. O

Also O
, O
integrating O
future O
information O
implicitly O
into O
SNMT O
system O
during O
training O
is O
ex- O
plored O
in O
Wu O
et O
al O
. O

( O
2020 O
) O
by O
simultaneously O
train- O
ing O
different O
wait- O
kSNMT O
systems O
. O

However O
, O
no O
previous O
works O
make O
use O
of O
explicit O
future O
informa- O
tion O
both O
during O
training O
and O
inference O
. O

To O
utilize O
explicit O
future O
information O
, O
we O
explored O
to O
inte- O
grate O
future O
information O
from O
LM O
directly O
into O
the O
output O
layer O
of O
the O
MMA O
model O
. O

However O
, O
it O
did O
not O
provide O
any O
improvements O
( O
refer O
to O
Appendix O
A O
) O
, O
thus O
motivating O
us O
to O
explore O
a O
tighter O
integra- O
tion O
of O
the O
LM O
information O
into O
SNMT O
model O
. O

In O
this O
work O
, O
we O
explicitly O
use O
plausible O
future38 O

Figure O
2 O
: O
Overview O
of O
the O
proposed O
language O
model O
augmented O
monotonic O
attention O
for O
SNMT O
. O

information O
from O
LM O
during O
training O
by O
transform- O
ing O
the O
monotonic O
attention O
mechanism O
. O

As O
shown O
in O
Figure O
1 O
, O
at O
each O
step O
, O
the O
LM O
takes O
the O
prefix O
target O
( O
and O
source O
, O
for O
cross O
- O
lingual O
LM O
) O
sequence O
and O
predicts O
the O
probable O
future O
information O
. O

We O
hypothesize O
that O
aiding O
the O
monotonic O
attention O
with O
this O
future O
information O
can O
improve O
MMA O
model O
’s O
read O
/ O
write O
policy O
, O
eventually O
leading O
to O
better O
translation O
with O
less O
delay O
. O

Several O
experi- O
ments O
on O
MuST O
- O
C O
( O
Di O
Gangi O
et O
al O
. O
, O
2019 O
) O

English- O

German O
and O
English O
- O
French O
speech O
- O
to O
- O
text O
transla- O
tion O
tasks O
with O
our O
proposed O
approach O
show O
clear O
improvements O
of O
latency O
- O
quality O
trade O
- O
offs O
over O
the O
state O
- O
of O
- O
the O
- O
art O
MMA O
models O
. O

2 O
Monotonic O
Attention O
with O
Future O
Information O
Model O
2.1 O
Monotonic O
Attention O
In O
simultaneous O
machine O
translation O
( O
SNMT O
) O
mod- O
els O
, O
the O
probability O
of O
predicting O
the O
target O
token O
yi∈ydepends O
on O
the O
partial O
source O
and O
target O
sequences O
( O
x≤j∈x O
, O
y O
< O
i∈y O
) O
. O

In O
sequence O
- O
to- O
sequence O
based O
SNMT O
model O
, O
each O
target O
token O
yi O
is O
generated O
as O
follows O
: O

hj O
= O
E(x≤j O
) O
( O
1 O
) O
si O
= O
D(y O
< O
i O
, O
ci O
= O
A(si−1 O
, O
h≤j O
) O
) O
( O
2 O
) O
yi O
= O
Output O
( O
si O
) O
( O
3 O
) O
where O
E(.)andD(.)are O
the O
encoder O
and O
decoder O
layers O
, O
and O
ciis O
a O
context O
vector O
. O

In O
monotonic O
attention O
based O
SNMT O
, O
the O
context O
vector O
is O
com- O
puted O
as O
follows O
: O
ei O
, O
j O
= O
MonotonicEnergy O
( O
si−1 O
, O
hj)(4 O
) O
pi O
, O
j O
= O
Sigmoid O
( O
ei O
, O
j O
) O
( O
5 O
) O
zi O
, O
j∼Bernoulli O
( O
pi O
, O
j O
) O
( O
6)When O
generating O
a O
target O
token O
yi O
, O
the O
decoder O
chooses O
whether O
to O
read O
/ O
write O
based O
on O
Bernoulli O
selection O
probability O
pi O
, O
j. O
When O
zi O
, O
j= O
1 O
( O
write O
) O
, O
model O
sets O
ti O
= O
j O
, O
ci O
= O
hjand O
generates O
the O
target O
token O
yi O
. O

Forzi O
, O
j= O
0(read O
) O
, O
it O
sets O
ti O
= O
j+ O
1and O
repeats O
Eq O
. O
4 O
to O
6 O
. O

Here O
tirefers O
to O
the O
index O
of O
the O
encoder O
when O
decoder O
needs O
to O
produce O
the O
ith O
target O
token O
. O

Instead O
of O
hard O
alignment O
of O
ci O
= O
hj O
, O
Raffel O
et O
al O
. O

( O
2017 O
) O
compute O
an O
expected O
alignment O
in O
a O
recurrent O
manner O
and O
propose O
a O
closed O
- O
form O
parallel O
solution O
. O

Arivazhagan O
et O

al O
. O

( O
2019 O
) O
adopt O
monotonic O
attention O
into O
SNMT O
and O
later O
, O
Ma O
et O
al O
. O

( O
2019c O
) O
extend O
it O
to O
MMA O
to O
integrate O
it O
into O
the O
Transformer O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O

2.2 O
Monotonic O
Attention O
with O
Future O
Information O

The O
monotonic O
attention O
described O
in O
Section O
2.1 O
performs O
anticipation O
based O
only O
on O
the O
currently O
available O
source O
and O
target O
information O
. O

To O
aug- O
ment O
this O
anticipation O
process O
using O
future O
informa- O
tion O
extracted O
using O
LMs O
, O
we O
propose O
the O
following O
modifications O
to O
the O
monotonic O
attention O
. O

Future O
Representation O
Layer O
: O

At O
every O
de- O
coding O
step O
i O
, O
the O
previous O
target O
token O
yi−1is O
equipped O
with O
a O
plausible O
future O
token O
ˆyias O
shown O
in O
the O
Figure O
2 O
. O

Since O
the O
token O
ˆyicomes O
from O
an O
LM O
possibly O
with O
a O
different O
tokenizer O
and O
vo- O
cabulary O
set O
, O
applying O
the O
model O
’s O
tokenizer O
and O
vocabulary O
might O
split O
the O
token O
ˆyifurther O
into O
mul- O
tiple O
sub O
- O
tokens O
{ O
ˆy1 O
i,ˆy2 O
i,···,ˆym O
i O
} O
. O

To O
get O
a O
single O
future O
token O
representation O
˜si∈ O
Rdfrom O
all O
the O
sub O
- O
tokens O
, O
we O
apply O
a O
sub O
- O
token O
summary O
layer O
: O

˜si= O
Γ({ˆy1 O
i,ˆy2 O
i,···,ˆym O
i O
} O
) O
( O
7 O
) O
TheΓrepresents O
a O
general O
sequence O
representation O
layer O
such O
as O
a O
Transformer O
encoder O
layer O
or O
a O
sim- O
ple O
normalized O
sum O
of O
sub O
- O
token O
representations.39 O

We O
enrich O
˜siat O
every O
layer O
lof O
the O
decoder O
block O
by O
applying O
a O
residual O
feed O
- O
forward O
network O
. O

˜sl O
i O
= O
FFN O
( O
˜yl−1 O
i O
) O
( O
8) O
Monotonic O
Energy O
Layer O
with O
Future O
Informa- O
tion O
: O
Despite O
the O
fact O
that O
we O
can O
add O
the O
plau- O
sible O
future O
information O
to O
the O
output O
layer O
( O
Ap- O
pendix O
A O
) O
or O
append O
it O
to O
the O
target O
token O
represen- O

tation O
yi−1 O
, O
the O
MMA O
read O
/ O
write O
decisions O
happen O
in O
Eq O
. O

4 O
. O

Therefore O
, O
we O
integrate O
˜siinto O
the O
Eq O
. O
4 O
instead O
. O

The O
integration O
is O
carried O
out O
by O
modifying O
Eq O
. O
4 O
- O
Eq O
. O

5 O
. O

We O
compute O
the O
monotonic O
energy O
for O
future O
information O
using O
the O
enriched O
future O
token O
representation O
˜siavailable O
at O
each O
layer O
: O
˜ei O
, O
j O
= O
MonotonicEnergy O
( O
˜si O
, O
hj O
) O
( O
9 O
) O
We O
integrate O
the O
future O
monotonic O
energy O
function O
into O
Eq O
. O
5 O
as O
follows O
: O
˜pi O
, O
j O
= O
Sigmoid O
( O
ei O
, O
j+ O
˜ei O
, O
j O
) O
( O
10 O
) O
After O
computing O
˜pi O
, O
j O
, O
we O
compute O
cisimilar O
to O
MMA O
model O
. O

This O
way O
of O
integration O
of O
future O
information O
allows O
the O
model O
to O
condition O
the O
LM O
output O
us- O
age O
on O
the O
input O
sequence O
. O

The O
model O
can O
control O
the O
relative O
weightage O
given O
to O
the O
LM O
output O
by O
varying O
the O
˜ei O
, O
j. O
In O
case O
of O
insufficient O
source O
in- O
formation O
in O
the O
low O
latency O
regime O
, O
we O
expect O
the O
model O
’s O
decision O
policy O
to O
rely O
more O
on O
˜ei O
, O
j. O
Inference O
: O

During O
inference O
, O
the O
start O
token O
does O
not O
contain O
any O
plausible O
information O
. O

After O
pre- O
dicting O
the O
first O
target O
token O
, O
for O
every O
subsequent O
prediction O
of O
target O
token O
yi O
, O
we O
invoke O
the O
LM O
to O
predict O
the O
next O
plausible O
future O
token O
and O
integrate O
this O
new O
information O
into O
Eq O
. O

10 O
. O
3 O
Experiments O
and O
Results O
3.1 O
Experimental O
Settings O
Datasets O
and O
Metrics O
: O
We O
conduct O
our O
experi- O
ments O
on O
the O
MuST O
- O
C O
English(En)-German(De O
) O
and O
English(En)-French(Fr O
) O
speech O
- O
to O
- O
text O
( O
ST O
) O
translation O
task O
. O

The O
speech O
sequence O
is O
repre- O
sented O
using O
80 O
- O
dimensional O
log O
- O
mel O
filter O
bank O
features O
. O

The O
target O
sequence O
is O
represented O
as O
sub- O
words O
using O
a O
SentencePiece O
( O
Kudo O
and O
Richard- O
son O
, O
2018 O
) O
model O
with O
a O
unigram O
vocabulary O
of O
size O
10,000 O
. O

We O
evaluate O
the O
performance O
of O
the O
models O
on O
both O
the O
latency O
and O
quality O
aspects O
. O

Weuse O
Average O
Lagging(AL O
) O
as O
our O
latency O
metric O
and O
case O
- O
sensitive O
detokenized O
SacreBLEU O
( O
Post O
, O
2018 O
) O
to O
measure O
the O
translation O
quality O
, O
similar O
to O
( O
Ma O
et O
al O
. O
, O
2020 O
) O
. O

The O
best O
models O
are O
chosen O
based O
on O
the O
dev O
set O
results O
and O
reported O
results O
are O
from O
the O
MuST O
- O
C O
test O
( O
tst O
- O
COMMON O
) O
sets O
. O

Language O
Models O
We O
use O
two O
language O
mod- O
els O
to O
train O
our O
proposed O
modified O
MMA O
model O
. O

Firstly O
, O
we O
use O
the O
pretrained O
XLM O
- O
RoBERTa O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
model O
from O
Huggingface O
Transformers1model O
repository O
. O

Since O
the O
LM O
out- O
put O
can O
be O
very O
open O
- O
ended O
and O
might O
not O
directly O
suit O
/ O
cater O
to O
our O
task O
and O
dataset O
, O
we O
finetune O
the O
head O
of O
the O
model O
using O
the O
MuST O
- O
C O
target O
text O
data O
for O
each O
task O
. O

We O
also O
train O
a O
smaller O
language O
model O
( O
SLM O
) O
, O
which O
contains O
6 O
Transformer O
decoder O
layers O
, O
512 O
hidden O
- O
states O
and O
24 O
M O
parameters O
. O

We O
use O
the O
MuST O
- O
C O
data O
along O
with O
additional O
data O
augmen- O
tation O
to O
reduce O
overfitting O
. O

The O
SLM O
helps O
to O
remove O
the O
issues O
related O
to O
vocabulary O
mismatch O
as O
discussed O
in O
the O
Section O
2.2 O
. O

Implementation O
Details O
: O
Our O
base O
model O
is O
adopted O
from O
Ma O
et O
al O
. O

( O
2020 O
) O
. O

We O
use O
a O
pre- O
decision O
ratio O
of O
7 O
, O
which O
means O
that O
the O
simultane- O
ousread O
/ O
write O
decisions O
are O
made O
after O
every O
seven O
encoder O
states O
. O

We O
use O
λorλlatency O
to O
refer O
to O
the O
hyperparameter O
corresponding O
to O
the O
weighted O
average O
( O
λavg O
) O
in O
MMA O
. O

The O
values O
of O
this O
hyperpa- O
rameter O
λare O
chosen O
from O
the O
set O
{ O
0.01,0.05,0.1 O
} O
. O

TheΓlayer O
in O
Eq O
. O

7 O
computes O
the O
normalized O
sum O
of O
the O
sub O
- O
token O
representations O
. O

For O
SLM O
, O
it O
sim- O
ply O
finds O
the O
embedding O
since O
it O
shares O
the O
same O
vocabulary O
set O
. O

All O
the O
models O
are O
trained O
on O
a O
NVIDIA O
v100 O
GPU O
with O
update O
_ O
freq O
set O
to O
8 O
. O

Simultaneous O
Translation O
Models O
: O
Even O
though O
future O
information O
can O
be O
integrated O
explicitly O
into O
the O
fixed O
policy O
approaches O
such O
as O
Wait O
- O
K O
( O
Ma O
et O
al O
. O
, O
2019b O
) O
, O
we O
choose O
monotonic O
attention O
as O
our O
baseline O
due O
to O
its O
superior O
performance O
( O
Arivazhagan O
et O
al O
. O
, O
2019 O
; O
Ma O
et O
al O
. O
, O
2019c O
) O
. O

We O
train O
a O
baseline O
based O
on O
Ma O
et O
al O
. O

( O
2020 O
) O
work O
, O
called O
as O
MMA O
model O
. O

The O
MMA O
model O
encoder O
and O
decoder O
embedding O
dimensions O
are O
set O
to O
392 O
, O
whereas O
our O
proposed O
model O
’s O
encoder O
and O
decoder O
embeddings O
are O
set O
to O
256 O
to O
have O
similar O
parameters O
( O
≈39 O
M O
) O
for O
a O
fair O
comparison O
. O

We O
train O
two O
models O
using O
the O
1https://huggingface.co/transformers/40 O

Figure O
3 O
: O
LM O
prediction O
weight O
vs O
λ O
modified O
MMA O
based O
on O
two O
LMs O
( O
XLM O
, O
SLM O
) O
, O
referred O
as O
MMA O
- O
XLM O
and O
MMA O
- O
SLM O
. O

3.2 O
Results O
We O
first O
analyze O
how O
the O
LM O
predictions O
are O
being O
utilized O
by O
the O
our O
model O
. O

In O
order O
to O
measure O
the O
relative O
weight O
given O
to O
model O
’s O
internal O
states O
ver- O

sus O
the O
predictions O
from O
the O
LM O
, O
we O
compare O
the O
norm O
of O
the O
monotonic O
energies O
corresponding O
to O
the O
LM O
predictions O
epred(Eq O
. O

9 O
) O
and O
the O
previous O
output O
tokens O
eoutput O
( O
Eq O
. O
4 O
) O
. O

Let O
us O
define O
LM O
prediction O
weight O
as O
follows O
: O
LMpw=/parenleftbigg∥epred∥ O
∥eoutput∥/parenrightbigg O
( O
11 O
) O
In O
Figure O
3 O
, O
we O
plot O
the O
variation O
of O
LMpw(av- O
eraged O
) O
vs. O
λ O
. O

We O
use O
two O
additional O
values O
of O
λ∈ O
{ O
0.005,0.001}to O
obtain O
this O
plot O
. O

We O
can O
observe O
that O
as O
the O
latency O
requirements O
become O
more O
and O
more O
strict O
, O
the O
model O
starts O
to O
give O
more O
weightage O
to O
the O
predictions O
coming O
from O
the O
LM O
. O

This O
shows O
that O
the O
model O
learns O
to O
utilize O
the O
in- O
formation O
coming O
from O
LM O
predictions O
based O
on O
latency O
requirements O
. O

Next O
, O
we O
discuss O
the O
performance O
improvements O
obtained O
from O
our O
proposed O
approach O
. O

By O
vary- O
ing O
the O
λ O
, O
we O
train O
separate O
models O
for O
different O
latency O
regimes O
. O

Moreover O
, O
the O
quality O
and O
latency O
for O
a O
particular O
model O
can O
also O
be O
varied O
by O
control- O
ling O
the O
speech O
segment O
size O
during O
the O
inference O
. O

Speech O
segment O
size O
or O
step O
size O
refers O
to O
the O
du- O
ration O
of O
speech O
( O
in O
ms O
) O
processed O
corresponding O
to O
each O
read O
decision O
. O

We O
vary O
these O
hyperparame- O
ters O
for O
all O
the O
three O
models O
, O
namely O
MMA O
, O
MMA- O
XLM O
and O
MMA O
- O
SLM O
. O

The O
BLEU O
- O
AL O
curves O
for O
all O
the O
models O
have O
been O
provided O
in O
Figure O
4 O
and O
BLEU O
- O
AL O
num- O
bers O
for O
all O
models O
are O
included O
in O
Appendix O
F O
500 O
1000 O
1500 O
2000 O
2500 O
3000 O
Latency O
( O
AL)4681012141618BLEU O
step O
size O
120 O
step O
size O
200 O
step O
size O
280 O
step O
size O
360 O
step O
size O
440 O
step O
size O
520MMA O
MMA- O

XLM O
MMA O
- O
SLM(a O
) O
EnDe O
Task O
500 O
1000 O
1500 O
2000 O
2500 O
Latency O
( O
AL)7.510.012.515.017.520.022.525.027.5BLEU O
step O
size O
120 O
step O
size O
200 O
step O
size O
280 O
step O
size O
360 O
step O
size O
440 O
step O
size O
520MMA O
MMA- O

XLM O
MMA O
- O
SLM O
( O
b O
) O
EnFr O
Task O
Figure O
4 O
: O
BLEU O
vs O
Average O
Lagging O
results O
for O
MMA O
, O
MMA O
- O
XLM O
and O
MMA O
- O
SLM O
models O
. O

for O
reference O
. O

We O
vary O
the O
step O
sizes O
in O
intervals O
of O
80ms O
from O
120 O
ms O
to O
520 O
ms O
in O
order O
to O
get O
performances O
corresponding O
to O
different O
latency O
regimes O
. O

We O
can O
observe O
that O
the O
LM O
- O
based O
mod- O
els O
using O
both O
XLM O
and O
SLM O
provide O
a O
significant O
performance O
improvement O
over O
the O
baseline O
MMA O
model O
. O

We O
observe O
improvements O
in O
the O
range O
of O
1 O
- O
2 O
BLEU O
scores O
consistently O
across O
all O
the O
latency O
regimes O
( O
λ= O
0.1,0.05,0.01 O
) O
. O

The O
MMA O
using O
SLM O
language O
model O
performs O
slightly O
better O
than O
MMA O
using O
XLM O
language O
model O
. O

This O
is O
due O
to O
SLM O
’s O
higher O
accuracy O
on O
the O
next O
token O
predic- O
tion O
task O
as O
compared O
to O
XLM O
, O
30.15 O
% O
vs. O
21.5 O
% O
for O
German O
& O
31.65 O
% O
vs. O
18.45 O
% O
for O
French O
. O

The O
high O
accuracy O
of O
SLM O
is O
attributed O
to O
its O
training O
on O
in O
- O
domain O
data O
. O

4 O
Conclusion O
In O
this O
work O
, O
we O
provide O
a O
generic O
framework O
to O
integrate O
the O
linguistic O
and O
extra O
- O
linguistic O
infor- O
mation O
into O
simultaneous O
models O
. O

We O
rely O
on O
lan-41 O

guage O
models O
to O
extract O
this O
plausible O
future O

in- O
formation O
and O
propose O
a O
new O
monotonic O
attention O
mechanism O
to O
infuse O
this O
information O
. O

Several O
ex- O
periments O
on O
speech O
- O
to O
- O
text O
translation O
tasks O
show O
the O
effectiveness O
of O
proposed O
approach O
on O
obtain- O
ing O
superior O
quality O
- O
latency O
trade O
- O
offs O
, O
compared O
to O
the O
state O
- O
of O
- O
the O
- O
art O
monotonic O
multihead O
attention O
. O

References O
Naveen O
Arivazhagan O
, O
Colin O
Cherry O
, O
Wolfgang O
Macherey O
, O
Chung O
- O
Cheng O
Chiu O
, O
Semih O
Yavuz O
, O
Ruom- O
ing O
Pang O
, O
Wei O
Li O
, O
and O
Colin O
Raffel O
. O

2019 O
. O

Monotonic O
infinite O
lookback O
attention O
for O
simultaneous O
machine O
translation O
. O

In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
1313–1323 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O

Alexis O
Conneau O
, O
Kartikay O
Khandelwal O
, O
Naman O
Goyal O
, O
Vishrav O
Chaudhary O
, O
Guillaume O
Wenzek O
, O
Francisco O
Guzmán O
, O
Edouard O
Grave O
, O
Myle O
Ott O
, O
Luke O
Zettle- O
moyer O
, O
and O
Veselin O
Stoyanov O
. O

2019 O
. O

Unsupervised O
cross O
- O
lingual O
representation O
learning O
at O
scale O
. O

arXiv O
preprint O
arXiv:1911.02116 O
. O

Fahim O
Dalvi O
, O
Nadir O
Durrani O
, O
Hassan O
Sajjad O
, O
and O
Stephan O
V O
ogel O
. O

2018 O
. O

Incremental O
decoding O
and O
training O
methods O
for O
simultaneous O
translation O
in O
neural O
ma- O
chine O
translation O
. O

In O
Proceedings O
of O
the O
2018 O
Con- O
ference O
of O
the O
North O
American O
Chapter O
of O
the O
Asso- O
ciation O
for O
Computational O
Linguistics O
: O
Human O
Lan- O
guage O
Technologies O
, O
Volume O
2 O
( O
Short O
Papers O
) O
, O
pages O
493–499 O
, O
New O
Orleans O
, O
Louisiana O
. O
Association O
for O
Computational O
Linguistics O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
under- O
standing O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Tech- O
nologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
, O
Minneapolis O
, O
Minnesota O
. O

Association O
for O
Computational O
Linguistics O
. O

Mattia O
A. O
Di O
Gangi O
, O
Roldano O
Cattoni O
, O
Luisa O
Bentivogli O
, O
Matteo O
Negri O
, O
and O
Marco O
Turchi O
. O

2019 O
. O

MuST O
- O
C O
: O
a O
Multilingual O
Speech O
Translation O
Corpus O
. O

In O
Proceed- O
ings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
2012–2017 O
, O
Min- O
neapolis O
, O
Minnesota O
. O

Association O
for O
Computational O
Linguistics O
. O

Caglar O
Gulcehre O
, O
Orhan O
Firat O
, O
Kelvin O
Xu O
, O
Kyunghyun O
Cho O
, O
Loic O
Barrault O
, O
Huei O
- O
Chi O
Lin O
, O
Fethi O
Bougares O
, O
Holger O
Schwenk O
, O
and O
Yoshua O
Bengio O
. O
2015 O
. O

On O
using O
monolingual O
corpora O
in O
neural O
machine O
trans- O
lation O
. O

Caglar O
Gulcehre O
, O
Orhan O
Firat O
, O
Kelvin O
Xu O
, O
Kyunghyun O
Cho O
, O
and O
Yoshua O
Bengio O
. O
2017 O
. O

On O
integrating O
a O
lan- O
guage O
model O
into O
neural O
machine O
translation O
. O

Com- O
puter O
Speech O
and O
Language O
, O
45:137–148 O
. O

Urvashi O
Khandelwal O
, O
Angela O
Fan O
, O
Dan O
Jurafsky O
, O
Luke O
Zettlemoyer O
, O
and O
Mike O
Lewis O
. O

2020 O
. O

Nearest O
neighbor O
machine O
translation O
. O

arXiv O
preprint O
arXiv:2010.00710 O
. O

Sosuke O
Kobayashi O
. O

2018 O
. O

Contextual O
augmentation O
: O
Data O
augmentation O
by O
words O
with O
paradigmatic O
re- O
lations O
. O

In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Tech- O
nologies O
, O
Volume O
2 O
( O
Short O
Papers O
) O
, O
pages O
452–457 O
, O
New O
Orleans O
, O
Louisiana O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Taku O
Kudo O
and O
John O
Richardson O
. O

2018 O
. O

Sentencepiece O
: O

A O
simple O
and O
language O
independent O
subword O
tok- O
enizer O
and O
detokenizer O
for O
neural O
text O
processing O
. O

arXiv O
preprint O
arXiv:1808.06226 O
. O

Mingbo O
Ma O
, O
Liang O
Huang O
, O
Hao O
Xiong O
, O
Renjie O
Zheng O
, O
Kaibo O
Liu O
, O
Baigong O
Zheng O
, O
Chuanqiang O
Zhang O
, O
Zhongjun O
He O
, O
Hairong O
Liu O
, O
Xing O
Li O
, O
Hua O
Wu O
, O
and O
Haifeng O
Wang O
. O
2019a O
. O

Stacl O
: O

Simultaneous O
trans- O
lation O
with O
implicit O
anticipation O
and O
controllable O
la- O
tency O
using O
prefix O
- O
to O
- O
prefix O
framework O
. O

Mingbo O
Ma O
, O
Liang O
Huang O
, O
Hao O
Xiong O
, O
Renjie O
Zheng O
, O
Kaibo O
Liu O
, O
Baigong O
Zheng O
, O
Chuanqiang O
Zhang O
, O
Zhongjun O
He O
, O
Hairong O
Liu O
, O
Xing O
Li O
, O
Hua O
Wu O
, O
and O
Haifeng O
Wang O
. O

2019b O
. O

STACL O
: O

Simultaneous O
trans- O
lation O
with O
implicit O
anticipation O
and O
controllable O
la- O
tency O
using O
prefix O
- O
to O
- O
prefix O
framework O
. O

In O
Proceed- O
ings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
3025–3036 O
, O
Flo- O
rence O
, O
Italy O
. O

Association O
for O
Computational O
Linguis- O
tics O
. O

Xutai O
Ma O
, O
Juan O
Pino O
, O
James O
Cross O
, O
Liezl O
Puzon O
, O
and O
Jiatao O
Gu O
. O
2019c O
. O

Monotonic O
multihead O
attention O
. O

Xutai O
Ma O
, O
Juan O
Pino O
, O
and O
Philipp O
Koehn O
. O
2020 O
. O

Simulmt O
to O
simulst O
: O
Adapting O
simultaneous O
text O
translation O
to O
end O
- O
to O
- O
end O
simultaneous O
speech O
trans- O
lation O
. O

arXiv O
preprint O
arXiv:2011.02048 O
. O

Matt O
Post O
. O
2018 O
. O

A O
call O
for O
clarity O
in O
reporting O
bleu O
scores O
. O

arXiv O
preprint O
arXiv:1804.08771 O
. O

Colin O
Raffel O
, O
Minh O
- O
Thang O
Luong O
, O
Peter O
J. O
Liu O
, O
Ron O
J. O
Weiss O
, O
and O
Douglas O
Eck O
. O
2017 O
. O

Online O
and O
linear- O
time O
attention O
by O
enforcing O
monotonic O
alignments O
. O

InProceedings O
of O
the O
34th O
International O
Conference O
on O
Machine O
Learning O
, O
volume O
70 O
of O
Proceedings O
of O
Machine O
Learning O
Research O
, O
pages O
2837–2846 O
. O
PMLR O
. O

Felix O
Stahlberg O
, O
James O
Cross O
, O
and O
Veselin O
Stoyanov O
. O

2018 O
. O

Simple O
fusion O
: O
Return O
of O
the O
language O
model O
. O

InProceedings O
of O
the O
Third O
Conference O
on O
Machine O
Translation O
: O
Research O
Papers O
, O
pages O
204–211 O
, O
Brus- O
sels O
, O
Belgium O
. O

Association O
for O
Computational O
Lin- O
guistics.42 O

Sonia O
Vandepitte O
. O

2001 O
. O

Anticipation O
in O
conference O
interpreting O
: O
a O
cognitive O
process O
. O

Alicante O
Journal O
of O
English O
Studies O
/ O
Revista O
Alicantina O
de O
Estudios O
Ingleses O
, O
0(14):323–335 O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
Ł O
ukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
Advances O
in O
Neural O
Information O
Pro- O

cessing O
Systems O
30 O
, O
pages O
5998–6008 O
. O

Curran O
Asso- O
ciates O
, O

Inc. O

Xueqing O
Wu O
, O
Yingce O
Xia O
, O
Lijun O
Wu O
, O
Shufang O
Xie O
, O
Weiqing O
Liu O
, O
Jiang O
Bian O
, O
Tao O
Qin O
, O
and O
Tie O
- O
Yan O
Liu O
. O
2020 O
. O

Learn O
to O
use O
future O
information O
in O
simultane- O
ous O
translation O
. O

A O
LM O
at O
MMA O
Output O
Layer O
We O
explored O
a O
naive O
approach O
of O
integrating O
LM O
information O
into O
the O
MMA O
. O

In O
this O
approach O
, O
we O
in- O
tegrate O
the O
future O
information O
obtained O
from O
the O
LM O
directly O
into O
the O
output O
layer O
of O
the O
MMA O
model O
. O

We O
refer O
to O
this O
experiment O
as O
‘ O
LM O
Rescor- O
ing(LMR O
) O
’ O
, O
and O
the O
corresponding O
model O
is O
called O
MMA O
- O
LMR O
. O

As O
observed O
in O
Figure O
5 O
, O
MMA O
- O
LMR O
has O
infe- O
rior O
performance O
compared O
to O
the O
MMA O
model O
. O

Since O
the O
LM O
information O
integration O
is O
only O
done O
at O
the O
output O
layer O
of O
the O
model O
, O
the O
MMA O
model O
can O
not O
easily O
discard O
the O
incorrect O
information O
from O
LM O
. O

This O
motivates O
us O
to O
tightly O
integrate O
the O
LM O
information O
into O
the O
simultaneous O
model O
. O

B O
Language O
Models O
As O
mentioned O
earlier O
, O
we O
train O
two O
different O
lan- O
guage O
models O
( O
LMs O
) O
and O
use O
them O
to O
improve O
the O
anticipation O
in O
monotonic O
attention O
based O
Simulta- O
neous O
models O
. O

B.1 O
XLM O
- O
Roberta(XLM O
- O
R O
) O
XLM O
- O
R O
Large O
model2was O
trained O
on O
the O
100 O
lan- O
guages O
CommonCrawl O
corpora O
total O
size O
of O
2.5 O
TB O
with O
550 O
M O
parameters O
from O
24 O
layers O
, O
1024 O
hid- O
den O
states O
, O
4096 O
feed O
- O
forward O
hidden O
- O
states O
, O
and O
16 O
heads O
. O

Total O
number O
of O
parameters O
is O
558M. O
We O
finetune O
the O
head O
of O
the O
XLM O
- O
R O
LM O
model O
using O
the O
Masked O
Language O
Modeling O
objective O
which O
accounts O
for O
0.23 O
% O
of O
the O
total O
model O
parameters O
, O
i.e. O
, O
1.3 O
M O
parameters O
. O

B.2 O
Smaller O
Language O
Model O
Since O
the O
LM O
predictions O
are O
computed O
serially O
during O
inference O
, O
the O
time O
taken O
to O
compute O
the O
2https://huggingface.co/xlm-roberta-largeLM O
token O
serves O
as O
a O
bottleneck O
to O
the O
latency O
re- O
quirements O
. O

To O
reduce O
the O
LM O
computation O
time O
, O
we O
train O
a O
smaller O
Language O
Model O
( O
SLM O
) O
from O
scratch O
using O
the O
Causal O
Language O
Modeling O
ob- O
jective O
. O

SLM O
is O
composed O
of O
6 O
Transformer O
de- O
coder O
blocks O
, O
512 O
hidden O
- O
states O
, O
2048 O
feed O
- O
forward O
hidden O
- O
states O
& O
8 O
attention O
heads O
. O

It O
alleviates O
the O
need O
for O
the O
sub O
- O
token O
summary O
layer O
since O
it O
shares O
the O
vocabulary O
and O
tokenization O
with O
the O
MMA O
models O
. O

The O
train O
examples O
are O
at O
the O
sen- O
tence O
level O
, O
rather O
than O
forming O
a O
block O
out O
of O
multi- O
ple O
sentences(which O
is O
the O
usual O
case O
for O
Language O
Models O
) O
. O

Since O
the O
target O
texts O
contain O
lesser O
than O
250k O
examples O
, O
we O
use O
additional O
data O
augmentation O
techniques O
to O
upsample O
the O
target O
data O
. O

We O
also O
use O
additional O
data O
to O
avoid O
overfitting O
on O
the O
MuST O
- O
C O
target O
text O
. O

Details O
have O
been O
provided O
in O
B.2.1 O
. O

B.2.1 O
Data O
Augmentation O
Up O
- O
Sampling O
: O
To O
boost O
the O
LM O
performance O
and O
mitigate O
overfitting O
, O
we O
use O
contextual O
data O
augmentation O
( O
Kobayashi O
, O
2018 O
) O
to O
upsample O
the O
MuST O
- O
C O
target O
text O
data O
by O
substituting O
and O
insert- O
ing O
words O
based O
on O
LM O
predictions O
. O

We O
use O
the O
NLPAUG3package O
to O
get O
similar O
words O
based O
on O
contextual O
embeddings O
. O

From O
the O
Hugging O
Face O
Repository O
, O
we O
use O
two O
different O
pretrained O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
models O
for O
German O
bert O
- O
base- O
german O
- O
dbmdz O
- O
cased O
& O
bert O
- O
base O
- O
german O
- O
dbmdz- O
uncased O
andbert O
- O
base O
- O
fr O
- O
cased O
for O
French O
. O

We O
upsample O
German O
to O
1.13 O
M O
examples O
and O
French O
to O
1.38 O
M O
examples O
. O

Additional O
Data O
: O
We O
also O
use O
additional O
data O
to O
avoid O
overfitting O
. O

For O
German O
we O
use O
the O
Newscrawl(WMT O
19 O
) O
data O
which O
includes O
58 O
M O
examples O
. O

For O
French O
, O
we O
use O
Common O
Crawl O
and O
Europarl O
to O
augment O
4 O
M O
extra O
training O
examples O
. O

We O
observe O
that O
both O
upsampling O
and O
data O
aug- O
mentation O
help O
us O
to O
reduce O
the O
overfitting O
on O
the O
MuST O
- O
C O
dev O
set O
. O

B.3 O
Token O
Prediction O
For O
each O
output O
token O
, O
the O
LM O
prediction O
is O
ob- O
tained O
by O
feeding O
the O
prefix O
upto O
that O
token O
to O
the O
LM O
model O
. O

These O
predictions O
are O
pre O
- O
computed O
for O
training O
and O
validation O
sets O
. O

This O
ensures O
par- O
allelization O
and O
avoids O
the O
overhead O
to O
run O
the O
LM O
simultaneously O
during O
the O
training O
process O
. O

During O
3https://pypi.org/project/nlpaug/43 O

( O
a O
) O
EnDe O
Task O
  O
( O
b O
) O
EnFr O
Task O
Figure O
5 O
: O
BLEU O
vs O
Average O
Lagging O
results O
for O
MMA O
and O
MMA O
- O
LMR O
models O
. O

Each O
model O
is O
trained O
with O
different O
λ= O
0.1,0.05,0.01values O
. O

Each O
BLUE O
- O
AL O
point O
obtained O
by O
varying O
step O
size O
and O
λ O
. O
inference O
, O
the O
LM O
model O
is O
called O
every O
time O
a O
new O
output O
token O
is O
written O
. O

C O
Dataset O

The O
MuST O
- O
C O
dataset O
comprises O
of O
English O
TED O
talks O
, O
the O
translations O
and O
transcriptions O
have O
been O
aligned O
with O
the O
speech O
at O
sentence O
level O
. O

Dataset O
statistics O
have O
been O
provided O
in O
the O
Table O
1 O
. O

D O
Effect O
of O
LM O
Size O
on O
Latency O
- O
Quality O
We O
train O
several O
SLM O
models O
with O
varying O
sizes O
in O
our O
experiments O
and O
choose O
the O
best O
model O
based O
on O
the O
top-1 O
accuracy O
. O

As O
we O
increase O
the O
number O
of O
layers O
in O
the O
LM O
model O
from O
2 O
to O
4 O
to O
6 O
layers O
, O
the O
SLM O
and O
the O
proposed O
MMA O
with O
future O
infor- O
mation O
models O
have O
shown O
performance O
improve- O
ments O
. O

However O
, O
increasing O
the O
number O
of O
layers O
greater O
than O
6 O
does O
not O
yield O
any O
performance O
im- O
provements O
. O

We O
also O
notice O
this O
degradation O
of O
performance O
with O
the O
XLM O
model O
while O
varying O
the O
number O
of O
hidden O
layers O
in O
the O
LM O
head O
. O

E O
Training O
Details O

We O
follow O
the O
training O
process O
similar O
to O
Ma O
et O
al O
. O

( O
2020 O
) O
training O
process O
. O

We O
train O
an O
English O
ASR O
model O
using O
the O
source O
speech O
data O
. O

Next O
, O
we O
train O
a O
simultaneous O
model O
without O
the O
latency O
loss O
( O
setting O
λlatency O
= O
0 O
) O
after O
initializing O
the O
encoder O
from O
the O
English O
ASR O
model O
. O

After O
this O
step O
, O
we O
finetune O
the O
simultaneous O
model O
for O
different O
λs O
. O

This O
training O
process O
is O
repeated O
for O
all O
the O
reportedmodels O
and O
for O
each O
task O
. O

The O
details O
regarding O
the O
hyperparameters O
for O
the O
model O
have O
been O
provided O
in O
Table O
2 O
. O
F O
BLEU O
- O
AL O
Numbers O
As O
mentioned O
in O
the O
results O
section O
of O
the O
main O
pa- O
per O
, O
we O
vary O
the O
latency O
weight O
hyperparameter O
( O
λ O
) O
to O
train O
different O
models O
to O
obtain O
different O
latency O
regimes O
. O

We O
also O
vary O
the O
step O
- O
size O
/ O
speech O
seg- O
ment O
size O
during O
inference O
. O

In O
total O
, O
we O
obtain O
18 O
different O
data O
points O
corresponding O
to O
each O
model O
. O

In O
Table O
3 O
, O
we O
compare O
the O
results O
obtained O
using O
MMA O
, O
MMA O
- O
XLM O
and O
MMA O
- O
SLM O
under O
similar O
hyperparameter O
settings O
. O

It O
will O
help O
the O
reader O
to O
quantify O
the O
benefits O
obtained O
from O
our O
proposed O
approach.44 O

Task O
# O
Hours O
# O
Sentences O
# O
Talks O
# O
Words O
Train O
Dev O
Test O
Source O
Target O
English O
- O
German O
408 O
225k O
1,423 O
2,641 O
2,093 O
4.3 O
M O
4 O
M O
English O
- O
French O
492 O
269k O
1,412 O
2,632 O
2,510 O
5.2 O
M O
5.4 O
M O
Table O
1 O
: O
Dataset O
Statistics O
( O
# O
- O
Number O
of O
) O

MMA O
MMA O
- O
XLM O
/ O
CLMHyperparameter O
encoder O
layers O
12 O
12 O
encoder O
embed O
dim O
292 O
256 O
encoder O
ffn O
embed O
dim O
2048 O
2048 O
encoder O
attention O
heads O
4 O
4 O
decoder O
layers O
6 O
6 O
decoder O
embed O
dim O
292 O
256 O
decoder O
ffn O
embed O
dim O
2048 O
2048 O
monotonic O
ffn O
embed O
dim O
– O
2048 O
decoder O
attention O
heads O
4 O
4 O
dropout O
0.1 O
0.1 O
optimizer O
adam O
adam O
adam- O
β O
( O
0.9 O
, O
0.999 O
) O
( O
0.9 O
, O
0.999 O
) O
clip O
- O
norm O
10.0 O
10.0 O
lr O
scheduler O
inverse O
sqrt O
inverse O
sqrt O
learning O
rate O
0.0001 O
0.0001 O
warmup O
- O
updates O
4000 O
4000 O
label O
- O
smoothing O
0.0 O
0.0 O
max O
tokens O
40000 O
40000 O
conv O
layers O
2 O
2 O
conv O
stride O
( O
2,2 O
) O
( O
2,2 O
) O
# O
params O
≈39 O
M O
≈39 O
M O
Table O
2 O
: O
Model O
Hyperparameters O
λ O
Modelstep O
size O
( O
AL(msec O
) O
/ O
BLEU O
) O
120 O
200 O
280 O
360 O
440 O
520 O
English O
- O
German O
Task O
0.1MMA O
378 O
/ O
3.54 O
887 O
/ O
7.17 O
1317 O
/ O
9.72 O
1671 O
/ O
11.54 O
1935 O
/ O
12.95 O
2376 O
/ O
13.98 O
MMA O
- O
XLM O
348 O
/ O
5.03 O
848 O
/ O
8.77 O
1269 O
/ O
10.4 O
1631 O
/ O
12.78 O
1961 O
/ O
14.22 O
2272 O
/ O
15.34 O

MMA O
- O
SLM O
748 O
/ O
8.83 O
1192 O
/ O
10.43 O
1566 O
/ O
12.43 O
1857 O
/ O
13.82 O
2156 O
/ O
14.29 O
2421 O
/ O
15.44 O
0.05MMA O
775 O
/ O
6.5 O
1220 O
/ O
10.08 O
1683 O
/ O
11.72 O
1891 O
/ O
12.92 O
2484 O
/ O
13.85 O
2441 O
/ O
14.51 O
MMA O
- O
XLM O
766 O
/ O
7.76 O
1200 O
/ O
10.84 O
1654 O
/ O
13.2 O
1873 O
/ O
13.72 O
2655 O
/ O
16.36 O
2456 O
/ O
15.90 O
MMA O
- O
SLM O
1250 O
/ O
12.12 O
1588 O
/ O
13.53 O
1899 O
/ O
14.68 O
2171 O
/ O
15.18 O
2424 O
/ O
15.72 O
2665 O
/ O
15.99 O
0.01MMA O
1841 O
/ O
13.33 O
2183 O
/ O
14.24 O
2455 O
/ O
14.58 O
2683 O
/ O
15.11 O
2839 O
/ O
16.05 O
3079 O
/ O
16.18 O
MMA O
- O
XLM O
1846 O
/ O
14.83 O
2167 O
/ O
15.57 O
2439 O
/ O
16.13 O
2662 O
/ O
16.21 O
2855 O
/ O
16.87 O
3085 O
/ O
17.24 O
MMA O
- O
SLM O
2047 O
/ O
14.43 O
2039 O
/ O
15.33 O
2420 O
/ O
16.87 O
2503 O
/ O
16.87 O
2786 O
/ O
17.01 O
2871 O
/ O
17.35 O
English O
- O
French O
Task O
0.1MMA O
471 O
/ O
6.77 O
924 O
/ O
13.2 O
1299 O
/ O
17.37 O
1667 O
/ O
19.78 O
1975 O
/ O
20.77 O
2269 O
/ O
22.28 O
MMA O
- O
XLM O
478 O
/ O
7.54 O
902 O
/ O
14.48 O
1284 O
/ O
17.74 O
1607 O
/ O
21.02 O
1972 O
/ O
22.17 O
2251 O
/ O
22.79 O
MMA O
- O
SLM O
460 O
/ O
7.89 O
701 O
/ O
14.86 O
1313 O
/ O
18.40 O
1604 O
/ O
21.23 O
1971 O
/ O
22.50 O
2211 O
/ O
22.91 O
0.05MMA O
806 O
/ O
12.9 O
1209 O
/ O
18.13 O
1533 O
/ O
20.57 O
1825 O
/ O
22.30 O
2137 O
/ O
22.95 O
2390 O
/ O
23.98 O
MMA O
- O
XLM O
796 O
/ O
14.06 O
1184 O
/ O
19.19 O
1512 O
/ O
21.25 O
1807 O
/ O
23.17 O
2117 O
/ O

23.75 O
2363 O
/ O
24.78 O

MMA O
- O
SLM O
794 O
/ O
14.20 O
1197 O
/ O
19.79 O
1504 O
/ O
21.44 O
1878 O
/ O
23.5 O
2122 O
/ O
24.27 O
2341 O
/ O
24.96 O
0.01MMA O
1728 O
/ O
22.84 O
1725 O
/ O
23.6 O
2204 O
/ O
25.07 O
2416 O
/ O
25.44 O
2632 O
/ O
25.56 O
2824 O
/ O
25.81 O
MMA O
- O
XLM O
1713 O
/ O
23.11 O
1701 O
/ O
24.89 O
2116 O
/ O
26.24 O
2420 O
/ O
26.19 O
2631 O
/ O
26.07 O
2796 O
/ O
26.17 O
MMA O
- O
SLM O
1725 O
/ O
23.33 O
1704 O
/ O
25.16 O
2217 O
/ O
26.54 O
2412 O
/ O
26.63 O
2631 O
/ O

26.57 O
2812 O
/ O
26.55 O
Table O
3 O
: O
BLEU O
vs O
Average O
Lagging O
results O
for O
MMA O
, O
MMA O
- O
XLM O
and O
MMA O
- O
SLM O
models O
on O
English O
- O
German O
and O
English O
- O
French O
tasks O
. O

The O
models O
are O
trained O
using O
different O
latency O
loss O
weights O
( O
λ= O
0.1,0.05,0.01).45 O

