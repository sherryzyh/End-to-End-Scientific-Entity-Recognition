The O
sentence O
means O
that O
Simultaneous B-TaskName
Translation I-TaskName
is O
better O
than O
Language B-MethodName
Model I-MethodName
Augmented I-MethodName
Monotonic I-MethodName
Attention I-MethodName
. O

The O
most O
advanced O
and O
up O
- O
to O
- O
date O
policies O
for O
simul- B-MethodName
taneous I-MethodName
neural I-MethodName
machine I-MethodName
translation I-MethodName
( O
SNMT B-MethodName
) O
utilize O
monotonic B-MethodName
attention I-MethodName
to O
carry O
out O
read B-TaskName
/ I-TaskName
write I-TaskName
decisions I-TaskName
based O
on O
the O
partial O
source O
and O
target O
sequences O
. O

If O
there O
is O
insufficient O
information O
, O
the O
monotonic B-MethodName
attention I-MethodName
might O
make O
poor O
read B-TaskName
/ I-TaskName
write I-TaskName
decisions I-TaskName
decisions O
, O
which O
would O
then O
have O
a O
negative O
effect O
on O
the O
performance O
of O
the O
SNMT B-MethodName
model O
. O

Human O
translators O
are O
better O
than O
machines O
at O
read B-TaskName
/ I-TaskName
write I-TaskName
decisions I-TaskName
because O
they O
can O
use O
linguistic O
information O
and O
domain O
knowledge O
to O
anticipate O
the O
next O
few O
words O
. O

In O
this O
work O
, O
we O
propose O
a O
framework O
to O
improve O
monotonic B-MethodName
attention I-MethodName
's O
decision O
- O
making O
by O
incorporating O
an O
external O
language O
model O
. O

Experiments O
conducted O
on O
English O
- O
German O
and O
English O
- O
French O
tasks O
show O
that O
future O
information O
from O
language O
model O
improves O
the O
state O
- O
of O
- O
the O
- O
art O
monotonic O
multi O
- O
head O
attention O
model O
even O
further O
. O

1 O
Introduction O
A O
typical O
application O
of O
simultaneous B-MethodName
neural I-MethodName
ma- I-MethodName
chine I-MethodName
translation I-MethodName
( O
SNMT B-MethodName
) O
is O
to O
use O
it O
for O
conversational B-TaskName
speech I-TaskName
or O
live B-TaskName
video I-TaskName
caption I-TaskName
translation I-TaskName
. O

To O
achieve O
live O
translation O
, O
the O
SNMT B-MethodName
model O
alternates O
between O
performing O
read B-TaskName
from O
the O
source O
sequence O
and O
write B-TaskName
to O
the O
target O
sequence O
. O

A O
model O
can O
use O
a O
fixed O
or O
adaptive O
read O
/ O
write O
policy O
to O
decide O
whether O
to O
read B-TaskName
orwrite B-TaskName
at O
a O
certain O
moment O
. O

We O
hope O
to O
augment B-TaskName
monotonic I-TaskName
attention I-TaskName
with O
future O
informa- O
tion O
using O
language B-MethodName
models I-MethodName
( O
LM B-MethodName
) O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Conneau O
et O
al O
. O
, O
2019 O
) O
, O
similar O
to O
how O
human O
translation O
experts O
operate O
. O

We O
explored O
integrating O
future O
information O
from O
LM B-MethodName
directly O
into O
the O
output O
layer O
of O
the O
MMA B-MethodName
model O
to O
utilize O
explicit O
future O
information O
. O

However O
, O
because O
it O
did O
not O
provide O
any O
improvements O
( O
refer O
to O
Appendix O
A O
) O
, O
we O
were O
motivated O
to O
explore O
a O
tighter O
integration O
of O
the O
LM B-MethodName
information O
into O
the O
SNMT B-MethodName
model O
. O

In O
this O
work O
, O
we O
use O
plausible O
future O
information O
from O
LM B-MethodName
during O
training O
by O
transforming O
the O
monotonic O
attention O
mechanism O
. O

As O
shown O
in O
Figure O
1 O
, O
the O
LM B-MethodName
predicts O
the O
probable O
future O
information O
at O
each O
step O
using O
the O
prefix O
target O
( O
and O
source O
, O
for O
cross O
- O
lingual O
LM B-MethodName
) O
sequence O
. O

We O
believe O
that O
providing O
additional O
information O
can O
improve O
the O
MMA B-MethodName
model O
's O
ability O
to O
read O
and O
write O
, O
which O
would O
eventually O
lead O
to O
better O
translation O
with O
less O
delay O
. O

Several O
experiments O
have O
been O
conducted O
on O
MuST-C B-DatasetName
( O
Di O
Gangi O
et O
al O
. O
, O
2019 O
) O
. O

Our O
proposed O
approach O
for O
German O
and O
English O
tasks O
shows O
clear O
improvements O
in O
latency O
- O
quality O
trade O
- O
offs O
over O
the O
state O
- O
of O
- O
the O
- O
art O
MMA B-MethodName
models O
. O

In O
the O
simultaneous B-MethodName
machine I-MethodName
translation I-MethodName
( O
SNMT B-MethodName
) O
models O
, O
the O
probability O
of O
predicting O
the O
target O
token O
yi∈ydepends O
only O
on O
the O
partial O
source O
and O
target O
sequences O
( O
x≤j∈x O
, O
y O
< O
i∈y O
) O
that O
come O
before O
it O
, O
not O
on O
any O
subsequent O
information O
. O

In O
the O
sequence O
- O
to O
- O
sequence O
based O
SNMT B-MethodName
model O
, O
each O
target O
token O
yi O
is O
generated O
as O
follows O
: O

6 O
) O


 O
In O
monotonic O
attention O
based O
SNMT B-MethodName
, O
the O
context O
vector O
is O
computed O
as O
follows O
: O
ei O
, O
j O
= O
MonotonicEnergy(si-1 O
, O
hj O
) O
( O
4 O
) O
pi O
, O
j O
= O
Sigmoid(ei O
, O
j O
) O
( O
5 O
) O
zi O
, O
j O
~ O
Bernoulli(pi O
, O
j O
) O
( O
6 O
) O

Monotonic O
attention O
was O
adopted O
into O
SNMT B-MethodName
and O
later O
by O
Ma O
et O
al O
. O

The O
Transformer O
model O
can O
be O
extended O
to O
include O
MMA B-MethodName
. O

2.2 O
is O
a O
number O
, O
ABCDEFG O
is O
a O
string O
of O
letters O
, O
and O
0 O
is O
a O
number O
. O

Future B-HyperparameterName
Representation I-HyperparameterName
Layer I-HyperparameterName
: O
. O


 O
There O
are O
seven O
letters O
in O
the O
alphabet O
. O

Since O
the O
token O
ˆyicomes O
from O
a O
different O
tokenizer O
and O
vocabulary O
set O
, O
it O
might O
be O
split O
into O
multiple O
sub O
- O
tokens O
{ O
ˆy1i,ˆy2i, O
... O
,ˆymi O
} O
when O
applying O
the O
model O
's O
tokenizer O
and O
vocabulary O
. O

In O
order O
to O
obtain O
a O
single O
future O
token O
representation O
˜si∈ O
Rd O
from O
all O
of O
the O
sub O
- O
tokens O
, O
we O
apply O
a O
sub B-HyperparameterName
- I-HyperparameterName
token I-HyperparameterName
summary I-HyperparameterName
layer I-HyperparameterName
: O
. O

The O
sentence O
means O
that O
˜si O
is O
a O
representation O
of O
the O
input O
sequence O
that O
is O
generated O
by O
Γ O
, O
where O
Γ O
is O
a O
function O
that O
takes O
as O
input O
a O
sequence O
of O
sub O
- O
token O
representations O
( O
{ O
ˆy1i,ˆy2i, O
... O
,ˆymi O
} O
) O
. O

Despite O
the O
fact O
that O
we O
can O
add O
the O
plau- O
sible O
future O
information O
to O
the O
output O
layer O
( O
Ap- O
pendix O
A O
) O
or O
append O
it O
to O
the O
target O
token O
represen- O
tation O
( O
Appendix O
B O
) O
, O
we O
choose O
not O
to O
do O
so O
. O

In O
the O
equation O
, O
the O
read O
/ O
write O
decisions O
for O
MMA B-MethodName
happen O
in O
yi-1 O
. O

We O
compute O
the O
monotonic O
energy O
for O
future O
information O
using O
the O
enriched O
future O
token O
representation O
available O
at O
each O
layer O
. O
This O
allows O
us O
to O
integrate O
the O
future O
monotonic O
energy O
function O
into O
the O
existing O
model O
, O
which O
then O
predicts O
the O
probability O
of O
each O
token O
in O
the O
sequence O
. O

The O
model O
can O
use O
the O
input O
sequence O
to O
predict O
the O
output O
sequence O
. O

The O
model O
can O
control O
the O
weight O
given O
to O
the O
LM B-MethodName
output O
by O
varying O
the O
˜ei O
, O
j. O
In O
case O
of O
insufficient O
source O
information O
in O
the O
low O
latency O
regime O
, O
we O
expect O
the O
model O
to O
rely O
more O
on O
˜ei O
, O
j. O

After O
predicting O
the O
first O
target O
token O
, O
for O
every O
subsequent O
prediction O
of O
a O
target O
token O
, O
we O
invoke O
the O
LM B-MethodName
to O
predict O
the O
next O
plausible O
future O
token O
and O
integrate O
this O
new O
information O
into O
our O
equation O
. O

We O
conduct O
our O
experiments O
on O
the O
English O
- O
German O
and O
English O
- O
French O
task O
using O
the O
datasets O
and O
metrics O
MuST-C B-DatasetName
, O
speech B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
, O
and O
text B-TaskName
( I-TaskName
ST I-TaskName
) I-TaskName
translation I-TaskName
. O

The O
speech O
sequence O
is O
represented O
using O
a O
log O
- O
mel O
filter O
bank O
, O
with O
each O
letter O
representing O
a O
different O
filter O
. O

The O
target O
sequence O
is O
represented O
as O
sub O
- O
words O
using O
a O
SentencePiece O
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
model O
with O
a O
unigram O
. O

Our O
latency O
metric O
is O
Average B-MetricName
Lagging I-MetricName
( O
AL B-MetricName
) O
and O
our O
translation O
quality O
measure O
is O
case O
- O
sensitive O
detokenized O
SacreBLEU B-MetricName
( O
Post O
, O
2018 O
) O
, O
which O
is O
similar O
to O
( O
Ma O
et O
al O
. O
, O
2020 O
) O
. O

The O
best O
models O
are O
chosen O
based O
on O
the O
dev O
set O
results O
and O
reported O
results O
are O
from O
the O
test O
sets O
. O

We O
train O
our O
proposed O
modified O
MMA B-MethodName
model O
using O
two O
language O
models O
. O

We O
use O
the O
pretrained B-MethodName
XLM I-MethodName
- I-MethodName
RoBERTa I-MethodName
model O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
from O
the O
Huggingface O
Transformers O
model O
repository O
. O

We O
fine O
- O
tune O
the O
head O
of O
the O
model O
using O
the O
target O
text O
data O
for O
each O
task O
since O
the O
LM B-MethodName
output O
can O
be O
very O
open O
- O
ended O
and O
might O
not O
directly O
suit O
/ O
cater O
to O
our O
task O
and O
dataset O
. O

We O
also O
train O
a O
model O
( O
SLM B-MethodName
) O
, O
which O
contains O
three O
sub O
- O
networks O
: O
6 B-HyperparameterValue
, O
Transformer B-HyperparameterName
decoder I-HyperparameterName
layers I-HyperparameterName
, O
and O
512 B-HyperparameterValue
. O

We O
use O
the O
MuST-C B-DatasetName
data O
, O
as O
well O
as O
additional O
data O
augmentation O
, O
to O
reduce O
overfitting O
. O

The O
SLM B-MethodName
helps O
remove O
vocabulary O
mismatch O
issues O
as O
discussed O
in O
Section O
2.2 O
. O

We O
use O
a O
system O
of O
7 B-HyperparameterValue
, O
which O
means O
that O
the O
read O
/ O
write O
decisions O
are O
made O
after O
every O
seven O
encoder O
states O
. O

The O
hyperparameter O
corresponding O
to O
weighted B-HyperparameterName
average I-HyperparameterName
in O
MMA B-MethodName
is O
referred O
to O
as O
λ B-HyperparameterName
or O
λlatency B-HyperparameterName
. O

The O
value O
of O
this O
hyperparameter O
can O
be O
any O
value O
from O
the O
set O
{ O
0.01,0.05,0.1 B-HyperparameterValue
} O
. O

It O
simply O
finds O
the O
embedding O
for O
SLM B-MethodName
since O
it O
shares O
the O
same O
vocabulary O
set O
. O

All O
the O
models O
are O
trained O
on O
a O
NVIDIA O
v100 O
GPU O
with O
the O
parameters O
set O
to O
8 B-HyperparameterValue
. O

Even O
though O
future O
information O
can O
be O
integrated O
explicitly O
into O
the O
fixed O
policy O
approaches O
such O
as O
Wait O
- O
K O
( O
Ma O
et O
al O
. O
, O
2019b O
) O
, O
we O
choose O
monotonic B-MethodName
attention I-MethodName
as O
our O
baseline O
due O
to O
its O
superior O
performance O
( O
Arivazhagan O
et O
al O
. O
, O
2019 O
; O
Ma O
et O
al O
. O
, O
2019c O
) O
. O

The O
work O
, O
called O
the O
MMA B-MethodName
model O
, O
was O
published O
in O
2020 O
. O

The O
encoder O
for O
the O
MMA B-MethodName
model O
and O
the O
decoder B-HyperparameterName
embedding I-HyperparameterName
dimensions I-HyperparameterName
are O
set O
to O
392 B-HyperparameterValue
, O
which O
is O
similar O
to O
our O
proposed O
model O
's O
encoder O
being O
set O
to O
decoder B-HyperparameterName
embeddings I-HyperparameterName
, O
in O
order O
to O
have O
a O
fair O
comparison O
of O
parameters O
( O
≈39 O
M O
) O
. O

We O
train O
two O
models O
using O
the O
modified O
MMA B-MethodName
based O
on O
two O
LMs B-MethodName
( O
XLM B-MethodName
, O
SLM B-MethodName
) O
, O
referred O
as O
MMA B-MethodName
- I-MethodName
XLM I-MethodName
and O
MMA B-MethodName
- I-MethodName
SLM I-MethodName
. O

We O
first O
analyze O
how O
our O
model O
is O
utilizing O
the O
LM B-MethodName
predictions O
. O

We O
compared O
the O
norm O
of O
the O
monotonic O
energies O
corresponding O
to O
the O
predictions O
from O
the O
LM B-MethodName
to O
the O
predictions O
of O
the O
LM B-MethodName
. O

We O
define O
the O
LM B-MethodName
prediction O
weight O
as O
follows O
: O
LMpw=/parenleftbigg∥epred∥ O
∥eoutput∥/parenrightbigg O
( O
11 O
) O
In O
Figure O
3 O
, O
we O
plot O
the O
average O
variation O
of O
LMpw B-MetricName
vs. O
λ B-HyperparameterName
. O

We O
use O
two O
more O
values O
of O
λ B-HyperparameterName
that O
are O
either O
0.005 B-HyperparameterValue
or O
0.001 B-HyperparameterValue
to O
get O
this O
graph O
. O

The O
model O
starts O
giving O
more O
weight O
to O
predictions O
from O
LM B-MethodName
as O
latency O
requirements O
become O
stricter O
. O

This O
sentence O
demonstrates O
that O
the O
model O
is O
able O
to O
use O
information O
from O
LM B-MethodName
predictions O
that O
are O
based O
on O
latency O
requirements O
. O

We O
train O
separate O
models O
for O
different O
latency O
regimes O
by O
varying O
the O
λ B-HyperparameterName
. O

We O
vary O
the O
same O
set O
of O
hyperparameters O
for O
all O
three O
models O
: O
MMA B-MethodName
, O
MMA- B-MethodName
XLM I-MethodName
, O
and O
MMA B-MethodName
- I-MethodName
SLM I-MethodName
. O

The O
curves O
for O
all O
the O
models O
have O
been O
provided O
in O
Figure O
4 O
and O
the O
numbers O
for O
all O
models O
are O
included O
in O
Appendix O
F O
for O
reference O
. O

We O
vary O
the O
intervals O
between O
80ms B-HyperparameterValue
from O
120ms B-HyperparameterValue
to O
520ms B-HyperparameterValue
in O
order O
to O
get O
performances O
corresponding O
to O
different O
latency O
regimes O
. O

The O
XLM B-MethodName
and O
SLM B-MethodName
models O
provide O
a O
significant O
performance O
improvement O
over O
the O
baseline O
MMA B-MethodName
model O
. O

We O
see O
an O
improvement O
of O
1 O
- O
2 O
BLEU O
scores O
in O
all O
latency O
regimes O
( O
λ B-HyperparameterName
= O
0.1 B-HyperparameterValue
0.05 B-HyperparameterValue
0.01 B-HyperparameterValue
) O
. O

The O
language O
model O
SLM B-MethodName
performs O
slightly O
better O
than O
the O
language O
model O
XLM B-MethodName
. O

This O
is O
because O
SLM B-MethodName
does O
better O
on O
the O
next O
token O
prediction O
task O
than O
XLM B-MethodName
, O
30.15% B-MetricValue
for O
German O
and O
31.65% B-MetricValue
vs. O
18.45% B-MetricValue
for O
French O
. O

The O
high O
performance O
of O
SLM B-MethodName
is O
attributed O
to O
its O
training O
on O
in O
- O
domain O
data O
. O

Several O
experiments O
on O
speech B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
text I-TaskName
translation I-TaskName
tasks O
have O
shown O
that O
the O
proposed O
approach O
is O
more O
effective O
in O
obtaining O
superior O
quality O
- O
latency O
trade O
- O
offs O
than O
the O
state O
- O
of O
- O
the O
- O
art O
monotonic O
multihead O
attention O
. O

We O
explored O
a O
naive O
approach O
of O
integrating O
third O
- O
party O
information O
into O
the O
output O
layer O
. O

In O
this O
approach O
, O
we O
integrate O
future O
information O
obtained O
from O
the O
LM O
directly O
into O
the O
output O
layer O
of O
the O
MMA B-MethodName
model O
. O

We O
call O
this O
experiment O
' O
LM B-MethodName
Rescoring(LMR O
) O
' O
, O
and O
the O
model O
it O
uses O
is O
called O
MMA O
- O
LMR O
. O

MMA B-MethodName
- I-MethodName
LMR I-MethodName
has O
worse O
performance O
than O
the O
MMA B-MethodName
model O
according O
to O
Figure O
5 O
. O

Since O
the O
information O
integration O
is O
only O
done O
at O
the O
output O
layer O
of O
the O
model O
, O
the O
model O
can O
not O
easily O
discard O
the O
incorrect O
information O
from O
the O
input O
. O

The O
LM B-MethodName
information O
motivates O
us O
to O
tightly O
integrate O
it O
into O
the O
simultaneous O
model O
. O

We O
train O
two O
different O
language O
models O
, O
lan- B-MethodName
guage I-MethodName
models I-MethodName
and O
LMs B-MethodName
, O
and O
use O
them O
to O
improve O
the O
anticipation O
in O
Simulta- O
neous O
models O
based O
on O
monotonic B-MethodName
attention I-MethodName
. O

A O
large O
model O
was O
trained O
on O
100 O
language O
corpora O
totaling O
2.5 O
TB O
, O
with O
550 O
M O
parameters O
from O
24 O
layers O
, O
1024 O
hidden O
states O
, O
4096 O
feed O
- O
forward O
hidden O
states O
, O
and O
16 O
heads O
. O

We O
finetune O
the O
head O
of O
the O
XLM B-MethodName
- O
R O
LM B-MethodName
model O
using O
the O
Masked O
Language O
Modeling O
objective O
, O
which O
uses O
1.3 O
million O
parameters O
, O
or O
0.23 O
% O
of O
the O
total O
model O
parameters O
. O

Since O
the O
predictions O
are O
computed O
one O
after O
the O
other O
during O
inference O
, O
the O
time O
it O
takes O
to O
compute O
each O
prediction O
becomes O
a O
bottleneck O
to O
meeting O
the O
latency O
requirements O
. O

We O
train O
a O
smaller B-MethodName
Language I-MethodName
Model I-MethodName
( O
SLM B-MethodName
) O
from O
scratch O
using O
the O
Causal B-MethodName
Language I-MethodName
Modeling I-MethodName
objective O
to O
reduce O
the O
computation O
time O
for O
the O
LM B-MethodName
. O

SLM B-MethodName
is O
composed O
of O
6 B-HyperparameterValue
, O
Transformer B-HyperparameterName
decoder I-HyperparameterName
blocks I-HyperparameterName
, O
512 B-HyperparameterValue
, O
hidden B-HyperparameterName
- I-HyperparameterName
states I-HyperparameterName
, O
2048 B-HyperparameterValue
, O
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
hidden I-HyperparameterName
- I-HyperparameterName
states I-HyperparameterName
& O
8 B-HyperparameterValue
attention I-HyperparameterValue
heads I-HyperparameterValue
. O

Since O
it O
shares O
the O
vocabulary O
and O
tokenization O
with O
the O
MMA B-MethodName
models O
, O
the O
sub B-HyperparameterName
- I-HyperparameterName
token I-HyperparameterName
summary I-HyperparameterName
layer I-HyperparameterName
is O
no O
longer O
necessary O
. O

We O
use O
extra O
data O
to O
avoid O
fitting O
the O
target O
text O
too O
closely O
. O

To O
improve O
performance O
and O
prevent O
overfitting O
, O
we O
use O
contextual O
data O
augmentation O
( O
Kobayashi O
, O
2018 O
) O
to O
upsample O
the O
target O
text O
data O
by O
substituting O
and O
inserting O
words O
based O
on O
predictions O
. O

We O
use O
the O
Newscrawl B-DatasetName
( O
WMT19 B-DatasetName
) O
data O
, O
which O
consists O
of O
58 O
million O
examples O
, O
for O
German O
. O

To O
augment O
the O
4 O
M O
extra O
training O
examples O
for O
French O
, O
we O
use O
Common B-DatasetName
Crawl I-DatasetName
and I-DatasetName
Europarl I-DatasetName
. O

We O
find O
that O
both O
upsampling O
the O
data O
and O
augmenting O
it O
help O
to O
reduce O
overfitting O
on O
the O
MuST-C B-DatasetName
development O
set O
. O

For O
each O
output O
token O
, O
the O
prediction O
is O
obtained O
by O
feeding O
the O
prefix O
up O
to O
that O
token O
to O
the O
model O
. O

This O
ensures O
that O
the O
LM B-MethodName
are O
run O
in O
parallel O
during O
the O
training O
process O
, O
which O
avoids O
any O
overhead O
. O

The O
LM B-MethodName
model O
is O
called O
every O
time O
a O
new O
output O
token O
is O
written O
during O
inference O
. O

This O
dataset O
includes O
English O
TED O
talks O
, O
their O
translations O
, O
and O
transcriptions O
that O
have O
been O
aligned O
with O
the O
speech O
at O
the O
sentence O
level O
. O

We O
train O
several O
models O
of O
varying O
sizes O
for O
our O
experiments O
and O
choose O
the O
best O
model O
based O
on O
the O
accuracy B-MetricName
. O

As O
we O
increase O
the O
number O
of O
layers O
in O
the O
model O
, O
the O
proposed O
model O
with O
future O
information O
shows O
performance O
improvements O
. O

We O
also O
notice O
a O
decrease O
in O
performance O
with O
the O
XLM B-MethodName
model O
when O
we O
change O
the O
number O
of O
hidden O
layers O
in O
the O
LM B-MethodName
head O
. O

After O
initializing O
the O
encoder O
from O
the O
English O
ASR O
model O
, O
we O
train O
a O
simultaneous O
model O
without O
the O
latency O
loss O
( O
setting O
λlatency B-HyperparameterName
= O
0 B-HyperparameterValue
) O
. O

After O
this O
step O
, O
we O
fine O
- O
tune O
the O
simultaneous O
model O
for O
different O
λs B-HyperparameterName
. O

We O
obtain O
different O
latency O
regimes O
by O
training O
different O
models O
with O
different O
values O
for O
the O
hyperparameter O
λ B-HyperparameterName
. O

We O
also O
change O
the O
step B-HyperparameterName
- I-HyperparameterName
size I-HyperparameterName
/ I-HyperparameterName
speech I-HyperparameterName
seg- I-HyperparameterName
ment I-HyperparameterName
size I-HyperparameterName
during O
inference O
. O

We O
compare O
the O
results O
of O
MMA B-MethodName
, O
MMA B-MethodName
- I-MethodName
XLM I-MethodName
, O
and O
MMA B-MethodName
- I-MethodName
SLM I-MethodName
in O
Table O
3 O
, O
which O
were O
all O
obtained O
under O
similar O
hyperparameter O
settings O
. O

Our O
proposed O
approach O
provides O
quantifiable O
benefits O
to O
the O
reader O
. O
