AligNART B-MethodName
: O
Non B-MethodName
- I-MethodName
autoregressive I-MethodName
Neural I-MethodName
Machine I-MethodName
Translation I-MethodName
by I-MethodName
Jointly I-MethodName
Learning I-MethodName
to I-MethodName
Estimate I-MethodName
Alignment I-MethodName
and I-MethodName
Translate I-MethodName

Jongyoon O
Song1;2Sungwon O
Kim1 O
1Data O
Science O
and O
AI O
Laboratory O
, O
Seoul O
National O
University O
, O
South O
Korea O
2Kakao O
Enterprise O
, O
South O
Korea O
3Interdisciplinary O
Program O
in O
Artiﬁcial O
Intelligence O
, O
Seoul O
National O
University O
, O
South O
Korea O
{ O
coms1580 O
, O
ksw0306 O
, O
sryoon}@snu.ac.krSungroh O
Yoon1;3y O

Abstract O
Non B-MethodName
- I-MethodName
autoregressive I-MethodName
neural I-MethodName
machine I-MethodName
translation I-MethodName
( O
NART B-MethodName
) O
models O
suffer O
from O
the O
multi O
- O
modality O
problem O
which O
causes O
translation O
inconsis- O
tency O
such O
as O
token O
repetition O
. O

Most O
recent O
ap- O
proaches O
have O
attempted O
to O
solve O
this O
problem O
by O
implicitly O
modeling O
dependencies O
between O
outputs O
. O

In O
this O
paper O
, O
we O
introduce O
AligNART B-MethodName
, O
which O
leverages O
full O
alignment O
information O
to O
explicitly O
reduce O
the O
modality O
of O
the O
target O
distribution O
. O

AligNART B-MethodName
divides O
the O
machine B-TaskName
translation I-TaskName
task O
into O
( O
i)alignment O
estimation O
and O
(ii)translation O
with O
aligned O
decoder O
in- O
puts O
, O
guiding O
the O
decoder O
to O
focus O
on O
sim- O
pliﬁed O
one O
- O
to O
- O
one O
translation O
. O

To O
alleviate O
the O
alignment O
estimation O
problem O
, O
we O
further O
pro- O
pose O
a O
novel O
alignment O
decomposition O
method O
. O

Our O
experiments O
show O
that O
AligNART B-MethodName
out- O
performs O
previous O
non O
- O
iterative O
NART O
models O
that O
focus O
on O
explicit O
modality O
reduction O
on O
WMT14 B-DatasetName
En$De I-DatasetName
and O
WMT16 B-DatasetName
Ro I-DatasetName
! I-DatasetName
En I-DatasetName
. O

Fur- O
thermore O
, O
AligNART B-MethodName
achieves O
BLEU B-MetricName
scores O
comparable O
to O
those O
of O
the O
state O
- O
of O
- O
the O
- O
art O
con- O
nectionist O
temporal O
classiﬁcation O
based O
mod- O
els O
on O
WMT14 B-DatasetName
En$De I-DatasetName
. O

We O
also O
observe O
that O
AligNART B-MethodName
effectively O
addresses O
the O
token O
rep- O
etition O
problem O
even O
without O
sequence O
- O
level O
knowledge O
distillation O
. O

1 O
Introduction O
In O
the O
neural B-TaskName
machine I-TaskName
translation I-TaskName
( O
NMT B-TaskName
) O
domain O
, O
non O
- O
autoregressive O
NMT O
( O
NART O
) O
models O
( O
Gu O
et O
al O
. O
, O
2018 O
) O
have O
been O
proposed O
to O
alleviate O
the O
low O
translation O
speeds O
of O
autoregressive O
NMT O
( O
ART O
) O
models O
. O

However O
, O
these O
models O
suffer O
from O
degen- O
erated O
translation O
quality O
( O
Gu O
et O
al O
. O
, O
2018 O
; O
Sun O
et O
al O
. O
, O
2019 O
) O
. O

To O
improve O
the O
translation O
quality O
of O
NART O
, O
several O
studies O
on O
NART O
iteratively O
reﬁne O
decoded O
outputs O
with O
minimal O
iterations O
( O
Ghazvininejad O
et O
al O
. O
, O
2019 O
; O

Kasai O
et O
al O
. O
, O
2020a O
; O
Lee O
et O
al O
. O
, O
2020 O
; O
Guo O
et O
al O
. O
, O
2020 O
; O
Saharia O
et O
al O
. O
, O
2020 O
) O
; O
other O
recent O
works O
target O
to O
improve O
NART B-MethodName
without O
iteration O
( O
Qian O
et O
al O
. O
, O
2021 O
; O
Gu O
and O
Kong O
, O
2021 O
) O
. O

One O
of O
the O
signiﬁcant O
limitations O
of O
non O
- O
iterative O
NART O
models O
is O
the O
multi O
- O
modality O
problem O
. O

This O
problem O
originates O
from O
the O
fact O
that O
the O
models O
should O
maximize O
the O
probabilities O
of O
multiple O
tar- O
gets O
without O
considering O
conditional O
dependencies O
between O
target O
tokens O
. O

For O
example O
, O
in O
English O
- O
to- O
German O
translation O
, O
a O
source O
sentence O
" O
Thank O
you O
very O
much O
. O
" O
can O
be O
translated O
to O
" O
Danke O
schön O
. O
" O

or O
" O
Vielen O
Dank O
. O
" O
. O

Under O
the O
conditional O
indepen- O
dence O
assumption O
, O
the O
non O
- O
iterative O
NART O
models O
are O
likely O
to O
generate O
improper O
translations O
such O
as O
" O
Danke O
Dank O
. O
" O

or O
" O
Vielen O
schön O
. O
" O

( O
Gu O
et O
al O
. O
, O
2018 O
) O
. O

For O
the O
same O
reason O
, O
other O
inconsistency O
prob- O
lems O
such O
as O
token O
repetition O
or O
omission O
occur O
frequently O
in O
non B-TaskName
- I-TaskName
iterative I-TaskName
NART I-TaskName
( O
Gu O
and O
Kong O
, O
2021 O
) O
. O

There O
are O
two O
main O
methods O
for O
non B-TaskName
- I-TaskName
iterative I-TaskName
NART I-TaskName
to O
address O
the O
multi O
- O
modality O
problem O
. O

Some O
works O
focus O
on O
an O
implicit O
modeling O
of O
the O
dependencies O
between O
the O
target O
tokens O
( O
Gu O
and O
Kong O
, O
2021 O
) O
. O

For O
example O
, O
Ghazvininejad O
et O
al O
. O
( O
2020 O
) O
, O
Saharia O
et O
al O
. O
( O
2020 O
) O
, O
and O
Gu O
and O
Kong O
( O
2021 O
) O
modify O
the O
objective O
function O
based O
on O
dy- O
namic O
programming O
, O
whereas O

Qian O
et O
al O
. O
( O
2021 O
) O
provide O
target O
tokens O
to O
the O
decoder O
during O
train- O
ing O
. O

On O
the O
other O
hand O
, O
other O
works O
focus O
on O
an O
ex- O
plicit O
reduction O
of O
the O
modality O
of O
the O
target O
dis- O
tribution O
by O
utilizing O
external O
source O
or O
target O
sen- O
tence O
information O
rather O
than O
modifying O
the O
objec- O
tive O
function O
. O

For O
example O
, O
Akoury O
et O
al O
. O

( O
2019 O
) O
and O
Liu O
et O
al O
. O
( O
2021 O
) O
use O
syntactic O
or O
semantic O
in- O
formation O
; O
Gu O
et O
al O
. O
( O
2018 O
) O
, O
Zhou O
et O
al O
. O
( O
2020b O
) O
, O
and O
Ran O
et O
al O
. O
( O
2021 O
) O
use O
the O
alignment O
informa- O
tion O
between O
source O
and O
target O
tokens O
. O

However O
, O
previous O
explicit O
modality O
reduction O
methods O
show O
suboptimal O
performance O
. O

Zhou O
et O

al O
. O
( O
2020b O
) O
and O
Ran O
et O
al O
. O
( O
2021 O
) O
ex- O
tract O
fertility O
( O
Brown O
et O
al O
. O
, O
1993 O
) O
and O
ordering O
information O
in O
word O
alignments O
, O
which O
enables O
the O
modeling O
of O
several O
types O
of O
mappings O
except O
for O
many O
- O
to O
- O
one O
and O
many O
- O
to O
- O
many O
cases O
. O

We O
hypoth- O
esize O
that O
leveraging O
entire O
mappings O
signiﬁcantly O
reduces O
the O
modality O
and O
is O
the O
key O
to O
performance O
improvement O
. O

In O
this O
work O
, O
we O
propose O
AligNART B-MethodName
, O
a O
non- O
iterative O
NART O
model O
that O
mitigates O
the O
multi- O
modality O
problem O
by O
utilizing O
complete O
informa- O
tion O
in O
word O
alignments O
. O

AligNART B-MethodName
divides O
the O
ma- O
chine O
translation O
task O
into O
( O
i)alignment O
estimation O
and(ii)non O
- O
autoregressive O
translation O
under O
the O
given O
alignments O
. O

Modeling O
all O
the O
type O
of O
mapping O
guides O
( O
ii)more O
close O
to O
one O
- O
to O
- O
one O
translation O
. O

In O
AligNART B-MethodName
, O
a O
module O
called O
Aligner O
is O
simply O
aug- O
mented O
to O
NAT O
( O
Gu O
et O
al O
. O
, O
2018 O
) O
which O
estimates O
alignments O
to O
generate O
aligned O
decoder O
inputs O
. O

However O
, O
it O
is O
challenging O
to O
estimate O
the O
com- O
plex O
alignment O
information O
using O
only O
source O
sentence O
during O
inference O
. O

Speciﬁcally O
, O
Aligner O
should O
simultaneously O
predict O
the O
number O
of O
tar- O
get O
tokens O
corresponding O
to O
each O
source O
token O
and O
their O
mapping O
. O

To O
overcome O
this O
problem O
, O
we O
further O
propose O
alignment O
decomposition O
which O
factorizes O
the O
alignment O
process O
into O
three O
sub- O
processes O
: O
duplication O
, O
permutation O
, O
and O
group- O
ing O
. O

Each O
sub O
- O
process O
corresponds O
to O
much O
feasi- O
ble O
sub O
- O
problems O
: O
one O
- O
to O
- O
many O
mapping O
, O
ordering O
, O
and O
many O
- O
to O
- O
one O
mapping O
, O
respectively O
. O

Our O
experimental O
results O
show O
that O
AligNART B-MethodName
outperforms O
previous O
non O
- O
iterative O
NART O
models O
of O
explicit O
modality O
reduction O
on O
WMT14 B-DatasetName
En I-DatasetName
$ I-DatasetName
De I-DatasetName
and O
WMT16 B-DatasetName
Ro!En I-DatasetName
. O

AligNART B-MethodName
achieves O
per- O
formance O
comparable O
to O
that O
of O
the O
recent O
state- O
of O
- O
the O
- O
art O
non O
- O
iterative O
NART O
model O
on O
WMT14 B-DatasetName
En$De I-DatasetName
. O

We O
observe O
that O
the O
modality O
reduction O
in O
AligNART O
addresses O
the O
token O
repetition O
issue O
even O
without O
sequence O
- O
level O
knowledge O
distillation O
( O
Kim O
and O
Rush O
, O
2016 O
) O
. O

We O
also O
conduct O
quantita- O
tive O
and O
qualitative O
analyses O
on O
the O
effectiveness O
of O
alignment O
decomposition O
. O

2 O
Background O
Given O
a O
source O
sentence O
x O
= O
fx1;x2;:::;x O
Mgand O
its O
translation O
y O
= O
fy1;y2;:::;y O
Ng O
, O
ART O
models O
with O
encoder O
- O
decoder O
architecture O
are O
trained O
with O
chained O
target O
distributions O
and O
infer O
the O
target O
sen- O
tence O
autoregressively O
: O
p(yjx O
) O

= O
NY O
n=1p(ynjy O
< O
n;x O
): O
( O
1)At O
each O
decoding O
position O
n O
, O
the O
decoder O
of O
the O
model O
is O
conditioned O
with O
previous O
target O
tokens O
y O
< O
n O
= O
fy1;:::;y O
n 1 O
g O
, O
which O
is O
the O
key O
factor O
of O
performance O
in O
ART O
models O
. O

Previous O
target O
tokens O
reduce O
the O
target O
distribution O
modality O
and O
provide O
information O
about O
the O
target O
sentence O
. O

However O
, O
the O
autoregressive O
decoding O
scheme O
enforces O
the O
decoder O
to O
iterate O
Ntimes O
to O
complete O
the O
transla- O
tion O
and O
increases O
the O
translation O
time O
linearly O
with O
respect O
to O
the O
length O
of O
the O
target O
sentence O
. O

Non O
- O
iterative O
NART O
models O
( O
Gu O
et O
al O
. O
, O
2018 O
; O
Sun O
et O
al O
. O
, O
2019 O
; O
Sun O
and O
Yang O
, O
2020 O
) O
assume O
con- O
ditional O
independence O
between O
the O
target O
tokens O
to O
improve O
the O
translation O
speed O
: O
p(yjx O
) O
= O
p(Njx)NY O
n=1p(ynjx O
) O
; O
( O
2 O
) O
where O
N B-HyperparameterName
is O
the O
predicted B-HyperparameterName
target I-HyperparameterName
length I-HyperparameterName
to O
parallelize O
the O
decoding O
process O
. O

Non O
- O
iterative O
NART O
models O
provide O
only O
the O
length O
information O
of O
the O
target O
sentence O
to O
the O
decoder O
, O
which O
is O
insufﬁcient O
to O
address O
the O
multi O
- O
modality O
problem O
. O

3 O
AligNART B-MethodName
3.1 O
Model O
Overview O
Given O
the O
word O
alignments O
between O
the O
source O
and O
target O
sentences O
A2f0;1gNM O
, O
we O
factorize O
the O
task O
into O
( O
i)alignment O
estimation O
and O
( O
ii)transla- O
tion O
with O
aligned O
decoder O
inputs O
as O
follows O
: O
p(yjx O
) O
= O
p(Ajx)NY O
n=1p(ynjx;A O
) O
; O
( O
3 O
) O
where O
M B-HyperparameterName
and O
N B-HyperparameterName
are O
the B-HyperparameterName
lengths I-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
source I-HyperparameterName
and I-HyperparameterName
target I-HyperparameterName
sentences I-HyperparameterName
, O
respectively O
. O

Although O
we O
can O
also O
modify O
the O
negative O
log O
- O
likelihood O
loss O
to O
model O
dependencies O
between O
outputs O
such O
as O
connectionist O
temporal O
classiﬁcation O
( O
CTC O
) O
loss O
( O
Graves O
et O
al O
. O
, O
2006 O
) O
, O
we O
focus O
on O
the O
effect O
of O
the O
introduction O
of O
alignment O
as O
additional O
informa- O
tion O
. O

AligNART B-MethodName
is O
based O
on O
the O
encoder O
- O
decoder O
architecture O
, O
with O
an O
alignment O
estimation O
module O
called O
Aligner O
as O
depicted O
in O
Figure O
1a O
. O

The O
en- O
coder O
maps O
the O
embedding O
of O
the O
source O
tokens O
into O
hidden O
representations O
h O
= O
fh1;h2;:::;h O
Mg O
. O

Aligner O
constructs O
the O
aligned O
decoder O
inputs O
d= O
fd1;d2;:::;d O
Ngas O
follows O
: O
where O
rn B-HyperparameterName
is O
the O
number B-HyperparameterName
of I-HyperparameterName
non I-HyperparameterName
- I-HyperparameterName
zero I-HyperparameterName
elements I-HyperparameterName
in O
the O
n B-HyperparameterName
- O
th O
row O
of
A. O

Given O
the O
aligned O
decoder O
in- O
puts O
, O
the O
decoder O
is O
guided O
to O
focus O
on O
a O
one O
- O
to- O
one O
translation O
from O
dn O
to O
yn O
. O

One O
- O
to O
- O
one O
mapping O
signiﬁcantly O
reduces O
the O
modality O
of O
the O
target O
dis- O
tribution O
. O

The O
key O
component O
of O
AligNART B-MethodName
, O
Aligner O
, O
mod- O
els O
a O
conditional O
distribution O
of O
alignments O
Agiven O
the O
source O
sentence O
xduring O
training O
, O
and O
aligns O
encoder O
outputs O
using O
the O
estimated O
alignments O
during O
inference O
, O
as O
depicted O
in O
Figure O
1b O
. O

The O
ground O
truth O
of O
the O
alignments O
is O
extracted O
using O
an O
external O
word O
alignment O
tool O
. O

However O
, O
align- O
ment O
estimation O
given O
only O
the O
source O
sentence O
is O
challenging O
since O
the O
alignment O
consists O
of O
two O
components O
related O
with O
target O
tokens O
: O
•The O
number O
of O
target O
tokens O
that O
correspond O
to O
each O
encoder O
output O
hm O
. O
•The O
positions O
of O
the O
target O
tokens O
to O
which O
hmcorresponds O
. O

The O
Aligner O
decomposes O
the O
alignment O
for O
effec- O
tive O
estimation O
, O
which O
is O
described O
in O
Section O
3.2 O
. O

3.2 O
Aligner O
To O
alleviate O
the O
alignment O
estimation O
problem O
, O
we O
start O
by O
factorizing O
the O
alignment O
process O
as O
shownin O
Figure O
1b O
. O

First O
, O
we O
copy O
each O
encoder O
output O
hmby O
the O
number O
of O
target O
tokens O
mapped O
to O
hm O
, O
which O
is O
denoted O
as O
cm O
= O
P O
nAn;m O
. O

Given O
the O
duplicated O
encoder O
outputs O
h0 O
, O
we O
have O
to O
predict O
the O
positions O
of O
target O
tokens O
to O
which O
each O
element O
inh0is O
mapped O
. O

We O
further O
decompose O
the O
remaining O
prediction O
process O
into O
permutation O
andgrouping O
, O
since O
non- O
iterative O
NART O
models O
have O
no O
information O
about O
the O
target O
length O
Nduring O
inference O
. O

In O
the O
per- O
mutation O
process O
, O
h' B-HyperparameterName
is O
re O
- O
ordered O
into O
d' B-HyperparameterName
such O
that O
elements O
corresponding O
to O
the O
same O
target O
token O
are O
placed O
adjacent O
to O
each O
other O
. O

In O
the O
group- O
ing O
process O
, O
each O
element O
in O
d' B-HyperparameterName
is O
clustered O
into O
Ngroups O
by O
predicting O
whether O
each O
element O
is O
mapped O
to O
the O
same O
target O
token O
as O
the O
previous O
element O
.rn O
= O
P O
mAn;m O
denotes O
the O
number B-HyperparameterName
of I-HyperparameterName
el- I-HyperparameterName
ements I-HyperparameterName
in I-HyperparameterName
the I-HyperparameterName
n I-HyperparameterName
- I-HyperparameterName
th I-HyperparameterName
group I-HyperparameterName
which O
is O
equivalent O
to O
rn O
in O
Equation O
4 O
. O

Finally O
, O
we O
can O
derive O
the O
decoder O
inputs O
d O
in O
Equation O
4 O
by O
averaging O
the O
elements O
in O
each O
group O
in O
d0 O
. O

In O
summary O
, O
we O
decompose O
the O
alignment O
estimation O
task O
into O
three O
sequential O
sub O
- O
tasks O
: O
duplication O
, O
permutation O
, O
and O
grouping O
. O

3.2.1 O
Alignment O
Decomposition O
As O
shown O
in O
Figure O
1b O
, O
we O
factorize O
the O
align- O
ment O
matrix O
Ainto O
duplication O
, O
permutation O
, O
and O

4grouping O
matrices O
that O
correspond O
to O
each O
pro- O

cess.h0 O
= O
fh1;1;:::;h O
1;c1;:::;h O
M;1;:::;d O
M;cMg O
denotes O
the O
duplicated O
encoder O
outputs O
where O
hi;jis O
thej O
- O
th O
copied O
element O
of O
hi O
. O

Similarly O
, O
d0 O
= O
fd1;1;:::;d O
1;r1;:::;d O
N;1;:::;d O
N;rNgdenotes O
the O
permuted O
encoder O
outputs O
where O
di;jis O
thej- O
th O
element O
in O
the O
i O
- O
th O
group O
. O

The B-HyperparameterName
number I-HyperparameterName
of I-HyperparameterName
non- I-HyperparameterName
zero I-HyperparameterName
elements I-HyperparameterName
in O
the O
alignment O
matrix O
is O
deﬁned O
as O
L B-HyperparameterName
= O
P O
mcm O
= O
P O
nrn O
. O

Duplication O
Matrix O
Aligner O
copies O
hmbycm O
to O
construct O
the O
duplicated O
encoder O
outputs O
h0with O
a O
duplication O
matrix O
D2f0;1gLM. O
LetCm O
= O
Pm O
i=1ciandC0= O
0 O
. O

Then O
, O
we O
can O
deﬁne O
Dusing O
cmas O
follows O
: O
Dl;m= O
( O
1ifCm 1 O
< O
lCm O
0else:(5 O
) O

We O
indexh0by O
the O
following O
rule O
: O

•For O
anyhm;iandhm;j(i O
< O
j O
) O
, O
which O
are O
matched O
to O
dxi;yianddxj;yj O
, O
respectively O
, O
xixjandyiyj O
. O

The O
duplication O
matrix O
Dcontains O
similar O
informa- O
tion O
to O
fertility O
( O
Gu O
et O
al O
. O
, O
2018 O
) O
. O

Permutation O
Matrix O
Aligner O
re O

-orders O
h0to O
constructd0with O
a O
permutation O
matrix O
P2 O
f0;1gLL. O
Since O
all O
the O
indexed O
elements O
in O
h0 O
andd0are O
distinct O
, O
the O
permutation O
matrix O
Pis O
uniquely O
deﬁned O
. O

Grouping O
Matrix O
Aligner O
ﬁnally O
aggregates O
d0 O
to O
construct O
d O
, O
the O
aligned O
decoder O
inputs O
, O
with O
a O
grouping O
matrix O
G2 O

f0;1gNL. O

LetRn= O

Pn O

i=1riandR0= O
0 O
. O

Then O
, O
Gcan O
be O
deﬁned O
using O
rnas O
follows O
: O

Gn;l= O
( O
1ifRn 1 O
< O
lRn O
0else:(6 O
) O

We O
indexd0by O
the O
following O
rule O
: O
•For O
anydn;ianddn;j(i O
< O
j O
) O
, O
which O
are O
matched O
to O
hxi;yiandhxj;yj O
, O
respectively O
, O
xixjandyiyj O
. O

We O
can O
derive O
the O
aligned O
decoder O
inputs O
by O
sepa- O
rately O
estimating O
the O
decomposed O
matrices O
D O
, O
P O
, O
andG O
, O
which O
approximately O
correspond O
to O
one O
- O
to- O
many O
mapping O
, O
ordering O
, O
and O
many O
- O
to O
- O
one O
map- O
ping O
, O
respectively O
. O

The O
decomposed O
matrices O
have O
an O
easily O
predictable O
form O
while O
recovering O
the O
complete O
alignment O
matrix.3.2.2 O

Training O
Aligner O
consists O
of O
three O
prediction O
sub O
- O
modules O
: O
duplication O
, O
permutation O
, O
and O
grouping O
predictors O
. O

Each O
of O
them O
estimates O
the O
decomposed O
alignment O
matrix O
as O
follows O
: O
p(Ajx O
) O

= O
p(Gjx;P;D O
) O
p(Pjx;D)p(Djx):(7 O
) O

The O
duplication O
predictor O
learns O
to O
classify O
the O
num- O
ber O
of O
copies O
of O
hm O
. O

The O
duplication O
loss O
is O
deﬁned O
as O
follows O
: O
LD= 1 O
MMX O
m=1logpm(cm O
) O
; O
( O
8) O
wherepmis O
the O
predicted O
probability O
distribution O
of O
the O
duplication O
at O
the O
position O
m. O
To O
discrimi- O
nate O
copied O
elements O
in O
h0 O
, O
we O
add O
copy O
position O
embedding O
tofhm;1;:::;h O
m;cmgfor O
the O
next O
two O
predictors O
. O

The O
permutation O
predictor O
takes O
the O
duplicated O
encoder O
outputs O
h0as O
inputs O
. O

We O
simplify O
the O
per- O
mutation O
prediction O
problem O
into O
a O
classiﬁcation O
of O
the O
re O
- O
ordered O
position O
. O

For O
the O
permutation O
loss O
, O
we O
minimize O
the O
KL B-MetricName
- I-MetricName
divergence I-MetricName
between O
the O
pre- O
dictionPpredand O
the O
ground O
truth O
PGT O
. O

LP= 1 O
LX O
iX O
jPGT O
i;jlogPpred O
i;j O
: O
( O
9 O
) O
Given O
the O
permuted O
encoder O
outputs O
, O
the O
grouping O
predictor O
conducts O
a O
binary O
classiﬁcation O
task O
of O
whetherd0 O
lis O
assigned O
to O
the O
same O
group O
as O
d0 O
l 1 O
. O

Let O
the O
label O
at O
the O
position O
lbegl O
. O

Then O
, O
we O
deﬁne O
glfromGas O
follows O
: O
gl= O
( O
1ifG;l O
= O
G;l 1andl>1 O
0else:(10 O
) O

The O
grouping O
loss O
is O
deﬁned O
as O
follows O
: O
LG= 1 O
LLX O
l=1logpl(gl O
) O
; O
( O
11 O
) O
whereplis O
the O
predicted O
probability O
distribution O
of O
the O
grouping O
predictor O
at O
position O
l. O

Our O
ﬁnal O
loss O
function O
is O
deﬁned O
as O
the O
sum O
of O
the O
negative O
log O
- O
likelihood O
based O
translation O
loss O
LTand O
alignment O
loss O
LA O
: O
L O
= O
LT+LA O
= O
LT+ O
LD+ O
LP+ O
LG;(12 O
) O
where O
we O
set O
= O
= O
= O
0.5 B-HyperparameterValue
for O
all O
the O
experi- O
ments O
. O

3.2.3 O
Inference O
During O
inference O
, O
Aligner O
sequentially O
predicts O
the O
duplication O
, O
permutation O
, O
and O
grouping O
matrices O
to O
compute O
the O
aligned O
decoder O
inputs O
das O
depicted O
in O
Figure O
1b O
. O

The O
duplication O
predictor O
in O
Aligner O
infers O
^cmat O
each O
position O
m O
; O
then O
, O
we O
can O
directly O
construct O
a O
duplication O
matrix O
^Dusing O
Equation O
5 O
. O

The O
permutation O
predictor O
predicts O
the O
distribution O
of O
the O
target O
position O
Ppred O
. O

We O
obtain O
a O
permuta- O
tion O
matrix O
^Pthat O
minimizes O
the O
KL B-MetricName
- I-MetricName
divergence I-MetricName
as O
follows O
: O
^P= B-HyperparameterName
arg O
min O
P( X O
iX O
jPi;jlogPpred O
i;j):(13 O
) O
We O
utilize O
the O
linear O
sum O
assignment O
problem O
solver O
provided O
by O
Jones O
et O
al O
. O
( O
2001 O
) O
to O
ﬁnd O
^P. B-HyperparameterName

The O
grouping O
predictor O
infers O
the O
binary O
predic- O
tions O
^gl O
from O
the O
permuted O
encoder O
outputs O
. O

We O
construct O
a O
grouping O
matrix O
^Gusing O
^gland O

Equa- O
tions O
6 O
and O
10 O
. O

With O
a O
predicted O
alignment O
matrix O
^A=^G^P^D O
, O
Aligner O
constructs O
the O
decoder O
inputs O
using O
Equation O
4 O
, O
and O
the O
decoder O
performs O
translation O
from O
the O
aligned O
inputs O
. O

3.2.4 O
Decoding O
Strategies O
For O
the O
re O
- O
scoring O
based O
decoding O
method O
, O
we O
se- O
lect O
candidates O
of O
alignments O
using O
the O
predicted O
distributions O
in O
the O
duplication O
and O
grouping O
pre- O
dictors O
. O

We O
identify O
m0positions O
in O
the O
outputs O
of O
the O
duplication O
predictor O
, O
where O
the O
probability O
of O
the O
predicted O
class O
is O
low O
. O

We O
then O
construct O
a O
2m0- O
candidate O
pool O
where O
the O
predictions O
in O
part O
of O
the O
m0positions O
are O
replaced O
with O
the O
second O
proba- O
ble O
class O
. O

Next O
, O
we O
identify O
the O
top- O
acandidates O
with O
the O
highest O
joint O
probabilities O
. O

Similarly O
, O
we O
construct O
a O
2l0 O
- O
candidate O
pool O
and O
identify O
bcandi- O
dates O
in O
the O
grouping O
predictor O
for O
the O
acandidates O
. O

Finally O
, O
we O
rank O
abtranslations O
for O
the O
alignments O
candidates O
using O
a O
teacher O
ART O
model O
and O
select O
the O
best O
translation O
among O
them O
. O

3.3 O
Architecture O
of O
AligNART B-MethodName

We O
use O
the O
deep O
- O
shallow O
( O
12 O
- O
1 O
for O
short O
) O
Trans- O
former O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
architecture O
( O
i.e. O
, O
12- O
layer O
encoder O
and O
1 O
- O
layer O
decoder O
) O
proposed O
by O
Kasai O
et O
al O
. O
( O
2020b O
) O
for O
two O
reasons O
. O

First O
, O
a O
deeper O
encoder O
assists O
Aligner O
to O
increase O
the O
estimation O
accuracy O
of O
the O
alignment O
matrix O
during O
inference O
. O

Second O
, O
the O
deep O
- O
shallow O
architecture O
improves O
the O
inference O
speed O
since O
the O
encoder O
layer O
has O
no O
cross O
- O
attention O
module O
compared O
to O
the O
decoderlayer O
. O

The O
architecture O
of O
the O
duplication O
, O
permu- O
tation O
, O
and O
grouping O
predictor O
is O
shown O
in O
the O
Ap- O
pendix O
. O

3.4 O
Alignment O
Score O
Filtering O
Some O
alignment O
tools O
such O
as O
GIZA++ O
( O
Och O
and O
Ney O
, O
2003 O
) O
provide O
an O
alignment O
score O
for O
each O
sentence O
pair O
as O
a O
default O
. O

Samples O
with O
low O
align- O
ment O
scores O
are O
more O
likely O
to O
contain O
noise O
caused O
by O
sentence O
pairs O
or O
alignment O
tools O
. O

For O
GIZA++ O
, O
we O
ﬁlter O
out O
a O
ﬁxed O
portion O
of O
samples O
with O
low O
alignment O
scores O
to O
ease O
the O
alignment O
estimation O
. O

Since O
the O
pair O
of O
long O
sentences O
tends O
to O
be O
aligned O
with O
a O
low O
score O
, O
we O
apply O
the O
same O
ﬁltering O
por- O
tion O
for O
each O
target O
sentence O
length O
. O

4 O
Experimental O
Setups O
4.1 O
Datasets O
and O
Preprocessing O
We O
evaluate O
our O
method O
on O
two O
translation O
datasets O
: O
WMT14 B-DatasetName
English I-DatasetName
- I-DatasetName
German I-DatasetName
( I-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
) I-DatasetName
and O
WMT16 B-DatasetName
English I-DatasetName
- I-DatasetName
Romanian I-DatasetName
( I-DatasetName
En I-DatasetName
- I-DatasetName
Ro I-DatasetName
) I-DatasetName
. O

WMT14 B-DatasetName
En- I-DatasetName
De I-DatasetName
/ O
WMT16 B-DatasetName
En I-DatasetName
- I-DatasetName
Ro I-DatasetName
datasets O
contain O
4.5M/610 O
K O
training O
pairs O
, O
respectively O
. O

For O
WMT14 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
dataset O
, O
we O
use O
preprocess- O
ing O
pipelines O
provided O
by O
fairseq1(Ott O
et O
al O
. O
, O
2019 O
) O
. O

For O
WMT16 O
En O
- O
Ro O
dataset O
, O
we O
use O
the O
prepro- O
cessed O
corpus O
provided O
by O
Lee O
et O
al O
. O

( O
2018 O
) O
. O

Pre- O
processed O
datasets O
share O
a O
vocabulary O
dictionary O
between O
the O
source O
and O
target O
languages O
. O

We O
use O
fast O
align O
( O
FA O
) O
( O
Dyer O
et O
al O
. O
, O
2013 O
) O
and O
GIZA++ O
( O
GZ O
) O
, O
which O
is O
known O
to O
be O
more O
accurate O
than O
fast O
align O
, O
as O
word O
alignment O
tools O
. O

All O
the O
corpus O
are O
passed O
to O
the O
alignment O
tools O
at O
the O
subword O
- O
level O
. O

We O
ﬁlter O
out O
samples O
where O
the O
maximum O
number O
of O
duplications O
exceed O
16 O
. O

We O
explain O
the O
details O
of O
the O
alignment O
processing O
in O
the O
Appendix O
. O

We O
use O
the O
sequence O
- O
level O
knowledge O
distillation O
method O
( O
KD O
) O
for O
the O
distillation O
set O
. O

Transformer O
ART O
models O
are O
trained O
to O
generate O
the O
distillation O
set O
for O
each O
translation O
direction O
. O

4.2 O
Models O
and O
Baselines O
We O
compare O
our O
model O
with O
several O
non O
- O
iterative O
NART O
baselines O
, O
and O
divide O
the O
non O
- O
iterative O
NART O
models O
into O
two O
types O
as O
aforementioned O
: O
implicit O
dependency O
modeling O
andexplicit O
modal- O
ity O
reduction O
( O
see O
Table O
1 O
) O
. O

We O
also O
train O
the O
ART O
models O
and O
deep O
- O
shallow O
NAT O
for O
the O
analysis O
. O

Our O
models O
are O
implemented O
based O
on O
fairseq O
. O

AligNART B-MethodName
is O
implemented O
based O
on O
the O
deep O
- O
shallow O
Transformer O
architecture O
. O

We O
set O
dmodel B-HyperparameterName
/ O
dhidden B-HyperparameterName
to O
512 B-HyperparameterValue
/ O
2048 B-HyperparameterValue
and O
the O
dropout B-HyperparameterName
rate I-HyperparameterName
to O
0.3 B-HyperparameterValue
. O

The O
number B-HyperparameterName
of I-HyperparameterName
heads I-HyperparameterName
in O
multi O
- O
head O
attention O
modules O
is O
8 B-HyperparameterValue
except O
for O
the O
last O
attention O
module O
of O
the O
permutation O
predictor O
which O
is O
1 B-HyperparameterValue
. O

We O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
approximately O
64 B-HyperparameterValue
K I-HyperparameterValue
tokens O
for O
all O
the O
models O
we O
implement O
. O

All O
these O
models O
we O
implement O
are O
trained O
for O
300 B-HyperparameterValue
K I-HyperparameterValue
/ O
50 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
on O
En- B-DatasetName
De I-DatasetName
/ O
En B-DatasetName
- I-DatasetName
Ro I-DatasetName
datasets O
, O
respectively O
. O

For O
AligNART B-MethodName
, O
we O
average O
5 B-HyperparameterValue
checkpoints O
with O
the O
highest O
valida- O
tion O
BLEU B-MetricName
scores O
in O
the O
20 B-HyperparameterValue
latest O
checkpoints O
. O

For O
optimization O
, O
we O
use O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
= O
( O
0.9 B-HyperparameterValue
; O
0.98 B-HyperparameterValue
) O
and O
= O
10-8 B-HyperparameterValue
. O

The O
learning B-HyperparameterName
rate I-HyperparameterName
scheduling O
follows O
that O
of O
Vaswani O
et O
al O
. O
( O
2017 O
) O
, O
starting O
from O
10-7 B-HyperparameterValue
and O
warms O
up O
to O
5e-4 B-HyperparameterValue
in O
10 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
. O

We O
use O
the O
label O
smoothing O
technique O
with O
ls B-HyperparameterName
= O
0.1 B-HyperparameterValue
for O
the O
target O
token O
distribution O
and O
each O
row O
of O
permuta- O
tion O
matrix O
. O

The O
translation O
latency O
is O
measured O
on O
an O
NVIDIA O
Tesla O
V100 O
GPU O
. O

5 O
Results O
5.1 O
Main O
Results O
Table O
1 O
shows O
the O
BLEU B-MetricName
scores O
, O
translation B-MetricName
latency I-MetricName
and O
speedup B-MetricName
on O
WMT14 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
and O
WMT16 B-DatasetName
En I-DatasetName
- I-DatasetName
Ro I-DatasetName
. O

In O
explicit O
modality O
reduction O
, O
AligNART B-MethodName
( I-MethodName
FA I-MethodName
) I-MethodName
achieves O
higher O
BLEU B-MetricName
scores O
than O
Distortion O
and O
ReorderNAT B-MethodName
, O
which O
utilize O
the O
same O
alignment O
tool O
, O
since O
we O
leverage O
the O
entire O
alignment O
information O
rather O
than O
partial O
information O
such O
as O
fertility O
or O
ordering O
. O

Moreover O
, O
AligNART B-MethodName
( I-MethodName
GZ I-MethodName
) I-MethodName
signiﬁcantly O
outperforms O
previous O
models O
for O
explicit O
modal- O
ity O
reduction O
except O
for O
SNAT B-MethodName
on O
En B-DatasetName
! I-DatasetName
Ro I-DatasetName
. O

In O
im- O
plicit O
dependency O
modeling O
, O
AligNART B-MethodName
( I-MethodName
GZ I-MethodName
) I-MethodName
out- O
performs O
Imputer O
and O
shows O
performance O
compara- O
ble O
to O
that O
of O
the O
state O
- O
of O
- O
the O
- O
art O
CTC O
- O
based O
model O
on O
En$De B-DatasetName
by O
simply O
augmenting O
Aligner O
module O
to O
deep O
- O
shallow O
NAT O
. O

In O
this O
study O
, O
we O
focus O
on O
introducing O
complete O
information O
in O
word O
align- O
ments O
; O
we O
do O
not O
modify O
the O
objective O
function O
, O
which O
can O
be O
explored O
in O
the O
future O
work O
. O

Table O
2 O
shows O
the O
BLEU B-MetricName
scores O
with O
re O
- O
scoring O
decoding O
strategies O
of O
the O
non O
- O
iterative O
NART O
mod- O
els O
. O

We O
set O
m0 B-HyperparameterName
= O
l0= B-HyperparameterName
4 B-HyperparameterValue
, O
a B-HyperparameterName
= O
4 B-HyperparameterValue
, O
and O
b B-HyperparameterName
= O
2 B-HyperparameterValue
for O
8 O
candidates O
. O

AligNART B-MethodName
outperforms O
the O
base- O
lines O
on O
En!De B-DatasetName
and O
Ro!En B-DatasetName
, O
and O
shows O
perfor- O
mance O
similar O
to O
that O
of O
GLAT B-MethodName
on O
De B-DatasetName
! I-DatasetName
En I-DatasetName
. O

In O
non O
- O
iterative O
NART O
for O
explicit O
modality O
reduc- O
tion O
, O
AligNART B-MethodName
shows O
the O
best O
performance O
on O
En$De B-DatasetName
and O
Ro!En B-DatasetName
. O

5.2 O
Analysis O
of O
Aligner O
Components O
In O
this O
section O
, O
we O
investigate O
the O
accuracy O
, O
exam- O
ple O
, O
and O
ablation O
results O
of O
Aligner O
components O
as O
shown O
in O
Table O
3 O
, O
4 O
, O
and O
5 O
, O
respectively O
. O

Note O
that O
we O
partially O
provide O
the O
ground O
truth O
D O
or O
P O
matrices O
during O
the O
accuracy O
measurement O
. O

Knowledge O
Distillation O
In O
Table O
3 O
, O
a O
com- O
parison O
of O
accuracy O
between O
raw O
and O
distilled O
datasets O
shows O
that O
KD O
signiﬁcantly O
decreases O
multi O
- O
modality O
of O
each O
component O
. O

After O
KD O
, O
Alig- O
NART O
shows O
marginally O
reduced O
accuracy O
on O
the O
raw O
dataset O
, O
but O
high O
prediction O
accuracy O
in O
each O
component O
on O
the O
distillation O
set O
, O
resulting O
in O
in- O
creased O
BLEU B-MetricName
scores O
. O

Alignment O
Tool O
Before O
KD O
, O
AligNART B-MethodName
using O
fast O
align O
and O
GIZA++ O
have O
accuracy O
bottlenecks O
in O
permutation O
and O
duplication O
predictors O
, O
respec- O
tively O
, O
as O
shown O
in O
Table O
3 O
. O

The O
results O
imply O
that O
the O
alignment O
tools O
have O
different O
degrees O
of O
multi- O
modality O
on O
the O
D O
, O
P O
, O
and O
G O
matrices O
, O
which O
can O
be O
explored O
in O
the O
future O
work O
. O

Qualitative O
Study O
Table O
4 O
shows O
an O
example O
of O
addressing O
the O
multi O
- O
modality O
problem O
. O

Deep- O
shallow O
NAT O
monotonically O
copies O
the O
encoder O
out- O
puts O
and O
suffers O
from O
repetition O
and O
omission O
prob- O
lems O
. O

AligNART B-MethodName
( I-MethodName
FA I-MethodName
) I-MethodName
does O
not O
show O
the O
inconsis- O
tency O
problems O
thanks O
to O
the O
well O
- O
aligned O
decoder O
inputs O
, O
which O
signiﬁcantly O
reduces O
the O
modality O
of O
the O
target O
distribution O
. O

We O
also O
conducted O
a O
case O
study O
on O
predicted O
alignments O
and O
their O
translations O
during O
re O
- O
scoring O
as O
shown O
in O
the O
Appendix O
. O

Ablation O
Study O
We O
conduct O
an O
analysis O
of O
align- O
ment O
estimation O
by O
ablating O
one O
of O
the O
predictors O
during O
inference O
. O

We O
ablate O
each O
module O
in O
Aligner O
by O
replacing O
the O
predicted O
matrix O
with O
an O
identical O
matrixI. O
The O
results O
in O
Table O
5 O
indicate O
that O
each O
module O
in O
Aligner O
properly O
estimates O
the O
decom- O
posed O
information O
in O
word O
alignments O
. O

However O
, O
there O
is O
an O
exception O
in O
GIZA++ O
where O
many O
- O
to- O
one O
mapping O
does O
not O
exist O
, O
resulting O
in O
perfor- O
mance O
equal O
to O
that O
without O
the O
grouping O
predic- O
tor O
. O

We O
observe O
that O
AligNART -MethodName
achieves O
BLEU B-MetricName
scores O
comparable O
to O
those O
of O
CTC B-MethodName
- O
based O
models O
on O
En$De B-DatasetName
even O
with O
the O
ground O
truth O
word O
align- O
ments O
of O
partial O
information O
. O

5.3 O
Analysis O
of O
Modality O
Reduction O
Effects O
To O
evaluate O
the O
modality O
reduction O
effects O
of O
Alig- B-MethodNameO
NART I-MethodName
, O
we O
conducted O
experiments O
on O
two O
aspects O
: O
BLEU B-MetricName
score O
and O
token O
repetition O
ratio O
. O

Table O
6 O
shows O
the O
BLEU B-MetricName
scores O
on O
WMT14 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
. O

For O
En!De B-DatasetName
, O
AligNART B-MethodName
using O
fast O
align O
without O
KD O
achieves O
higher O
BLEU B-MetricName
scores O
than O
previous O
mod- O
els O
without O
KD O
and O
deep O
- O
shallow O
NAT O
with O
KD O
. O

The O
results O
indicate O
that O
our O
method O
is O
effective O
even O
without O
KD O
, O
which O
is O
known O
to O
decrease O
data O
complexity O
( O
Zhou O
et O
al O
. O
, O
2020a O
) O
. O

On O
the O
other O
hand O
, O
alignments O
from O
GIZA++ O
without O
KD O
are O
more O
complex O
for O
AligNART O
to O
learn O
, O
resulting O
in O
lower O
BLEU O
scores O
than O
deep O
- O
shallow O
NAT O
with O
KD O
. O

Ghazvininejad O
et O
al O
. O

( O
2020 O
) O
measured O
the O
token O
repetition O
ratio O
as O
a O
proxy O
for O
measuring O
multi- O
modality O
. O

The O
token O
repetition O
ratio O
represents O
the O
degree O
of O
the O
inconsistency O
problem O
. O

In O
Table O
7 O
, O
the O
token O
repetition O
ratio O
of O
AligNART O
is O
less O
than O
that O
of O
the O
CMLM O
- O
base O
( O
Ghazvininejad O
et O
al O
. O
, O
2019 O
) O
of O
5 O
iterations O
, O
AXE B-MethodName
, O
and O
GLAT B-MethodName
. O

We O
also O
observe O
that O
the O
decline O
in O
the O
token O
repetition O
ratio O
from O
Aligner O
is O
signiﬁcantly O
larger O
than O
that O
from O
KD O
. O

Combined O
with O
the O
results O
from O
Table O
6 O
, O
alignment O
information O
adequately O
alleviates O
the O
token O
repeti- O
tion O
issue O
even O
in O
the O
case O
where O
the O
BLEU B-MetricName
score O
is O
lower O
than O
that O
of O
deep O
- O
shallow O
NAT O
with O
KD O
. O

5.4 O
Ablation O
Study O
We O
conduct O
several O
extensive O
experiments O
to O
ana- O
lyze O
our O
method O
further O
as O
shown O
in O
Table O
8 O
and O
9 O
. O

Each O
of O
our O
method O
consistently O
improves O
the O
performance O
of O
AligNART B-MethodName
. O

Cross O
Attention O
As O
shown O
in O
Table O
8 O
, O
we O
ab- O
late O
the O
cross O
attention O
module O
in O
the O
decoder O
to O
observe O
the O
relationship O
between O
aligned O
decoder O
inputs O
and O
alignment O
learning O
of O
the O
cross O
atten- O
tion O
module O
. O

We O
train O
AligNART B-MethodName
and O
deep O
- O
shallow O
NAT O
without O
a O
cross O
attention O
module O
for O
compari- O
son O
. O

AligNART O
without O
the O
cross O
attention O
module O
has O
a O
smaller O
impact O
on O
the O
BLEU B-MetricName
score O
than O
the O
deep O
- O
shallow O
NAT O
. O

The O
cross O
attention O
module O
is O
known O
to O
learn O
alignments O
between O
source O
and O
tar- O
get O
tokens O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
, O
and O
the O
result O
implies O
that O
aligned O
decoder O
inputs O
signiﬁcantly O
ofﬂoad O
the O
role O
of O
the O
cross O
attention O
module O
. O

Deep O
- O
shallow O
Architecture O
Deep O
- O
shallow O
ar- O
chitecture O
heavily O
affects O
the O
BLEU B-MetricName
scores O
of O
Alig- B-MethodName
NART I-MethodName
as O
shown O
in O
Table O
8 O
. O

The O
results O
indicate O
that O
the O
deep O
encoder O
assists O
alignment O
estimation O
, O
whereas O
the O
shallow O
decoder O
with O
aligned O
inputs O
has O
a O
lower O
impact O
on O
performance O
degeneration O
. O

Alignment O
Score O
Filtering O
We O
investigate O
the O
trade O
- O
off O
between O
the O
alignment B-HyperparameterName
score I-HyperparameterName
ﬁltering I-HyperparameterName
ratio I-HyperparameterName
and O
BLEU B-MetricName
score O
using O
AligNART B-MethodName
( I-MethodName
GZ I-MethodName
) I-MethodName
presented O
in O
Table O
9 O
. O

Samples O
with O
low O
alignment O
scores O
are O
more O
likely O
to O
contain O
noise O
caused O
by O
distilled O
tar- O
gets O
or O
an O
alignment O
tool O
. O

We O
observe O
that O
ﬁltering O
out O
of O
5 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
samples O
improves O
the O
BLEU B-MetricName
score O
in O
both O
the O
directions O
. O

Surprisingly O
, O
increasing O
the O
ﬁltering O
ratio O
up O
to O
20 B-HyperparameterValue
% I-HyperparameterValue
preserves O
the O
performance O
thanks O
to O
the O
noise O
ﬁltering O
capability O
. O

6 O
Related O
Work O
6.1 O
Non O
- O
iterative O
NART O
After O
Gu O
et O
al O
. O
( O
2018 O
) O
proposed O
NAT O
, O
non O
- O
iterative O
NART O
has O
been O
investigated O
in O
various O
directions O
to O
maximize O
translation O
speed O
while O
maintaining O
translation O
quality O
. O

Shao O
et O
al O
. O
( O
2019 O
) O
, O
Shao O
et O
al O
. O
( O
2020 O
) O
, O
and O
Ghazvininejad O
et O
al O
. O
( O
2020 O
) O
address O
the O
limitations O
of O
conventional O
cross O
entropy O
based O
ob- O
jectives O
that O
overly O
penalize O
consistent O
predictions O
. O

Lee O
et O
al O
. O
( O
2018 O
) O
, O
Ma O
et O
al O
. O
( O
2019 O
) O
, O
Shu O
et O
al O
. O
( O
2020 O
) O
, O
and O
Lee O
et O
al O
. O
( O
2020 O
) O
introduce O
latent O
variables O
to O
model O
the O
complex O
dependencies O
between O
target O
to- O
kens O
. O

Saharia O
et O
al O
. O
( O
2020 O
) O
and O
Gu O
and O
Kong O
( O
2021 O
) O
apply O
CTC O
loss O
to O
the O
NMT O
domain O
. O

Qian O
et O
al O
. O
( O
2021 O
) O
provide O
target O
tokens O
to O
the O
decoder O
during O
training O
using O
the O
glancing O
sampling O
technique O
. O

6.2 O
Alignment O
in O
Parallel O
Generative O
Models O
In O
other O
domains O
, O
such O
as O
text O
- O
to O
- O
speech O
( O
Ren O
et O
al O
. O
, O
2019 O
; O
Kim O
et O
al O
. O
, O
2020 O
; O
Donahue O
et O
al O
. O
, O
2020 O
) O
, O
a O
common O
assumption O
is O
a O
monotonicity O
in O
the O
align- O
ments O
between O
text O
and O
speech O
. O

Given O
this O
assump- O
tion O
, O
only O
a O
duration O
predictor O
is O
required O
to O
alle- O
viate O
the O
length O
- O
mismatch O
problem O
between O
text O
and O
speech O
. O

On O
the O
other O
hand O
, O
modeling O
the O
align- O
ment O
in O
the O
NMT O
domain O
is O
challenging O
since O
the O
alignment O
contains O
additional O
ordering O
and O
group- O
ing O
information O
. O

Our O
method O
estimates O
an O
arbitrary O
alignment O
matrix O
using O
alignment O
decomposition O
.6.3 O

Improving O
NMT O
with O
Enhanced O
Information O
To O
alleviate O
the O
multi O
- O
modality O
problem O
of O
NART O
models O
, O
Gu O
et O
al O
. O
( O
2018 O
) O
, O
Akoury O
et O
al O
. O
( O
2019 O
) O
, O
Zhou O
et O
al O
. O
( O
2020b O
) O
, O
Ran O
et O
al O
. O
( O
2021 O
) O
, O
and O
Liu O
et O
al O
. O
( O
2021 O
) O
provide O
additional O
sentence O
information O
to O
the O
decoder O
. O

Alignment O
is O
considered O
as O
a O
major O
factor O
in O
machine O
translation O
( O
Li O
et O
al O
. O
, O
2007 O
; O
Zhang O
et O
al O
. O
, O
2017 O
) O
. O

Alkhouli O
et O
al O
. O
( O
2018 O
) O
decompose O
the O
ART O
model O
into O
alignment O
and O
lexical O
models O
. O

Song O
et O
al O
. O
( O
2020 O
) O
use O
the O
predicted O
alignment O
in O
ART O
models O
to O
constrain O
vocabulary O
candidates O
during O
decoding O
. O

However O
, O
the O
alignment O
estimation O
in O
NART O
is O
much O
challenging O
since O
the O
information O
of O
decoding O
outputs O
is O
limited O
. O

In O
NART B-MethodName
, O
Gu O
et O
al O
. O
( O
2018 O
) O
, O
Zhou O
et O
al O
. O
( O
2020b O
) O
, O
and O
Ran O
et O
al O
. O
( O
2021 O
) O
exploit O
partial O
information O
from O
the O
ground O
truth O
alignments O
. O

In O
contrast O
, O
we O
propose O
the O
alignment O
decomposition O
method O
for O
effective O
alignment O
esti- O
mation O
in O
NART O
where O
we O
leverage O
the O
complete O
alignment O
information O
. O

7 O
Conclusion O
and O
Future O
Work O

In O
this O
study O
, O
we O
leverage O
full O
alignment O
informa- O
tion O
to O
directly O
reduce O
the O
degree O
of O
the O
multi- O
modality O
in O
non O
- O
iterative O
NART O
and O
propose O
an O
alignment O
decomposition O
method O
for O
alignment O
estimation O
. O

AligNART B-MethodName
with O
GIZA++ O
shows O
per- O
formance O
comparable O
to O
that O
of O
the O
recent O
CTC- O
based O
implicit O
dependency O
modeling O
approach O
on O
WMT14 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
and O
modality O
reduction O
capability O
. O

However O
, O
we O
observe O
that O
AligNART O
depends O
on O
the O
quality O
of O
the O
ground O
truth O
word O
alignments O
, O
which O
can O
be O
studied O
in O
the O
future O
work O
. O

Further- O
more O
, O
we O
can O
study O
on O
the O
combination O
of O
Alig- O
NART O
and O
implicit O
dependency O
modeling O
meth- O
ods O
. O

Acknowledgement O
This O
work O
was O
supported O
by O
the O
National O
Research O
Foundation O
of O
Korea O
( O
NRF O
) O
grant O
funded O
by O
the O
Korea O
government O
( O
Ministry O
of O
Science O
and O
ICT O
) O
[ O
2018R1A2B3001628 O
] O
, O
the O
BK21 O
FOUR O
program O
of O
the O
Education O
and O
Research O
Program O
for O
Future O
ICT O
Pioneers O
, O
Seoul O
National O
University O
in O
2021 O
, O
AIRS O
Company O
in O
Hyundai O
& O
Kia O
Motor O
Company O
through O
HKMC O
- O
SNU O
AI O
Consortium O
Fund O
, O
and O
Kakao O
Enterprise O
. O

10References O
Nader O
Akoury O
, O
Kalpesh O
Krishna O
, O
and O
Mohit O
Iyyer O
. O

2019 O
. O

Syntactically O
supervised O
transformers O
for O
faster O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Com- O
putational O
Linguistics O
, O
pages O
1269–1281 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O

Tamer O
Alkhouli O
, O
Gabriel O
Bretschner O
, O
and O
Hermann O
Ney O
. O

2018 O
. O

On O
the O
alignment O
problem O
in O
multi O
- O
head O
attention O
- O
based O
neural O
machine O
translation O
. O

In O
Pro- O
ceedings O
of O
the O
Third O
Conference O
on O
Machine O
Trans- O
lation O
: O
Research O
Papers O
, O
pages O
177–185 O
, O
Brussels O
, O
Belgium O
. O

Association O
for O
Computational O
Linguis- O
tics O
. O

Dzmitry O
Bahdanau O
, O
Kyunghyun O
Cho O
, O
and O
Yoshua O
Ben- O
gio O
. O
2015 O
. O

Neural O
machine O
translation O
by O
jointly O
learning O
to O
align O
and O
translate O
. O

In O
3rd O
Inter- O
national O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2015 O
, O
San O
Diego O
, O
CA O
, O
USA O
, O
May O
7 O
- O
9 O
, O
2015 O
, O
Conference O
Track O
Proceedings O
. O

Peter O
F. O
Brown O
, O
Stephen O
A. O
Della O
Pietra O
, O
Vincent O
J. O
Della O
Pietra O
, O
and O
Robert O
L. O
Mercer O
. O

1993 O
. O

The O
math- O
ematics O
of O
statistical O
machine O
translation O
: O
Parameter O
estimation O
. O

Computational O
Linguistics O
, O
19(2):263 O
– O
311 O
. O

Jeff O
Donahue O
, O
Sander O
Dieleman O
, O
Mikolaj O
Binkowski O
, O
Erich O
Elsen O
, O
and O
Karen O
Simonyan O
. O

2020 O
. O

End O
- O
to- O
end O
adversarial O
text O
- O
to O
- O
speech O
. O

In O
International O
Con- O
ference O
on O
Learning O
Representations O
. O

Chris O
Dyer O
, O
Victor O
Chahuneau O
, O
and O
Noah O
A. O
Smith O
. O

2013 O
. O

A O
simple O
, O
fast O
, O
and O
effective O
reparameter- O
ization O
of O
IBM O
model O
2 O
. O

In O
Proceedings O
of O
the O
2013 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Hu- O
man O
Language O
Technologies O
, O
pages O
644–648 O
, O
At- O
lanta O
, O
Georgia O
. O

Association O
for O
Computational O
Lin- O
guistics O
. O

Marjan O
Ghazvininejad O
, O
Vladimir O
Karpukhin O
, O
Luke O
Zettlemoyer O
, O
and O
Omer O
Levy O
. O

2020 O
. O

Aligned O
cross O
entropy O
for O
non O
- O
autoregressive O
machine O
translation O
. O

InProceedings O
of O
the O
37th O
International O
Conference O
on O
Machine O
Learning O
, O
ICML O
2020 O
, O
13 O
- O
18 O
July O
2020 O
, O
Virtual O
Event O
, O
volume O
119 O
of O
Proceedings O
of O
Ma- O
chine O

Learning O
Research O
, O
pages O
3515–3523 O
. O

PMLR O
. O

Marjan O
Ghazvininejad O
, O
Omer O
Levy O
, O
Yinhan O
Liu O
, O
and O
Luke O
Zettlemoyer O
. O

2019 O
. O

Mask O
- O
predict O
: O
Parallel O
de- O
coding O
of O
conditional O
masked O
language O
models O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Lan- O
guage O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
6112 O
– O
6121 O
, O
Hong O
Kong O
, O
China O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Alex O
Graves O
, O
Santiago O
Fernández O
, O
Faustino O
J. O
Gomez O
, O
and O
Jürgen O
Schmidhuber O
. O

2006 O
. O

Connectionist O
tem- O
poral O
classiﬁcation O
: O
labelling O
unsegmented O
sequence O
data O
with O
recurrent O
neural O
networks O
. O

In O
MachineLearning O
, O
Proceedings O
of O
the O
Twenty O
- O
Third O
Interna- O
tional O
Conference O
( O
ICML O
2006 O
) O
, O
Pittsburgh O
, O
Penn- O
sylvania O
, O
USA O
, O
June O
25 O
- O
29 O
, O
2006 O
, O
volume O
148 O
of O
ACM O
International O
Conference O
Proceeding O
Series O
, O
pages O
369–376 O
. O

ACM O
. O

Jiatao O
Gu O
, O
James O
Bradbury O
, O
Caiming O
Xiong O
, O
Vic- O
tor O
O. O
K. O
Li O
, O
and O
Richard O
Socher O
. O

2018 O
. O

Non- O
autoregressive O
neural O
machine O
translation O
. O

In O
6th O
International O
Conference O
on O
Learning O
Representa- O
tions O
, O
ICLR O
2018 O
, O
Vancouver O
, O
BC O
, O
Canada O
, O
April O
30 O
- O
May O
3 O
, O
2018 O
, O
Conference O
Track O
Proceedings O
. O

Open- O
Review.net O
. O

Jiatao O
Gu O
and O
Xiang O
Kong O
. O
2021 O
. O

Fully O
non- O

autoregressive O
neural O
machine O
translation O
: O
Tricks O
of O
the O
trade O
. O

In O
Findings O
of O
the O
Association O
for O
Compu- O
tational O
Linguistics O
: O
ACL O
- O
IJCNLP O
2021 O
, O
pages O
120 O
– O
133 O
, O
Online O
. O

Association O
for O
Computational O
Linguis- O
tics O
. O

Junliang O
Guo O
, O
Linli O
Xu O
, O
and O
Enhong O
Chen O
. O

2020 O
. O

Jointly O
masked O
sequence O
- O
to O
- O
sequence O
model O
for O
non- O
autoregressive O
neural O
machine O
translation O
. O

In O
Pro- O
ceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Associa- O
tion O
for O
Computational O
Linguistics O
, O
pages O
376–385 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Eric O
Jones O
, O
Travis O
Oliphant O
, O
Pearu O
Peterson O
, O
et O
al O
. O
2001 O
. O

SciPy O
: O

Open O
source O
scientiﬁc O
tools O
for O
Python O
. O

Jungo O
Kasai O
, O
James O
Cross O
, O
Marjan O
Ghazvininejad O
, O
and O
Jiatao O
Gu O
. O
2020a O
. O

Non O
- O
autoregressive O
machine O
translation O
with O
disentangled O
context O
transformer O
. O

InProceedings O
of O
the O
37th O
International O
Conference O
on O
Machine O
Learning O
, O
ICML O
2020 O
, O
13 O
- O
18 O
July O
2020 O
, O
Virtual O
Event O
, O
volume O
119 O
of O
Proceedings O
of O
Ma- O
chine O

Learning O
Research O
, O
pages O
5144–5155 O
. O

PMLR O
. O

Jungo O
Kasai O
, O
Nikolaos O
Pappas O
, O
Hao O
Peng O
, O
James O
Cross O
, O
and O
Noah O
Smith O
. O

2020b O
. O

Deep O
encoder O
, O
shallow O
decoder O
: O
Reevaluating O
non O
- O
autoregressive O
machine O
translation O
. O

In O
International O
Conference O
on O
Learn- O
ing O
Representations O
. O

Jaehyeon O
Kim O
, O
Sungwon O
Kim O
, O
Jungil O
Kong O
, O
and O
Sun- O
groh O
Yoon O
. O

2020 O
. O

Glow O
- O
tts O
: O
A O
generative O
ﬂow O
for O
text O
- O
to O
- O
speech O
via O
monotonic O
alignment O
search O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
33 O
: O
Annual O
Conference O
on O
Neural O
Information O
Pro- O
cessing O
Systems O
2020 O
, O
NeurIPS O
2020 O
, O
December O
6- O
12 O
, O
2020 O
, O
virtual O
. O

Yoon O
Kim O
and O
Alexander O
M. O
Rush O
. O
2016 O
. O

Sequence- O
level O
knowledge O
distillation O
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natu- O
ral O
Language O
Processing O
, O
pages O
1317–1327 O
, O
Austin O
, O
Texas O
. O

Association O
for O
Computational O
Linguistics O
. O

Diederik O
P. O
Kingma O
and O
Jimmy O
Ba O
. O
2015 O
. O

Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O

In O
3rd O
Inter- O
national O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2015 O
, O
San O
Diego O
, O
CA O
, O
USA O
, O
May O
7 O
- O
9 O
, O
2015 O
, O
Conference O
Track O
Proceedings O
. O

11Jason O
Lee O
, O
Elman O
Mansimov O
, O
and O
Kyunghyun O
Cho O
. O
2018 O
. O

Deterministic O
non O
- O
autoregressive O
neural O
se- O
quence O
modeling O
by O
iterative O
reﬁnement O
. O

In O
Pro- O
ceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Meth- O
ods O
in O
Natural O
Language O
Processing O
, O
pages O
1173 O
– O
1182 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computa- O
tional O
Linguistics O
. O

Jason O
Lee O
, O
Raphael O
Shu O
, O
and O
Kyunghyun O
Cho O
. O
2020 O
. O

Iterative O
reﬁnement O
in O
the O
continuous O
space O
for O
non O
- O
autoregressive O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
1006–1015 O
, O
Online O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Chi O
- O
Ho O
Li O
, O
Minghui O
Li O
, O
Dongdong O
Zhang O
, O
Mu O
Li O
, O
Ming O
Zhou O
, O
and O
Yi O
Guan O
. O

2007 O
. O

A O
probabilistic O
approach O
to O
syntax O
- O
based O
reordering O
for O
statistical O
machine O
translation O
. O

In O
Proceedings O
of O
the O
45th O
An- O
nual O
Meeting O
of O
the O
Association O
of O
Computational O
Linguistics O
, O
pages O
720–727 O
, O
Prague O
, O
Czech O
Repub- O
lic O
. O

Association O
for O
Computational O
Linguistics O
. O

Ye O
Liu O
, O
Yao O
Wan O
, O
Jianguo O
Zhang O
, O
Wenting O
Zhao O
, O
and O
Philip O
Yu O
. O
2021 O
. O

Enriching O
non O
- O
autoregressive O
trans- O
former O
with O
syntactic O
and O
semantic O
structures O
for O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
the O
16th O
Conference O
of O
the O
European O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Main O
Volume O
, O
pages O
1235–1244 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Xuezhe O
Ma O
, O
Chunting O
Zhou O
, O
Xian O
Li O
, O
Graham O
Neu- O
big O
, O
and O
Eduard O
Hovy O
. O

2019 O
. O

FlowSeq O
: O
Non- O
autoregressive O
conditional O
sequence O
generation O
with O
generative O
ﬂow O
. O

In O
Proceedings O
of O
the O
2019 O
Con- O
ference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Confer- O
ence O
on O
Natural O
Language O
Processing O
( O
EMNLP- O
IJCNLP O
) O
, O
pages O
4282–4292 O
, O
Hong O
Kong O
, O
China O
. O

As- O
sociation O
for O
Computational O
Linguistics O
. O

Franz O
Josef O
Och O
and O
Hermann O
Ney O
. O

2003 O
. O

A O
systematic O
comparison O
of O
various O
statistical O
alignment O
models O
. O

Computational O
Linguistics O
, O
29(1):19–51 O
. O

Myle O
Ott O
, O
Sergey O
Edunov O
, O
Alexei O
Baevski O
, O
Angela O
Fan O
, O
Sam O
Gross O
, O
Nathan O
Ng O
, O
David O
Grangier O
, O
and O
Michael O
Auli O
. O

2019 O
. O

fairseq O
: O

A O
fast O
, O
extensible O
toolkit O
for O
sequence O
modeling O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chap- O
ter O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Demonstrations O
) O
, O
pages O
48–53 O
, O
Minneapolis O
, O
Min- O
nesota O
. O

Association O
for O
Computational O
Linguistics O
. O

Lihua O
Qian O
, O
Hao O
Zhou O
, O
Yu O
Bao O
, O
Mingxuan O
Wang O
, O
Lin O
Qiu O
, O
Weinan O
Zhang O
, O
Yong O
Yu O
, O
and O
Lei O
Li O
. O
2021 O
. O

Glancing O
transformer O
for O
non O
- O
autoregressive O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
the O
59th O
An- O
nual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Confer- O
ence O
on O
Natural O
Language O
Processing O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1993–2003 O
, O
Online O
. O

Associa- O
tion O
for O
Computational O
Linguistics O
. O

Qiu O
Ran O
, O
Yankai O
Lin O
, O
Peng O
Li O
, O
and O
Jie O
Zhou O
. O

2021 O
. O

Guiding O
non O
- O
autoregressive O
neural O
machine O
transla- O
tion O
decoding O
with O
reordering O
information O
. O

Proceed- O
ings O
of O
the O
AAAI O
Conference O
on O
Artiﬁcial O
Intelli- O
gence O
, O
35(15):13727–13735 O
. O

Yi O
Ren O
, O
Yangjun O
Ruan O
, O
Xu O
Tan O
, O
Tao O
Qin O
, O
Sheng O
Zhao O
, O
Zhou O
Zhao O
, O
and O
Tie O
- O
Yan O
Liu O
. O
2019 O
. O

Fastspeech O
: O
Fast O
, O
robust O
and O
controllable O
text O
to O
speech O
. O

In O
Ad- O
vances O
in O
Neural O
Information O
Processing O
Systems O
32 O
: O
Annual O
Conference O
on O
Neural O
Information O
Pro- O
cessing O
Systems O
2019 O
, O
NeurIPS O
2019 O
, O
December O
8 O
- O
14 O
, O
2019 O
, O
Vancouver O
, O
BC O
, O
Canada O
, O
pages O
3165 O
– O
3174 O
. O

Chitwan O
Saharia O
, O
William O
Chan O
, O
Saurabh O
Saxena O
, O
and O
Mohammad O
Norouzi O
. O
2020 O
. O

Non O
- O
autoregressive O
ma- O
chine O
translation O
with O
latent O
alignments O
. O

In O
Proceed- O
ings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
1098–1108 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Chenze O
Shao O
, O
Yang O
Feng O
, O
Jinchao O
Zhang O
, O
Fandong O
Meng O
, O
Xilin O
Chen O
, O
and O
Jie O
Zhou O
. O

2019 O
. O

Retrieving O
sequential O
information O
for O
non O
- O
autoregressive O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
the O
57th O
An- O
nual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
3013–3024 O
, O
Florence O
, O
Italy O
. O

Asso- O
ciation O
for O
Computational O
Linguistics O
. O

Chenze O
Shao O
, O
Jinchao O
Zhang O
, O
Yang O
Feng O
, O
Fandong O
Meng O
, O
and O
Jie O
Zhou O
. O

2020 O
. O

Minimizing O
the O
bag O
- O
of- O
ngrams O
difference O
for O
non O
- O
autoregressive O
neural O
ma- O
chine O
translation O
. O

In O
The O
Thirty O
- O
Fourth O
AAAI O
Con- O
ference O
on O
Artiﬁcial O
Intelligence O
, O
AAAI O
2020 O
, O
The O
Thirty O
- O
Second O
Innovative O
Applications O
of O
Artiﬁcial O
Intelligence O
Conference O
, O
IAAI O
2020 O
, O
The O
Tenth O
AAAI O
Symposium O
on O
Educational O
Advances O
in O
Artiﬁcial O
In- O
telligence O
, O
EAAI O
2020 O
, O
New O
York O
, O
NY O
, O
USA O
, O
Febru- O
ary O
7 O
- O
12 O
, O
2020 O
, O
pages O
198–205 O
. O

AAAI O
Press O
. O

Raphael O
Shu O
, O
Jason O
Lee O
, O
Hideki O
Nakayama O
, O
and O
Kyunghyun O
Cho O
. O
2020 O
. O

Latent O
- O
variable O
non- O
autoregressive O
neural O
machine O
translation O
with O
deter- O
ministic O
inference O
using O
a O
delta O
posterior O
. O

In O
Pro- O
ceedings O
of O
the O
AAAI O
Conference O
on O
Artiﬁcial O
Intel- O
ligence O
, O
volume O
34 O
, O
pages O
8846–8853 O
. O

Kai O
Song O
, O
Kun O
Wang O
, O
Heng O
Yu O
, O
Yue O
Zhang O
, O
Zhongqiang O
Huang O
, O
Weihua O
Luo O
, O
Xiangyu O
Duan O
, O
and O
Min O
Zhang O
. O

2020 O
. O

Alignment O
- O
enhanced O
trans- O
former O
for O
constraining O
nmt O
with O
pre O
- O
speciﬁed O
trans- O
lations O
. O

In O
AAAI O
, O
pages O
8886–8893 O
. O

Zhiqing O
Sun O
, O
Zhuohan O
Li O
, O
Haoqing O
Wang O
, O
Di O
He O
, O
Zi O
Lin O
, O
and O
Zhi O
- O
Hong O
Deng O
. O
2019 O
. O

Fast O
structured O
decoding O
for O
sequence O
models O
. O

In O
Advances O
in O
Neu- O
ral O
Information O
Processing O
Systems O
32 O
: O
Annual O
Con- O
ference O
on O
Neural O
Information O
Processing O
Systems O
2019 O
, O
NeurIPS O
2019 O
, O
December O
8 O
- O
14 O
, O
2019 O
, O
Vancou- O
ver O
, O
BC O
, O
Canada O
, O
pages O
3011–3020 O
. O

Zhiqing O
Sun O
and O
Yiming O
Yang O
. O

2020 O
. O

An O
EM O
approach O
to O
non O
- O
autoregressive O
conditional O
sequence O
genera- O

12tion O
. O

In O
Proceedings O
of O
the O
37th O
International O
Con- O
ference O
on O
Machine O
Learning O
, O
ICML O
2020 O
, O
13 O
- O
18 O
July O
2020 O
, O
Virtual O
Event O
, O
volume O
119 O
of O
Proceedings O
of O
Machine O
Learning O
Research O
, O
pages O
9249–9258 O
. O

PMLR O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N. O
Gomez O
, O
Lukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
Advances O
in O
Neural O
Information O
Pro- O
cessing O
Systems O
30 O
: O
Annual O
Conference O
on O
Neural O
Information O
Processing O
Systems O
2017 O
, O
December O
4- O
9 O
, O
2017 O
, O
Long O
Beach O
, O
CA O
, O
USA O
, O
pages O
5998–6008 O
. O

Jinchao O
Zhang O
, O
Mingxuan O
Wang O
, O
Qun O
Liu O
, O
and O
Jie O
Zhou O
. O
2017 O
. O

Incorporating O
word O
reordering O
knowl- O
edge O
into O
attention O
- O
based O
neural O
machine O
transla- O
tion O
. O

In O
Proceedings O
of O
the O
55th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Vol- O
ume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1524–1534 O
, O
Vancouver O
, O
Canada O
. O

Association O
for O
Computational O
Linguistics O
. O

Chunting O
Zhou O
, O
Jiatao O
Gu O
, O
and O
Graham O
Neubig O
. O
2020a O
. O

Understanding O
knowledge O
distillation O
in O
non- O
autoregressive O
machine O
translation O
. O

In O
8th O
Inter- O
national O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2020 O
, O
Addis O
Ababa O
, O
Ethiopia O
, O
April O
26 O
- O
30 O
, O
2020 O
. O

OpenReview.net O
. O

Long O
Zhou O
, O
Jiajun O
Zhang O
, O
Yang O
Zhao O
, O
and O
Chengqing O
Zong O
. O

2020b O
. O

Non O
- O
autoregressive O
neural O
machine O
translation O
with O
distortion O
model O
. O

In O
CCF O
Interna- O
tional O
Conference O
on O
Natural O
Language O
Processing O
and O
Chinese O
Computing O
, O
pages O
403–415 O
. O

Springer O
. O

Appendix O
A O
Mappings O
in O
Alignment O
In O
general O
, O
there O
are O
one O
- O
to O
- O
one O
, O
one O
- O
to O
- O
many O
, O
many O
- O
to O
- O
one O
, O
and O
many O
- O
to O
- O
many O
mappings O
exclud- O
ing O
zero O
- O
fertility O
and O
spurious O
word O
cases O
( O
see O
Fig- O
ure O
2 O
) O
. O

Distortion O
and O
ReorderNAT O
can O
not O
rep- O
resent O
many O
- O
to O
- O
one O
, O
many O
- O
to O
- O
many O
, O
and O
spurious O
word O
cases O
. O

The O
grouping O
predictor O
in O
AligNART B-MethodName
models O
many O
- O
to O
- O
one O
and O
many O
- O
to O
- O
many O
mappings O
. O

The O
addition O
of O
a O
spurious O
token O
, O
which O
is O
applied O
to O
AligNART B-MethodName
( I-MethodName
FA I-MethodName
) I-MethodName
, O
enables O
us O
to O
address O
the O
spuri- O
ous O
word O
case O
, O
which O
is O
explained O
in O
Section O
C.2 O
. O

During O
the O
experiments O
, O
we O
observe O
that O
the O
intro- O
duction O
of O
a O
spurious O
token O
degrades O
the O
perfor- O
mance O
for O
GIZA++ O
. O

We O
guess O
the O
reason O
of O
the O
degradation O
is O
that O
alignment O
matrix O
from O
GIZA++ O
contains O
more O
than O
two O
times O

as O
many O
empty O
rows O
as O
that O
of O
fast O
align O
on O
WMT14 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
. O
B O
Architecture O
of O
Aligner O

The O
duplication O
predictor O
and O
grouping O
predictor O
modules O
consist O
of O
a O
convolutional O
layer O
, O
ReLU O
ac- O
tivation O
, O
layer O
normalization O
, O
dropout O
, O
and O
a O
projec- O
tion O
layer O
, O
same O
as O
the O
phoneme O
duration O
predictor O
in O
FastSpeech O
( O
Ren O
et O
al O
. O
, O
2019 O
) O
, O
which O
is O
a O
parallel O
text O
- O
to O
- O
speech O
model O
. O

The O
permutation O
predictor O
in O
Aligner O
consists O
of O
three O
encoder O
layers O
: O
pre O
- O
network O
, O
query O
= O
key O
network O
, O
and O
single O
- O
head O
attention O
module O
for O
the O
outputs O
. O

Note O
that O
the O
outputs O
of O
the O
pre O
- O
network O
are O
passed O
to O
the O
query O
and O
key O
networks O
. O

To O
pre- O
vent O
the O
predicted O
permutation O
matrix O
from O
being O
an O
identity O
matrix O
, O
we O
apply O
a O
gate O
function O
to O
the O
last O
attention O
module O
in O
the O
permutation O
predictor O
to O
modulate O
the O
probabilities O
of O
un O
- O
permuted O
and O
permuted O
cases O
. O

We O
formulate O
the O
output O
of O
gated O
attention O
as O
follows O
: O
g=(Qu O
) O
( O
14 O
) O
Ppred O
= O
softmax O
( O
M+QKT O
) O
( O
15 O
) O
Ppred O
= O
Dg+ O
( O
I Dg)Ppred O
; O
( O
16 O
) O
where O
 O
is O
the O
sigmoid O
function O
and O
Q O
= O
K O
is O
the O
output O
of O
the O
query O
= O
key O
network O
, O
respectively O
. O

g B-HyperparameterName
is O
the B-HyperparameterName
probability I-HyperparameterName
of I-HyperparameterName
an I-HyperparameterName
un I-HyperparameterName
- I-HyperparameterName
permuted I-HyperparameterName
case I-HyperparameterName
. O

Mis O
a O
diagonal O
mask O
matrix O
, O
where O
the O
values O
of O
the O
di- O
agonal O
elements O
are O
 inf O
. O

Iis O
an O
identical O
matrix O
andDgis O
a O
diagonal O
matrix O
with O
gas O
the O
main O
diagonal O
. O

C O
Alignment O
Processing O
C.1 O
Word O
- O
to O
- O
subword O
Alignment O

To O
reduce O
the O
complexity O
of O
alignment O
, O
we O
further O
assume O
that O
the O
alignment O
process O
is O
conducted O
at O
the O
word O
- O
level O
. O

We O
decompose O
the O
alignment O
ma- O
trix O
into O
the O
source O
subword O
to O
source O
word O
matrix O
Sand O
the O
source O
word O
to O
target O
subword O
matrix O
Awsas O
depicted O
in O
Figure O
3 O
. O

Since O
Sis O
always O
given O
, O
Awsis O
the O
only O
target O
to O
be O
learned O
. O

First O
, O
we O
derive O
the O
source O
subword O
to O
target O
subword O
matrixAusing O
the O
alignment O
tool O
. O

Awsis O
achieved O
by O
clipping O
the O
maximum O
value O
of O
AS O
> O
to O
1.Aws O
reduces O
the O
search O
space O
because O
of O
the O
assumption O
that O
source O
tokens O
duplicate O
, O
permute O
, O
and O
group O
at O
the O
word O
- O
level O
. O

However O
, O
there O
is O
a O
trade O
- O
off O
be- O
tween O
the O
simplicity O
and O
resolution O
of O
information O
. O

The O
recovered O
source O
subword O
to O
target O
subword O
matrixAwsSloses O
the O
subword O
- O
level O
information O
as O
shown O
in O
the O
rightmost O
matrix O
in O
Figure O
3 O
. O

C.2 O
Filling O
Null O
Rows O
in O
Alignment O
Matrix O

The O
output O
of O
the O
alignment O
tool O
usually O
contains O
empty O
rows O
which O
means O
that O
no O
aligned O
source O
token O
exists O
for O
certain O
target O
tokens O
. O

We O
select O
two O
strategies O
to O
ﬁll O
the O
null O
rows O
: O
( O
i)copy O
the O
alignment O
from O
the O
previous O
target O
token O
, O
or O
( O
ii O
) O
introduce O
a O
special O
spurious O
token O
. O

For O
the O
second O
strategy O
, O
we O
concatenate O
a O
special O
spurious O
token O
at O
the O
end O
of O
the O
source O
sentence O
. O

If O
the O
current O
and O
previous O
target O
tokens O
belong O
to O
the O
same O
word O
, O
we O
follow O
( O
i O
) O
. O

The O
remaining O
target O
tokens O
of O
the O
null O
alignment O
are O
aligned O
to O
the O
spurious O
token O
. O

C.3 O
Details O
of O
Alignment O
Tool O
Conﬁguration O
Forfast O
align O
, O
we O
follow O
the O
default O
setting O
for O
for- O
ward O
/ O
backward O
directions O
and O
obtain O
symmetrized O
alignment O
with O
the O
grow O
- O
diag-ﬁnal O
- O
and O
option O
. O

We O
apply O
the O
word O
- O
to O
- O
subword O
alignment O
technique O
andspurious O
token O
strategy O
for O
null O
alignments O
. O

For O
GIZA++ O
, O
we O
apply O
the O
word O
- O
to O
- O
subword O
align- O
ment O
technique O
and O
copy O
the O
alignment O
from O
the O
previous O
target O
token O
for O
null O
alignment O
. O

We O
set O
the O
alignment B-HyperparameterName
score I-HyperparameterName
ﬁltering I-HyperparameterName
ratio I-HyperparameterName
to O
5 B-HyperparameterValue
% I-HyperparameterValue
. O

D O
Case O
Study O
To O
analyze O
various O
alignments O
and O
their O
transla- O
tions O
during O
re O
- O
scoring O
decoding O
, O
we O
conduct O
acase O
study O
on O
WMT14 B-DatasetName
De I-DatasetName
! I-DatasetName
En I-DatasetName
validation O
set O
as O
shown O
in O
Figure O
4 O
. O

The O
two O
translations O
have O
differ- O
ent O
orderings O
: O
the O
telescope O
’s O
tasks O
andthe O
tasks O
of O
the O
telescope O
. O

In O
this O
sample O
, O
we O
observe O
that O
Alig- B-MethodName
NART I-MethodName
( O
i) O
can O
capture O
non O
- O
diagonal O
alignments O
, O
( O
ii)models O
multiple O
alignments O
, O
and O
( O
iii)trans- O
lates O
corresponding O
to O
the O
given O
alignments O
. O

