BERT B-MethodName
: O
Pre B-MethodName
- I-MethodName
training I-MethodName
of I-MethodName
Deep I-MethodName
Bidirectional I-MethodName
Transformers I-MethodName
for O
Language B-TaskName
Understanding I-TaskName
Jacob O
Devlin O
Ming O
- O
Wei O
Chang O
Kenton O
Lee O
Kristina O
Toutanova O
Google O
AI O
Language O
fjacobdevlin O
, O
mingweichang O
, O
kentonl O
, O
kristout O
g@google.com O

Abstract O
We O
introduce O
a O
new O
language O
representa- O
tion O
model O
called O
BERT B-MethodName
, O
which O
stands O
for O
Bidirectional B-MethodName
Encoder I-MethodName
Representations I-MethodName
from I-MethodName
Transformers I-MethodName
. O

Unlike O
recent O
language O
repre- O
sentation O
models O
( O
Peters O
et O
al O
. O
, O
2018a O
; O
Rad- O
ford O
et O
al O
. O
, O
2018 O
) O
, O
BERT B-MethodName
is O
designed O
to O
pre- O
train O
deep O
bidirectional O
representations O
from O
unlabeled O
text O
by O
jointly O
conditioning O
on O
both O
left O
and O
right O
context O
in O
all O
layers O
. O

As O
a O
re- O
sult O
, O
the O
pre O
- O
trained O
BERT B-MethodName
model O
can O
be O
ﬁne- O
tuned O
with O
just O
one O
additional O
output O
layer O
to O
create O
state O
- O
of O
- O
the O
- O
art O
models O
for O
a O
wide O
range O
of O
tasks O
, O
such O
as O
question B-TaskName
answering I-TaskName
and O
language B-TaskName
inference I-TaskName
, O
without O
substantial O
task- O
speciﬁc O
architecture O
modiﬁcations O
. O

BERT B-MethodName
is O
conceptually O
simple O
and O
empirically O
powerful O
. O

It O
obtains O
new O
state O
- O
of O
- O
the O
- O
art O
re- O
sults O
on O
eleven O
natural O
language O
processing O
tasks O
, O
including O
pushing O
the O
GLUE B-MetricName
score I-MetricName
to O
80.5% B-MetricValue
( O
7.7% B-MetricValue
point O
absolute O
improvement O
) O
, O
MultiNLI B-MetricName
accuracy I-MetricName
to O
86.7% B-MetricValue
( O
4.6% B-MetricValue
absolute O
improvement O
) O
, O
SQuAD B-DatasetName
v1.1 I-DatasetName
question B-TaskName
answering I-TaskName
Test O
F1 B-MetricName
to O
93.2 B-MetricValue
( O
1.5 B-MetricValue
point O
absolute O
im- O
provement O
) O
and O
SQuAD B-DatasetName
v2.0 I-DatasetName
Test O
F1 B-MetricName
to O
83.1 B-MetricValue
( O
5.1 B-MetricValue
point O
absolute O
improvement O
) O
. O

1 O
Introduction O
Language B-MethodName
model I-MethodName
pre I-MethodName
- I-MethodName
training I-MethodName
has O
been O
shown O
to O
be O
effective O
for O
improving O
many O
natural O
language O
processing O
tasks O
( O
Dai O
and O
Le O
, O
2015 O
; O
Peters O
et O
al O
. O
, O
2018a O
; O
Radford O
et O
al O
. O
, O
2018 O
; O
Howard O
and O
Ruder O
, O
2018 O
) O
. O

These O
include O
sentence O
- O
level O
tasks O
such O
as O
natural B-TaskName
language I-TaskName
inference I-TaskName
( O
Bowman O
et O
al O
. O
, O
2015 O
; O
Williams O
et O
al O
. O
, O
2018 O
) O
and O
paraphrasing B-TaskName
( O
Dolan O
and O
Brockett O
, O
2005 O
) O
, O
which O
aim O
to O
predict O
the O
re- O
lationships O
between O
sentences O
by O
analyzing O
them O
holistically O
, O
as O
well O
as O
token O
- O
level O
tasks O
such O
as O
named B-TaskName
entity I-TaskName
recognition I-TaskName
and O
question B-TaskName
answering I-TaskName
, O
where O
models O
are O
required O
to O
produce O
ﬁne O
- O
grained O
output O
at O
the O
token O
level O
( O
Tjong O
Kim O
Sang O
and O
De O
Meulder O
, O
2003 O
; O

Rajpurkar O
et O
al O
. O
, O
2016). O

There O
are O
two O
existing O
strategies O
for O
apply- O
ing O
pre O
- O
trained O
language O
representations O
to O
down- O
stream O
tasks O
: O
feature O
- O
based O
andﬁne O
- O
tuning O
. O

The O
feature O
- O
based O
approach O
, O
such O
as O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018a O
) O
, O
uses O
task O
- O
speciﬁc O
architectures O
that O
include O
the O
pre O
- O
trained O
representations O
as O
addi- O
tional O
features O
. O

The O
ﬁne O
- O
tuning O
approach O
, O
such O
as O
the O
Generative B-MethodName
Pre I-MethodName
- I-MethodName
trained I-MethodName
Transformer I-MethodName
( O
OpenAI B-MethodName
GPT I-MethodName
) O
( O
Radford O
et O
al O
. O
, O
2018 O
) O
, O
introduces O
minimal O
task O
- O
speciﬁc O
parameters O
, O
and O
is O
trained O
on O
the O
downstream O
tasks O
by O
simply O
ﬁne O
- O
tuning O
allpre- O
trained O
parameters O
. O

The O
two O
approaches O
share O
the O
same O
objective O
function O
during O
pre O
- O
training O
, O
where O
they O
use O
unidirectional B-MethodName
language I-MethodName
models I-MethodName
to O
learn O
general O
language O
representations O
. O

We O
argue O
that O
current O
techniques O
restrict O
the O
power O
of O
the O
pre O
- O
trained O
representations O
, O
espe- O
cially O
for O
the O
ﬁne O
- O
tuning O
approaches O
. O

The O
ma- O
jor O
limitation O
is O
that O
standard O
language O
models O
are O
unidirectional O
, O
and O
this O
limits O
the O
choice O
of O
archi- O
tectures O
that O
can O
be O
used O
during O
pre O
- O
training O
. O

For O
example O
, O
in O
OpenAI B-MethodName
GPT I-MethodName
, O
the O
authors O
use O
a O
left O
- O
to- O
right O
architecture O
, O
where O
every O
token O
can O
only O
at- O
tend O
to O
previous O
tokens O
in O
the O
self O
- O
attention O
layers O
of O
the O
Transformer O
( O
Vaswani O
et O

al O
. O
, O
2017 O
) O
. O

Such O
re- O
strictions O
are O
sub O
- O
optimal O
for O
sentence O
- O
level O
tasks O
, O
and O
could O
be O
very O
harmful O
when O
applying O
ﬁne- O
tuning O
based O
approaches O
to O
token O
- O
level O
tasks O
such O
as O
question B-TaskName
answering I-TaskName
, O
where O
it O
is O
crucial O
to O
incor- O
porate O
context O
from O
both O
directions O
. O

In O
this O
paper O
, O
we O
improve O
the O
ﬁne O
- O
tuning O
based O
approaches O
by O
proposing O
BERT B-MethodName
: O
Bidirectional B-MethodName
Encoder I-MethodName
Representations I-MethodName
from I-MethodName
Transformers I-MethodName
. O

BERT B-MethodName
alleviates O
the O
previously O
mentioned O
unidi- O
rectionality O
constraint O
by O
using O
a O
“ O
masked B-MethodName
lan- I-MethodName
guage I-MethodName
model I-MethodName
” O
( O
MLM B-MethodName
) O
pre O
- O
training O
objective O
, O
in- O
spired O
by O
the O
Cloze O
task O
( O
Taylor O
, O
1953 O
) O
. O

The O
masked B-MethodName
language I-MethodName
model I-MethodName
randomly O
masks O
some O
of O
the O
tokens O
from O
the O
input O
, O
and O
the O
objective O
is O
to O
predict O
the O
original O
vocabulary O
i O
d O
of O
the O
masked O
word O
based O
only O
on O
its O
context O
. O

Unlike O
left O
- O
to- O
right O
language O
model O
pre O
- O
training O
, O
the O
MLM B-MethodName
ob- O
jective O
enables O
the O
representation O
to O
fuse O
the O
left O
and O
the O
right O
context O
, O
which O
allows O
us O
to O
pre- O
train O
a O
deep O
bidirectional O
Transformer O
. O

In O
addi- O
tion O
to O
the O
masked B-MethodName
language I-MethodName
model I-MethodName
, O
we O
also O
use O
a O
“ O
next B-TaskName
sentence I-TaskName
prediction I-TaskName
” O
task O
that O
jointly O
pre- O
trains O
text O
- O
pair O
representations O
. O

The O
contributions O
of O
our O
paper O
are O
as O
follows O
: O
• O
We O
demonstrate O
the O
importance O
of O
bidirectional O
pre O
- O
training O
for O
language O
representations O
. O

Un- O
like O
Radford O
et O
al O
. O

( O
2018 O
) O
, O
which O
uses O
unidirec- O
tional O
language O
models O
for O
pre O
- O
training O
, O
BERT B-MethodName
uses O
masked B-MethodName
language I-MethodName
models I-MethodName
to O
enable O
pre- O
trained O
deep O
bidirectional O
representations O
. O

This O
is O
also O
in O
contrast O
to O
Peters O
et O
al O
. O
( O
2018a O
) O
, O
which O
uses O
a O
shallow O
concatenation O
of O
independently O
trained O
left O
- O
to O
- O
right O
and O
right O
- O
to O
- O
left O
LMs O
. O

• O

We O
show O
that O
pre O
- O
trained O
representations O
reduce O
the O
need O
for O
many O
heavily O
- O
engineered O
task- O
speciﬁc O
architectures O
. O

BERT B-MethodName
is O
the O
ﬁrst O
ﬁne- O
tuning O
based O
representation O
model O
that O
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
a O
large O
suite O
of O
sentence O
- O
level O
andtoken O
- O
level O
tasks O
, O
outper- O
forming O
many O
task O
- O
speciﬁc O
architectures O
. O

• O
BERT B-MethodName
advances O
the O
state O
of O
the O
art O
for O
eleven O
NLP O
tasks O
. O

The O
code O
and O
pre O
- O
trained O
mod- O
els O
are O
available O
at O
https://github.com/ O
google O
- O
research O
/ O
bert O
. O

2 O
Related O
Work O
There O
is O
a O
long O
history O
of O
pre O
- O
training O
general O
lan- O
guage O
representations O
, O
and O
we O
brieﬂy O
review O
the O
most O
widely O
- O
used O
approaches O
in O
this O
section O
. O

2.1 O
Unsupervised O
Feature O
- O
based O
Approaches O
Learning O
widely O
applicable O
representations O
of O
words O
has O
been O
an O
active O
area O
of O
research O
for O
decades O
, O
including O
non O
- O
neural O
( O
Brown O
et O
al O
. O
, O
1992 O
; O
Ando O
and O
Zhang O
, O
2005 O
; O
Blitzer O
et O
al O
. O
, O
2006 O
) O
and O
neural O
( O
Mikolov O
et O
al O
. O
, O
2013 O
; O
Pennington O
et O
al O
. O
, O
2014 O
) O
methods O
. O

Pre O
- O
trained O
word O
embeddings O
are O
an O
integral O
part O
of O
modern O
NLP O
systems O
, O
of- O
fering O
signiﬁcant O
improvements O
over O
embeddings O
learned O
from O
scratch O
( O
Turian O
et O

al O
. O
, O
2010 O
) O
. O

To O
pre- O
train O
word O
embedding O
vectors O
, O
left O
- O
to O
- O
right O
lan- O
guage O
modeling O
objectives O
have O
been O
used O
( O
Mnih O
and O
Hinton O
, O
2009 O
) O
, O
as O
well O
as O
objectives O
to O
dis- O
criminate O
correct O
from O
incorrect O
words O
in O
left O
and O
right O
context O
( O
Mikolov O
et O
al O
. O
, O
2013).These O
approaches O
have O
been O
generalized O
to O
coarser O
granularities O
, O
such O
as O
sentence O
embed- O
dings O
( O
Kiros O
et O
al O
. O
, O
2015 O
; O
Logeswaran O
and O
Lee O
, O
2018 O
) O
or O
paragraph O
embeddings O
( O
Le O
and O
Mikolov O
, O
2014 O
) O
. O

To O
train O
sentence O
representations O
, O
prior O
work O
has O
used O
objectives O
to O
rank O
candidate O
next O
sentences O
( O
Jernite O
et O
al O
. O
, O
2017 O
; O
Logeswaran O
and O
Lee O
, O
2018 O
) O
, O
left O
- O
to O
- O
right O
generation O
of O
next O
sen- O
tence O
words O
given O
a O
representation O
of O
the O
previous O
sentence O
( O
Kiros O
et O
al O
. O
, O
2015 O
) O
, O
or O
denoising O
auto- O
encoder O
derived O
objectives O
( O
Hill O
et O
al O
. O
, O
2016 O
) O
. O

ELMo B-MethodName
and O
its O
predecessor O
( O
Peters O
et O
al O
. O
, O
2017 O
, O
2018a O
) O
generalize O
traditional O
word O
embedding O
re- O
search O
along O
a O
different O
dimension O
. O

They O
extract O
context O
- O
sensitive O
features O
from O
a O
left O
- O
to O
- O
right O
and O
a O
right O
- O
to O
- O
left O
language O
model O
. O

The O
contextual O
rep- O
resentation O
of O
each O
token O
is O
the O
concatenation O
of O
the O
left O
- O
to O
- O
right O
and O
right O
- O
to O
- O
left O
representations O
. O

When O
integrating O
contextual O
word O
embeddings O
with O
existing O
task O
- O
speciﬁc O
architectures O
, O
ELMo B-MethodName
advances O
the O
state O
of O
the O
art O
for O
several O
major O
NLP O
benchmarks O
( O
Peters O
et O
al O
. O
, O
2018a O
) O
including O
ques- B-TaskName
tion I-TaskName
answering I-TaskName
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
, O
sentiment B-TaskName
analysis I-TaskName
( O
Socher O
et O
al O
. O
, O
2013 O
) O
, O
and O
named B-TaskName
entity I-TaskName
recognition I-TaskName
( O
Tjong O
Kim O
Sang O
and O
De O
Meulder O
, O
2003 O
) O
. O

Melamud O
et O

al O
. O

( O
2016 O
) O
proposed O
learning O
contextual O
representations O
through O
a O
task O
to O
pre- O
dict O
a O
single O
word O
from O
both O
left O
and O
right O
context O
using O
LSTMs O
. O

Similar O
to O
ELMo B-MethodName
, O
their O
model O
is O
feature O
- O
based O
and O
not O
deeply O
bidirectional O
. O

Fedus O
et O

al O
. O

( O
2018 O
) O
shows O
that O
the O
cloze O
task O
can O
be O
used O
to O
improve O
the O
robustness O
of O
text O
generation O
mod- O
els O
. O

2.2 O
Unsupervised O
Fine O
- O
tuning O
Approaches O
As O
with O
the O
feature O
- O
based O
approaches O
, O
the O
ﬁrst O
works O
in O
this O
direction O
only O
pre O
- O
trained O
word O
em- O
bedding O
parameters O
from O
unlabeled O
text O
( O
Col- O
lobert O
and O
Weston O
, O
2008 O
) O
. O

More O
recently O
, O
sentence O
or O
document O
encoders O
which O
produce O
contextual O
token O
representations O
have O
been O
pre O
- O
trained O
from O
unlabeled O
text O
and O
ﬁne O
- O
tuned O
for O
a O
supervised O
downstream O
task O
( O
Dai O
and O
Le O
, O
2015 O
; O
Howard O
and O
Ruder O
, O
2018 O
; O
Radford O
et O
al O
. O
, O
2018 O
) O
. O

The O
advantage O
of O
these O
approaches O
is O
that O
few O
parameters O
need O
to O
be O
learned O
from O
scratch O
. O

At O
least O
partly O
due O
to O
this O
advantage O
, O
OpenAI B-MethodName
GPT I-MethodName
( O
Radford O
et O
al O
. O
, O
2018 O
) O
achieved O
pre- O
viously O
state O
- O
of O
- O
the O
- O
art O
results O
on O
many O
sentence- O
level O
tasks O
from O
the O
GLUE O
benchmark O
( O
Wang O
et O
al O
. O
, O
2018a O
) O
. O

Left O
- O
to O
- O
right O
language O
model- O
ing O
and O
auto O
- O
encoder O
objectives O
have O
been O
used O
for O
pre O
- O
training O
such O
models O
( O
Howard O
and O
Ruder O
, O
2018 O
; O
Radford O
et O

al O
. O
, O
2018 O
; O
Dai O
and O
Le O
, O
2015 O
) O

2.3 O
Transfer O
Learning O
from O
Supervised O
Data O
There O
has O
also O
been O
work O
showing O
effective O
trans- O
fer O
from O
supervised O
tasks O
with O
large O
datasets O
, O
such O
as O
natural O
language O
inference O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
and O
machine O
translation O
( O
McCann O
et O
al O
. O
, O
2017 O
) O
. O

Computer O
vision O
research O
has O
also O
demon- O
strated O
the O
importance O
of O
transfer O
learning O
from O
large O
pre O
- O
trained O
models O
, O
where O
an O
effective O
recipe O
is O
to O
ﬁne O
- O
tune O
models O
pre O
- O
trained O
with O
Ima- O
geNet O
( O
Deng O
et O
al O
. O
, O
2009 O
; O
Yosinski O
et O
al O
. O
, O
2014 O
) O
. O

3 O
BERT B-MethodName
We O
introduce O
BERT B-MethodName
and O
its O
detailed O
implementa- O
tion O
in O
this O
section O
. O

There O
are O
two O
steps O
in O
our O
framework O
: O
pre O
- O
training O
and O
ﬁne O
- O
tuning O
. O

Dur- O
ing O
pre O
- O
training O
, O
the O
model O
is O
trained O
on O
unlabeled O
data O
over O
different O
pre O
- O
training O
tasks O
. O

For O
ﬁne- O
tuning O
, O
the O
BERT B-MethodName
model O
is O
ﬁrst O
initialized O
with O
the O
pre O
- O
trained O
parameters O
, O
and O
all O
of O
the O
param- O
eters O
are O
ﬁne O
- O
tuned O
using O
labeled O
data O
from O
the O
downstream O
tasks O
. O

Each O
downstream O
task O
has O
sep- O
arate O
ﬁne O
- O
tuned O
models O
, O
even O
though O
they O
are O
ini- O
tialized O
with O
the O
same O
pre O
- O
trained O
parameters O
. O

The O
question O
- O
answering O
example O
in O
Figure O
1 O
will O
serve O
as O
a O
running O
example O
for O
this O
section O
. O

A O
distinctive O
feature O
of O
BERT B-MethodName
is O
its O
uniﬁed O
ar- O
chitecture O
across O
different O
tasks O
. O

There O
is O
mini O
- O
mal O
difference O
between O
the O
pre O
- O
trained O
architec- O
ture O
and O
the O
ﬁnal O
downstream O
architecture O
. O

Model O
Architecture O
BERT B-MethodName
’s O
model O
architec- O
ture O
is O
a O
multi B-MethodName
- I-MethodName
layer I-MethodName
bidirectional I-MethodName
Transformer I-MethodName
en- O
coder O
based O
on O
the O
original O
implementation O
de- O
scribed O
in O
Vaswani O

et O

al O
. O
( O
2017 O
) O
and O
released O
in O
thetensor2tensor O
library.1Because O
the O
use O
of O
Transformers B-MethodName
has O
become O
common O
and O
our O
im- O
plementation O
is O
almost O
identical O
to O
the O
original O
, O
we O
will O
omit O
an O
exhaustive O
background O
descrip- O
tion O
of O
the O
model O
architecture O
and O
refer O
readers O
to O
Vaswani O
et O

al O
. O
( O
2017 O
) O
as O
well O
as O
excellent O
guides O
such O
as O
“ O
The O
Annotated O
Transformer O
. O
”2 O

In O
this O
work O
, O
we O
denote O
the O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
( O
i.e. O
, O
Transformer B-HyperparameterName
blocks I-HyperparameterName
) O
as O
L B-HyperparameterName
, O
the O
hidden B-HyperparameterName
size I-HyperparameterName
as O
H B-HyperparameterName
, O
and O
the O
number B-HyperparameterName
of I-HyperparameterName
self I-HyperparameterName
- I-HyperparameterName
attention I-HyperparameterName
heads I-HyperparameterName
as O
A B-HyperparameterName
. O

We O
primarily O
report O
results O
on O
two O
model O
sizes O
: O
BERT B-MethodName
BASE I-MethodName
( O
L B-HyperparameterName
= O
12 B-HyperparameterValue
, O
H B-HyperparameterName
= O
768 B-HyperparameterValue
, O
A B-HyperparameterName
= O
12 B-HyperparameterValue
, O
Total O
Param- O
eters=110 O
M O
) O
and O
BERT B-MethodName
LARGE I-MethodName
( O
L B-HyperparameterName
= O
24 B-HyperparameterValue
, O
H B-HyperparameterName
= O
1024 B-HyperparameterValue
, O
A B-HyperparameterName
= O
16 B-HyperparameterValue
, O
Total O
Parameters=340 O
M O
) O
. O

BERT B-MethodName
BASE I-MethodName
was O
chosen O
to O
have O
the O
same O
model O
size O
as O
OpenAI B-MethodName
GPT I-MethodName
for O
comparison O
purposes O
. O

Critically O
, O
however O
, O
the O
BERT B-MethodName
Transformer I-MethodName
uses O
bidirectional O
self O
- O
attention O
, O
while O
the O
GPT B-MethodName
Trans- I-MethodName
former I-MethodName
uses O
constrained O
self O
- O
attention O
where O
every O
token O
can O
only O
attend O
to O
context O
to O
its O
left O
. O

Input O
/ O
Output O
Representations O
To O
make O
BERT B-MethodName
handle O
a O
variety O
of O
down O
- O
stream O
tasks O
, O
our O
input O
representation O
is O
able O
to O
unambiguously O
represent O
both O
a O
single O
sentence O
and O
a O
pair O
of O
sentences O
( O
e.g. O
,hQuestion O
, O
Answeri O
) O
in O
one O
token O
sequence O
. O

Throughout O
this O
work O
, O
a O
“ O
sentence O
” O
can O
be O
an O
arbi- O
trary O
span O
of O
contiguous O
text O
, O
rather O
than O
an O
actual O
linguistic O
sentence O
. O

A O
“ O
sequence O
” O
refers O
to O
the O
in- O
put O
token O
sequence O
to O
BERT B-MethodName
, O
which O
may O
be O
a O
sin- O
gle O
sentence O
or O
two O
sentences O
packed O
together O
. O

We O
use O
WordPiece B-DatasetName
embeddings O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
with O
a O
30,000 O
token O
vocabulary O
. O

The O
ﬁrst O
token O
of O
every O
sequence O
is O
always O
a O
special O
clas- O
siﬁcation O
token O

( O
[ O
CLS O
] O
) O
. O

The O
ﬁnal O
hidden O
state O
corresponding O
to O
this O
token O
is O
used O
as O
the O
ag- O
gregate O
sequence O
representation O
for O
classiﬁcation O
tasks O
. O

Sentence O
pairs O
are O
packed O
together O
into O
a O
single O
sequence O
. O

We O
differentiate O
the O
sentences O
in O
two O
ways O
. O

First O
, O
we O
separate O
them O
with O
a O
special O
token O
( O
[ O
SEP O
] O
) O
. O

Second O
, O
we O
add O
a O
learned O
embed- O
ding O
to O
every O
token O
indicating O
whether O
it O
belongs O
to O
sentence O
Aor O
sentence O
B. O
As O
shown O
in O
Figure O
1 O
, O
we O
denote O
input O
embedding O
as O
E O
, O
the O
ﬁnal O
hidden O
vector O
of O
the O
special O
[ O
CLS O
] O
token O
asC2RH O
, O
and O
the O
ﬁnal O
hidden O
vector O
for O
the O
ithinput O
token O
asTi2RH O
. O

For O
a O
given O
token O
, O
its O
input O
representation O
is O
constructed O
by O
summing O
the O
corresponding O
token O
, O
segment O
, O
and O
position O
embeddings O
. O

A O
visualiza- O
tion O
of O
this O
construction O
can O
be O
seen O
in O
Figure O
2 O
. O

3.1 O
Pre O
- O
training O
BERT B-MethodName
Unlike O
Peters O
et O
al O
. O
( O
2018a O
) O
and O
Radford O
et O

al O
. O

( O
2018 O
) O
, O
we O
do O
not O
use O
traditional O
left O
- O
to O
- O
right O
or O
right O
- O
to O
- O
left O
language O
models O
to O
pre O
- O
train O
BERT B-MethodName
. O

Instead O
, O
we O
pre O
- O
train O
BERT B-MethodName
using O
two O
unsuper- O
vised O
tasks O
, O
described O
in O
this O
section O
. O

This O
step O
is O
presented O
in O
the O
left O
part O
of O
Figure O
1 O
. O

Task O
# O
1 O
: O
Masked O
LM O

Intuitively O
, O
it O
is O
reason- O
able O
to O
believe O
that O
a O
deep O
bidirectional O
model O
is O
strictly O
more O
powerful O
than O
either O
a O
left O
- O
to O
- O
right O
model O
or O
the O
shallow O
concatenation O
of O
a O
left O
- O
to- O
right O
and O
a O
right O
- O
to O
- O
left O
model O
. O

Unfortunately O
, O
standard O
conditional O
language O
models O
can O
only O
be O
trained O
left O
- O
to O
- O
right O
orright O
- O
to O
- O
left O
, O
since O
bidirec- O
tional O
conditioning O
would O
allow O
each O
word O
to O
in- O

directly O
“ O
see O
itself O
” O
, O
and O
the O
model O
could O
trivially O
predict O
the O
target O
word O
in O
a O
multi O
- O
layered O
context O
. O

former O
is O
often O
referred O
to O
as O
a O
“ O
Transformer O
encoder O
” O
while O
the O
left O
- O
context O
- O
only O
version O
is O
referred O
to O
as O
a O
“ O
Transformer O
decoder O
” O
since O
it O
can O
be O
used O
for O
text O
generation O
. O

In O
order O
to O
train O
a O
deep O
bidirectional O
representa- O
tion O
, O
we O
simply O
mask O
some O
percentage O
of O
the O
input O
tokens O
at O
random O
, O
and O
then O
predict O
those O
masked O
tokens O
. O

We O
refer O
to O
this O
procedure O
as O
a O
“ O
masked O
LM O
” O
( O
MLM O
) O
, O
although O
it O
is O
often O
referred O
to O
as O
a O
Cloze O
task O
in O
the O
literature O
( O
Taylor O
, O
1953 O
) O
. O

In O
this O
case O
, O
the O
ﬁnal O
hidden O
vectors O
corresponding O
to O
the O
mask O
tokens O
are O
fed O
into O
an O
output O
softmax O
over O
the O
vocabulary O
, O
as O
in O
a O
 standard O
LM O
. O

In O
all O
of O
our O
experiments O
, O
we O
mask O
15% B-HyperparameterValue
of O
all O
WordPiece B-DatasetName
to- O
kens O
in O
each O
sequence O
at O
random O
. O

In O
contrast O
to O
denoising O
auto O
- O
encoders O
( O
Vincent O
et O
al O
. O
, O
2008 O
) O
, O
we O
only O
predict O
the O
masked O
words O
rather O
than O
recon- O
structing O
the O
entire O
input O
. O

Although O
this O
allows O
us O
to O
obtain O
a O
bidirec- O
tional O
pre O
- O
trained O
model O
, O
a O
downside O
is O
that O
we O
are O
creating O
a O
mismatch O
between O
pre O
- O
training O
and O
ﬁne O
- O
tuning O
, O
since O
the O
[ O
MASK O
] O
token O
does O
not O
ap- O
pear O
during O
ﬁne O
- O
tuning O
. O

To O
mitigate O
this O
, O
we O
do O
not O
always O
replace O
“ O
masked O
” O
words O
with O
the O
ac- O

tual[MASK O
] O
token O
. O

The O
training O
data O
generator O
chooses O
15% B-HyperparameterValue
of O
the O
token O
positions O
at O
random O
for O
prediction O
. O

If O
the O
i O
- O
th O
token O
is O
chosen O
, O
we O
replace O
thei O
- O
th O
token O
with O
( O
1 O
) O
the O
[ O
MASK O
] O
token O
80% B-HyperparameterValue
of O
the O
time O
( O
2 O
) O
a O
random O
token O
10% B-HyperparameterValue
of O
the O
time O
( O
3 O
) O
the O
unchanged O
i O
- O
th O
token O
10% B-HyperparameterValue
of O
the O
time O
. O

Then O
, O
Tiwill O
be O
used O
to O
predict O
the O
original O
token O
with O
cross O
entropy O
loss O
. O

We O
compare O
variations O
of O
this O
procedure O
in O
Appendix O
C.2 O
. O

Task O
# O
2 O
: O
Next O
Sentence O
Prediction O
( O
NSP O
) O
Many O
important O
downstream O
tasks O
such O
as O
Ques- B-TaskName
tion I-TaskName
Answering O
( O
QA B-TaskName
) O
and O
Natural B-TaskName
Language I-TaskName
Infer- I-TaskName
ence I-TaskName
( O
NLI B-TaskName
) O
are O
based O
on O
understanding O
the O
rela- O
tionship O
between O
two O
sentences O
, O
which O
is O
not O
di- O
rectly O
captured O
by O
language O
modeling O
. O

In O
order O
to O
train O
a O
model O
that O
understands O
sentence O
rela- O
tionships O
, O
we O
pre O
- O
train O
for O
a O
binarized O
next O
sen- O
tence O
prediction O
task O
that O
can O
be O
trivially O
gener- O
ated O
from O
any O
monolingual O
corpus O
. O

Speciﬁcally O
, O
when O
choosing O
the O
sentences O
AandBfor O
each O
pre- O
training O
example O
, O
50% B-HyperparameterValue
of O
the O
time O
Bis O
the O
actual O
next O
sentence O
that O
follows O
A(labeled O
as O
IsNext O
) O
, O
and O
50% B-HyperparameterValue
of O
the O
time O
it O
is O
a O
random O
sentence O
from O
the O
corpus O
( O
labeled O
as O
NotNext O
) O
. O

As O
we O
show O
in O
Figure O
1 O
, O
Cis O
used O
for O
next O
sentence O
predic- O
tion O
( O
NSP).5Despite O
its O
simplicity O
, O
we O
demon- O
strate O
in O
Section O
5.1 O
that O
pre O
- O
training O
towards O
this O
task O
is O
very O
beneﬁcial O
to O
both O
QA B-TaskName
and O
NLI B-TaskName
. O

The O
NSP O
task O
is O
closely O
related O
to O
representation- O
learning O
objectives O
used O
in O
Jernite O
et O
al O
. O

( O
2017 O
) O
and O
Logeswaran O
and O
Lee O
( O
2018 O
) O
. O

However O
, O
in O
prior O
work O
, O
only O
sentence O
embeddings O
are O
transferred O
to O
down O
- O
stream O
tasks O
, O
where O
BERT B-MethodName
transfers O
all O
pa- O
rameters O
to O
initialize O
end O
- O
task O
model O
parameters O
. O

Pre O
- O
training O
data O
The O
pre O
- O
training O
procedure O
largely O
follows O
the O
existing O
literature O
on O
language O
model O
pre O
- O
training O
. O

For O
the O
pre O
- O
training O
corpus O
we O
use O
the O
BooksCorpus B-DatasetName
( O
800 O
M O
words O
) O

( O
Zhu O
et O
al O
. O
, O
2015 O
) O
and O
English B-DatasetName
Wikipedia I-DatasetName
( O
2,500 O
M O
words O
) O
. O

For O
Wikipedia B-DatasetName
we O
extract O
only O
the O
text O
passages O
and O
ignore O
lists O
, O
tables O
, O
and O
headers O
. O

It O
is O
criti- O
cal O
to O
use O
a O
document O
- O
level O
corpus O
rather O
than O
a O
shufﬂed O
sentence O
- O
level O
corpus O
such O
as O
the O
Billion B-DatasetName
Word I-DatasetName
Benchmark I-DatasetName
( O
Chelba O
et O
al O
. O
, O
2013 O
) O
in O
order O
to O
extract O
long O
contiguous O
sequences O
. O

3.2 O
Fine O
- O
tuning O
BERT B-MethodName
Fine O
- O
tuning O
is O
straightforward O
since O
the O
self- O
attention O
mechanism O
in O
the O
Transformer B-MethodName
al- O
lows O
BERT B-MethodName
to O
model O
many O
downstream O
tasks O
— O
whether O
they O
involve O
single O
text O
or O
text O
pairs O
— O
by O
swapping O
out O
the O
appropriate O
inputs O
and O
outputs O
. O

For O
applications O
involving O
text O
pairs O
, O
a O
common O
pattern O
is O
to O
independently O
encode O
text O
pairs O
be- O

fore O
applying O
bidirectional O
cross O
attention O
, O
such O
as O
Parikh O
et O
al O
. O

( O
2016 O
) O
; O

Seo O
et O
al O
. O
( O
2017 O
) O
. O

BERT B-MethodName
instead O
uses O
the O
self O
- O
attention O
mechanism O
to O
unify O
these O
two O
stages O
, O
as O
encoding O
a O
concatenated O
text O
pair O
with O
self O
- O
attention O
effectively O
includes O
bidi- O
rectional O
cross O
attention O
between O
two O
sentences O
. O

For O
each O
task O
, O
we O
simply O
plug O
in O
the O
task- O
speciﬁc O
inputs O
and O
outputs O
into O
BERT O
and O
ﬁne- O
tune O
all O
the O
parameters O
end O
- O
to O
- O
end O
. O

At O
the O
in- O
put O
, O
sentence O
Aand O
sentence O
Bfrom O
pre O
- O
training O
are O
analogous O
to O
( O
1 O
) O
sentence O
pairs O
in O
paraphras- B-TaskName
ing I-TaskName
, O
( O
2 O
) O
hypothesis O
- O
premise O
pairs O
in O
entailment B-TaskName
, O
( O
3 O
) O
question O
- O
passage O
pairs O
in O
question B-TaskName
answering I-TaskName
, O
and(4 O
) O
a O
degenerate O
text- O
? O

pair O
in O
text B-TaskName
classiﬁcation I-TaskName
or O
sequence B-TaskName
tagging I-TaskName
. O

At O
the O
output O
, O
the O
token O
rep- O
resentations O
are O
fed O
into O
an O
output O
layer O
for O
token- O
level O
tasks O
, O
such O
as O
sequence B-TaskName
tagging I-TaskName
or O
question B-TaskName
answering I-TaskName
, O
and O
the O
[ O
CLS O
] O
representation O
is O
fed O
into O
an O
output O
layer O
for O
classiﬁcation O
, O
such O
as O
en- B-TaskName
tailment I-TaskName
or O
sentiment B-TaskName
analysis I-TaskName
. O

Compared O
to O
pre O
- O
training O
, O
ﬁne O
- O
tuning O
is O
rela- O
tively O
inexpensive O
. O

All O
of O
the O
results O
in O
the O
pa- O
per O
can O
be O
replicated O
in O
at O
most O
1 O
hour O
on O
a O
sin- O
gle O
Cloud O
TPU O
, O
or O
a O
few O
hours O
on O
a O
GPU O
, O
starting O
from O
the O
exact O
same O
pre O
- O
trained O
model.7We O
de- O
scribe O
the O
task O
- O
speciﬁc O
details O
in O
the O
correspond- O
ing O
subsections O
of O
Section O
4 O
. O

More O
details O
can O
be O
found O
in O
Appendix O
A.5 O
. O

4 O
Experiments O
In O
this O
section O
, O
we O
present O
BERT B-MethodName
ﬁne O
- O
tuning O
re- O
sults O
on O
11 O
NLP O
tasks O
. O

4.1 O
GLUE O
The O
General B-DatasetName
Language I-DatasetName
Understanding I-DatasetName
Evaluation I-DatasetName
( O
GLUE B-DatasetName
) O
benchmark O
( O
Wang O
et O
al O
. O
, O
2018a O
) O
is O
a O
col- O
lection O
of O
diverse O
natural O
language O
understanding O
tasks O
. O

Detailed O
descriptions O
of O
GLUE B-DatasetName
datasets O
are O
included O
in O
Appendix O
B.1 O
. O

To O
ﬁne O
- O
tune O
on O
GLUE B-DatasetName
, O
we O
represent O
the O
input O
sequence O
( O
for O
single O
sentence O
or O
sentence O
pairs O
) O
as O
described O
in O
Section O
3 O
, O
and O
use O
the O
ﬁnal O
hid- O
den O
vectorC2RHcorresponding O
to O
the O
ﬁrst O
input O
token O
( O
[ O
CLS O
] O
) O
as O
the O
aggregate O
representa- O
tion O
. O

The O
only O
new O
parameters O
introduced O
during O
ﬁne O
- O
tuning O
are O
classiﬁcation O
layer B-HyperparameterName
weights I-HyperparameterName
W B-HyperparameterName
RKH O
, O
where O
K B-HyperparameterName
is O
the O
number B-HyperparameterName
of I-HyperparameterName
labels I-HyperparameterName
. O

We O
com- O
pute O
a O
standard O
classiﬁcation O
loss O
with O
CandW O
, O
i.e. O
,log(softmax O
( O
CWT O
) O
) O
. O

We O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
and O
ﬁne O
- O
tune O
for O
3 B-HyperparameterValue
epochs B-HyperparameterName
over O
the O
data O
for O
all O
GLUE B-DatasetName
tasks O
. O

For O
each O
task O
, O
we O
selected O
the O
best O
ﬁne O
- O
tuning O
learning B-HyperparameterName
rate I-HyperparameterName
( O
among O
5e-5 B-HyperparameterValue
, O
4e-5 B-HyperparameterValue
, O
3e-5 B-HyperparameterValue
, O
and O
2e-5 B-HyperparameterValue
) O
on O
the O
Dev O
set O
. O

Additionally O
, O
for O
BERT B-MethodName
LARGE I-MethodName
we O
found O
that O
ﬁne- O
tuning O
was O
sometimes O
unstable O
on O
small O
datasets O
, O
so O
we O
ran O
several O
random O
restarts O
and O
selected O
the O
best O
model O
on O
the O
Dev O
set O
. O

With O
random O
restarts O
, O
we O
use O
the O
same O
pre O
- O
trained O
checkpoint O
but O
per- O
form O
different O
ﬁne O
- O
tuning O
data O
shufﬂing O
and O

clas- O
siﬁer O
layer O
initialization.9 O
Results O
are O
presented O
in O
Table O
1 O
. O

Both O
BERT B-MethodName
BASE I-MethodName
and O
BERT B-MethodName
LARGE I-MethodName
outperform O
all O
sys- O
tems O
on O
all O
tasks O
by O
a O
substantial O
margin O
, O
obtaining O
4.5% B-MetricValue
and O
7.0% B-MetricValue
respective O
average O
accuracy B-MetricName
im- O
provement O
over O
the O
prior O
state O
of O
the O
art O
. O

Note O
that O
BERT B-MethodName
BASE I-MethodName
and O
OpenAI B-MethodName
GPT I-MethodName
are O
nearly O
identical O
in O
terms O
of O
model O
architecture O
apart O
from O
the O
at- O
tention O
masking O
. O

For O
the O
largest O
and O
most O
widely O
reported O
GLUE B-DatasetName
task O
, O
MNLI B-TaskName
, O
BERT B-MethodName
obtains O
a O
4.6% B-MetricValue
absolute O
accuracy B-MetricName
improvement O
. O

On O
the O
ofﬁcial O
GLUE B-DatasetName
leaderboard10 O
, O
BERT B-MethodName
LARGE I-MethodName
obtains O
a O
score O
of O
80.5 B-MetricValue
, O
compared O
to O
OpenAI B-MethodName
GPT I-MethodName
, O
which O
obtains O
72.8 B-MetricValue
as O
of O
the O
date O
of O
writing O
. O

We O
ﬁnd O
that O
BERT B-MethodName
LARGE I-MethodName
signiﬁcantly O
outper- O
forms O
BERT B-MethodName
BASE I-MethodName
across O
all O
tasks O
, O
especially O
those O
with O
very O
little O
training O
data O
. O

The O
effect O
of O
model O
size O
is O
explored O
more O
thoroughly O
in O
Section O
5.2 O
. O

4.2 O
SQuAD O
v1.1 O
The O
Stanford B-DatasetName
Question I-DatasetName
Answering I-DatasetName
Dataset I-DatasetName
( O
SQuAD B-DatasetName
v1.1 I-DatasetName
) O
is O
a O
collection O
of O
100k O
crowd- O
sourced O
question O
/ O
answer O
pairs O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
. O

Given O
a O
question O
and O
a O
passage O
from O
Wikipedia O
containing O
the O
answer O
, O
the O
task O
is O
to O
predict O
the O
answer O
text O
span O
in O
the O
passage O
. O

As O
shown O
in O
Figure O
1 O
, O
in O
the O
question B-TaskName
answer- I-TaskName
ing I-TaskName
task O
, O
we O
represent O
the O
input O
question O
and O
pas- O
sage O
as O
a O
single O
packed O
sequence O
, O
with O
the O
ques- O
tion O
using O
the O
Aembedding O
and O
the O
passage O
using O
theBembedding O
. O

We O
only O
introduce O
a O
start O
vec- O
torS2RHand O
an O
end O
vector O
E2RHduring O
ﬁne O
- O
tuning O
. O

The O
probability O
of O
word O
ibeing O
the O
start O
of O
the O
answer O
span O
is O
computed O
as O
a O
dot O
prod- O
uct O
between O
TiandSfollowed O
by O
a O
softmax O
over O
all O
of O
the O
words O
in O
the O
paragraph O
: O
Pi O
= O
eSTiP O
jeSTj O
. O

The O
analogous O
formula O
is O
used O
for O
the O
end O
of O
the O
answer O
span O
. O

The O
score O
of O
a O
candidate O
span O
from O
positionito O
positionjis O
deﬁned O
as O
STi+ETj O
, O
and O
the O
maximum O
scoring O
span O
where O
jiis O
used O
as O
a O
prediction O
. O

The O
training O
objective O
is O
the O
sum O
of O
the O
log O
- O
likelihoods O
of O
the O
correct O
start O
and O
end O
positions O
. O

We O
ﬁne O
- O
tune O
for O
3 B-HyperparameterValue
epochs B-HyperparameterName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5e-5 B-HyperparameterValue
and O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
. O

Table O
2 O
shows O
top O
leaderboard O
entries O
as O
well O
as O
results O
from O
top O
published O
systems O
( O
Seo O
et O
al O
. O
, O
2017 O
; O
Clark O
and O
Gardner O
, O
2018 O
; O
Peters O
et O
al O
. O
, O
2018a O
; O
Hu O
et O
al O
. O
, O
2018 O
) O
. O

The O
top O
results O
from O
the O
SQuAD B-DatasetName
leaderboard O
do O
not O
have O
up O
- O
to O
- O
date O
public O
system O
descriptions O
available,11and O
are O
allowed O
to O
use O
any O
public O
data O
when O
training O
their O
systems O
. O

We O
therefore O
use O
modest O
data O
augmentation O
in O
our O
system O
by O
ﬁrst O
ﬁne O
- O
tuning O
on O
TriviaQA B-DatasetName
( O
Joshi O
et O
al O
. O
, O
2017 O
) O
befor O
ﬁne O
- O
tuning O
on O
SQuAD B-DatasetName
. O

Our O
best O
performing O
system O
outperforms O
the O
top O
leaderboard O
system O
by O
+1.5 B-MetricValue
F1 B-MetricName
in O
ensembling O
and O
+1.3 B-MetricValue
F1 B-MetricName
as O
a O
single O
system O
. O

In O
fact O
, O
our O
single O
BERT B-MethodName
model O
outperforms O
the O
top O
ensemble O
sys- O
tem O
in O
terms O
of O
F1 B-MetricName
score O
. O

Without O
TriviaQA B-DatasetName
ﬁne- O
tuning O
data O
, O
we O
only O
lose O
0.1 B-MetricValue
- I-MetricValue
0.4 I-MetricValue
F1 B-MetricName
, O
still O
outper- O
forming O
all O
existing O
systems O
by O
a O
wide O
margin O
. O

4.3 O
SQuAD O
v2.0 O

The O
SQuAD B-DatasetName
2.0 I-DatasetName
task O
extends O
the O
SQuAD B-DatasetName
1.1 I-DatasetName
problem O
deﬁnition O
by O
allowing O
for O
the O
possibility O
that O
no O
short O
answer O
exists O
in O
the O
provided O
para- O
graph O
, O
making O
the O
problem O
more O
realistic O
. O

We O
use O
a O
simple O
approach O
to O
extend O
the O
SQuAD B-DatasetName
v1.1 I-DatasetName
BERT B-MethodName
model O
for O
this O
task O
. O

We O
treat O
ques- O
tions O
that O
do O
not O
have O
an O
answer O
as O
having O
an O
an- O
swer O
span O
with O
start O
and O
end O
at O
the O
[ O
CLS O
] O
to- O
ken O
. O

The O
probability O
space O
for O
the O
start O
and O
end O
answer O
span O
positions O
is O
extended O
to O
include O
the O
position O
of O
the O
[ O
CLS O
] O
token O
. O

For O
prediction O
, O
we O
compare O
the O
score O
of O
the O
no O
- O
answer O
span O
: O
snull= O
SC+ECto O
the O
score O
of O
the O
best O
non O
- O
null O
span O
^si;j O
= O
maxjiSTi+ETj O
. O

We O
predict O
a O
non O
- O
null O
answer O
when O
^si;j O
> O
s O
null+ O

, O
where O
the O
thresh- O
old O

is O
selected O
on O
the O
dev O
set O
to O
maximize O
F1 B-MetricName
. O

We O
did O
not O
use O
TriviaQA B-DatasetName
data O
for O
this O
model O
. O

We O
ﬁne O
- O
tuned O
for O
2 B-HyperparameterValue
epochs B-HyperparameterName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5e-5 B-HyperparameterValue
and O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
48 B-HyperparameterValue
. O

The O
results O
compared O
to O
prior O
leaderboard O
en- O

tries O
and O
top O
published O
work O
( O
Sun O
et O
al O
. O
, O
2018 O
; O
Wang O
et O
al O
. O
, O
2018b O
) O
are O
shown O
in O
Table O
3 O
, O
exclud- O
ing O
systems O
that O
use O
BERT B-MethodName
as O
one O
of O
their O
com- O
ponents O
. O

We O
observe O
a O
+5.1 B-MetricValue
F1 B-MetricName
improvement O
over O
the O
previous O
best O
system O
. O

4.4 O
SWAG O
The O
Situations B-DatasetName
With I-DatasetName
Adversarial I-DatasetName
Generations I-DatasetName
( O
SWAG B-DatasetName
) O
dataset O
contains O
113k O
sentence O
- O
pair O
com- O
pletion O
examples O
that O
evaluate O
grounded O
common- O
sense O
inference O
( O
Zellers O
et O
al O
. O
, O
2018 O
) O
. O

Given O
a O
sen- O
tence O
, O
the O
task O
is O
to O
choose O
the O
most O
plausible O
con- O
tinuation O
among O
four O
choices O
. O

When O
ﬁne O
- O
tuning O
on O
the O
SWAG B-DatasetName
dataset O
, O
we O
construct O
four O
input O
sequences O
, O
each O
containing O
the O
concatenation O
of O
the O
given O
sentence O
( O
sentence O
A O
) O
and O
a O
possible O
continuation O
( O
sentence O
B O
) O
. O

The O
only O
task O
- O
speciﬁc O
parameters O
introduced O
is O
a O
vec- O
tor O
whose O
dot O
product O
with O
the O
[ O
CLS O
] O
token O
rep- O

resentation O
Cdenotes O
a O
score O
for O
each O
choice O
which O
is O
normalized O
with O
a O
softmax O
layer O
. O

We O
ﬁne O
- O
tune O
the O
model O
for O
3 B-HyperparameterValue
epochs B-HyperparameterName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
2e-5 B-HyperparameterValue
and O
a O
batch B-HyperparameterName
size I-HyperparameterValue
of O
16 B-HyperparameterValue
. O

Re- O
sults O
are O
presented O
in O
Table O
4 O
. O

BERT B-MethodName
LARGE I-MethodName
out- O
performs O
the O
authors O
’ O
baseline O
ESIM+ELMo B-MethodName
sys- O
tem O
by O
+27.1% B-MetricValue
and O
OpenAI B-MethodName
GPT I-MethodName
by O
8.3% B-MetricValue
. O

5 O
Ablation O
Studies O

In O
this O
section O
, O
we O
perform O
ablation O
experiments O
over O
a O
number O
of O
facets O
of O
BERT B-MethodName
in O
order O
to O
better O
understand O
their O
relative O
importance O
. O

Additional O
ablation O
studies O
can O
be O
found O
in O
Appendix O
C O
. O

5.1 O
Effect O
of O
Pre O
- O
training O
Tasks O
We O
demonstrate O
the O
importance O
of O
the O
deep O
bidi- O
rectionality O
of O
BERT B-MethodName
by O
evaluating O
two O
pre- O
training O
objectives O
using O
exactly O
the O
same O
pre- O
training O
data O
, O
ﬁne O
- O
tuning O
scheme O
, O
and O
hyperpa- O
rameters O
as O
BERT B-MethodName
BASE I-MethodName
: O
No O
NSP O
: O
A O
bidirectional O
model O
which O
is O
trained O
using O
the O
“ O
masked O
LM O
” O
( O
MLM O
) O
but O
without O
the O
“ O
next O
sentence O
prediction O
” O
( O
NSP O
) O
task O
. O

LTR O
& O
No O
NSP O
: O
A O
left O
- O
context O
- O
only O
model O
which O
is O
trained O
using O
a O
standard O
Left O
- O
to O
- O
Right O
( O
LTR O
) O
LM O
, O
rather O
than O
an O
MLM O
. O

The O
left O
- O
only O
constraint O
was O
also O
applied O
at O
ﬁne O
- O
tuning O
, O
because O
removing O
it O
introduced O
a O
pre O
- O
train/ﬁne O
- O
tune O
mismatch O
that O
degraded O
downstream O
performance O
. O

Additionally O
, O
this O
model O
was O
pre O
- O
trained O
without O
the O
NSP O
task O
. O

This O
is O
directly O
comparable O
to O
OpenAI B-MethodName
GPT I-MethodName
, O
but O
using O
our O
larger O
training O
dataset O
, O
our O
input O
repre- O
sentation O
, O
and O
our O
ﬁne O
- O
tuning O
scheme O
. O

We O
ﬁrst O
examine O
the O
impact O
brought O
by O
the O
NSP O
task O
. O

In O
Table O
5 O
, O
we O
show O
that O
removing O
NSP O
hurts O
performance O
signiﬁcantly O
on O
QNLI B-DatasetName
, O
MNLI B-DatasetName
, O
and O
SQuAD B-DatasetName
1.1 I-DatasetName
. O

Next O
, O
we O
evaluate O
the O
impact O
of O
training O
bidirectional O
representations O
by O
com- O
paring O
“ O
No O
NSP O
” O
to O
“ O
LTR O
& O
No O
NSP O
” O
. O

The O
LTR O
model O
performs O
worse O
than O
the O
MLM O
model O
on O
all O
tasks O
, O
with O
large O
drops O
on O
MRPC B-DatasetName
and O
SQuAD B-DatasetName
. O

For O
SQuAD B-DatasetName
it O
is O
intuitively O
clear O
that O
a O
LTR O
model O
will O
perform O
poorly O
at O
token O
predictions O
, O
since O
the O
token O
- O
level O
hidden O
states O
have O
no O
right- O
side O
context O
. O

In O
order O
to O
make O
a O
good O
faith O
at- O
tempt O
at O
strengthening O
the O
LTR O
system O
, O
we O
added O
a O
randomly O
initialized O
BiLSTM O
on O
top O
. O

This O
does O
signiﬁcantly O
improve O
results O
on O
SQuAD B-DatasetName
, O
but O
theresults O
are O
still O
far O
worse O
than O
those O
of O
the O
pre- O
trained O
bidirectional O
models O
. O

The O
BiLSTM O
hurts O
performance O
on O
the O
GLUE B-DatasetName
tasks O
. O

We O
recognize O
that O
it O
would O
also O
be O
possible O
to O
train O
separate O
LTR O
and O
RTL O
models O
and O
represent O
each O
token O
as O
the O
concatenation O
of O
the O
two O
mod- O
els O
, O
as O
ELMo B-MethodName
does O
. O

However O
: O
( O
a O
) O
this O
is O
twice O
as O
expensive O
as O
a O
single O
bidirectional O
model O
; O
( O
b O
) O
this O
is O
non O
- O
intuitive O
for O
tasks O
like O
QA B-TaskName
, O
since O
the O
RTL O
model O
would O
not O
be O
able O
to O
condition O
the O
answer O
on O
the O
question O
; O
( O
c O
) O
this O
it O
is O
strictly O
less O
powerful O
than O
a O
deep O
bidirectional O
model O
, O
since O
it O
can O
use O
both O
left O
and O
right O
context O
at O
every O
layer O
. O

5.2 O
Effect O
of O
Model O
Size O
In O
this O
section O
, O
we O
explore O
the O
effect O
of O
model O
size O
on O
ﬁne O
- O
tuning O
task O
accuracy O
. O

We O
trained O
a O
number O
of O
BERT B-MethodName
models O
with O
a O
differing O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
, O
hidden B-HyperparameterName
units I-HyperparameterName
, O
and O
attention B-HyperparameterName
heads I-HyperparameterName
, O
while O
otherwise O
using O
the O
same O
hyperparameters O
and O
training O
pro- O
cedure O
as O
described O
previously O
. O

Results O
on O
selected O
GLUE B-DatasetName
tasks O
are O
shown O
in O
Table O
6 O
. O

In O
this O
table O
, O
we O
report O
the O
average B-MetricName
Dev I-MetricName
Set I-MetricName
accuracy I-MetricName
from O
5 O
random O
restarts O
of O
ﬁne O
- O
tuning O
. O

We O
can O
see O
that O
larger O
models O
lead O
to O
a O
strict O
ac- O
curacy O
improvement O
across O
all O
four O
datasets O
, O
even O
for O
MRPC B-DatasetName
which O
only O
has O
3,600 O
labeled O
train- O
ing O
examples O
, O
and O
is O
substantially O
different O
from O
the O
pre O
- O
training O
tasks O
. O

It O
is O
also O
perhaps O
surpris- O
ing O
that O
we O
are O
able O
to O
achieve O
such O
signiﬁcant O
improvements O
on O
top O
of O
models O
which O
are O
al- O
ready O
quite O
large O
relative O
to O
the O
existing O
literature O
. O

For O
example O
, O
the O
largest O
Transformer O
explored O
in O
Vaswani O
et O
al O
. O

( O
2017 O
) O
is O
( O
L B-HyperparameterName
= O
6 B-HyperparameterValue
, O
H B-HyperparameterName
= O
1024 B-HyperparameterValue
, O
A B-HyperparameterName
= O
16 B-HyperparameterValue
) O
with O
100M B-HyperparameterValue
parameters O
for O
the O
encoder O
, O
and O
the O
largest O
Transformer O
we O
have O
found O
in O
the O
literature O
is O
( O
L B-HyperparameterName
= O
64 B-HyperparameterValue
, O
H B-HyperparameterName
= O
512 B-HyperparameterValue
, O
A B-HyperparameterName
= O
2 B-HyperparameterValue
) O
with O
235M B-HyperparameterValue
parameters O
( O
Al O
- O
Rfou O
et O
al O
. O
, O
2018 O
) O
. O

By O
contrast O
, O
BERT B-MethodName
BASE I-MethodName
contains O
110M B-HyperparameterValue
parameters O
and O
BERT B-MethodName
LARGE I-MethodName
con- O
tains O
340M B-HyperparameterValue
parameters O
. O

It O
has O
long O
been O
known O
that O
increasing O
the O
model O
size O
will O
lead O
to O
continual O
improvements O
on O
large O
- O
scale O
tasks O
such O
as O
machine O
translation O
and O
language O
modeling O
, O
which O
is O
demonstrated O
by O
the O
LM O
perplexity O
of O
held O
- O
out O
training O
data O
shown O
in O
Table O
6 O
. O

However O
, O
we O
believe O
that O
this O
is O
the O
ﬁrst O
work O
to O
demonstrate O
convinc- O
ingly O
that O
scaling O
to O
extreme O
model O
sizes O
also O
leads O
to O
large O
improvements O
on O
very O
small O
scale O
tasks O
, O
provided O
that O
the O
model O
has O
been O
sufﬁ- O
ciently O
pre O
- O
trained O
. O

Peters O
et O
al O
. O
( O
2018b O
) O
presented O
mixed O
results O
on O
the O
downstream O
task O
impact O
of O
increasing O
the O
pre O
- O
trained O
bi O
- O
LM O
size O
from O
two O
to O
four O
layers O
and O
Melamud O
et O
al O
. O

( O
2016 O
) O

men- O
tioned O
in O
passing O
that O
increasing O
hidden O
dimen- O
sion O
size O
from O
200 O
to O
600 O
helped O
, O
but O
increasing O
further O
to O
1,000 O
did O
not O
bring O
further O
improve- O

ments O
. O

Both O
of O
these O
prior O
works O
used O
a O
feature- O
based O
approach O
— O
we O
hypothesize O
that O
when O
the O
model O
is O
ﬁne O
- O
tuned O
directly O
on O
the O
downstream O
tasks O
and O
uses O
only O
a O
very O
small O
number O
of O
ran- O
domly O
initialized O
additional O
parameters O
, O
the O
task- O
speciﬁc O
models O
can O
beneﬁt O
from O
the O
larger O
, O
more O
expressive O
pre O
- O
trained O
representations O
even O
when O
downstream O
task O
data O
is O
very O
small O
. O

5.3 O
Feature O
- O
based O
Approach O
with O
BERT O
All O
of O
the O
BERT B-MethodName
results O
presented O
so O
far O
have O
used O
the O
ﬁne O
- O
tuning O
approach O
, O
where O
a O
simple O
classiﬁ- O
cation O
layer O
is O
added O
to O
the O
pre O
- O
trained O
model O
, O
and O
all O
parameters O
are O
jointly O
ﬁne O
- O
tuned O
on O
a O
down- O
stream O
task O
. O

However O
, O
the O
feature O
- O
based O
approach O
, O
where O
ﬁxed O
features O
are O
extracted O
from O
the O
pre- O
trained O
model O
, O
has O
certain O
advantages O
. O

First O
, O
not O
all O
tasks O
can O
be O
easily O
represented O
by O
a O
Trans- O
former O
encoder O
architecture O
, O
and O
therefore O
require O
a O
task O
- O
speciﬁc O
model O
architecture O
to O
be O
added O
. O

Second O
, O
there O
are O
major O
computational O
beneﬁts O
to O
pre O
- O
compute O
an O
expensive O
representation O
of O
the O
training O
data O
once O
and O
then O
run O
many O
experiments O
with O
cheaper O
models O
on O
top O
of O
this O
representation O
. O

In O
this O
section O
, O
we O
compare O
the O
two O
approaches O
by O
applying O
BERT B-MethodName
to O
the O
CoNLL-2003 B-TaskName
Named I-TaskName
Entity I-TaskName
Recognition I-TaskName
( O
NER B-TaskName
) O
task O
( O
Tjong O
Kim O
Sang O
and O
De O
Meulder O
, O
2003 O
) O
. O

In O
the O
input O
to O
BERT B-MethodName
, O
we O
use O
a O
case O
- O
preserving O
WordPiece B-MethodName
model O
, O
and O
we O
include O
the O
maximal O
document O
context O
provided O
by O
the O
data O
. O

Following O
standard O
practice O
, O
we O
for- O
mulate O
this O
as O
a O
tagging B-TaskName
task O
but O
do O
not O
use O
a O
CRF O
layer O
in O
the O
output O
. O

We O
use O
the O
representation O
of O
the O
ﬁrst O
sub O
- O
token O
as O
the O
input O
to O
the O
token O
- O
level O
classiﬁer O
over O
the O
NER B-TaskName
label O
set O
. O

To O
ablate O
the O
ﬁne O
- O
tuning O
approach O
, O
we O
apply O
the O
feature O
- O
based O
approach O
by O
extracting O
the O
activa- O
tions O
from O
one O
or O
more O
layers O
without O
ﬁne O
- O
tuning O
any O
parameters O
of O
BERT B-MethodName
. O

These O
contextual O
em- O
beddings O
are O
used O
as O
input O
to O
a O
randomly O
initial- O
ized O
two B-HyperparameterValue
- O
layer B-HyperparameterName
768 B-HyperparameterValue
- O
dimensional B-HyperparameterName
BiLSTM O
before O
the O
classiﬁcation O
layer O
. O

Results O
are O
presented O
in O
Table O
7 O
. O

BERT B-MethodName
LARGE I-MethodName
performs O
competitively O
with O
state O
- O
of O
- O
the O
- O
art O
meth- O
ods O
. O

The O
best O
performing O
method O
concatenates O
the O
token O
representations O
from O
the O
top O
four O
hidden O
lay- O
ers O
of O
the O
pre O
- O
trained O
Transformer O
, O
which O
is O
only O
0.3 B-MetricValue
F1 B-MetricName
behind O
ﬁne O
- O
tuning O
the O
entire O
model O
. O

This O
demonstrates O
that O
BERT B-MethodName
is O
effective O
for O
both O
ﬁne- O
tuning O
and O
feature O
- O
based O
approaches O
. O

6 O
Conclusion O
Recent O
empirical O
improvements O
due O
to O
transfer O
learning O
with O
language O
models O
have O
demonstrated O
that O
rich O
, O
unsupervised O
pre O
- O
training O
is O
an O
integral O
part O
of O
many O
language O
understanding O
systems O
. O

In O
particular O
, O
these O
results O
enable O
even O
low O
- O
resource O
tasks O
to O
beneﬁt O
from O
deep O
unidirectional O
architec- O
tures O
. O

Our O
major O
contribution O
is O
further O
general- O
izing O
these O
ﬁndings O
to O
deep O
bidirectional O
architec- O
tures O
, O
allowing O
the O
same O
pre O
- O
trained O
model O
to O
suc- O
cessfully O
tackle O
a O
broad O
set O
of O
NLP O
tasks O
. O

References O
Alan O
Akbik O
, O
Duncan O
Blythe O
, O
and O
Roland O
V O
ollgraf O
. O

2018 O
. O

Contextual O
string O
embeddings O
for O
sequence O
labeling O
. O

In O
Proceedings O
of O
the O
27th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
1638–1649 O
. O

Rami O
Al O
- O
Rfou O
, O
Dokook O
Choe O
, O
Noah O
Constant O
, O
Mandy O
Guo O
, O
and O
Llion O
Jones O
. O

2018 O
. O

Character O
- O
level O
lan- O
guage O
modeling O
with O
deeper O
self O
- O
attention O
. O

arXiv O
preprint O
arXiv:1808.04444 O
. O

Rie O
Kubota O
Ando O
and O
Tong O
Zhang O
. O

2005 O
. O

A O
framework O
for O
learning O
predictive O
structures O
from O
multiple O
tasks O
and O
unlabeled O
data O
. O

Journal O
of O
Machine O
Learning O
Research O
, O
6(Nov):1817–1853 O
. O

Luisa O
Bentivogli O
, O
Bernardo O
Magnini O
, O
Ido O
Dagan O
, O
Hoa O
Trang O
Dang O
, O
and O
Danilo O
Giampiccolo O
. O

2009 O
. O

The O
ﬁfth O
PASCAL O
recognizing O
textual O
entailment O
challenge O
. O

In O
TAC O
. O

NIST O
. O

John O
Blitzer O
, O
Ryan O
McDonald O
, O
and O
Fernando O
Pereira O
. O
2006 O
. O

Domain O
adaptation O
with O
structural O
correspon- O
dence O
learning O
. O

In O
Proceedings O
of O
the O
2006 O
confer- O
ence O
on O
empirical O
methods O
in O
natural O
language O
pro- O

cessing O
, O
pages O
120–128 O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Samuel O
R. O
Bowman O
, O
Gabor O
Angeli O
, O
Christopher O
Potts O
, O
and O
Christopher O
D. O
Manning O
. O

2015 O
. O

A O
large O
anno- O
tated O
corpus O
for O
learning O
natural O
language O
inference O
. O

InEMNLP O
. O

Association O
for O
Computational O
Linguis- O
tics O
. O

Peter O
F O
Brown O
, O
Peter O
V O
Desouza O
, O
Robert O
L O
Mercer O
, O
Vincent O
J O
Della O
Pietra O
, O
and O
Jenifer O
C O
Lai O
. O

1992 O
. O

Class O
- O
based O
n O
- O
gram O
models O
of O
natural O
language O
. O

Computational O
linguistics O
, O
18(4):467–479 O
. O

Daniel O
Cer O
, O
Mona O
Diab O
, O
Eneko O
Agirre O
, O
Inigo O
Lopez- O
Gazpio O
, O
and O
Lucia O
Specia O
. O

2017 O
. O

Semeval-2017 O
task O
1 O
: O
Semantic O
textual O
similarity O
multilingual O
and O
crosslingual O
focused O
evaluation O
. O

In O
Proceedings O
of O
the O
11th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval-2017 O
) O
, O
pages O
1–14 O
, O
Vancou- O
ver O
, O
Canada O
. O

Association O
for O
Computational O
Lin- O
guistics O
. O

Ciprian O
Chelba O
, O
Tomas O
Mikolov O
, O
Mike O
Schuster O
, O
Qi O
Ge O
, O
Thorsten O
Brants O
, O
Phillipp O
Koehn O
, O
and O
Tony O
Robin- O
son O
. O
2013 O
. O

One O
billion O
word O
benchmark O
for O
measur- O
ing O
progress O
in O
statistical O
language O
modeling O
. O

arXiv O
preprint O
arXiv:1312.3005 O
. O

Z. O
Chen O
, O
H. O
Zhang O
, O
X. O
Zhang O
, O
and O
L. O
Zhao O
. O

2018 O
. O

Quora O
question O
pairs O
. O

Christopher O
Clark O
and O
Matt O
Gardner O
. O

2018 O
. O

Simple O
and O
effective O
multi O
- O
paragraph O
reading O
comprehen- O
sion O
. O

In O
ACL.Kevin O
Clark O
, O
Minh O
- O
Thang O
Luong O
, O
Christopher O
D O
Man- O
ning O
, O
and O
Quoc O
Le O
. O
2018 O
. O

Semi O
- O
supervised O
se- O
quence O
modeling O
with O
cross O
- O
view O
training O
. O

In O
Pro- O
ceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Meth- O
ods O
in O
Natural O
Language O
Processing O
, O
pages O
1914 O
– O
1925 O
. O

Ronan O
Collobert O
and O
Jason O
Weston O
. O

2008 O
. O

A O
uniﬁed O
architecture O
for O
natural O
language O
processing O
: O

Deep O
neural O
networks O
with O
multitask O
learning O
. O

In O
Pro- O
ceedings O
of O
the O
25th O
international O
conference O
on O
Machine O
learning O
, O
pages O
160–167 O
. O
ACM O
. O

Alexis O
Conneau O
, O
Douwe O
Kiela O
, O
Holger O
Schwenk O
, O
Lo O
¨ıc O
Barrault O
, O
and O
Antoine O
Bordes O
. O

2017 O
. O

Supervised O
learning O
of O
universal O
sentence O
representations O
from O
natural O
language O
inference O
data O
. O

In O
Proceedings O
of O
the O
2017 O
Conference O
on O
Empirical O
Methods O
in O
Nat- O
ural O
Language O
Processing O
, O
pages O
670–680 O
, O
Copen- O
hagen O
, O
Denmark O
. O

Association O
for O
Computational O
Linguistics O
. O

Andrew O
M O
Dai O
and O
Quoc O
V O
Le O
. O
2015 O
. O

Semi O
- O
supervised O
sequence O
learning O
. O

In O
Advances O
in O
neural O
informa- O
tion O
processing O
systems O
, O
pages O
3079–3087 O
. O

J. O
Deng O
, O
W. O
Dong O
, O
R. O
Socher O
, O
L.-J. O
Li O
, O
K. O
Li O
, O
and O
L. O
Fei- O
Fei O
. O
2009 O
. O

ImageNet O
: O

A O
Large O
- O
Scale O
Hierarchical O
Image O
Database O
. O

In O
CVPR09 O
. O

William O
B O
Dolan O
and O
Chris O
Brockett O
. O

2005 O
. O

Automati- O
cally O
constructing O
a O
corpus O
of O
sentential O
paraphrases O
. O

InProceedings O
of O
the O
Third O
International O
Workshop O
on O
Paraphrasing O
( O
IWP2005 O
) O
. O

William O
Fedus O
, O
Ian O
Goodfellow O
, O
and O
Andrew O
M O
Dai O
. O
2018 O
. O

Maskgan O
: O
Better O
text O
generation O
via O
ﬁlling O
in O
the.arXiv O
preprint O
arXiv:1801.07736 O
. O

Dan O
Hendrycks O
and O
Kevin O
Gimpel O
. O

2016 O
. O

Bridging O
nonlinearities O
and O
stochastic O
regularizers O
with O
gaus- O
sian O
error O
linear O
units O
. O

CoRR O
, O
abs/1606.08415 O
. O

Felix O
Hill O
, O
Kyunghyun O
Cho O
, O
and O
Anna O
Korhonen O
. O

2016 O
. O

Learning O
distributed O
representations O
of O
sentences O
from O
unlabelled O
data O
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Jeremy O
Howard O
and O
Sebastian O
Ruder O
. O

2018 O
. O

Universal O
language O
model O
ﬁne O
- O
tuning O
for O
text O
classiﬁcation O
. O

In O
ACL O
. O

Association O
for O
Computational O
Linguistics O
. O

Minghao O
Hu O
, O
Yuxing O
Peng O
, O
Zhen O
Huang O
, O
Xipeng O
Qiu O
, O
Furu O
Wei O
, O
and O
Ming O
Zhou O
. O

2018 O
. O

Reinforced O
mnemonic O
reader O
for O
machine O
reading O
comprehen- O
sion O
. O

In O
IJCAI O
. O

Yacine O
Jernite O
, O
Samuel O
R. O
Bowman O
, O
and O
David O
Son- O
tag O
. O

2017 O
. O

Discourse O
- O
based O
objectives O
for O
fast O
un- O
supervised O
sentence O
representation O
learning O
. O

CoRR O
, O
abs/1705.00557 O
. O

4181Mandar O
Joshi O
, O
Eunsol O
Choi O
, O
Daniel O
S O
Weld O
, O
and O
Luke O
Zettlemoyer O
. O
2017 O
. O

Triviaqa O
: O

A O
large O
scale O
distantly O
supervised O
challenge O
dataset O
for O
reading O
comprehen- O
sion O
. O

In O
ACL O
. O

Ryan O
Kiros O
, O
Yukun O
Zhu O
, O
Ruslan O
R O
Salakhutdinov O
, O
Richard O
Zemel O
, O
Raquel O
Urtasun O
, O
Antonio O
Torralba O
, O
and O
Sanja O
Fidler O
. O

2015 O
. O

Skip O
- O
thought O
vectors O
. O

In O
Advances O
in O
neural O
information O
processing O
systems O
, O
pages O
3294–3302 O
. O

Quoc O
Le O
and O
Tomas O
Mikolov O
. O

2014 O
. O

Distributed O
rep- O
resentations O
of O
sentences O
and O
documents O
. O

In O
Inter- O
national O
Conference O
on O
Machine O
Learning O
, O
pages O
1188–1196 O
. O

Hector O
J O
Levesque O
, O
Ernest O
Davis O
, O
and O
Leora O
Morgen- O
stern O
. O

2011 O
. O

The O
winograd O
schema O
challenge O
. O

In O
Aaai O
spring O
symposium O
: O
Logical O
formalizations O
of O
commonsense O
reasoning O
, O
volume O
46 O
, O
page O
47 O
. O

Lajanugen O
Logeswaran O
and O
Honglak O
Lee O
. O
2018 O
. O

An O
efﬁcient O
framework O
for O
learning O
sentence O
represen- O

tations O
. O

In O
International O
Conference O
on O
Learning O
Representations O
. O

Bryan O
McCann O
, O
James O
Bradbury O
, O
Caiming O
Xiong O
, O
and O
Richard O
Socher O
. O

2017 O
. O

Learned O
in O
translation O
: O
Con- O
textualized O
word O
vectors O
. O

In O
NIPS O
. O

Oren O
Melamud O
, O
Jacob O
Goldberger O
, O
and O
Ido O
Dagan O
. O

2016 O
. O

context2vec O
: O
Learning O
generic O
context O
em- O
bedding O
with O
bidirectional O
LSTM O
. O

In O
CoNLL O
. O

Tomas O
Mikolov O
, O
Ilya O
Sutskever O
, O
Kai O
Chen O
, O
Greg O
S O
Cor- O
rado O
, O
and O
Jeff O
Dean O
. O

2013 O
. O

Distributed O
representa- O
tions O
of O
words O
and O
phrases O
and O
their O
compositional- O
ity O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
26 O
, O
pages O
3111–3119 O
. O

Curran O
Associates O
, O
Inc. O
Andriy O
Mnih O
and O
Geoffrey O
E O
Hinton O
. O

2009 O
. O

A O
scal- O
able O
hierarchical O
distributed O
language O
model O
. O

In O
D. O
Koller O
, O
D. O
Schuurmans O
, O
Y O
. O

Bengio O
, O
and O
L. O
Bot- O
tou O
, O
editors O
, O
Advances O
in O
Neural O
Information O
Pro- O

cessing O
Systems O
21 O
, O
pages O
1081–1088 O
. O

Curran O
As- O
sociates O
, O
Inc. O

Ankur O
P O
Parikh O
, O
Oscar O
T O
¨ackstr O
¨om O
, O
Dipanjan O
Das O
, O
and O
Jakob O
Uszkoreit O
. O

2016 O
. O

A O
decomposable O
attention O
model O
for O
natural O
language O
inference O
. O

In O
EMNLP O
. O

Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christo- O
pher O
D. O
Manning O
. O

2014 O
. O

Glove O
: O
Global O
vectors O
for O
word O
representation O
. O

In O
Empirical O
Methods O
in O
Nat- O
ural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
1532 O
– O
1543 O
. O

Matthew O
Peters O
, O
Waleed O
Ammar O
, O
Chandra O
Bhagavat- O
ula O
, O
and O
Russell O
Power O
. O
2017 O
. O

Semi O
- O
supervised O
se- O
quence O
tagging O
with O
bidirectional O
language O
models O
. O

InACL O
. O

Matthew O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O

2018a O
. O

Deep O
contextualized O
word O
rep- O
resentations O
. O

In O
NAACL O
.Matthew O

Peters O
, O
Mark O
Neumann O
, O
Luke O
Zettlemoyer O
, O
and O
Wen O
- O
tau O
Yih O
. O
2018b O
. O

Dissecting O
contextual O
word O
embeddings O
: O
Architecture O
and O
representation O
. O

InProceedings O
of O
the O
2018 O
Conference O
on O
Empiri- O
cal O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1499–1509 O
. O

Alec O
Radford O
, O
Karthik O
Narasimhan O
, O
Tim O
Salimans O
, O
and O
Ilya O
Sutskever O
. O
2018 O
. O

Improving O
language O
under- O
standing O
with O
unsupervised O
learning O
. O

Technical O
re- O
port O
, O
OpenAI O
. O

Pranav O
Rajpurkar O
, O
Jian O
Zhang O
, O
Konstantin O
Lopyrev O
, O
and O
Percy O
Liang O
. O

2016 O
. O

Squad O
: O
100,000 O
+ O
questions O
for O
machine O
comprehension O
of O
text O
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Nat- O
ural O
Language O
Processing O
, O
pages O
2383–2392 O
. O

Minjoon O
Seo O
, O
Aniruddha O
Kembhavi O
, O
Ali O
Farhadi O
, O
and O
Hannaneh O
Hajishirzi O
. O
2017 O
. O

Bidirectional O
attention O
ﬂow O
for O
machine O
comprehension O
. O

In O
ICLR O
. O

Richard O
Socher O
, O
Alex O
Perelygin O
, O
Jean O
Wu O
, O
Jason O
Chuang O
, O
Christopher O
D O
Manning O
, O
Andrew O
Ng O
, O
and O
Christopher O
Potts O
. O
2013 O
. O

Recursive O
deep O
models O
for O
semantic O
compositionality O
over O
a O
sentiment O
tree- O
bank O
. O

In O
Proceedings O
of O
the O
2013 O
conference O
on O
empirical O
methods O
in O
natural O
language O
processing O
, O
pages O
1631–1642 O
. O

Fu O
Sun O
, O
Linyang O
Li O
, O
Xipeng O
Qiu O
, O
and O
Yang O
Liu O
. O

2018 O
. O

U O
- O
net O
: O
Machine O
reading O
comprehension O
with O
unanswerable O
questions O
. O

arXiv O
preprint O
arXiv:1810.06638 O
. O

Wilson O
L O
Taylor O
. O

1953 O
. O

Cloze O
procedure O
: O
A O
new O
tool O
for O
measuring O
readability O
. O

Journalism O
Bulletin O
, O
30(4):415–433 O
. O

Erik O
F O
Tjong O
Kim O
Sang O
and O
Fien O
De O
Meulder O
. O

2003 O
. O

Introduction O
to O
the O
conll-2003 O
shared O
task O
: O
Language O
- O
independent O
named O
entity O
recognition O
. O

In O
CoNLL O
. O

Joseph O
Turian O
, O
Lev O
Ratinov O
, O
and O
Yoshua O
Bengio O
. O

2010 O
. O

Word O
representations O
: O
A O
simple O
and O
general O
method O
for O
semi O
- O
supervised O
learning O
. O

In O
Proceedings O
of O
the O
48th O
Annual O
Meeting O
of O
the O
Association O
for O
Compu- O
tational O
Linguistics O
, O
ACL O
’ O
10 O
, O
pages O
384–394 O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
Lukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
Advances O
in O
Neural O
Information O
Pro- O
cessing O
Systems O
, O
pages O
6000–6010 O
. O

Pascal O
Vincent O
, O
Hugo O
Larochelle O
, O
Yoshua O
Bengio O
, O
and O
Pierre O
- O
Antoine O
Manzagol O
. O
2008 O
. O

Extracting O
and O
composing O
robust O
features O
with O
denoising O
autoen- O
coders O
. O

In O
Proceedings O
of O
the O
25th O
international O
conference O
on O
Machine O
learning O
, O
pages O
1096–1103 O
. O
ACM O
. O

Alex O
Wang O
, O
Amanpreet O
Singh O
, O
Julian O
Michael O
, O
Fe- O
lix O
Hill O
, O
Omer O
Levy O
, O
and O
Samuel O
Bowman O
. O

2018a O
. O

Glue O
: O
A O
multi O
- O
task O
benchmark O
and O
analysis O
platform O

4182for O
natural O
language O
understanding O
. O

In O
Proceedings O
of O
the O
2018 O
EMNLP O
Workshop O
BlackboxNLP O
: O
An- O
alyzing O
and O
Interpreting O
Neural O
Networks O
for O
NLP O
, O
pages O
353–355 O
. O

Wei O
Wang O
, O
Ming O
Yan O
, O
and O
Chen O
Wu O
. O
2018b O
. O

Multi- O

granularity O
hierarchical O
attention O
fusion O
networks O
for O
reading O
comprehension O
and O
question O
answering O
. O

InProceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
As- O
sociation O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
. O

Association O
for O
Computational O
Lin- O
guistics O
. O

Alex O
Warstadt O
, O
Amanpreet O
Singh O
, O
and O
Samuel O
R O
Bow- O
man O
. O

2018 O
. O

Neural O
network O
acceptability O
judg- O
ments O
. O

arXiv O
preprint O
arXiv:1805.12471 O
. O

Adina O
Williams O
, O
Nikita O
Nangia O
, O
and O
Samuel O
R O
Bow- O
man O
. O

2018 O
. O

A O
broad O
- O
coverage O
challenge O
corpus O
for O
sentence O
understanding O
through O
inference O
. O

In O
NAACL O
. O

Yonghui O
Wu O
, O
Mike O
Schuster O
, O
Zhifeng O
Chen O
, O
Quoc O
V O
Le O
, O
Mohammad O
Norouzi O
, O
Wolfgang O
Macherey O
, O
Maxim O
Krikun O
, O
Yuan O
Cao O
, O
Qin O
Gao O
, O
Klaus O
Macherey O
, O
et O
al O
. O
2016 O
. O

Google O
’s O
neural O
ma- O
chine O
translation O
system O
: O
Bridging O
the O
gap O
between O
human O
and O
machine O
translation O
. O

arXiv O
preprint O
arXiv:1609.08144 O
. O

Jason O
Yosinski O
, O
Jeff O
Clune O
, O
Yoshua O
Bengio O
, O
and O
Hod O
Lipson O
. O

2014 O
. O

How O
transferable O
are O
features O
in O
deep O
neural O
networks O
? O

In O
Advances O
in O
neural O
information O
processing O
systems O
, O
pages O
3320–3328 O
. O

Adams O
Wei O
Yu O
, O
David O
Dohan O
, O
Minh O
- O
Thang O
Luong O
, O
Rui O
Zhao O
, O
Kai O
Chen O
, O
Mohammad O
Norouzi O
, O
and O
Quoc O
V O
Le O
. O

2018 O
. O

QANet O
: O

Combining O
local O
convolution O
with O
global O
self O
- O
attention O
for O
reading O
comprehen- O
sion O
. O

In O
ICLR O
. O

Rowan O
Zellers O
, O
Yonatan O
Bisk O
, O
Roy O
Schwartz O
, O
and O
Yejin O
Choi O
. O

2018 O
. O

Swag O
: O
A O
large O
- O
scale O
adversarial O
dataset O
for O
grounded O
commonsense O
inference O
. O

In O
Proceed- O
ings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
. O

Yukun O
Zhu O
, O
Ryan O
Kiros O
, O
Rich O
Zemel O
, O
Ruslan O
Salakhut- O
dinov O
, O
Raquel O
Urtasun O
, O
Antonio O
Torralba O
, O
and O
Sanja O
Fidler O
. O
2015 O
. O

Aligning O
books O
and O
movies O
: O
Towards O
story O
- O
like O
visual O
explanations O
by O
watching O
movies O
and O
reading O
books O
. O

In O
Proceedings O
of O
the O
IEEE O
international O
conference O
on O
computer O
vision O
, O
pages O
19–27 O
. O

Appendix O
for O
“ O
BERT O
: O
Pre O
- O
training O
of O
Deep O
Bidirectional O
Transformers O
for O
Language O
Understanding O
” O
We O
organize O
the O
appendix O
into O
three O
sections O
: O

• O
Additional O
implementation O
details O
for O
BERT B-MethodName
are O
presented O
in O
Appendix O
A;

• O
Additional O
details O
for O
our O
experiments O
are O
presented O
in O
Appendix O
B O
; O
and O

• O
Additional O
ablation O
studies O
are O
presented O
in O
Appendix O
C O
. O

We O
present O
additional O
ablation O
studies O
for O
BERT B-MethodName
including O
: O
– O
Effect O
of O
Number O
of O
Training O
Steps O
; O
and O
– O
Ablation O
for O
Different O
Masking O
Proce- O
dures O
. O

A O
Additional O
Details O
for O
BERT O
A.1 O
Illustration O
of O
the O
Pre O
- O
training O
Tasks O
We O
provide O
examples O
of O
the O
pre O
- O
training O
tasks O
in O
the O
following O
. O

Masked O
LM O
and O
the O
Masking O
Procedure O
As- O
suming O
the O
unlabeled O
sentence O
is O
my O
dog O
is O
hairy O
, O
and O
during O
the O
random O
masking O
procedure O
we O
chose O
the O
4 O
- O
th O
token O
( O
which O
corresponding O
to O
hairy O
) O
, O
our O
masking O
procedure O
can O
be O
further O
il- O
lustrated O
by O
• O
80% B-HyperparameterValue
of O
the O
time O
: O

Replace O
the O
word O
with O
the O
[ O
MASK O
] O
token O
, O
e.g. O
, O
my O
dog O
is O
hairy O
! O

my O
dog O
is O
[ O
MASK O
] O
• O
10% B-HyperparameterValue
of O
the O
time O
: O

Replace O
the O
word O
with O
a O
random O
word O
, O
e.g. O
, O
my O
dog O
is O
hairy O
! O

my O
dog O
is O
apple O
• O
10% B-HyperparameterValue
of O
the O
time O
: O
Keep O
the O
word O
un- O
changed O
, O
e.g. O
, O
my O
dog O
is O
hairy O
! O

my O
dog O
is O
hairy O
. O

The O
purpose O
of O
this O
is O
to O
bias O
the O
representation O
towards O
the O
actual O
observed O
word O
. O

The O
advantage O
of O
this O
procedure O
is O
that O
the O
Transformer O
encoder O
does O
not O
know O
which O
words O
it O
will O
be O
asked O
to O
predict O
or O
which O
have O
been O
re- O
placed O
by O
random O
words O
, O
so O
it O
is O
forced O
to O
keep O
a O
distributional O
contextual O
representation O
of O
ev- O
eryinput O
token O
. O

Additionally O
, O
because O
random O
replacement O
only O
occurs O
for O
1.5% B-HyperparameterValue
of O
all O
tokens O
( O
i.e. O
, O
10 O
% O
of O
15 O
% O
) O
, O
this O
does O
not O
seem O
to O
harm O
the O
model O
’s O
language O
understanding O
capability O
. O

In O
Section O
C.2 O
, O
we O
evaluate O
the O
impact O
this O
proce- O
dure O
. O

Compared O
to O
standard O
langauge O
model O
training O
, O
the O
masked O
LM O
only O
make O
predictions O
on O
15 O
% O
of O
tokens O
in O
each O
batch O
, O
which O
suggests O
that O
more O
pre O
- O
training O
steps O
may O
be O
required O
for O
the O
model O
to O
converge O
. O

In O
Section O
C.1 O
we O
demonstrate O
that O
MLM O
does O
converge O
marginally O
slower O
than O
a O
left- O
to O
- O
right O
model O
( O
which O
predicts O
every O
token O
) O
, O
but O
the O
empirical O
improvements O
of O
the O
MLM O
model O
far O
outweigh O
the O
increased O
training O
cost O
. O

Next O
Sentence O
Prediction O
The O
next O
sentence O
prediction O
task O
can O
be O
illustrated O
in O
the O
following O
examples O
. O

Input O
=[ O
CLS O
] O
the O
man O
went O
to O
[ O
MASK O
] O
store O
[ O
SEP O
] O
he O
bought O
a O
gallon O
[ O
MASK O
] O
milk O

[ O
SEP O
] O
Label O
= O
IsNext O
Input O
=[ O
CLS O
] O
the O
man O
[ O
MASK O
] O
to O
the O
store O
[ O
SEP O
] O
penguin O

[ O
MASK O
] O
are O
flight O
# O
# O
less O
birds O

[ O
SEP O
] O
Label O
= O
NotNext O
A.2 O
Pre O
- O
training O
Procedure O
To O
generate O
each O
training O
input O
sequence O
, O
we O
sam- O
ple O
two O
spans O
of O
text O
from O
the O
corpus O
, O
which O
we O
refer O
to O
as O
“ O
sentences O
” O
even O
though O
they O
are O
typ- O
ically O
much O
longer O
than O
single O
sentences O
( O
but O
can O
be O
shorter O
also O
) O
. O

The O
ﬁrst O
sentence O
receives O
the O
A O
embedding O
and O
the O
second O
receives O
the O
Bembed- O
ding O
. O

50% B-HyperparameterValue
of O
the O
time O
Bis O
the O
actual O
next O
sentence O
that O
follows O
Aand O
50% B-HyperparameterValue
of O
the O
time O
it O
is O
a O
random O
sentence O
, O
which O
is O
done O
for O
the O
“ O
next O
sentence O
pre- O
diction O
” O
task O
. O

They O
are O
sampled O
such O
that O
the O
com- O
bined O
length O
is O
<512 B-HyperparameterValue
tokens O
. O

The O
LM O
masking O
is O
applied O
after O
WordPiece B-MethodName
tokenization O
with O
a O
uni- B-HyperparameterName
form I-HyperparameterName
masking I-HyperparameterName
rate I-HyperparameterName
of O
15% B-HyperparameterValue
, O
and O
no O
special O
consid- O
eration O
given O
to O
partial O
word O
pieces O
. O

We O
train O
with O
batch B-HyperparameterName
size I-HyperparameterName
of O
256 B-HyperparameterValue
sequences O
( O
256 B-HyperparameterValue
sequences O
* O
512 B-HyperparameterValue
tokens O
= O
128,000 O
tokens O
/ O
batch O
) O
for O
1,000,000 B-HyperparameterValue
steps B-HyperparameterName
, O
which O
is O
approximately O
40 B-HyperparameterValue
epochs B-HyperparameterName
over O
the O
3.3 O
billion O
word O
corpus O
. O

We O
use O
Adam O
with O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-4 B-HyperparameterValue
, O

1= O
0.9 B-HyperparameterValue
, O

2= O
0.999 B-HyperparameterValue
, O
L2 B-HyperparameterName
weight I-HyperparameterName
decay I-HyperparameterName
of O
0:01 B-HyperparameterValue
, O
learning O
rate O
warmup O
over O
the O
ﬁrst O
10,000 B-HyperparameterValue
steps B-HyperparameterName
, O
and O
linear O
decay O
of O
the O
learning O
rate O
. O

We O
use O
a O
dropout B-HyperparameterName
prob- I-HyperparameterName
ability I-HyperparameterName
of O
0.1 B-HyperparameterValue
on O
all O
layers O
. O

We O
use O
a O
gelu O
acti- O
vation O
( O
Hendrycks O
and O
Gimpel O
, O
2016 O
) O
rather O
than O
the O
standard O
relu O
, O
following O
OpenAI B-MethodName
GPT I-MethodName
. O

The O
training O
loss O
is O
the O
sum O
of O
the O
mean O
masked O
LM O
likelihood O
and O
the O
mean O
next O
sentence O
prediction O
likelihood O
. O

Training O
of O
BERT B-MethodName
BASE I-MethodName
was O
performed O
on O
4 O
Cloud O
TPUs O
in O
Pod O
conﬁguration O
( O
16 O
TPU O
chips O
total).13Training O
of O
BERT B-MethodName
LARGE I-MethodName
was O
performed O
on O
16 O
Cloud O
TPUs O
( O
64 O
TPU O
chips O
total O
) O
. O

Each O
pre- O
training O
took O
4 O
days O
to O
complete O
. O

Longer O
sequences O
are O
disproportionately O
expen- O
sive O
because O
attention O
is O
quadratic O
to O
the O
sequence O
length O
. O

To O
speed O
up O
pretraing O
in O
our O
experiments O
, O
we O
pre O
- O
train O
the O
model O
with O
sequence B-HyperparameterName
length I-HyperparameterName
of O
128 B-HyperparameterValue
for O
90% B-HyperparameterValue
of O
the O
steps B-HyperparameterName
. O

Then O
, O
we O
train O
the O
rest O
10% B-HyperparameterValue
of O
the O
steps B-HyperparameterName
of O
sequence O
of O
512 B-HyperparameterValue
to O
learn O
the O
positional O
embeddings O
. O

A.3 O
Fine O
- O
tuning O
Procedure O
For O
ﬁne O
- O
tuning O
, O
most O
model O
hyperparameters O
are O
the O
same O
as O
in O
pre O
- O
training O
, O
with O
the O
exception O
of O
the O
batch B-HyperparameterName
size I-HyperparameterName
, O
learning B-HyperparameterName
rate I-HyperparameterName
, O
and O
number B-HyperparameterName
of I-HyperparameterName
train- I-HyperparameterName
ing I-HyperparameterName
epochs I-HyperparameterName
. O

The O
dropout B-HyperparameterName
probability I-HyperparameterName
was O
always O
kept O
at O
0.1 B-HyperparameterValue
. O

The O
optimal O
hyperparameter O
values O
are O
task O
- O
speciﬁc O
, O
but O
we O
found O
the O
following O
range O
of O
possible O
values O
to O
work O
well O
across O
all O
tasks O
: O
• O
Batch B-HyperparameterName
size I-HyperparameterName
: O
16 B-HyperparameterValue
, O
32 B-HyperparameterValue

• O
Learning B-HyperparameterName
rate I-HyperparameterName
( O
Adam O
) O
: O
5e-5 B-HyperparameterValue
, O
3e-5 B-HyperparameterValue
, O
2e-5 B-HyperparameterValue
• O
Number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
: O
2 B-HyperparameterValue
, O
3 B-HyperparameterValue
, O
4 B-HyperparameterValue
We O
also O
observed O
that O
large O
data O
sets O
( O
e.g. O
, O
100k+ O
labeled O
training O
examples O
) O
were O
far O
less O
sensitive O
to O
hyperparameter O
choice O
than O
small O
data O
sets O
. O

Fine O
- O
tuning O
is O
typically O
very O
fast O
, O
so O
it O
is O
rea- O
sonable O
to O
simply O
run O
an O
exhaustive O
search O
over O
the O
above O
parameters O
and O
choose O
the O
model O
that O
performs O
best O
on O
the O
development O
set O
. O

A.4 O
Comparison O
of O
BERT O
, O
ELMo O
, O
and O
OpenAI O
GPT O
Here O
we O
studies O
the O
differences O
in O
recent O
popular O
representation O
learning O
models O
including O
ELMo B-MethodName
, O
OpenAI B-MethodName
GPT I-MethodName
and O
BERT B-MethodName
. O

The O
comparisons O
be- O
tween O
the O
model O
architectures O
are O
shown O
visually O
in O
Figure O
3 O
. O

Note O
that O
in O
addition O
to O
the O
architec- O
ture O
differences O
, O
BERT B-MethodName
and O
OpenAI B-MethodName
GPT I-MethodName
are O
ﬁne- O
tuning O
approaches O
, O
while O
ELMo B-MethodName
is O
a O
feature O
- O
based O
approach O
. O

The O
most O
comparable O
existing O
pre O
- O
training O
method O
to O
BERT B-MethodName
is O
OpenAI B-MethodName
GPT I-MethodName
, O
which O
trains O
a O
left O
- O
to O
- O
right O
Transformer O
LM O
on O
a O
large O
text O
cor- O
pus O
. O

In O
fact O
, O
many O
of O
the O
design O
decisions O
in O
BERT B-HyperparameterName
were O
intentionally O
made O
to O
make O
it O
as O
close O
to O
GPT B-MethodName
as O
possible O
so O
that O
the O
two O
methods O
could O
be O
minimally O
compared O
. O

The O
core O
argument O
of O
this O
work O
is O
that O
the O
bi O
- O
directionality O
and O
the O
two O
pre- O
training O
tasks O
presented O
in O
Section O
3.1 O
account O
for O
the O
majority O
of O
the O
empirical O
improvements O
, O
but O
we O
do O
note O
that O
there O
are O
several O
other O
differences O
between O
how O
BERT B-MethodName
and O
GPT B-MethodName
were O
trained O
: O

• O
GPT B-MethodName
is O
trained O
on O
the O
BooksCorpus B-DatasetName
( O
800 O
M O
words O
) O
; O
BERT B-MethodName
is O
trained O
on O
the O
BooksCor- B-DatasetName
pus I-DatasetName
( O
800 O
M O
words O
) O
and O
Wikipedia B-DatasetName
( O
2,500 O
M O
words O
) O
. O

• O
GPT B-MethodName
uses O
a O
sentence O
separator O
( O
[ O
SEP O
] O
) O
and O
classiﬁer O
token O
( O
[ O
CLS O
] O
) O
which O
are O
only O
in- O
troduced O
at O
ﬁne O
- O
tuning O
time O
; O
BERT B-MethodName
learns O
[ O
SEP O
] O
, O
[ O
CLS O
] O
and O
sentence O
A O
/ O
Bembed- O

dings O
during O
pre O
- O
training O
. O

• O
GPT B-MethodName
was O
trained O
for O
1M B-HyperparameterValue
steps B-HyperparameterName
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32,000 B-HyperparameterValue
words O
; O
BERT B-MethodName
was O
trained O
for O
1M B-HyperparameterValue
steps B-HyperparameterName
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
128,000 B-HyperparameterValue
words O
. O

• O
GPT B-MethodName
used O
the O
same O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5e-5 B-HyperparameterValue
for O
all O
ﬁne O
- O
tuning O
experiments O
; O
BERT B-HyperparameterName
chooses O
a O
task O
- O
speciﬁc O
ﬁne O
- O
tuning O
learning B-HyperparameterName
rate I-HyperparameterName
which O
performs O
the O
best O
on O
the O
development O
set O
. O

To O
isolate O
the O
effect O
of O
these O
differences O
, O
we O
per- O
form O
ablation O
experiments O
in O
Section O
5.1 O
which O
demonstrate O
that O
the O
majority O
of O
the O
improvements O
are O
in O
fact O
coming O
from O
the O
two O
pre O
- O
training O
tasks O
and O
the O
bidirectionality O
they O
enable O
. O

A.5 O
Illustrations O
of O
Fine O
- O
tuning O
on O
Different O
Tasks O
The O
illustration O
of O
ﬁne O
- O
tuning O
BERT B-MethodName
on O
different O
tasks O
can O
be O
seen O
in O
Figure O
4 O
. O

Our O
task O
- O
speciﬁc O
models O
are O
formed O
by O
incorporating O
BERT B-MethodName
with O
one O
additional O
output O
layer O
, O
so O
a O
minimal O
num- O
ber O
of O
parameters O
need O
to O
be O
learned O
from O
scratch O
. O

Among O
the O
tasks O
, O
( O
a O
) O
and O
( O
b O
) O
are O
sequence O
- O
level O
tasks O
while O
( O
c O
) O
and O
( O
d O
) O
are O
token O
- O
level O
tasks O
. O

In O
the O
ﬁgure O
, O
Erepresents O
the O
input O
embedding O
, O
Ti O
represents O
the O
contextual O
representation O
of O
token O
i O
, O

[ O
CLS O
] O
is O
the O
special O
symbol O
for O
classiﬁcation O
out- O
put O
, O
and O
[ O
SEP O
] O
is O
the O
special O
symbol O
to O
separate O
non O
- O
consecutive O
token O
sequences O
. O

B O
Detailed O
Experimental O
Setup O
B.1 O
Detailed O
Descriptions O
for O
the O
GLUE O
Benchmark O
Experiments O
. O

The O
GLUE B-DatasetName
benchmark O
includes O
the O
following O
datasets O
, O
the O
descriptions O
of O
which O
were O
originally O
summarized O
in O
Wang O
et O
al O
. O

( O
2018a O
): O
MNLI O
Multi B-TaskName
- I-TaskName
Genre I-TaskName
Natural I-TaskName
Language I-TaskName
Inference I-TaskName
is O
a O
large O
- O
scale O
, O
crowdsourced O
entailment O
classiﬁ- O
cation O
task O
( O
Williams O
et O
al O
. O
, O
2018 O
) O
. O

Given O
a O
pair O
of O
sentences O
, O
the O
goal O
is O
to O
predict O
whether O
the O
sec- O
ond O
sentence O
is O
an O
entailment O
, O
contradiction O
, O
or O
neutral O
with O
respect O
to O
the O
ﬁrst O
one O
. O

QQP O
Quora B-TaskName
Question I-TaskName
Pairs I-TaskName
is O
a O
binary O
classiﬁ- O
cation O
task O
where O
the O
goal O
is O
to O
determine O
if O
two O
questions O
asked O
on O
Quora O
are O
semantically O
equiv- O
alent O
( O
Chen O
et O
al O
. O
, O
2018 O
) O
. O

QNLI O

Question B-DatasetName
Natural I-DatasetName
Language I-DatasetName
Inference I-DatasetName
is O
a O
version O
of O
the O
Stanford B-DatasetName
Question I-DatasetName
Answering I-DatasetName
Dataset O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
which O
has O
been O
converted O
to O
a O
binary O
classiﬁcation O
task O
( O
Wang O
et O
al O
. O
, O
2018a O
) O
. O

The O
positive O
examples O
are O
( O
ques- O
tion O
, O
sentence O
) O
pairs O
which O
do O
contain O
the O
correct O
answer O
, O
and O
the O
negative O
examples O
are O
( O
question O
, O
sentence O
) O
from O
the O
same O
paragraph O
which O
do O
not O
contain O
the O
answer O
. O

SST-2 O

The O
Stanford B-TaskName
Sentiment I-TaskName
Treebank I-TaskName
is O
a O
binary O
single O
- O
sentence O
classiﬁcation O
task O
consist- O
ing O
of O
sentences O
extracted O
from O
movie O
reviews O
with O
human O
annotations O
of O
their O
sentiment O
( O
Socher O
et O
al O
. O
, O
2013 O
) O
. O

CoLA O

The O
Corpus B-TaskName
of I-TaskName
Linguistic I-TaskName
Acceptability I-TaskName
is O
a O
binary O
single O
- O
sentence O
classiﬁcation O
task O
, O
where O
the O
goal O
is O
to O
predict O
whether O
an O
English O
sentence O
is O
linguistically O
“ O
acceptable O
” O
or O
not O
( O
Warstadt O
et O
al O
. O
, O
2018 O
) O
. O

STS O
- O
B O
The O
Semantic B-DatasetName
Textual I-DatasetName
Similarity I-DatasetName
Bench- I-DatasetName
mark I-DatasetName
is O
a O
collection O
of O
sentence O
pairs O
drawn O
from O
news O
headlines O
and O
other O
sources O
( O
Cer O
et O
al O
. O
, O
2017 O
) O
. O

They O
were O
annotated O
with O
a O
score O
from O
1 O
to O
5 O
denoting O
how O
similar O
the O
two O
sentences O
are O
in O
terms O
of O
semantic O
meaning O
. O

MRPC O
Microsoft B-DatasetName
Research I-DatasetName
Paraphrase I-DatasetName
Corpus I-DatasetName
consists O
of O
sentence O
pairs O
automatically O
extracted O
from O
online O
news O
sources O
, O
with O
human O
annotations O
for O
whether O
the O
sentences O
in O
the O
pair O
are O
semanti- O
cally O
equivalent O
( O
Dolan O
and O
Brockett O
, O
2005).RTE O

Recognizing B-TaskName
Textual I-TaskName
Entailment I-TaskName
is O
a O
bi- O
nary O
entailment O
task O
similar O
to O
MNLI B-TaskName
, O
but O
with O
much O
less O
training O
data O
( O
Bentivogli O
et O
al O
. O
, O

2009).14 O
WNLI O
Winograd B-DatasetName
NLI I-DatasetName
is O
a O
small O
natural O
lan- O
guage O
inference O
dataset O
( O
Levesque O
et O
al O
. O
, O
2011 O
) O
. O

The O
GLUE B-DatasetName
webpage O
notes O
that O
there O
are O
issues O
with O
the O
construction O
of O
this O
dataset,15and O
every O
trained O
system O
that O
’s O
been O
submitted O
to O
GLUE B-DatasetName
has O
performed O
worse O
than O
the O
65.1 O
baseline O
accuracy O
of O
predicting O
the O
majority O
class O
. O

We O
therefore O
ex- O
clude O
this O
set O
to O
be O
fair O
to O
OpenAI B-MethodName
GPT I-MethodName
. O

For O
our O
GLUE B-DatasetName
submission O
, O
we O
always O
predicted O
the O
ma- O
jority O
class O
. O

C O
Additional O
Ablation O
Studies O
C.1 O
Effect O
of O
Number O
of O
Training O
Steps O
Figure O
5 O
presents O
MNLI B-TaskName
Dev B-MetricName
accuracy I-MetricName
after O
ﬁne- O
tuning O
from O
a O
checkpoint O
that O
has O
been O
pre O
- O
trained O
forksteps O
. O

This O
allows O
us O
to O
answer O
the O
following O
questions O
: O
1 O
. O
Question O
: O
Does O
BERT B-MethodName
really O
need O
such O
a O
large O
amount O
of O
pre O
- O
training O
( O
128,000 B-HyperparameterValue
words B-HyperparameterName
/ I-HyperparameterName
batch I-HyperparameterName
* O
1,000,000 B-HyperparameterValue
steps B-HyperparameterName
) O
to O
achieve O
high O
ﬁne O
- O
tuning O
accuracy O
? O

Answer O
: O

Yes O
, O
BERT B-MethodName
BASE I-MethodName
achieves O
almost O
1.0% B-MetricValue
additional O
accuracy B-MetricName
on O
MNLI B-TaskName
when O
trained O
on O
1M B-HyperparameterValue
steps B-HyperparameterName
compared O
to O
500k B-HyperparameterValue
steps B-HyperparameterName
. O

2 O
. O
Question O
: O
Does O
MLM O
pre O
- O
training O
converge O
slower O
than O
LTR O
pre O
- O
training O
, O
since O
only O
15% B-HyperparameterValue
of O
words O
are O
predicted O
in O
each O
batch O
rather O
than O
every O
word O
? O

Answer O
: O

The O
MLM O
model O
does O
converge O
slightly O
slower O
than O
the O
LTR O
model O
. O

How- O
ever O
, O
in O
terms O
of O
absolute O
accuracy O
the O
MLM O
model O
begins O
to O
outperform O
the O
LTR O
model O
almost O
immediately O
. O

C.2 O
Ablation O
for O
Different O
Masking O
Procedures O
In O
Section O
3.1 O
, O
we O
mention O
that O
BERT B-MethodName
uses O
a O
mixed O
strategy O
for O
masking O
the O
target O
tokens O
when O
pre O
- O
training O
with O
the O
masked O
language O
model O
( O
MLM O
) O
objective O
. O

The O
following O
is O
an O
ablation O
study O
to O
evaluate O
the O
effect O
of O
different O
masking O
strategies O
. O

Note O
that O
the O
purpose O
of O
the O
masking O
strategies O
is O
to O
reduce O
the O
mismatch O
between O
pre O
- O
training O
and O
ﬁne O
- O
tuning O
, O
as O
the O
[ O
MASK O
] O
symbol O
never O
ap- O
pears O
during O
the O
ﬁne O
- O
tuning O
stage O
. O

We O
report O
the O
Dev O
results O
for O
both O
MNLI B-TaskName
and O
NER B-TaskName
. O

For O
NER B-TaskName
, O
we O
report O
both O
ﬁne O
- O
tuning O
and O
feature O
- O
based O
ap- O
proaches O
, O
as O
we O
expect O
the O
mismatch O
will O
be O
am- O
pliﬁed O
for O
the O
feature O
- O
based O
approach O
as O
the O
model O
will O
not O
have O
the O
chance O
to O
adjust O
the O
representa- O
tions O
. O

The O
results O
are O
presented O
in O
Table O
8 O
. O

In O
the O
table O
, O
MASK O
means O
that O
we O
replace O
the O
target O
token O
with O
the[MASK O
] O
symbol O
for O
MLM O
; O
S O
AME O
means O
that O
we O
keep O
the O
target O
token O
as O
is O
; O
R O
NDmeans O
that O
we O
replace O
the O
target O
token O
with O
another O
random O
token O
. O

The O
numbers O
in O
the O
left O
part O
of O
the O
table O
repre- O
sent O
the O
probabilities O
of O
the O
speciﬁc O
strategies O
used O
during O
MLM O
pre O
- O
training O
( O
BERT B-MethodName
uses O
80% B-HyperparameterValue
, O
10% B-HyperparameterValue
, O
10% B-HyperparameterValue
) O
. O

The O
right O
part O
of O
the O
paper O
represents O
the O
Dev O
set O
results O
. O

For O
the O
feature O
- O
based O
approach O
, O
we O
concatenate O
the O
last O
4 O
layers O
of O
BERT B-MethodName
as O
the O
features O
, O
which O
was O
shown O
to O
be O
the O
best O
approach O
in O
Section O
5.3 O
. O

From O
the O
table O
it O
can O
be O
seen O
that O
ﬁne O
- O
tuning O
is O
surprisingly O
robust O
to O
different O
masking O
strategies O
. O

However O
, O
as O
expected O
, O
using O
only O
the O
M O
ASK O
strat- O
egy O
was O
problematic O
when O
applying O
the O
feature- O
based O
approach O
to O
NER B-TaskName
. O

Interestingly O
, O
using O
only O
the O
R O
NDstrategy O
performs O
much O
worse O
than O
our O
strategy O
as O
well O
. O

