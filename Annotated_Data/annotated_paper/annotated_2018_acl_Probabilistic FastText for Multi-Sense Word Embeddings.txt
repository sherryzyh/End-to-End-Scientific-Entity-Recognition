Probabilistic B-MethodName
FastText I-MethodName
for O
Multi O
- O
Sense O
Word O
Embeddings O
Ben O
Athiwaratkun O
Cornell O
University O
pa338@cornell.eduAndrew O
Gordon O
Wilson O
Cornell O
University O
andrew@cornell.eduAnima O
Anandkumar O
AWS O
& O
Caltech O
anima@amazon.com O

Abstract O
We O
introduce O
Probabilistic B-MethodName
FastText I-MethodName
, O
a O
new O
model O
for O
word O
embeddings O
that O
can O
cap- O
ture O
multiple O
word O
senses O
, O
sub O
- O
word O
struc- O
ture O
, O
and O
uncertainty O
information O
. O

In O
particular O
, O
we O
represent O
each O
word O
with O
a O
Gaussian O
mixture O
density O
, O
where O
the O
mean O
of O
a O
mixture O
component O
is O
given O
by O
the O
sum O
of O
n O
- O
grams O
. O

This O
represen- O
tation O
allows O
the O
model O
to O
share O
statis- O
tical O
strength O
across O
sub O
- O
word O
structures O
( O
e.g. O
Latin O
roots O
) O
, O
producing O
accurate O
rep- O
resentations O
of O
rare O
, O
misspelt O
, O
or O
even O
un- O
seen O
words O
. O

Moreover O
, O
each O
component O
of O
the O
mixture O
can O
capture O
a O
different O
word O
sense O
. O

Probabilistic B-MethodName
FastText I-MethodName
out- O
performs O
both O
F B-MethodName
ASTTEXT I-MethodName
, O
which O
has O
no O
probabilistic O
model O
, O
and O
dictionary B-MethodName
- I-MethodName
level I-MethodName
probabilistic I-MethodName
embeddings I-MethodName
, O
which O
do O
not O
incorporate O
subword O
structures O
, O
on O
sev- O
eral O
word O
- O
similarity O
benchmarks O
, O
includ- O
ing O
English B-DatasetName
RareWord I-DatasetName
and O
foreign O
lan- O
guage O
datasets O
. O

We O
also O
achieve O
state O
- O
of- O
art O
performance O
on O
benchmarks O
that O
mea- O
sure O
ability O
to O
discern O
different O
meanings O
. O

Thus O
, O
the O
proposed O
model O
is O
the O
ﬁrst O
to O
achieve O
multi O
- O
sense O
representations O
while O
having O
enriched O
semantics O
on O
rare O
words O
. O

1 O
Introduction O
Word O
embeddings O
are O
foundational O
to O
natural O
language O
processing O
. O

In O
order O
to O
model O
lan- O
guage O
, O
we O
need O
word O
representations O
to O
contain O
as O
much O
semantic O
information O
as O
possible O
. O

Most O
re- O

search O
has O
focused O
on O
vector O
word O
embeddings O
, O
such O
as O
W B-MethodName
ORD2VEC I-MethodName
(Mikolov O
et O
al O
. O
, O
2013a O
) O
, O
where O
words O
with O
similar O
meanings O
are O
mapped O
to O
nearby O
points O
in O
a O
vector O
space O
. O

Following O
the O
seminal O
work O
of O
Mikolov O
et O
al O
. O

( O
2013a O
) O
, O
there O
have O
been O
numerous O
works O
looking O
to O
learn O
efﬁcient O
word O
embeddings O
. O

One O
shortcoming O
with O
the O
above O
approaches O
to O
word O
embedding O
that O
are O
based O
on O
a O
prede- O
ﬁned O
dictionary O
( O
termed O
as O
dictionary O
- O
based O
em- O
beddings O
) O
is O
their O
inability O
to O
learn O
representa- O
tions O
of O
rare O
words O
. O

To O
overcome O
this O
limitation O
, O
character O
- O
level O
word O
embeddings O
have O
been O
pro- O
posed O
. O

F B-MethodName
ASTTEXT I-MethodName
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
is O
the O
state O
- O
of O
- O
the O
- O
art O
character O
- O
level O
approach O
to O
em- O
beddings O
. O

In O
F B-MethodName
ASTTEXT I-MethodName
, O
each O
word O
is O
modeled O
by O
a O
sum O
of O
vectors O
, O
with O
each O
vector O
represent- O
ing O
an O
n O
- O
gram O
. O

The O
beneﬁt O
of O
this O
approach O
is O
that O
the O
training O
process O
can O
then O
share O
strength O
across O
words O
composed O
of O
common O
roots O
. O

For O
exam- O
ple O
, O
with O
individual O
representations O
for O
“ O
circum O
” O
and O
“ O
navigation O
” O
, O
we O
can O
construct O
an O
informa- O
tive O
representation O
for O
“ O
circumnavigation O
” O
, O
which O
would O
otherwise O
appear O
too O
infrequently O
to O
learn O
a O
dictionary O
- O
level O
embedding O
. O

In O
addition O
to O
effec- O
tively O
modelling O
rare O
words O
, O
character O
- O
level O
em- O
beddings O
can O
also O
represent O
slang O
or O
misspelled O
words O
, O
such O
as O
“ O
dogz O
” O
, O
and O
can O
share O
strength O
across O
different O
languages O
that O
share O
roots O
, O
e.g. O
Romance O
languages O
share O
latent O
roots O
. O

A O
different O
promising O
direction O
involves O
repre- O
senting O
words O
with O
probability O
distributions O
, O
in- O
stead O
of O
point O
vectors O
. O

For O
example O
, O
Vilnis O
and O
McCallum O
( O
2014 O
) O
represents O
words O
with O
Gaussian O
distributions O
, O
which O
can O
capture O
uncertainty O
infor- O
mation O
. O

Athiwaratkun O
and O
Wilson O
( O
2017 O
) O
gen- O
eralizes O
this O
approach O
to O
multimodal O
probability O
distributions O
, O
which O
can O
naturally O
represent O
words O
with O
different O
meanings O
. O

For O
example O
, O
the O
distri- O
bution O
for O
“ O
rock O
” O
could O
have O
mass O
near O
the O
word O
“ O
jazz O
” O
and O
“ O
pop O
” O
, O
but O
also O
“ O
stone O
” O
and O
“ O
basalt O
” O
. O

Athiwaratkun O
and O
Wilson O
( O
2018 O
) O
further O
devel- O
oped O
this O
approach O
to O
learn O
hierarchical O
word O
rep- O
resentations O
: O
for O
example O
, O
the O
word O
“ O
music O
” O
can O

2be O
learned O
to O
have O
a O
broad O
distribution O
, O
which O
en- O
capsulates O
the O
distributions O
for O
“ O
jazz O
” O
and O
“ O
rock O
” O
. O

In O
this O
paper O
, O
we O
propose O
Probabilistic B-MethodName
Fast- I-MethodName
Text I-MethodName
( O
PFT B-MethodName
) O
, O
which O
provides O
probabilistic O
character- O
level O
representations O
of O
words O
. O

The O
resulting O
word O
embeddings O
are O
highly O
expressive O
, O
yet O
straightfor- O
ward O
and O
interpretable O
, O
with O
simple O
, O
efﬁcient O
, O
and O
intuitive O
training O
procedures O
. O

PFT B-MethodName
can O
model O
rare O
words O
, O
uncertainty O
information O
, O
hierarchical O
rep- O
resentations O
, O
and O
multiple O
word O
senses O
. O

In O
partic- O
ular O
, O
we O
represent O
each O
word O
with O
a O
Gaussian O
or O
a O
Gaussian O
mixture O
density O
, O
which O
we O
name O
PFT B-MethodName
- I-MethodName
G I-MethodName
and O
PFT B-MethodName
- I-MethodName
GM I-MethodName
respectively O
. O

Each O
component O
of O
the O
mixture O
can O
represent O
different O
word O
senses O
, O
and O
the O
mean O
vectors O
of O
each O
component O
decompose O
into O
vectors O
of O
n O
- O
grams O
, O
to O
capture O
character O
- O
level O
information O
. O

We O
also O
derive O
an O
efﬁcient O
energy- O
based O
max O
- O
margin O
training O
procedure O
for O
PFT B-MethodName
. O

We O
perform O
comparison O
with O
F B-MethodName
ASTTEXT I-MethodName
as O
well O
as O
existing O
density O
word O
embeddings O
W2 B-MethodName
G I-MethodName
( O
Gaussian O
) O
and O
W2GM B-MethodName
(Gaussian O
mixture O
) O
. O

Our O
models O
extract O
high O
- O
quality O
semantics O
based O
on O
multiple O
word O
- O
similarity O
benchmarks O
, O
including O
the O
rare B-DatasetName
word I-DatasetName
dataset O
. O

We O
obtain O
an O
average O
weighted O
improvement O
of O
3.7% B-MetricValue
over O
F B-MethodName
ASTTEXT I-MethodName
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
and O
3.1 B-MetricValue
% I-MetricValue
over O
the O
dictionary B-MethodName
- I-MethodName
level I-MethodName
density I-MethodName
- I-MethodName
based I-MethodName
models I-MethodName
. O

We O
also O
observe O
meaningful O
nearest O
neighbors O
, O
particu- O
larly O
in O
the O
multimodal O
density O
case O
, O
where O
each O
mode O
captures O
a O
distinct O
meaning O
. O

Our O
models O
are O
also O
directly O
portable O
to O
foreign O
languages O
with- O
out O
any O
hyperparameter O
modiﬁcation O
, O
where O
we O
observe O
strong O
performance O
, O
outperforming O
F B-MethodName
AST- I-MethodName
TEXT I-MethodName
on O
many O
foreign O
word O
similarity O
datasets O
. O

Our O
multimodal O
word O
representation O
can O
also O
dis- O
entangle O
meanings O
, O
and O
is O
able O
to O
separate O
differ- O
ent O
senses O
in O
foreign O
polysemies O
. O

In O
particular O
, O
our O
models O
attain O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
SCWS B-DatasetName
, O
a O
benchmark O
to O
measure O
the O
ability O
to O
sep- O
arate O
different O
word O
meanings O
, O
achieving O
1.0% B-MetricValue
im- O
provement O
over O
a O
recent O
density O
embedding O
model O
W2GM B-MethodName
(Athiwaratkun O
and O
Wilson O
, O
2017 O
) O
. O

To O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
ﬁrst O
to O
develop O
multi O
- O
sense O
embeddings O
with O
high O
se- O
mantic O
quality O
for O
rare O
words O
. O

Our O
code O
and O
em- O
beddings O
are O
publicly O
available.1 O
2 O
Related O
Work O
Early O
word O
embeddings O
which O
capture O
semantic O
information O
include O
Bengio O

et O
al O
. O

( O
2003 O
) O
, O
Col- O
lobert O
and O
Weston O
( O
2008 O
) O
, O
and O
Mikolov O

et O

al O
. O
( O
2011 O
) O
. O

Later O
, O
Mikolov O
et O
al O
. O

( O
2013a O
) O
developed O
the O
popular O
W B-MethodName
ORD2VEC I-MethodName
method O
, O
which O
proposes O
a O
log O
- O
linear O
model O
and O
negative O
sampling O
ap- O
proach O
that O
efﬁciently O
extracts O
rich O
semantics O
from O
text O
. O

Another O
popular O
approach O
G B-MethodName
LOVE I-MethodName
learns O
word O
embeddings O
by O
factorizing O
co O
- O
occurrence O
matrices O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

Recently O
there O
has O
been O
a O
surge O
of O
interest O
in O
making O
dictionary O
- O
based O
word O
embeddings O
more O
ﬂexible O
. O

This O
ﬂexibility O
has O
valuable O
applica- O

tions O
in O
many O
end O
- O
tasks O
such O
as O
language O
mod- O
eling O
( O
Kim O
et O
al O
. O
, O
2016 O
) O
, O
named O
entity O
recogni- O
tion O
( O
Kuru O
et O
al O
. O
, O
2016 O
) O
, O
and O
machine O
translation O
( O
Zhao O
and O
Zhang O
, O
2016 O
; O
Lee O
et O
al O
. O
, O
2017 O
) O
, O
where O
unseen O
words O
are O
frequent O
and O
proper O
handling O
of O
these O
words O
can O
greatly O
improve O
the O
performance O
. O

These O
works O
focus O
on O
modeling O
subword O
informa- O
tion O
in O
neural O
networks O
for O
tasks O
such O
as O
language O
modeling O
. O

Besides O
vector O
embeddings O
, O
there O
is O
recent O
work O
on O
multi O
- O
prototype O
embeddings O
where O
each O
word O
is O
represented O
by O
multiple O
vectors O
. O

The O
learn- O
ing O
approach O
involves O
using O
a O
cluster O
centroid O
of O
context O
vectors O
( O
Huang O
et O
al O
. O
, O
2012 O
) O
, O
or O
adapt- O
ing O
the O
skip O
- O
gram O
model O
to O
learn O
multiple O
latent O
representations O
( O
Tian O
et O
al O
. O
, O
2014 O
) O
. O

Neelakan- O
tan O
et O
al O
. O

( O
2014 O
) O
furthers O
adapts O
skip O
- O
gram O
with O
a O
non O
- O
parametric O
approach O
to O
learn O
the O
embed- O
dings O
with O
an O
arbitrary O
number O
of O
senses O
per O
word O
. O

Chen O
et O
al O
. O

( O
2014 O
) O
incorporates O
an O
external O
dataset O
WORDNET B-DatasetName
to O
learn O
sense O
vectors O
. O

We O
compare O
these O
models O
with O
our O
multimodal O
embeddings O
in O
Section O
4 O
. O
3 O
Probabilistic B-MethodName
FastText I-MethodName
We O
introduce O
Probabilistic B-MethodName
FastText I-MethodName
, O
which O
com- O
bines O
a O
probabilistic O
word O
representation O
with O
the O
ability O
to O
capture O
subword O
structure O
. O

We O
describe O
the O
probabilistic O
subword O
representation O
in O
Sec- O
tion O
3.1 O
. O

We O
then O
describe O
the O
similarity O
measure O
and O
the O
loss O
function O
used O
to O
train O
the O
embeddings O
in O
Sections O
3.2 O
and O
3.3 O
. O

We O
conclude O
by O
brieﬂy O
presenting O
a O
simpliﬁed O
version O
of O
the O
energy O
func- O
tion O
for O
isotropic O
Gaussian O
representations O
( O
Sec- O
tion O
3.4 O
) O
, O
and O
the O
negative O
sampling O
scheme O
we O
use O
in O
training O
( O
Section O
3.5 O
) O
. O

3.1 O
Probabilistic O
Subword O
Representation O
We O
represent O
each O
word O
with O
a O
Gaussian O
mixture O
with O
K B-HyperparameterName
Gaussian O
components O
. O

That O
is O
, O
a O
word O
wis O
associated O
with O
a O
density O
function O
f(x O
) O

= O
PK O
i=1pw;iN(x;~ O
w;i;w;i)wherefw;igK O
k=1are O
the O
mean O
vectors O
and O
fw;igare O
the O
covariance O
matrices O
, O
andfpw;igK O
k=1are O
the O
component O
prob- O
abilities O
which O
sum O
to O
1 O
. O

The O
mean O
vectors O
of O
Gaussian O
components O
hold O
much O
of O
the O
semantic O
information O
in O
density O
em- O

beddings O
. O

While O
these O
models O
are O
successful O
based O
on O
word O
similarity O
and O
entailment O
bench- O
marks O
( O
Vilnis O
and O
McCallum O
, O
2014 O
; O
Athiwaratkun O
and O
Wilson O
, O
2017 O
) O
, O
the O
mean O
vectors O
are O
often O
dictionary O
- O
level O
, O
which O
can O
lead O
to O
poor O
semantic O
estimates O
for O
rare O
words O
, O
or O
the O
inability O
to O
handle O
words O
outside O
the O
training O
corpus O
. O

We O
propose O
us- O
ing O
subword O
structures O
to O
estimate O
the O
mean O
vec- O
tors O
. O

We O
outline O
the O
formulation O
below O
. O

For O
wordw O
, O
we O
estimate O
the O
mean O
vector O
w O
with O
the O
average O
over O
n O
- O
gram O
vectors O
and O
its O
dictionary O
- O
level O
vector O
. O

That O
is O
, O
w=1 O
jNGwj+ O
10 O
@vw+X O
g2NGwzg1 O
A O
( O
1 O
) O
wherezgis O
a O
vector O
associated O
with O
an O
n O
- O
gram O
g O
, O
vwis O
the O
dictionary O
representation O
of O
word O
w O
, O
and O
NGwis O
a O
set O
ofn O
- O
grams O
of O
word O
w. O
Examples O
of O
3,4 O
- O
grams O
for O
a O
word O
“ O
beautiful O
” O
, O
including O
thebeginning O
- O
of O
- O
word O
character O
‘ O
h O
’ O
and O
end O
- O
of O
- O
word O
character O
‘ O
i O
’ O
, O
are O
: O
3 O
- O
grams O
: O
hbe O
, O
bea O
, O
eau O
, O
aut O
, O
uti O
, O
tif O
, O
ful O
, O
ul O
i O
4 O
- O
grams O
: O
hbea O
, O
beau O
.. O
, O
iful O
, O
ful O

i O
This O
structure O
is O
similar O
to O
that O
of O
F B-MethodName
ASTTEXT I-MethodName
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
; O
however O
, O
we O
note O
that O
F B-MethodName
ASTTEXT I-MethodName
uses O
single O
- O
prototype O
determinis- O
tic O
embeddings O
as O
well O
as O
a O
training O
approach O
that O
maximizes O
the O
negative O
log O
- O
likelihood O
, O
whereas O
we O
use O
a O
multi O
- O
prototype O
probabilistic O
embedding O
and O
for O
training O
we O
maximize O
the O
similarity O
be- O
tween O
the O
words O
’ O
probability O
densities O
, O
as O
de- O
scribed O
in O
Sections O
3.2 O
and O
3.3 O
Figure O
1a O
depicts O
the O
subword O
structure O
for O
the O
mean O
vector O
. O

Figure O
1b O
and O
1c O
depict O
our O
models O
, O
Gaussian B-MethodName
probabilistic I-MethodName
F I-MethodName
ASTTEXT I-MethodName
( O
PFT- B-MethodName
G I-MethodName
) O
and O
Gaussian B-MethodName
mixture I-MethodName
probabilistic I-MethodName
F I-MethodName
ASTTEXT I-MethodName
( O
PFT B-MethodName
- I-MethodName
GM I-MethodName
) O
. O

In O
the O
Gaussian O
case O
, O
we O
represent O
each O
mean O
vector O
with O
a O
subword O
estimation O
. O

For O
the O
Gaussian O
mixture O
case O
, O
we O
represent O
one O
Gaus- O
sian O
component O
’s O
mean O
vector O
with O
the O
subword O
structure O
whereas O
other O
components O
’ O
mean O
vec- O
tors O
are O
dictionary O
- O
based O
. O

This O
model O
choice O
to O
use O
dictionary O
- O
based O
mean O
vectors O
for O
other O
com- O
ponents O
is O
to O
reduce O
to O
constraint O
imposed O
by O
the O
subword O
structure O
and O
promote O
independence O
for O
meaning O
discovery O
. O

3.2 O
Similarity O
Measure O
between O
Words O
Traditionally O
, O
if O
words O
are O
represented O
by O
vec- O
tors O
, O
a O
common O
similarity O
metric O
is O
a O
dot O
prod- O
uct O
. O

In O
the O
case O
where O
words O
are O
represented O
by O
distribution O
functions O
, O
we O
use O
the O
general- O
ized O
dot O
product O
in O
Hilbert O
space O
h;iL2 O
, O
which O
is O
called O
the O
expected B-MetricName
likelihood I-MetricName
kernel I-MetricName
( O
Jebara O
et O
al O
. O
, O
2004 O
) O
. O

We O
deﬁne O
the O
energy O
E(f;g O
) O
between O
two O
words O
fandgto O
beE(f;g O
) O

= O
loghf;giL2= O

logR O
f(x)g(x)dx O
. O

With O
Gaussian O
mixturesf(x O
) O
= O
PK O
i=1piN(x;~ O
f;i;f;i)and O
g(x O
) O
= O
PK O
i=1qiN(x;~ O
g;i;g;i),PK O
i=1pi= O
1 O
, O
andPK O
i=1qi= O
1 O
, O
the O
energy O
has O
a O
closed O
form O
: O
E(f;g O
) O
= O

logKX O
j=1KX O
i=1piqjei;j(2 O
) O
wherej;jis O
the O
partial O
energy O
which O
corresponds O
to O
the O
similarity O
between O
component O
iof O
the O
ﬁrst O

4wordfand O
component O
jof O
the O
second O
word O
g.2 O
i;jlogN(0;~ O
f;i ~ O
g;j;f;i+ O
g;j O
) O

= O
 1 O
2log O
det( O
f;i+ O
g;j) D O
2log(2 O
) O
 1 O

2(~ O
f;i ~ O
g;j)>(f;i+ O
g;j) 1(~ O
f;i ~ O
g;j O
) O
( O
3 O
) O
Figure O
2 O
demonstrates O
the O
partial O
energies O
among O
the O
Gaussian O
components O
of O
two O
words O
. O

3.3 O
Loss O
Function O
The O
model O
parameters O
that O
we O
seek O
to O
learn O
are O
vw O
for O
each O
word O
wandzgfor O
each O
n O
- O
gram O
g. O
We O
train O
the O
model O
by O
pushing O
the O
energy O
of O
a O
true O
context O
pair O
wandcto O
be O
higher O
than O
the O
nega- O
tive O
context O
pair O
wandnby O

a O
margin O
m. O
We O
use O
Adagrad O
( O
Duchi O
et O
al O
. O
, O
2011 O
) O
to O
minimize O
the O
fol- O
lowing O
loss O
to O
achieve O
this O
outcome O
: O
L(f;g O
) O
= O
max O
[ O
0;m E(f;g O
) O
+ O
E(f;n)]:(4 O
) O
We O
describe O
how O
to O
sample O
words O
as O
well O
as O
its O
positive O
and O
negative O
contexts O
in O
Section O
3.5 O
. O

This O
loss O
function O
together O
with O
the O
Gaussian O
mixture O
model O
with O
K B-HyperparameterName
> B-HyperparameterValue
1 I-HyperparameterValue
has O
the O
ability O
to O
extract O
multiple O
senses O
of O
words O
. O

That O
is O
, O
for O
a O
word O
with O
multiple O
meanings O
, O
we O
can O
observe O
each O
mode O
to O
represent O
a O
distinct O
meaning O
. O

For O
in- O
stance O
, O
one O
density O
mode O
of O
“ O
star O
” O
is O
close O
to O
the O
densities O
of O
“ O
celebrity O
” O
and O
“ O
hollywood O
” O
whereas O
another O
mode O
of O
“ O
star O
” O
is O
near O
the O
densities O
of O
“ O
constellation O
” O
and O
“ O
galaxy O
” O
. O

3.4 O
Energy O
Simpliﬁcation O
In O
theory O
, O
it O
can O
be O
beneﬁcial O
to O
have O
covari- O
ance O
matrices O
as O
learnable O
parameters O
. O

In O
prac- O
tice O
, O
Athiwaratkun O
and O
Wilson O
( O
2017 O
) O
observe O
that O
spherical O
covariances O
often O
perform O
on O
par O
with O
diagonal O
covariances O
with O
much O
less O
computa- O
tional O
resources O
. O

Using O
spherical O
covariances O
for O
each O
component O
, O
we O
can O
further O
simplify O
the O
en- O
ergy O
function O
as O
follows O
: O
i;j=  O

  O
2jjf;i g;jjj2 O
; O
( O
5 O
) O
where O
the O
hyperparameter O

is O
the O
scale B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
in- I-HyperparameterName
verse I-HyperparameterName
covariance I-HyperparameterName
term O
in O
Equation O
3 O
. O

We O
note O
that O
Equation O
5 O
is O
equivalent O
to O
Equation O
3 O
up O
to O
an O
ad- O
ditive O
constant O
given O
that O
the O
covariance O
matrices O
are O
spherical O
and O
the O
same O
for O
all O
components O
. O

3.5 O
Word O
Sampling O
To O
generate O
a O
context O
word O
cof O
a O
given O
word O
w O
, O
we O
pick O
a O
nearby O
word O
within O
a O
context O
window O
of O
a O
ﬁxed B-HyperparameterName
length I-HyperparameterName
` O
. O

We O
also O
use O
a O
word O
sampling O
technique O
similar O
to O
Mikolov O
et O

al O
. O
( O
2013b O
) O
. O

This O
subsampling O
procedure O
selects O
words O
for O
training O
with O
lower O
probabilities O
if O
they O
appear O
frequently O
. O

This O
technique O
has O
an O
effect O
of O
reducing O
the O
impor- O
tance O
of O
words O
such O
as O
‘ O
the O
’ O
, O
‘ O
a O
’ O
, O
‘ O
to O
’ O
which O
can O
be O
predominant O
in O
a O
text O
corpus O
but O
are O
not O
as O
mean- O
ingful O
as O
other O
less O
frequent O
words O
such O
as O
‘ O
city O
’ O
, O
‘ O
capital O
’ O
, O
‘ O
animal O
’ O
, O
etc O
. O

In O
particular O
, O
word O
whas O
probabilityP(w O
) O

= O
1 p O
t O
= O
f(w)wheref(w)is O
the O
frequency O
of O
word O
win O
the O
corpus O
and O
tis O
the O
frequency O
threshold O
. O

A O
negative O
context O
word O
is O
selected O
using O
a O
dis- O
tributionPn(w)/U(w)3=4whereU(w)is O
a O
un- O
igram O
probability O
of O
word O

w. O

The O
exponent O
3=4 O
also O
diminishes O
the O
importance O
of O
frequent O
words O
and O
shifts O
the O
training O
focus O
to O
other O
less O
frequent O
words O
. O

4 O
Experiments O
We O
have O
proposed O
a O
probabilistic B-MethodName
F I-MethodName
ASTTEXT I-MethodName
model O
which O
combines O
the O
ﬂexibility O
of O
subword O
structure O
with O
the O
density O
embedding O
approach O
. O

In O
this O
section O
, O
we O
show O
that O
our O
probabilistic O
representation O
with O
subword O
mean O
vectors O
with O
the O
simpliﬁed O
energy O
function O
outperforms O
many O
word O
similarity O
baselines O
and O
provides O
disentan- O
gled O
meanings O
for O
polysemies O
. O

First O
, O
we O
describe O
the O
training O
details O
in O
Section O
4.1 O
. O

We O
provide O
qualitative O
evaluation O
in O
Section O

54.2 O
, O
showing O
meaningful O
nearest O
neighbors O
for O
the O
Gaussian O
embeddings O
, O
as O
well O
as O
the O
ability O
to O
capture O
multiple O
meanings O
by O
Gaussian O
mixtures O
. O

Our O
quantitative O
evaluation O
in O
Section O
4.3 O
demon- O
strates O
strong O
performance O
against O
the O
baseline O
models O
F B-MethodName
ASTTEXT I-MethodName
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
and O
the O
dictionary O
- O
level O
Gaussian O
( O
W2 O
G O
) O
( O
Vilnis O
and O
McCallum O
, O
2014 O
) O
and O
Gaussian O
mixture O
embed- O
dings O
( O
Athiwaratkun O
and O
Wilson O
, O
2017 O
) O
( O
W2GM O
) O
. O

We O
train O
our O
models O
on O
foreign O
language O
corpuses O
and O
show O
competitive O
results O
on O
foreign O
word O
sim- O
ilarity O
benchmarks O
in O
Section O
4.4 O
. O

Finally O
, O
we O
ex- O
plain O
the O
importance O
of O
the O
n O
- O
gram O
structures O
for O
semantic O
sharing O
in O
Section O
4.5 O
. O

4.1 O
Training O
Details O
We O
train O
our O
models O
on O
both O
English O
and O
for- O
eign O
language O
datasets O
. O

For O
English O
, O
we O
use O
the O
concatenation O
of O
UKWAC B-DatasetName
and O
W B-DatasetName
ACKY I-DatasetName
PEDIA I-DatasetName
( O
Ba- O
roni O
et O
al O
. O
, O
2009 O
) O
which O
consists O
of O
3:376billion O
words O
. O

We O
ﬁlter O
out O
word O
types O
that O
occur O
fewer O
than5times O
which O
results O
in O
a O
vocabulary O
size O
of O
2,677,466 O
. O

For O
foreign O
languages O
, O
we O
demonstrate O
the O
training O
of O
our O
model O
on O
French O
, O
German O
, O
and O
Ital- O
ian O
text O
corpuses O
. O

We O
note O
that O
our O
model O
should O
be O
applicable O
for O
other O
languages O
as O
well O
. O

We O
use O
F B-DatasetName
RWAC I-DatasetName
(French O
) O
, O
D B-DatasetName
EWAC I-DatasetName
(German O
) O
, O
I B-DatasetName
TWAC I-DatasetName
( O
Italian O
) O
datasets O
( O
Baroni O
et O
al O
. O
, O
2009 O
) O
for O
text O
cor- O
puses O
, O
consisting O
of O
1:634,1:716and1:955billion O
words O
respectively O
. O

We O
use O
the O
same O
threshold O
, O
ﬁltering O
out O
words O
that O
occur O
less O
than O
5times O
in O
each O
corpus O
. O

We O
have O
dictionary O
sizes O
of O
1:3,2:7 O
, O
and1:4million O
words O
for O
F B-DatasetName
RWAC I-DatasetName
, O
DEWAC B-DatasetName
, O
and O
ITWAC B-DatasetName
. O

We O
adjust O
the O
hyperparameters O
on O
the O
English O
corpus O
and O
use O
them O
for O
foreign O
languages O
. O

Note O
that O
the O
adjustable O
parameters O
for O
our O
models O
are O
the O
loss B-HyperparameterName
margin I-HyperparameterName
m B-HyperparameterName
in O
Equation O
4 O
and O
the O
scale B-HyperparameterName

  O
in O
Equation O
5 O
. O

We O
search O
for O
the O
optimal O
hyperpa- O
rameters O
in O
a O
grid O
m B-HyperparameterName
2f O
0.01 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
1 B-HyperparameterValue
, O
10 B-HyperparameterValue
, O
100 B-HyperparameterValue
, O
g O
and O

2f1 O

5*10-3/1 B-HyperparameterValue
, O
10-3/1 B-HyperparameterValue
, O
2*10-4/1 B-HyperparameterValue
, O
1*10-4/1 B-HyperparameterValue
gon O
our O
En- O
glish O
corpus O
. O

The O
hyperpameter O

affects O
the O
scale O
of O
the O
loss O
function O
; O
therefore O
, O
we O
adjust O
the O
learn- B-HyperparameterName
ing I-HyperparameterName
rate I-HyperparameterName
appropriately O
for O
each O

. O

In O
particular O
, O
the O
learning B-HyperparameterName
rates I-HyperparameterName
used O
are O

= O
f
10-4 B-HyperparameterValue
, O
10-5 B-HyperparameterValue
, O
10-6 B-HyperparameterValue
g O
for O
the O
respective O

values O
. O

Other O
ﬁxed O
hyperparameters O
include O
the O
num- B-HyperparameterName
ber I-HyperparameterName
of I-HyperparameterName
Gaussian I-HyperparameterName
components I-HyperparameterName
K B-HyperparameterName
= O
2 B-HyperparameterValue
, O
the O
con- B-HyperparameterName
text I-HyperparameterName
window I-HyperparameterName
length I-HyperparameterName
` O
= O
10 B-HyperparameterValue
and O
the O
subsampling B-HyperparameterName
threshold I-HyperparameterName
t B-HyperparameterName
= O
10-5 B-HyperparameterValue
. O

Similar O
to O
the O
setup O
in O
F B-MethodName
AST I-MethodName
- I-MethodName
TEXT I-MethodName
, O
we O
use O
n O
- O
grams O
where O
n B-HyperparameterName
= O

3 B-HyperparameterValue
, O
4 B-HyperparameterValue
, O
5 B-HyperparameterValue
, O
6 B-HyperparameterValue
to O
es- O
timate O
the O
mean O
vectors O
. O

4.2 O
Qualitative O
Evaluation O
- O
Nearest O
neighbors O
We O
show O
that O
our O
embeddings O
learn O
the O
word O
se- O
mantics O
well O
by O
demonstrating O
meaningful O
nearest O
neighbors O
. O

Table O
1 O
shows O
examples O
of O
polysemous O
words O
such O
as O
rock O
, O
star O
, O
andcell O
. O

Table O
1 O
shows O
the O
nearest O
neighbors O
of O
polyse- O
mous O
words O
. O

We O
note O
that O
subword O
embeddings O
prefer O
words O
with O
overlapping O
characters O
as O
near- O
est O
neighbors O
. O

For O
instance O
, O
“ O
rock O
- O
y O
” O
, O
“ O
rockn O
” O
, O
and O
“ O
rock O
” O
are O
both O
close O
to O
the O
word O
“ O
rock O
” O
. O

For O
the O
purpose O
of O
demonstration O
, O
we O
only O
show O
words O
with O
meaningful O
variations O
and O
omit O
words O
with O
small O
character O
- O
based O
variations O
previously O
men- O
tioned O
. O

However O
, O
all O
words O
shown O
are O
in O
the O
top- O
100 O
nearest O
words O
. O

We O
observe O
the O
separation O
in O
meanings O
for O
the O
multi O
- O
component O
case O
; O
for O
instance O
, O
one O
compo- O
nent O
of O
the O
word O
“ O
bank O
” O
corresponds O
to O
a O
ﬁnancial O
bank O
whereas O
the O
other O
component O
corresponds O
to O
a O
river O
bank O
. O

The O
single O
- O
component O
case O
also O
has O
interesting O
behavior O
. O

We O
observe O
that O
the O
subword O
embeddings O
of O
polysemous O
words O
can O
represent O
both O
meanings O
. O

For O
instance O
, O
both O
“ O
lava O
- O
rock O
” O
and O
“ O
rock O
- O
pop O
” O
are O
among O
the O
closest O
words O
to O
“ O
rock O
” O
. O

4.3 O
Word O
Similarity O
Evaluation O
We O
evaluate O
our O
embeddings O
on O
several O
standard O
word O
similarity O
datasets O
, O
namely O
, O
SL-999 B-DatasetName
( O
Hill O
et O
al O
. O
, O
2014 O
) O
, O
WS-353 B-DatasetName
( O
Finkelstein O
et O
al O
. O
, O
2002 O
) O
, O
MEN-3k B-DatasetName
( O
Bruni O
et O
al O
. O
, O
2014 O
) O
, O
MC-30 B-DatasetName
( O
Miller O
and O
Charles O
, O
1991 O
) O
, O
RG-65 B-DatasetName
( O
Rubenstein O
and O
Goode- O
nough O
, O
1965 O
) O
, O
YP-130 B-DatasetName
( O
Yang O
and O
Powers O
, O
2006 O
) O
, O
MTurk(-287,-771 B-DatasetName
) I-DatasetName
( O
Radinsky O
et O
al O
. O
, O
2011 O
; O
Halawi O
et O
al O
. O
, O
2012 O
) O
, O
and O
RW-2k B-DatasetName
( O
Luong O
et O
al O
. O
, O
2013 O
) O
. O

Each O
dataset O
contains O
a O
list O
of O
word O
pairs O
with O
a O
human O
score O
of O
how O
related O
or O
similar O
the O
two O
words O
are O
. O

We O
use O
the O
notation O
DATASET O
-NUM O
to O
denote O
the O
number O
of O
word O
pairs O
NUM O
in O
each O
evaluation O
set O
. O

We O
note O
that O
the O
dataset O
RW B-DatasetName
fo- O
cuses O
more O
on O
infrequent O
words O
and O
SimLex-999 B-DatasetName
focuses O
on O
the O
similarity O
of O
words O
rather O
than O
re- O
latedness O
. O

We O
also O
compare O
PFT B-MethodName
- I-MethodName
GM I-MethodName
with O
other O
multi O
- O
prototype O
embeddings O
in O
the O
literature O
us- O
ing O

SCWS B-DatasetName
( O
Huang O
et O
al O
. O
, O
2012 O
) O
, O
a O
word O
similar- O

ity O
dataset O
that O
is O
aimed O
to O
measure O
the O
ability O
of O
embeddings O
to O
discern O
multiple O
meanings O
. O

We O
calculate O
the O
Spearman B-MetricName
correlation I-MetricName
( O
Spear- O
man O
, O
1904 O
) O
between O
the O
labels O
and O
our O
scores O
gen- O
erated O
by O
the O
embeddings O
. O

The O
Spearman B-MetricName
corre- I-MetricName
lation I-MetricName
is O
a O
rank O
- O
based O
correlation O
measure O
that O
as- O
sesses O
how O
well O
the O
scores O
describe O
the O
true O
labels O
. O

The O
scores O
we O
use O
are O
cosine O
- O
similarity O
scores O
be- O
tween O
the O
mean O
vectors O
. O

In O
the O
case O
of O
Gaussian O
mixtures O
, O
we O
use O
the O
pairwise O
maximum O
score O
: O
s(f;g O
) O

= O
max O
i21;:::;Kmax O
j21;:::;Kf;ig;j O
jjf;ijjjjg;jjj:(6 O
) O

The O
pair O
( O
i;j)that O
achieves O
the O
maximum O
cosine O
similarity O
corresponds O
to O
the O
Gaussian O
component O
pair O
that O
is O
the O
closest O
in O
meanings O
. O

Therefore O
, O
this O
similarity O
score O
yields O
the O
most O
related O
senses O
of O
a O
given O
word O
pair O
. O

This O
score O
reduces O
to O
a O
cosine O
similarity O
in O
the O
Gaussian O
case O
( O
K B-HyperparameterName
= O
1 B-HyperparameterValue
) O
.4.3.1 O

Comparison O
Against O
Dictionary B-MethodName
- I-MethodName
Level I-MethodName
Density I-MethodName
Embeddings I-MethodName
and O
FASTTEXT B-MethodName
We O
compare O
our O
models O
against O
the O
dictionary- B-MethodName
level I-MethodName
Gaussian I-MethodName
and I-MethodName
Gaussian I-MethodName
mixture I-MethodName
embed- I-MethodName
dings I-MethodName
in O
Table O
2 O
, O
with O
50 O
- O
dimensional O
and O
300- O
dimensional O
mean O
vectors O
. O

The O
50 O
- O
dimensional O
results O
for O
W2G B-MethodName
and O
W2GM B-MethodName
are O
obtained O
directly O
from O
Athiwaratkun O
and O
Wilson O
( O
2017 O
) O
. O

For O
com- O
parison O
, O
we O
use O
the O
public O
code3to O
train O
the O
300- O
dimensional O
W2G B-MethodName
and O
W2GM B-MethodName
models O
and O
the O
pub- O
licly O
available O
F B-MethodName
ASTTEXT I-MethodName
model4 O
. O

We O
calculate O
Spearman B-MetricName
’s I-MetricName
correlations I-MetricName
for O
each O
of O
the O
word O
similarity O
datasets O
. O

These O
datasets O
vary O
greatly O
in O
the O
number O
of O
word O
pairs O
; O
there- O
fore O
, O
we O
mark O
each O
dataset O
with O
its O
size O
for O
visibil- O
ity O
. O

For O
a O
fair O
and O
objective O
comparison O
, O
we O
cal- O
culate O
a O
weighted O
average O
of O
the O
correlation O
scores O
for O
each O
model O
. O

Our O
PFT B-MethodName
- I-MethodName
GM I-MethodName
achieves O
the O
highest O
average O
score O
among O
all O
competing O
models O
, O
outperforming O
both O
FASTTEXT B-MethodName
and O
the O
dictionary O
- O
level O
embeddings O
W2G B-MethodName
and O
W2GM B-MethodName
. O

Our O
unimodal O
model O
PFT B-MethodName
- I-MethodName
G I-MethodName
also O
outperforms O
the O
dictionary O
- O
level O
counterpart O
W2 B-MethodName
G I-MethodName
and O
F B-MethodName
ASTTEXT I-MethodName
. O

We O
note O
that O
the O
model O
W2GM B-MethodName
appears O
quite O
strong O
according O
to O
Table O
2 O
, O
beating O
PFT B-MethodName
- I-MethodName
GM I-MethodName
on O
many O
word O
similarity O
datasets O
. O

How- O
ever O
, O
the O
datasets O
that O
W2GM B-MethodName
performs O
better O
than O
PFT B-MethodName
- I-MethodName
GM I-MethodName
often O
have O
small O
sizes O
such O
as O
MC-30 B-DatasetName
or O
RG-65 B-DatasetName
, O
where O
the O
Spearman B-MetricName
’s I-MetricName
correlations I-MetricName
are O
more O
subject O
to O
noise O
. O

Overall O
, O
PFT B-MethodName
- I-MethodName
GM I-MethodName
outper- O
forms O
W2GM B-DatasetName
by O
3.1% B-MetricValue
and O
8.7% B-MetricValue
in O
300 O
and O 
50di- O
mensional O
models O
. O

In O
addition O
, O
PFT B-MethodName
- I-MethodName
G I-MethodName
and O
PFT B-MethodName
- I-MethodName
GM I-MethodName
also O
outperform O
F B-MethodName
ASTTEXT I-MethodName
by O
1.2% B-MetricValue
and O
3.7% B-MetricValue
re- O
spectively O
. O

4.3.2 O
Comparison O
Against O
Multi O
- O
Prototype O
Models O
In O
Table O
3 O
, O
we O
compare O
50and300dimensional O
PFT B-MethodName
- I-MethodName
GM I-MethodName
models O
against O
the O
multi O
- O
prototype O
em- O
beddings O
described O
in O
Section O
2 O
and O
the O
existing O
multimodal O
density O
embeddings O
W2GM B-MethodName
. O

We O
use O
the O
word O
similarity O
dataset O
SCWS B-DatasetName
( O
Huang O
et O
al O
. O
, O
2012 O
) O
which O
contains O
words O
with O
potentially O
many O
meanings O
, O
and O
is O
a O
benchmark O
for O
distinguishing O
senses O
. O

We O
use O
the O
maximum B-MetricName
similarity I-MetricName
score O
( O
Equation O
6 O
) O
, O
denoted O
as O
M B-MetricName
AXSIM I-MetricName
. O

AVESIM B-MetricName
de- O
notes O
the O
average B-MetricName
of I-MetricName
the I-MetricName
similarity I-MetricName
scores I-MetricName
, O
rather O
than O
the O
maximum O
. O

We O
outperform O
the O
dictionary O
- O
based O
density O
embeddings O
W2GM B-MethodName
in O
both O
50and300 O
dimen- O
sions O
, O
demonstrating O
the O
beneﬁts O
of O
subword O
in- O

formation O
. O

Our O
model O
achieves O
state O
- O
of O
- O
the O
- O
art O
re- O
sults O
, O
similar O
to O
that O
of O
Neelakantan O
et O
al O
. O

( O
2014 O
) O
. O

4.4 O
Evaluation O
on O
Foreign O
Language O
Embeddings O
We O
evaluate O
the O
foreign O
- O
language O
embeddings O
on O
word O
similarity O
datasets O
in O
respective O
lan- O
guages O
. O

We O
use O
Italian B-DatasetName
W I-DatasetName
ORDSIM353 I-DatasetName
and O
Ital- B-DatasetName
ian I-DatasetName
S I-DatasetName
IMLEX-999 I-DatasetName
( O
Leviant O
and O
Reichart O
, O
2015 O
) O
for O
Italian O
models O
, O
GUR350 B-DatasetName
and O
GUR65 B-DatasetName
( O
Gurevych O
, O
2005 O
) O
for O
German O
models O
, O
and O
French O
W B-DatasetName
ORD- I-DatasetName
SIM353 I-DatasetName
( O
Finkelstein O
et O
al O
. O
, O
2002 O
) O
for O
French O
mod- O
els O
. O

For O
datasets O
GUR350 B-DatasetName
and O
GUR65 B-DatasetName
, O
we O
use O
the O
results O
reported O
in O
the O
F B-MethodName
ASTTEXT I-MethodName
publication O
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
. O

For O
other O
datasets O
, O
we O
train O
F B-MethodName
ASTTEXT I-MethodName
models O
for O
comparison O
using O
the O
public O
code5on O
our O
text O
corpuses O
. O

We O
also O
train O
dictionary O
- O
level O
models O
W2 B-MethodName
G I-MethodName
, O
and O
W2GM B-MethodName
for O
com- O
parison O
. O

Table O
4 O
shows O
the O
Spearman B-MetricName
’s I-MetricName
correlation I-MetricName
re- O
sults O
of O
our O
models O
. O

We O
outperform O
F B-MethodName
ASTTEXT I-MethodName
on O
many O
word O
similarity O
benchmarks O
. O

Our O
results O
are O
also O
signiﬁcantly O
better O
than O
the O
dictionary O
- O
based O
models O
, O
W2G B-MethodName
and O
W2GM B-MethodName
. O

We O
hypothesize O
that O
W2G B-MethodName
and O
W2GM B-MethodName
can O
perform O
better O
than O
the O
cur- O
rent O
reported O
results O
given O
proper O
pre O
- O
processing O
of O
words O
due O
to O
special O
characters O
such O
as O
accents O
. O

We O
investigate O
the O
nearest O
neighbors O
of O
poly- O
semies O
in O
foreign O
languages O
and O
also O
observe O
clear O
sense O
separation O
. O

For O
example O
, O
piano O
in O
Italian O
can O
mean O
“ O
ﬂoor O
” O
or O
“ O
slow O
” O
. O

These O
two O
meanings O
are O
reﬂected O
in O
the O
nearest O
neighbors O
where O
one O
component O
is O
close O
to O
piano O
- O
piano O
, O
pianod O
which O
mean O
“ O
slowly O
” O
whereas O
the O
other O
component O
is O
close O
to O
piani O
( O
ﬂoors O
) O
, O
istrutturazione O
( O
renovation O
) O
orinfrastruttre O
( O
infrastructure O
) O
. O

Table O
5 O
shows O
ad- O
ditional O
results O
, O
demonstrating O
that O
the O
disentan- O
gled O
semantics O
can O
be O
observed O
in O
multiple O
lan- O
guages O
. O

4.5 O
Qualitative O
Evaluation O
- O
Subword O
Decomposition O
One O
of O
the O
motivations O
for O
using O
subword O
infor- O
mation O
is O
the O
ability O
to O
handle O
out O
- O
of O
- O
vocabulary O
words O
. O

Another O
beneﬁt O
is O
the O
ability O
to O
help O
im- O
prove O
the O
semantics O
of O
rare O
words O
via O
subword O
sharing O
. O

Due O
to O
an O
observation O
that O
text O
corpuses O
follow O
Zipf O
’s O
power O
law O
( O
Zipf O
, O
1949 O
) O
, O
words O
at O
the O
tail O
of O
the O
occurrence O
distribution O
appears O
much O
less O
frequently O
. O

Training O
these O
words O
to O
have O
a O
good O
semantic O
representation O
is O
challenging O
if O
done O
at O
the O
word O
level O
alone O
. O

However O
, O
an O
n- O
gram O
such O
as O
‘ O
abnorm O
’ O
is O
trained O
during O
both O
oc- O
currences O
of O
“ O
abnormal O
” O
and O
“ O
abnormality O
” O
in O
the O
corpus O
, O
hence O
further O
augments O
both O
words O
’s O
se- O
mantics O
. O

Figure O
3 O
shows O
the O
contribution O
of O
n O
- O
grams O
to O
the O
ﬁnal O
representation O
. O

We O
ﬁlter O
out O
to O
show O
only O
the O
n O
- O
grams O
with O
the O
top-5 O
and O
bottom-5 O
similarity O
scores O
. O

We O
observe O
that O
the O
ﬁnal O
representations O
of O
both O
words O
align O
with O
n O
- O
grams O
“ O
abno O
” O
, O
“ O
bnor O
” O
, O
“ O
abnorm O
” O
, O
“ O
anbnor O
” O
, O
“ O
< O
abn O
” O
. O

In O
fact O
, O
both O
“ O
ab- O
normal O
” O
and O
“ O
abnormality O
” O
share O
the O
same O
top-5 O
n O
- O
grams O
. O

Due O
to O
the O
fact O
that O
many O
rare O
words O
such O
as O
“ O
autobiographer O
” O
, O
“ O
circumnavigations O
” O
, O
or O
“ O
hypersensitivity O
” O
are O
composed O
from O
many O
com- O
mon O
sub O
- O
words O
, O
the O
n O
- O
gram O
structure O
can O
help O
im- O
prove O
the O
representation O
quality O
. O

5 O
Numbers O
of O
Components O
It O
is O
possible O
to O
train O
our O
approach O
with O
K B-HyperparameterName
> B-HyperparameterValue
2 I-HyperparameterValue
mixture O
components O
; O
however O
, O
Athiwaratkun O
and O
Wilson O
( O
2017 O
) O
observe O
that O
dictionary O
- O
level O
Gaus- O
sian O
mixtures O
with O
K B-HyperparameterName
= O
3 B-HyperparameterValue
do O
not O
overall O
im- O
prove O
word O
similarity O
results O
, O
even O
though O
these O
mixtures O
can O
discover O
3distinct O
senses O
for O
certain O
words O
. O

Indeed O
, O
while O
K B-HyperparameterName
> B-HyperparameterValue
2 I-HyperparameterValue
in O
principle O
allows O
for O
greater O
ﬂexibility O
than O
K B-HyperparameterName
= O
2 B-HyperparameterValue
, O
most O
words O
can O
be O
very O
ﬂexibly O
modelled O
with O
a O
mixture O
of O
two O
Gaussians O
, O
leading O
to O
K B-HyperparameterName
= O
2 B-HyperparameterValue
representing O
a O
good O
balance O
between O
ﬂexibility O
and O
Occam O
’s O
razor O
. O

Even O
for O
words O
with O
single O
meanings O
, O
our O
PFT B-MethodName
model O
with O
K B-HyperparameterName
= O
2 B-HyperparameterValue
often O
learns O
richer O
repre- O
sentations O
than O
a O
K B-HyperparameterName
= O
1 B-HyperparameterValue
model O
. O

For O
example O
, O
the O
two O
mixture O
components O
can O
learn O
to O
cluster O
to- O

9gether O
to O
form O
a O
more O
heavy O
tailed O
unimodal O
distri- O
bution O
which O
captures O
a O
word O
with O
one O
dominant O
meaning O
but O
with O
close O
relationships O
to O
a O
wide O
range O
of O
other O
words O
. O

In O
addition O
, O
we O
observe O
that O
our O
model O
with O
K B-HyperparameterName
components O
can O
capture O
more O
than O
K B-HyperparameterName
meanings O
. O

For O
instance O
, O
in O
K B-HyperparameterName
= O
1 B-HyperparameterValue
model O
, O
the O
word O
pairs O
( O
“ O
cell O
” O
, O
“ O
jail O
” O
) O
and O
( O
“ O
cell O
” O
, O
“ O
biology O
” O
) O
and O
( O
“ O
cell O
” O
, O
“ O
phone O
” O
) O
will O
all O
have O
positive O
similarity O
scores O
based O
on O
K B-HyperparameterName
= O
1 B-HyperparameterValue
model O
. O

In O
general O
, O
if O
a O
word O
has O
multiple O
meanings O
, O
these O
meanings O
are O
usually O
compressed O
into O
the O
linear O
substructure O
of O
the O
em- O
beddings O
( O
Arora O
et O
al O
. O
, O
2016 O
) O
. O

However O
, O
the O
pairs O
of O
non O
- O
dominant O
words O
often O
have O
lower O
similar- O
ity O
scores O
, O
which O
might O
not O
accurately O
reﬂect O
their O
true O
similarities O
. O

6 O
Conclusion O
and O
Future O
Work O
We O
have O
proposed O
models O
for O
probabilistic O
word O
representations O
equipped O
with O
ﬂexible O
sub O
- O
word O
structures O
, O
suitable O
for O
rare O
and O
out O
- O
of O
- O
vocabulary O
words O
. O

The O
proposed O
probabilistic O
formulation O
in- O
corporates O
uncertainty O
information O
and O
naturally O
allows O
one O
to O
uncover O
multiple O
meanings O
with O
multimodal O
density O
representations O
. O

Our O
models O
offer O
better O
semantic O
quality O
, O
outperforming O
com- O
peting O
models O
on O
word O
similarity O
benchmarks O
. O

Moreover O
, O
our O
multimodal O
density O
models O
can O
provide O
interpretable O
and O
disentangled O
representa- O
tions O
, O
and O
are O
the O
ﬁrst O
multi O
- O
prototype O
embeddings O
that O
can O
handle O
rare O
words O
. O

Future O
work O
includes O
an O
investigation O
into O
the O
trade O
- O
off O
between O
learning O
full O
covariance O
ma- O

trices O
for O
each O
word O
distribution O
, O
computational O
complexity O
, O
and O
performance O
. O

This O
direction O
can O
potentially O
have O
a O
great O
impact O
on O
tasks O
where O
the O
variance O
information O
is O
crucial O
, O
such O
as O
for O
hi- O
erarchical O
modeling O
with O
probability O
distributions O
( O
Athiwaratkun O
and O
Wilson O
, O
2018 O
) O
. O

Other O
future O
work O
involves O
co O
- O
training O
PFT B-MethodName
on O
many O
languages O
. O

Currently O
, O
existing O
work O
on O
multi O
- O
lingual O
embeddings O
align O
the O
word O
seman- O
tics O
on O
pre O
- O
trained O
vectors O
( O
Smith O
et O
al O
. O
, O
2017 O
) O
, O
which O
can O
be O
suboptimal O
due O
to O
polysemies O
. O

We O
envision O
that O
the O
multi O
- O
prototype O
nature O
can O
help O
disambiguate O
words O
with O
multiple O
meanings O
and O
facilitate O
semantic O
alignment O
. O

References O
Sanjeev O
Arora O
, O
Yuanzhi O
Li O
, O
Yingyu O
Liang O
, O
Tengyu O
Ma O
, O
and O
Andrej O
Risteski O
. O

2016 O
. O

Linear O
al O
- O
gebraic O
structure O
of O
word O
senses O
, O
with O
appli- O
cations O
to O
polysemy O
. O

CoRR O
abs/1601.03764 O
. O

http://arxiv.org/abs/1601.03764 O
. O

Ben O
Athiwaratkun O
and O
Andrew O
Gordon O
Wilson O
. O
2017 O
. O

Multimodal O
word O
distributions O
. O

In O
ACL O
. O
https://arxiv.org/abs/1704.08424 O
. O

Ben O
Athiwaratkun O
and O
Andrew O
Gordon O
Wilson O
. O

2018 O
. O

On O
modeling O
hierarchical O
data O
via O
probabilistic O
or- O
der O
embeddings O
. O

ICLR O
. O

Marco O
Baroni O
, O
Silvia O
Bernardini O
, O
Adriano O
Fer- O
raresi O
, O
and O
Eros O
Zanchetta O
. O

2009 O
. O

The O
wacky O
wide O
web O
: O
a O
collection O
of O
very O
large O
linguis- O
tically O
processed O
web O
- O
crawled O
corpora O
. O

Lan- O
guage O
Resources O
and O
Evaluation O
43(3):209–226 O
. O

https://doi.org/10.1007/s10579-009-9081-4 O
. O

Yoshua O
Bengio O
, O
R O
´ O
ejean O
Ducharme O
, O
Pascal O
Vin- O
cent O
, O
and O
Christian O
Janvin O
. O

2003 O
. O

A O
neu- O
ral O
probabilistic O
language O
model O
. O

Journal O
of O
Machine O
Learning O
Research O
3:1137–1155 O
. O
http://www.jmlr.org/papers/v3/bengio03a.html O
. O

Piotr O
Bojanowski O
, O
Edouard O
Grave O
, O
Armand O
Joulin O
, O
and O
Tomas O
Mikolov O
. O

2016 O
. O

Enriching O
word O
vectors O
with O
subword O
information O
. O

CoRR O
abs/1607.04606 O
. O

http://arxiv.org/abs/1607.04606 O
. O

Elia O
Bruni O
, O
Nam O
Khanh O
Tran O
, O
and O
Marco O
Ba- O
roni O
. O

2014 O
. O

Multimodal O
distributional O
se- O
mantics O
. O

J. O
Artif O
. O

Int O
. O

Res O
. O

49(1):1–47 O
. O

http://dl.acm.org/citation.cfm?id=2655713.2655714 O
. O

Xinxiong O
Chen O
, O
Zhiyuan O
Liu O
, O
and O
Maosong O
Sun O
. O

2014 O
. O

A O
uniﬁed O
model O
for O
word O
sense O
represen- O
tation O
and O
disambiguation O
. O

In O
Proceedings O
of O
the O
2014 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
EMNLP O
2014 O
, O
October O
25- O
29 O
, O
2014 O
, O
Doha O
, O
Qatar O
, O
A O
meeting O
of O
SIGDAT O
, O
a O
Spe- O
cial O
Interest O
Group O
of O
the O
ACL O
. O

pages O
1025–1035 O
. O

http://aclweb.org/anthology/D/D14/D14-1110.pdf O
. O

Ronan O
Collobert O
and O
Jason O
Weston O
. O

2008 O
. O

A O
uni- O
ﬁed O
architecture O
for O
natural O
language O
processing O
: O
deep O
neural O
networks O
with O
multitask O
learning O
. O

In O
Machine O
Learning O
, O
Proceedings O
of O
the O
Twenty- O
Fifth O
International O
Conference O
( O
ICML O
2008 O
) O
, O
Helsinki O
, O
Finland O
, O
June O
5 O
- O
9 O
, O
2008 O
. O

pages O
160–167 O
. O

http://doi.acm.org/10.1145/1390156.1390177 O
. O

John O
C. O
Duchi O
, O
Elad O
Hazan O
, O
and O
Yoram O
Singer O
. O

2011 O
. O

Adaptive O
subgradient O
methods O
for O
on- O
line O
learning O
and O
stochastic O
optimization O
. O

Jour- O
nal O
of O
Machine O
Learning O
Research O
12:2121–2159 O
. O

http://dl.acm.org/citation.cfm?id=2021068 O
. O

Lev O
Finkelstein O
, O
Evgeniy O
Gabrilovich O
, O
Yossi O
Matias O
, O
Ehud O
Rivlin O
, O
Zach O
Solan O
, O
Gadi O
Wolfman O
, O
and O
Eytan O
Ruppin O
. O

2002 O
. O

Placing O
search O
in O
context O
: O
the O
con- O
cept O
revisited O
. O

ACM O
Trans O
. O

Inf O
. O
Syst O
. O

20(1):116–131 O
. O

http://doi.acm.org/10.1145/503104.503110 O
. O

10Iryna O
Gurevych O
. O
2005 O
. O

Using O
the O
structure O
of O
a O
concep- O
tual O
network O
in O
computing O
semantic O
relatedness O
. O

In O
Natural O
Language O
Processing O
- O
IJCNLP O
2005 O
, O
Sec- O
ond O
International O
Joint O
Conference O
, O
Jeju O
Island O
, O
Ko- O
rea O
, O
October O
11 O
- O
13 O
, O
2005 O
, O
Proceedings O
. O

pages O
767 O
– O
778 O
. O

Guy O
Halawi O
, O
Gideon O
Dror O
, O
Evgeniy O
Gabrilovich O
, O
and O
Yehuda O
Koren O
. O

2012 O
. O

Large O
- O
scale O
learning O
of O
word O
relatedness O
with O
constraints O
. O

In O
The O
18th O
ACM O
SIGKDD O
International O
Conference O
on O
Knowl- O
edge O
Discovery O
and O
Data O
Mining O
, O
KDD O
’ O
12 O
, O
Bei- O
jing O
, O
China O
, O
August O
12 O
- O
16 O
, O
2012 O
. O

pages O
1406–1414 O
. O

http://doi.acm.org/10.1145/2339530.2339751 O
. O

Felix O
Hill O
, O
Roi O
Reichart O
, O
and O
Anna O
Korhonen O
. O

2014 O
. O

Simlex-999 O
: O
Evaluating O
semantic O
models O
with O
( O
gen- O
uine O
) O
similarity O
estimation O
. O

CoRR O
abs/1408.3456 O
. O

http://arxiv.org/abs/1408.3456 O
. O

Eric O
H. O
Huang O
, O
Richard O
Socher O
, O
Christopher O
D. O
Man- O
ning O
, O
and O
Andrew O
Y O
. O

Ng O
. O
2012 O
. O

Improving O
word O
representations O
via O
global O
context O
and O
multiple O
word O
prototypes O
. O

In O
The O
50th O
Annual O
Meeting O
of O
the O
As- O
sociation O
for O
Computational O
Linguistics O
, O
Proceed- O
ings O
of O
the O
Conference O
, O
July O
8 O
- O
14 O
, O
2012 O
, O
Jeju O
Island O
, O
Korea O
- O
Volume O
1 O
: O
Long O
Papers O
. O

pages O
873–882 O
. O

http://www.aclweb.org/anthology/P12-1092 O
. O

Tony O
Jebara O
, O
Risi O
Kondor O
, O
and O
Andrew O
Howard O
. O

2004 O
. O

Probability O
product O
kernels O
. O

Journal O
of O
Machine O
Learning O
Research O
5:819–844 O
. O

Yoon O
Kim O
, O
Yacine O
Jernite O
, O
David O
Sontag O
, O
and O
Alexan- O
der O
M. O
Rush O
. O

2016 O
. O

Character O
- O
aware O
neural O
lan- O
guage O
models O
. O

In O
Proceedings O
of O
the O
Thirtieth O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
February O
12- O
17 O
, O
2016 O
, O
Phoenix O
, O
Arizona O
, O
USA O
. O
. O

pages O
2741 O
– O
2749 O
. O

Onur O
Kuru O
, O
Ozan O
Arkan O
Can O
, O
and O
Deniz O
Yuret O
. O
2016 O
. O

Charner O
: O
Character O
- O
level O
named O
entity O
recogni- O
tion O
. O

In O
COLING O
2016 O
, O
26th O
International O
Con- O
ference O
on O
Computational O
Linguistics O
, O
Proceed- O
ings O
of O
the O
Conference O
: O
Technical O
Papers O
, O
Decem- O
ber O
11 O
- O
16 O
, O
2016 O
, O
Osaka O
, O
Japan O
. O

pages O
911–921 O
. O

http://aclweb.org/anthology/C/C16/C16-1087.pdf O
. O

Jason O
Lee O
, O
Kyunghyun O
Cho O
, O
and O
Thomas O
Hofmann O
. O
2017 O
. O

Fully O
character O
- O
level O
neural O
machine O
translation O
without O
ex- O
plicit O
segmentation O
. O

TACL O
5:365–378 O
. O

https://transacl.org/ojs/index.php/tacl/article/view/1051 O
. O

Ira O
Leviant O
and O
Roi O
Reichart O
. O

2015 O
. O

Judgment O
lan- O
guage O
matters O
: O
Multilingual O
vector O
space O
models O
for O
judgment O
language O
aware O
lexical O
semantics O
. O

CoRR O
abs/1508.00106 O
. O

http://arxiv.org/abs/1508.00106 O
. O

Minh O
- O
Thang O
Luong O
, O
Richard O
Socher O
, O
and O
Christo- O
pher O
D. O
Manning O
. O

2013 O
. O

Better O
word O
representations O
with O
recursive O
neural O
networks O
for O
morphology O
. O

In O
CoNLL O
. O

Soﬁa O
, O
Bulgaria O
. O

Tomas O
Mikolov O
, O
Kai O
Chen O
, O
Greg O
Corrado O
, O
and O
Jeffrey O
Dean O
. O

2013a O
. O

Efﬁcient O
estimation O
of O
word O
repre- O
sentations O
in O
vector O
space O
. O

CoRR O
abs/1301.3781 O
. O

http://arxiv.org/abs/1301.3781 O
. O

Tomas O
Mikolov O
, O
Kai O
Chen O
, O
Greg O
Corrado O
, O
and O
Jeffrey O
Dean O
. O

2013b O
. O

Efﬁcient O
estimation O
of O
word O
repre- O
sentations O
in O
vector O
space O
. O

CoRR O
abs/1301.3781 O
. O

http://arxiv.org/abs/1301.3781 O
. O

Tomas O
Mikolov O
, O
Stefan O
Kombrink O
, O
Luk O
´ O
as O
Burget O
, O
Jan O
Cernock O
´ O
y O
, O
and O
Sanjeev O
Khudanpur O
. O

2011 O
. O

Exten- O
sions O
of O
recurrent O
neural O
network O
language O
model O
. O

InProceedings O
of O
the O
IEEE O
International O
Confer- O

ence O
on O
Acoustics O
, O
Speech O
, O
and O
Signal O
Processing O
, O
ICASSP O
2011 O
, O
May O
22 O
- O
27 O
, O
2011 O
, O
Prague O
Congress O
Center O
, O
Prague O
, O
Czech O
Republic O
. O

pages O
5528–5531 O
. O
https://doi.org/10.1109/ICASSP.2011.5947611 O
. O

George O
A. O
Miller O
and O
Walter O
G. O
Charles O
. O

1991 O
. O

Contextual O
Correlates O
of O
Semantic O
Similarity O
. O

Language O
& O
Cognitive O
Processes O
6(1):1–28 O
. O

https://doi.org/10.1080/01690969108406936 O
. O

Arvind O
Neelakantan O
, O
Jeevan O
Shankar O
, O
Alexandre O
Pas- O
sos O
, O
and O
Andrew O
McCallum O
. O

2014 O
. O

Efﬁcient O
non- O
parametric O
estimation O
of O
multiple O
embeddings O
per O
word O
in O
vector O
space O
. O

In O
Proceedings O
of O
the O
2014 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Lan- O
guage O
Processing O
, O
EMNLP O
2014 O
, O
October O
25 O
- O
29 O
, O
2014 O
, O
Doha O
, O
Qatar O
, O
A O
meeting O
of O
SIGDAT O
, O
a O
Spe- O
cial O
Interest O
Group O
of O
the O
ACL O
. O

pages O
1059–1069 O
. O

http://aclweb.org/anthology/D/D14/D14-1113.pdf O
. O

Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christo- O
pher O
D. O
Manning O
. O

2014 O
. O

Glove O
: O
Global O
vectors O
for O
word O
representation O
. O

In O
Proceedings O
of O
the O
2014 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Lan- O
guage O
Processing O
, O
EMNLP O
2014 O
, O
October O
25 O
- O
29 O
, O
2014 O
, O
Doha O
, O
Qatar O
, O
A O
meeting O
of O
SIGDAT O
, O
a O
Spe- O
cial O
Interest O
Group O
of O
the O
ACL O
. O

pages O
1532–1543 O
. O

http://aclweb.org/anthology/D/D14/D14-1162.pdf O
. O
Kira O
Radinsky O
, O
Eugene O
Agichtein O
, O
Evgeniy O
Gabrilovich O
, O
and O
Shaul O
Markovitch O
. O
2011 O
. O

A O
word O
at O
a O
time O
: O

Computing O
word O
relatedness O
using O
temporal O
semantic O
analysis O
. O

In O
Proceed- O
ings O
of O
the O
20th O
International O
Conference O
on O
World O
Wide O
Web O
. O

WWW O
’ O
11 O
, O
pages O
337–346 O
. O

http://doi.acm.org/10.1145/1963405.1963455 O
. O

Herbert O
Rubenstein O
and O
John O
B. O
Goode- O
nough O
. O

1965 O
. O

Contextual O
correlates O
of O
syn- O
onymy O
. O

Commun O
. O

ACM O
8(10):627–633 O
. O

http://doi.acm.org/10.1145/365628.365657 O
. O

Samuel O
L. O
Smith O
, O
David O
H. O
P. O
Turban O
, O
Steven O
Ham- O
blin O
, O
and O
Nils O
Y O
. O

Hammerla O
. O
2017 O
. O

Ofﬂine O
bilin- O

gual O
word O
vectors O
, O
orthogonal O
transformations O
and O
the O
inverted O
softmax O
. O

CoRR O
abs/1702.03859 O
. O

http://arxiv.org/abs/1702.03859 O
. O

C. O
Spearman O
. O

1904 O
. O

The O
proof O
and O
measurement O
of O
association O
between O
two O
things O
. O

American O
Journal O
of O
Psychology O
15:88–103 O
. O

11Fei O
Tian O
, O
Hanjun O
Dai O
, O
Jiang O
Bian O
, O
Bin O
Gao O
, O
Rui O
Zhang O
, O
Enhong O
Chen O
, O
and O
Tie O
- O
Yan O
Liu O
. O

2014 O
. O

A O
prob- O
abilistic O
model O
for O
learning O
multi O
- O
prototype O
word O
embeddings O
. O

In O
COLING O
2014 O
, O
25th O
International O
Conference O
on O
Computational O
Linguistics O
, O
Proceed- O
ings O
of O
the O
Conference O
: O
Technical O
Papers O
, O
Au- O

gust O
23 O
- O
29 O
, O
2014 O
, O
Dublin O
, O
Ireland O
. O

pages O
151–160 O
. O

http://aclweb.org/anthology/C/C14/C14-1016.pdf O
. O
Luke O
Vilnis O
and O
Andrew O
McCallum O
. O

2014 O
. O

Word O
representations O
via O
gaussian O
embedding O
. O

CoRR O
abs/1412.6623 O
. O

http://arxiv.org/abs/1412.6623 O
. O

Dongqiang O
Yang O
and O
David O
M. O
W. O
Powers O
. O

2006 O
. O

Verb O
similarity O
on O
the O
taxonomy O
of O
wordnet O
. O

In O
In O
the O
3rd O
International O
WordNet O
Conference O
( O
GWC-06 O
) O
, O
Jeju O
Island O
, O
Korea O
. O

Shenjian O
Zhao O
and O
Zhihua O
Zhang O
. O

2016 O
. O

An O
efﬁcient O
character O
- O
level O
neural O
machine O
translation O
. O

CoRR O
abs/1608.04738 O
. O
http://arxiv.org/abs/1608.04738 O
. O

G.K. O
Zipf O
. O

1949 O
. O

Human O
behavior O
and O
the O
principle O
of O
least O
effort O
: O
an O
introduction O
to O
human O
ecology O
. O

Addison O
- O
Wesley O
Press O
. O

https://books.google.com/books?id=1tx9AAAAIAAJ O
. O

