Language B-MethodName
Model I-MethodName
Augmented I-MethodName
Monotonic I-MethodName
Attention I-MethodName
for O
Simultaneous B-TaskName
Translation I-TaskName
Sathish O
Indurthi§∗Mohd O
Abbas O
Zaidi‡ O
Beomseok O
Lee‡†Nikhil O
Kumar O
Lakumarapu‡†Sangha O
Kim‡ O
‡Samsung O
Research O
, O
South O
Korea§Zoom O
AI O
Lab O
, O
Singapore O
sathishreddy.indurthi@zoom.us O
, O
{ O
abbas.zaidi O
, O
bsgunn.lee O
, O
n07.kumar O
, O
sangha01.kim}@samsung.com O

Abstract O
The O
state O
- O
of O
- O
the O
- O
art O
adaptive O
policies O
for O
simul- B-MethodName
taneous I-MethodName
neural I-MethodName
machine I-MethodName
translation I-MethodName
( O
SNMT B-MethodName
) O
use O
monotonic B-MethodName
attention I-MethodName
to O
perform O
read B-TaskName
/ I-TaskName
write I-TaskName
decisions I-TaskName
based O
on O
the O
partial O
source O
and O
target O
sequences O
. O

The O
lack O
of O
sufficient O
information O
might O
cause O
the O
monotonic B-MethodName
attention I-MethodName
to O
take O
poor O
read B-TaskName
/ I-TaskName
write I-TaskName
decisions I-TaskName
, O
which O
in O
turn O
neg- O
atively O
affects O
the O
performance O
of O
the O
SNMT B-MethodName
model O
. O

On O
the O
other O
hand O
, O
human O
translators O
make O
better O
read B-TaskName
/ I-TaskName
write I-TaskName
decisions I-TaskName
since O
they O
can O
anticipate O
the O
immediate O
future O
words O
using O
linguistic O
information O
and O
domain O
knowledge O
. O

In O
this O
work O
, O
we O
propose O
a O
framework O
to O
aid O
monotonic B-MethodName
attention I-MethodName
with O
an O
external O
language O
model O
to O
improve O
its O
decisions O
. O

Experiments O
on O
MuST-C B-DatasetName
English O
- O
German O
and O
English O
- O
French O
speech B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
text I-TaskName
translation I-TaskName
tasks O
show O
the O
future O
information O
from O
language O
model O
improves O
the O
state O
- O
of O
- O
the O
- O
art O
monotonic O
multi O
- O
head O
attention O
model O
further O
. O

1 O
Introduction O
A O
typical O
application O
of O
simultaneous B-MethodName
neural I-MethodName
ma- I-MethodName
chine I-MethodName
translation I-MethodName
( O
SNMT B-MethodName
) O
is O
conversational B-TaskName
speech I-TaskName
or O
live B-TaskName
video I-TaskName
caption I-TaskName
translation I-TaskName
. O

In O
order O
to O
achieve O
live O
translation O
, O
an O
SNMT B-MethodName
model O
alternates O
be- O
tween O
performing O
read B-TaskName
from O
source O
sequence O
and O
write B-TaskName
to O
target O
sequence O
. O

For O
a O
model O
to O
decide O
whether O
to O
read B-TaskName
orwrite B-TaskName
at O
certain O
moment O
, O
either O
a O
fixed O
or O
an O
adaptive O
read O
/ O
write O
policy O
can O
be O
used O
. O

Earlier O
approaches O
in O
simultaneous O
translation O
such O
as O
Ma O
et O
al O
. O
( O
2019a O
) O
and O
Dalvi O
et O
al O
. O

( O
2018 O
) O
employ O
a O
fixed O
policy O
that O
alternate O
between O
read O
andwrite O
after O
the O
waiting O
period O
of O
ktokens O
. O

To O
alleviate O
possible O
long O
delay O
of O
fixed O
polices O
, O
re- O
cent O
works O
such O
as O
monotonic O
infinite O
lookback O
attention O
( O
MILk O
) O
( O
Arivazhagan O
et O
al O
. O
, O
2019 O
) O
, O
and O
monotonic O
multihead O
attention O
( O
MMA O
) O
( O
Ma O
et O
al O
. O
, O
2019c O
) O
developed O
flexible O
policies O
using O
monotonic O
attention O
( O
Raffel O
et O
al O
. O
, O
2017 O
) O
. O

While O
these O
monotonic O
attention O
anticipates O
tar- O
get O
words O
using O
only O
available O
prefix O
source O
and O
target O
sequence O
, O
human O
translators O
anticipate O
the O
target O
words O
using O
their O
language O
expertise O
( O
linguis- O
tic O
anticipation O
) O
as O
well O
as O
contextual O
information O
( O
extra O
- O
linguistic O
anticipation O
) O
( O
Vandepitte O
, O
2001 O
) O
. O

Inspired O
by O
human O
translation O
experts O
, O
we O
aim O
to O
augment B-TaskName
monotonic I-TaskName
attention I-TaskName
with O
future O
informa- O
tion O
using O
language B-MethodName
models I-MethodName
( O
LM B-MethodName
) O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Conneau O
et O
al O
. O
, O
2019 O
) O
. O

Integrating O
the O
external O
information O
effectively O
into O
text O
- O
to O
- O
text O
machine O
translation O
( O
MT O
) O
systems O
has O
been O
explored O
by O
several O
works O
( O
Khandelwal O
et O
al O
. O
, O
2020 O
; O
Gulcehre O
et O
al O
. O
, O
2015 O
, O
2017 O
; O
Stahlberg O
et O
al O
. O
, O
2018 O
) O
. O

Also O
, O
integrating O
future O
information O
implicitly O
into O
SNMT O
system O
during O
training O
is O
ex- O
plored O
in O
Wu O
et O
al O
. O

( O
2020 O
) O
by O
simultaneously O
train- O
ing O
different O
wait-K O
SNMT O
systems O
. O

However O
, O
no O
previous O
works O
make O
use O
of O
explicit O
future O
informa- O
tion O
both O
during O
training O
and O
inference O
. O

To O
utilize O
explicit O
future O
information O
, O
we O
explored O
to O
inte- O
grate O
future O
information O
from O
LM B-MethodName
directly O
into O
the O
output O
layer O
of O
the O
MMA B-MethodName
model O
. O

However O
, O
it O
did O
not O
provide O
any O
improvements O
( O
refer O
to O
Appendix O
A O
) O
, O
thus O
motivating O
us O
to O
explore O
a O
tighter O
integra- O
tion O
of O
the O
LM B-MethodName
information O
into O
SNMT B-MethodName
model O
. O

In O
this O
work O
, O
we O
explicitly O
use O
plausible O
future O
information O
from O
LM B-MethodName
during O
training O
by O
transform- O
ing O
the O
monotonic O
attention O
mechanism O
. O

As O
shown O
in O
Figure O
1 O
, O
at O
each O
step O
, O
the O
LM B-MethodName
takes O
the O
prefix O
target O
( O
and O
source O
, O
for O
cross O
- O
lingual O
LM B-MethodName
) O
sequence O
and O
predicts O
the O
probable O
future O
information O
. O

We O
hypothesize O
that O
aiding O
the O
monotonic O
attention O
with O
this O
future O
information O
can O
improve O
MMA B-MethodName
model O
’s O
read O
/ O
write O
policy O
, O
eventually O
leading O
to O
better O
translation O
with O
less O
delay O
. O

Several O
experi- O
ments O
on O
MuST-C B-DatasetName
( O
Di O
Gangi O
et O
al O
. O
, O
2019 O
) O

English- O

German O
and O
English O
- O
French O
speech B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
text I-TaskName
translation I-TaskName
tasks O
with O
our O
proposed O
approach O
show O
clear O
improvements O
of O
latency O
- O
quality O
trade O
- O
offs O
over O
the O
state O
- O
of O
- O
the O
- O
art O
MMA B-MethodName
models O
. O

2 O
Monotonic O
Attention O
with O
Future O
Information O
Model O
2.1 O
Monotonic O
Attention O
In O
simultaneous B-MethodName
machine I-MethodName
translation I-MethodName
( O
SNMT B-MethodName
) O
mod- O
els O
, O
the O
probability O
of O
predicting O
the O
target O
token O
yi∈ydepends O
on O
the O
partial O
source O
and O
target O
sequences O
( O
x≤j∈x O
, O
y O
< O
i∈y O
) O
. O

In O
sequence O
- O
to- O
sequence O
based O
SNMT B-MethodName
model O
, O
each O
target O
token O
yi O
is O
generated O
as O
follows O
: O

hj O
= O
E(x≤j O
) O
( O
1 O
) O
si O
= O
D(y O
< O
i O
, O
ci O
= O
A(si−1 O
, O
h≤j O
) O
) O
( O
2 O
) O
yi O
= O
Output O
( O
si O
) O
( O
3 O
) O
where O
E(.)andD(.)are O
the O
encoder O
and O
decoder O
layers O
, O
and O
ciis O
a O
context O
vector O
. O

In O
monotonic O
attention O
based O
SNMT B-MethodName
, O
the O
context O
vector O
is O
com- O
puted O
as O
follows O
: O
ei O
, O
j O
= O
MonotonicEnergy O
( O
si−1 O
, O
hj)(4 O
) O
pi O
, O
j O
= O
Sigmoid O
( O
ei O
, O
j O
) O
( O
5 O
) O
zi O
, O
j∼Bernoulli O
( O
pi O
, O
j O
) O
( O

When O
generating O
a O
target O
token O
yi O
, O
the O
decoder O
chooses O
whether O
to O
read O
/ O
write O
based O
on O
Bernoulli O
selection O
probability O
pi O
, O
j. O
When O
zi O
, O
j= O
1 O
( O
write O
) O
, O
model O
sets O
ti O
= O
j O
, O
ci O
= O
hjand O
generates O
the O
target O
token O
yi O
. O

Forzi O
, O
j= O
0(read O
) O
, O
it O
sets O
ti O
= O
j+ O
1and O
repeats O
Eq O
. O
4 O
to O
6 O
. O

Here O
tirefers O
to O
the O
index O
of O
the O
encoder O
when O
decoder O
needs O
to O
produce O
the O
ith O
target O
token O
. O

Instead O
of O
hard O
alignment O
of O
ci O
= O
hj O
, O
Raffel O
et O
al O
. O

( O
2017 O
) O
compute O
an O
expected O
alignment O
in O
a O
recurrent O
manner O
and O
propose O
a O
closed O
- O
form O
parallel O
solution O
. O

Arivazhagan O
et O

al O
. O

( O
2019 O
) O
adopt O
monotonic O
attention O
into O
SNMT B-MethodName
and O
later O
, O
Ma O
et O
al O
. O

( O
2019c O
) O
extend O
it O
to O
MMA B-MethodName
to O
integrate O
it O
into O
the O
Transformer O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O

2.2 O
Monotonic B-MethodName
Attention I-MethodName
with I-MethodName
Future I-MethodName
Information I-MethodName

The O
monotonic O
attention O
described O
in O
Section O
2.1 O
performs O
anticipation O
based O
only O
on O
the O
currently O
available O
source O
and O
target O
information O
. O

To O
aug- O
ment O
this O
anticipation O
process O
using O
future O
informa- O
tion O
extracted O
using O
LMs O
, O
we O
propose O
the O
following O
modifications O
to O
the O
monotonic O
attention O
. O

Future B-HyperparameterName
Representation I-HyperparameterName
Layer I-HyperparameterName
: O

At O
every O
de- O
coding O
step O
i O
, O
the O
previous O
target O
token O
yi−1is O
equipped O
with O
a O
plausible O
future O
token O
ˆyias O
shown O
in O
the O
Figure O
2 O
. O

Since O
the O
token O
ˆyicomes O
from O
an O
LM B-MethodName
possibly O
with O
a O
different O
tokenizer O
and O
vo- O
cabulary O
set O
, O
applying O
the O
model O
’s O
tokenizer O
and O
vocabulary O
might O
split O
the O
token O
ˆyifurther O
into O
mul- O
tiple O
sub O
- O
tokens O
{ O
ˆy1 O
i,ˆy2 O
i,···,ˆym O
i O
} O
. O

To O
get O
a O
single O
future O
token O
representation O
˜si∈ O
Rdfrom O
all O
the O
sub O
- O
tokens O
, O
we O
apply O
a O
sub B-HyperparameterName
- I-HyperparameterName
token I-HyperparameterName
summary I-HyperparameterName
layer I-HyperparameterName
: O

˜si= O
Γ({ˆy1 O
i,ˆy2 O
i,···,ˆym O
i O
} O
) O
( O
7 O
) O
TheΓrepresents O
a O
general B-HyperparameterName
sequence I-HyperparameterName
representation I-HyperparameterName
layer I-HyperparameterName
such O
as O
a O
Transformer O
encoder O
layer O
or O
a O
sim- O
ple O
normalized O
sum O
of O
sub O
- O
token O
representations O
. O

We O
enrich O
˜siat O
every O
layer O
lof O
the O
decoder O
block O
by O
applying O
a O
residual O
feed O
- O
forward O
network O
. O

˜sl O
i O
= O
FFN O
( O
˜yl−1 O
i O
) O
( O
8) O
Monotonic B-HyperparameterName
Energy I-HyperparameterName
Layer I-HyperparameterName
with O
Future O
Informa- O
tion O
: O
Despite O
the O
fact O
that O
we O
can O
add O
the O
plau- O
sible O
future O
information O
to O
the O
output O
layer O
( O
Ap- O
pendix O
A O
) O
or O
append O
it O
to O
the O
target O
token O
represen- O

tation O
yi−1 O
, O
the O
MMA B-MethodName
read O
/ O
write O
decisions O
happen O
in O
Eq O
. O

4 O
. O

Therefore O
, O
we O
integrate O
˜siinto O
the O
Eq O
. O
4 O
instead O
. O

The O
integration O
is O
carried O
out O
by O
modifying O
Eq O
. O
4 O
- O
Eq O
. O

5 O
. O

We O
compute O
the O
monotonic O
energy O
for O
future O
information O
using O
the O
enriched O
future O
token O
representation O
˜siavailable O
at O
each O
layer O
: O
˜ei O
, O
j O
= O
MonotonicEnergy O
( O
˜si O
, O
hj O
) O
( O
9 O
) O
We O
integrate O
the O
future O
monotonic O
energy O
function O
into O
Eq O
. O
5 O
as O
follows O
: O
˜pi O
, O
j O
= O
Sigmoid O
( O
ei O
, O
j+ O
˜ei O
, O
j O
) O
( O
10 O
) O
After O
computing O
˜pi O
, O
j O
, O
we O
compute O
cisimilar O
to O
MMA B-MethodName
model O
. O

This O
way O
of O
integration O
of O
future O
information O
allows O
the O
model O
to O
condition O
the O
LM B-MethodName
output O
us- O
age O
on O
the O
input O
sequence O
. O

The O
model O
can O
control O
the O
relative O
weightage O
given O
to O
the O
LM B-MethodName
output O
by O
varying O
the O
˜ei O
, O
j. O
In O
case O
of O
insufficient O
source O
in- O
formation O
in O
the O
low O
latency O
regime O
, O
we O
expect O
the O
model O
’s O
decision O
policy O
to O
rely O
more O
on O
˜ei O
, O
j. O
Inference O
: O

During O
inference O
, O
the O
start O
token O
does O
not O
contain O
any O
plausible O
information O
. O

After O
pre- O
dicting O
the O
first O
target O
token O
, O
for O
every O
subsequent O
prediction O
of O
target O
token O
yi O
, O
we O
invoke O
the O
LM B-MethodName
to O
predict O
the O
next O
plausible O
future O
token O
and O
integrate O
this O
new O
information O
into O
Eq O
. O

10 O
. O
3 O
Experiments O
and O
Results O
3.1 O
Experimental O
Settings O
Datasets O
and O
Metrics O
: O
We O
conduct O
our O
experi- O
ments O
on O
the O
MuST-C B-DatasetName
English(En)-German(De O
) O
and O
English(En)-French(Fr O
) O
speech B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
text B-TaskName
( I-TaskName
ST I-TaskName
) I-TaskName
translation I-TaskName
task O
. O

The O
speech O
sequence O
is O
repre- O
sented O
using O
80 B-HyperparameterValue
- I-HyperparameterValue
dimensional I-HyperparameterValue
log O
- O
mel O
filter O
bank O
features O
. O

The O
target O
sequence O
is O
represented O
as O
sub- O
words O
using O
a O
SentencePiece O
( O
Kudo O
and O
Richard- O
son O
, O
2018 O
) O
model O
with O
a O
unigram O
vocabulary B-HyperparameterName
of I-HyperparameterName
size I-HyperparameterName
10,000 B-HyperparameterValue
. O

We O
evaluate O
the O
performance O
of O
the O
models O
on O
both O
the O
latency O
and O
quality O
aspects O
. O

We O
use O
Average B-MetricName
Lagging I-MetricName
( O
AL B-MetricName
) O
as O
our O
latency O
metric O
and O
case O
- O
sensitive O
detokenized O
SacreBLEU B-MetricName
( O
Post O
, O
2018 O
) O
to O
measure O
the O
translation O
quality O
, O
similar O
to O
( O
Ma O
et O
al O
. O
, O
2020 O
) O
. O

The O
best O
models O
are O
chosen O
based O
on O
the O
dev O
set O
results O
and O
reported O
results O
are O
from O
the O
MuST-C B-DatasetName
test O
( O
tst-COMMON B-DatasetName
) O
sets O
. O

Language O
Models O
We O
use O
two O
language O
mod- O
els O
to O
train O
our O
proposed O
modified O
MMA B-MethodName
model O
. O

Firstly O
, O
we O
use O
the O
pretrained B-MethodName
XLM I-MethodName
- I-MethodName
RoBERTa I-MethodName
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
model O
from O
Huggingface O
Transformers1model O
repository O
. O

Since O
the O
LM B-MethodName
out- O
put O
can O
be O
very O
open O
- O
ended O
and O
might O
not O
directly O
suit O
/ O
cater O
to O
our O
task O
and O
dataset O
, O
we O
finetune O
the O
head O
of O
the O
model O
using O
the O
MuST-C B-DatasetName
target O
text O
data O
for O
each O
task O
. O

We O
also O
train O
a O
smaller B-MethodName
language I-MethodName
model I-MethodName
( O
SLM B-MethodName
) O
, O
which O
contains O
6 B-HyperparameterValue
Transformer B-HyperparameterName
decoder I-HyperparameterName
layers I-HyperparameterName
, O
512 B-HyperparameterValue
hidden B-HyperparameterName
- I-HyperparameterName
states I-HyperparameterName
and O
24 B-HyperparameterValue
M I-HyperparameterValue
parameters I-HyperparameterValue
. O

We O
use O
the O
MuST-C B-DatasetName
data O
along O
with O
additional O
data O
augmen- O
tation O
to O
reduce O
overfitting O
. O

The O
SLM B-MethodName
helps O
to O
remove O
the O
issues O
related O
to O
vocabulary O
mismatch O
as O
discussed O
in O
the O
Section O
2.2 O
. O

Implementation O
Details O
: O
Our O
base O
model O
is O
adopted O
from O
Ma O
et O
al O
. O

( O
2020 O
) O
. O

We O
use O
a O
pre- B-HyperparameterName
decision I-HyperparameterName
ratio I-HyperparameterName
of O
7 B-HyperparameterValue
, O
which O
means O
that O
the O
simultane- O
ousread O
/ O
write O
decisions O
are O
made O
after O
every O
seven O
encoder O
states O
. O

We O
use O
λ B-HyperparameterName
or O
λlatency B-HyperparameterName
to O
refer O
to O
the O
hyperparameter O
corresponding O
to O
the O
weighted B-HyperparameterName
average I-HyperparameterName
( O
λavg B-HyperparameterName
) O
in O
MMA B-MethodName
. O

The O
values O
of O
this O
hyperpa- O
rameter O
λ B-HyperparameterName
are O
chosen O
from O
the O
set O
{ O
0.01,0.05,0.1 B-HyperparameterValue
} O
. O

TheΓlayer O
in O
Eq O
. O

7 O
computes O
the O
normalized O
sum O
of O
the O
sub O
- O
token O
representations O
. O

For O
SLM B-MethodName
, O
it O
sim- O
ply O
finds O
the O
embedding O
since O
it O
shares O
the O
same O
vocabulary O
set O
. O

All O
the O
models O
are O
trained O
on O
a O
NVIDIA O
v100 O
GPU O
with O
update_freq B-HyperparameterName
set O
to O
8 B-HyperparameterValue
. O

Simultaneous O
Translation O
Models O
: O
Even O
though O
future O
information O
can O
be O
integrated O
explicitly O
into O
the O
fixed O
policy O
approaches O
such O
as O
Wait O
- O
K O
( O
Ma O
et O
al O
. O
, O
2019b O
) O
, O
we O
choose O
monotonic B-MethodName
attention I-MethodName
as O
our O
baseline O
due O
to O
its O
superior O
performance O
( O
Arivazhagan O
et O
al O
. O
, O
2019 O
; O
Ma O
et O
al O
. O
, O
2019c O
) O
. O

We O
train O
a O
baseline O
based O
on O
Ma O
et O
al O
. O

( O
2020 O
) O
work O
, O
called O
as O
MMA B-MethodName
model O
. O

The O
MMA B-MethodName
model O
encoder O
and O
decoder B-HyperparameterName
embedding I-HyperparameterName
dimensions I-HyperparameterName
are O
set O
to O
392 B-HyperparameterValue
, O
whereas O
our O
proposed O
model O
’s O
encoder O
and O
decoder B-HyperparameterName
embeddings I-HyperparameterName
are O
set O
to O
256 B-HyperparameterValue
to O
have O
similar O
parameters O
( O
≈39 O
M O
) O
for O
a O
fair O
comparison O
. O

We O
train O
two O
models O
using O
the O
modified O
MMA B-MethodName
based O
on O
two O
LMs B-MethodName
( O
XLM B-MethodName
, O
SLM B-MethodName
) O
, O
referred O
as O
MMA B-MethodName
- I-MethodName
XLM I-MethodName
and O
MMA B-MethodName
- I-MethodName
SLM I-MethodName
. O

3.2 O
Results O
We O
first O
analyze O
how O
the O
LM B-MethodName
predictions O
are O
being O
utilized O
by O
the O
our O
model O
. O

In O
order O
to O
measure O
the O
relative O
weight O
given O
to O
model O
’s O
internal O
states O
ver- O

sus O
the O
predictions O
from O
the O
LM B-MethodName
, O
we O
compare O
the O
norm O
of O
the O
monotonic O
energies O
corresponding O
to O
the O
LM B-MethodName
predictions O
epred(Eq O
. O

9 O
) O
and O
the O
previous O
output O
tokens O
eoutput O
( O
Eq O
. O
4 O
) O
. O

Let O
us O
define O
LM B-MethodName
prediction O
weight O
as O
follows O
: O
LMpw=/parenleftbigg∥epred∥ O
∥eoutput∥/parenrightbigg O
( O
11 O
) O
In O
Figure O
3 O
, O
we O
plot O
the O
variation O
of O
LMpw B-MetricName
( O
averaged O
) O
vs. O
λ B-HyperparameterName
. O

We O
use O
two O
additional O
values O
of O
λ B-HyperparameterName
∈ O
{ O
0.005 B-HyperparameterValue
0.001 B-HyperparameterValue
}to O
obtain O
this O
plot O
. O

We O
can O
observe O
that O
as O
the O
latency O
requirements O
become O
more O
and O
more O
strict O
, O
the O
model O
starts O
to O
give O
more O
weightage O
to O
the O
predictions O
coming O
from O
the O
LM B-MethodName
. O

This O
shows O
that O
the O
model O
learns O
to O
utilize O
the O
in- O
formation O
coming O
from O
LM B-MethodName
predictions O
based O
on O
latency O
requirements O
. O

Next O
, O
we O
discuss O
the O
performance O
improvements O
obtained O
from O
our O
proposed O
approach O
. O

By O
vary- O
ing O
the O
λ B-HyperparameterName
, O
we O
train O
separate O
models O
for O
different O
latency O
regimes O
. O

Moreover O
, O
the O
quality O
and O
latency O
for O
a O
particular O
model O
can O
also O
be O
varied O
by O
control- O
ling O
the O
speech O
segment O
size O
during O
the O
inference O
. O

Speech O
segment O
size O
or O
step O
size O
refers O
to O
the O
du- O
ration O
of O
speech O
( O
in O
ms O
) O
processed O
corresponding O
to O
each O
read O
decision O
. O

We O
vary O
these O
hyperparame- O
ters O
for O
all O
the O
three O
models O
, O
namely O
MMA B-MethodName
, O
MMA- B-MethodName
XLM I-MethodName
and O
MMA B-MethodName
- I-MethodName
SLM I-MethodName
. O

The O
BLEU B-MetricName
- I-MetricName
AL I-MetricName
curves O
for O
all O
the O
models O
have O
been O
provided O
in O
Figure O
4 O
and O
BLEU B-MetricName
- I-MetricName
AL I-MetricName
num- O
bers O
for O
all O
models O
are O
included O
in O
Appendix O
F O
for O
reference O
. O

We O
vary O
the O
step B-HyperparameterName
sizes I-HyperparameterName
in O
intervals O
of O
80ms B-HyperparameterValue
from O
120ms B-HyperparameterValue
to O
520ms B-HyperparameterValue
in O
order O
to O
get O
performances O
corresponding O
to O
different O
latency O
regimes O
. O

We O
can O
observe O
that O
the O
LM B-MethodName
- O
based O
mod- O
els O
using O
both O
XLM B-MethodName
and O
SLM B-MethodName
provide O
a O
significant O
performance O
improvement O
over O
the O
baseline O
MMA B-MethodName
model O
. O

We O
observe O
improvements O
in O
the O
range O
of O
1 O
- O
2 O
BLEU O
scores O
consistently O
across O
all O
the O
latency O
regimes O
( O
λ B-HyperparameterName
= O
0.1 B-HyperparameterValue
0.05 B-HyperparameterValue
0.01 B-HyperparameterValue
) O
. O

The O
MMA B-MethodName
using O
SLM B-MethodName
language O
model O
performs O
slightly O
better O
than O
MMA B-MethodName
using O
XLM B-MethodName
language O
model O
. O

This O
is O
due O
to O
SLM B-MethodName
’s O
higher O
accuracy B-MetricName
on O
the O
next O
token O
predic- O
tion O
task O
as O
compared O
to O
XLM B-MethodName
, O
30.15% B-MetricValue
vs. O
21.5% B-MetricValue
for O
German O
& O
31.65% B-MetricValue
vs. O
18.45% B-MetricValue
for O
French O
. O

The O
high O
accuracy B-MetricName
of O
SLM B-MethodName
is O
attributed O
to O
its O
training O
on O
in O
- O
domain O
data O
. O

4 O
Conclusion O
In O
this O
work O
, O
we O
provide O
a O
generic O
framework O
to O
integrate O
the O
linguistic O
and O
extra O
- O
linguistic O
infor- O
mation O
into O
simultaneous O
models O
. O

We O
rely O
on O
lan- O
guage O
models O
to O
extract O
this O
plausible O
future O
in- O
formation O
and O
propose O
a O
new O
monotonic O
attention O
mechanism O
to O
infuse O
this O
information O
. O

Several O
ex- O
periments O
on O
speech B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
text I-TaskName
translation I-TaskName
tasks O
show O
the O
effectiveness O
of O
proposed O
approach O
on O
obtain- O
ing O
superior O
quality O
- O
latency O
trade O
- O
offs O
, O
compared O
to O
the O
state O
- O
of O
- O
the O
- O
art O
monotonic O
multihead O
attention O
. O

References O
Naveen O
Arivazhagan O
, O
Colin O
Cherry O
, O
Wolfgang O
Macherey O
, O
Chung O
- O
Cheng O
Chiu O
, O
Semih O
Yavuz O
, O
Ruom- O
ing O
Pang O
, O
Wei O
Li O
, O
and O
Colin O
Raffel O
. O

2019 O
. O

Monotonic O
infinite O
lookback O
attention O
for O
simultaneous O
machine O
translation O
. O

In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
1313–1323 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O

Alexis O
Conneau O
, O
Kartikay O
Khandelwal O
, O
Naman O
Goyal O
, O
Vishrav O
Chaudhary O
, O
Guillaume O
Wenzek O
, O
Francisco O
Guzmán O
, O
Edouard O
Grave O
, O
Myle O
Ott O
, O
Luke O
Zettle- O
moyer O
, O
and O
Veselin O
Stoyanov O
. O

2019 O
. O

Unsupervised O
cross O
- O
lingual O
representation O
learning O
at O
scale O
. O

arXiv O
preprint O
arXiv:1911.02116 O
. O

Fahim O
Dalvi O
, O
Nadir O
Durrani O
, O
Hassan O
Sajjad O
, O
and O
Stephan O
V O
ogel O
. O

2018 O
. O

Incremental O
decoding O
and O
training O
methods O
for O
simultaneous O
translation O
in O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
the O
2018 O
Con- O
ference O
of O
the O
North O
American O
Chapter O
of O
the O
Asso- O
ciation O
for O
Computational O
Linguistics O
: O
Human O
Lan- O
guage O
Technologies O
, O
Volume O
2 O
( O
Short O
Papers O
) O
, O
pages O
493–499 O
, O
New O
Orleans O
, O
Louisiana O
. O
Association O
for O
Computational O
Linguistics O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
under- O
standing O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Tech- O
nologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
, O
Minneapolis O
, O
Minnesota O
. O

Association O
for O
Computational O
Linguistics O
. O

Mattia O
A. O
Di O
Gangi O
, O
Roldano O
Cattoni O
, O
Luisa O
Bentivogli O
, O
Matteo O
Negri O
, O
and O
Marco O
Turchi O
. O

2019 O
. O

MuST-C O
: O
a O
Multilingual O
Speech O
Translation O
Corpus O
. O

In O
Proceed- O
ings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
2012–2017 O
, O
Min- O
neapolis O
, O
Minnesota O
. O

Association O
for O
Computational O
Linguistics O
. O

Caglar O
Gulcehre O
, O
Orhan O
Firat O
, O
Kelvin O
Xu O
, O
Kyunghyun O
Cho O
, O
Loic O
Barrault O
, O
Huei O
- O
Chi O
Lin O
, O
Fethi O
Bougares O
, O
Holger O
Schwenk O
, O
and O
Yoshua O
Bengio O
. O
2015 O
. O

On O
using O
monolingual O
corpora O
in O
neural O
machine O
trans- O
lation O
. O

Caglar O
Gulcehre O
, O
Orhan O
Firat O
, O
Kelvin O
Xu O
, O
Kyunghyun O
Cho O
, O
and O
Yoshua O
Bengio O
. O
2017 O
. O

On O
integrating O
a O
lan- O
guage O
model O
into O
neural O
machine O
translation O
. O

Com- O
puter O
Speech O
and O
Language O
, O
45:137–148 O
. O

Urvashi O
Khandelwal O
, O
Angela O
Fan O
, O
Dan O
Jurafsky O
, O
Luke O
Zettlemoyer O
, O
and O
Mike O
Lewis O
. O

2020 O
. O

Nearest O
neighbor O
machine O
translation O
. O

arXiv O
preprint O
arXiv:2010.00710 O
. O

Sosuke O
Kobayashi O
. O

2018 O
. O

Contextual O
augmentation O
: O
Data O
augmentation O
by O
words O
with O
paradigmatic O
re- O
lations O
. O

In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Tech- O
nologies O
, O
Volume O
2 O
( O
Short O
Papers O
) O
, O
pages O
452–457 O
, O
New O
Orleans O
, O
Louisiana O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Taku O
Kudo O
and O
John O
Richardson O
. O

2018 O
. O

Sentencepiece O
: O

A O
simple O
and O
language O
independent O
subword O
tok- O
enizer O
and O
detokenizer O
for O
neural O
text O
processing O
. O

arXiv O
preprint O
arXiv:1808.06226 O
. O

Mingbo O
Ma O
, O
Liang O
Huang O
, O
Hao O
Xiong O
, O
Renjie O
Zheng O
, O
Kaibo O
Liu O
, O
Baigong O
Zheng O
, O
Chuanqiang O
Zhang O
, O
Zhongjun O
He O
, O
Hairong O
Liu O
, O
Xing O
Li O
, O
Hua O
Wu O
, O
and O
Haifeng O
Wang O
. O
2019a O
. O

Stacl O
: O

Simultaneous O
trans- O
lation O
with O
implicit O
anticipation O
and O
controllable O
la- O
tency O
using O
prefix O
- O
to O
- O
prefix O
framework O
. O

Mingbo O
Ma O
, O
Liang O
Huang O
, O
Hao O
Xiong O
, O
Renjie O
Zheng O
, O
Kaibo O
Liu O
, O
Baigong O
Zheng O
, O
Chuanqiang O
Zhang O
, O
Zhongjun O
He O
, O
Hairong O
Liu O
, O
Xing O
Li O
, O
Hua O
Wu O
, O
and O
Haifeng O
Wang O
. O

2019b O
. O

STACL O
: O

Simultaneous O
trans- O
lation O
with O
implicit O
anticipation O
and O
controllable O
la- O
tency O
using O
prefix O
- O
to O
- O
prefix O
framework O
. O

In O
Proceed- O
ings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
3025–3036 O
, O
Flo- O
rence O
, O
Italy O
. O

Association O
for O
Computational O
Linguis- O
tics O
. O

Xutai O
Ma O
, O
Juan O
Pino O
, O
James O
Cross O
, O
Liezl O
Puzon O
, O
and O
Jiatao O
Gu O
. O
2019c O
. O

Monotonic O
multihead O
attention O
. O

Xutai O
Ma O
, O
Juan O
Pino O
, O
and O
Philipp O
Koehn O
. O
2020 O
. O

Simulmt O
to O
simulst O
: O
Adapting O
simultaneous O
text O
translation O
to O
end O
- O
to O
- O
end O
simultaneous O
speech O
trans- O
lation O
. O

arXiv O
preprint O
arXiv:2011.02048 O
. O

Matt O
Post O
. O
2018 O
. O

A O
call O
for O
clarity O
in O
reporting O
bleu O
scores O
. O

arXiv O
preprint O
arXiv:1804.08771 O
. O

Colin O
Raffel O
, O
Minh O
- O
Thang O
Luong O
, O
Peter O
J. O
Liu O
, O
Ron O
J. O
Weiss O
, O
and O
Douglas O
Eck O
. O
2017 O
. O

Online O
and O
linear- O
time O
attention O
by O
enforcing O
monotonic O
alignments O
. O

InProceedings O
of O
the O
34th O
International O
Conference O
on O
Machine O
Learning O
, O
volume O
70 O
of O
Proceedings O
of O
Machine O
Learning O
Research O
, O
pages O
2837–2846 O
. O
PMLR O
. O

Felix O
Stahlberg O
, O
James O
Cross O
, O
and O
Veselin O
Stoyanov O
. O

2018 O
. O

Simple O
fusion O
: O
Return O
of O
the O
language O
model O
. O

InProceedings O
of O
the O
Third O
Conference O
on O
Machine O
Translation O
: O
Research O
Papers O
, O
pages O
204–211 O
, O
Brus- O
sels O
, O
Belgium O
. O

Association O
for O
Computational O
Lin- O
guistics.42 O

Sonia O
Vandepitte O
. O

2001 O
. O

Anticipation O
in O
conference O
interpreting O
: O
a O
cognitive O
process O
. O

Alicante O
Journal O
of O
English O
Studies O
/ O
Revista O
Alicantina O
de O
Estudios O
Ingleses O
, O
0(14):323–335 O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
Ł O
ukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
Advances O
in O
Neural O
Information O
Pro- O

cessing O
Systems O
30 O
, O
pages O
5998–6008 O
. O

Curran O
Asso- O
ciates O
, O

Inc. O

Xueqing O
Wu O
, O
Yingce O
Xia O
, O
Lijun O
Wu O
, O
Shufang O
Xie O
, O
Weiqing O
Liu O
, O
Jiang O
Bian O
, O
Tao O
Qin O
, O
and O
Tie O
- O
Yan O
Liu O
. O
2020 O
. O

Learn O
to O
use O
future O
information O
in O
simultane- O
ous O
translation O
. O

A O
LM B-MethodName
at O
MMA B-MethodName
Output O
Layer O
We O
explored O
a O
naive O
approach O
of O
integrating O
LM B-MethodName
information O
into O
the O
MMA B-MethodName
. O

In O
this O
approach O
, O
we O
in- O
tegrate O
the O
future O
information O
obtained O
from O
the O
LM O
directly O
into O
the O
output O
layer O
of O
the O
MMA B-MethodName
model O
. O

We O
refer O
to O
this O
experiment O
as O
‘ O
LM B-MethodName
Rescor- O
ing(LMR O
) O
’ O
, O
and O
the O
corresponding O
model O
is O
called O
MMA O
- O
LMR O
. O

As O
observed O
in O
Figure O
5 O
, O
MMA B-MethodName
- I-MethodName
LMR I-MethodName
has O
infe- O
rior O
performance O
compared O
to O
the O
MMA B-MethodName
model O
. O

Since O
the O
LM B-MethodName
information O
integration O
is O
only O
done O
at O
the O
output O
layer O
of O
the O
model O
, O
the O
MMA B-MethodName
model O
can O
not O
easily O
discard O
the O
incorrect O
information O
from O
LM B-MethodName
. O

This O
motivates O
us O
to O
tightly O
integrate O
the O
LM B-MethodName
information O
into O
the O
simultaneous O
model O
. O

B O
Language O
Models O
As O
mentioned O
earlier O
, O
we O
train O
two O
different O
lan- B-MethodName
guage I-MethodName
models I-MethodName
( O
LMs B-MethodName
) O
and O
use O
them O
to O
improve O
the O
anticipation O
in O
monotonic B-MethodName
attention I-MethodName
based O
Simulta- O
neous O
models O
. O

B.1 O
XLM B-MethodName
- I-MethodName
RoBERTa I-MethodName
- O
R O
) O
XLM B-MethodName
- O
R O
Large O
model2was O
trained O
on O
the O
100 O
lan- O
guages O
Common B-DatasetName
Crawl I-DatasetName
corpora O
total O
size O
of O
2.5 O
TB O
with O
550 O
M O
parameters O
from O
24 O
layers O
, O
1024 O
hid- O
den O
states O
, O
4096 O
feed O
- O
forward O
hidden O
- O
states O
, O
and O
16 O
heads O
. O

Total O
number O
of O
parameters O
is O
558M. O
We O
finetune O
the O
head O
of O
the O
XLM B-MethodName
- O
R O
LM B-MethodName
model O
using O
the O
Masked O
Language O
Modeling O
objective O
which O
accounts O
for O
0.23 O
% O
of O
the O
total O
model O
parameters O
, O
i.e. O
, O
1.3 O
M O
parameters O
. O

B.2 O
Smaller O
Language O
Model O
Since O
the O
LM B-MethodName
predictions O
are O
computed O
serially O
during O
inference O
, O
the O
time O
taken O
to O
compute O
the O
token O
serves O
as O
a O
bottleneck O
to O
the O
latency O
re- O
quirements O
. O

To O
reduce O
the O
LM B-MethodName
computation O
time O
, O
we O
train O
a O
smaller B-MethodName
Language I-MethodName
Model I-MethodName
( O
SLM B-MethodName
) O
from O
scratch O
using O
the O
Causal B-MethodName
Language I-MethodName
Modeling I-MethodName
ob- O
jective O
. O

SLM B-MethodName
is O
composed O
of O
6 B-HyperparameterValue
Transformer B-HyperparameterName
decoder I-HyperparameterName
blocks I-HyperparameterName
, O
512 B-HyperparameterValue
hidden B-HyperparameterName
- I-HyperparameterName
states I-HyperparameterName
, O
2048 B-HyperparameterValue
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
hidden I-HyperparameterName
- I-HyperparameterName
states I-HyperparameterName
& O
8 B-HyperparameterValue
attention I-HyperparameterValue
heads I-HyperparameterValue
. O

It O
alleviates O
the O
need O
for O
the O
sub B-HyperparameterName
- I-HyperparameterName
token I-HyperparameterName
summary I-HyperparameterName
layer I-HyperparameterName
since O
it O
shares O
the O
vocabulary O
and O
tokenization O
with O
the O
MMA B-MethodName
models O
. O

The O
train O
examples O
are O
at O
the O
sen- O
tence O
level O
, O
rather O
than O
forming O
a O
block O
out O
of O
multi- O
ple O
sentences(which O
is O
the O
usual O
case O
for O
Language O
Models O
) O
. O

Since O
the O
target O
texts O
contain O
lesser O
than O
250k O
examples O
, O
we O
use O
additional O
data O
augmentation O
techniques O
to O
upsample O
the O
target O
data O
. O

We O
also O
use O
additional O
data O
to O
avoid O
overfitting O
on O
the O
MuST-C B-DatasetName
target O
text O
. O

Details O
have O
been O
provided O
in O
B.2.1 O
. O

B.2.1 O
Data O
Augmentation O
Up O
- O
Sampling O
: O
To O
boost O
the O
LM B-MethodName
performance O
and O
mitigate O
overfitting O
, O
we O
use O
contextual O
data O
augmentation O
( O
Kobayashi O
, O
2018 O
) O
to O
upsample O
the O
MuST-C B-DatasetName
target O
text O
data O
by O
substituting O
and O
insert- O
ing O
words O
based O
on O
LM B-MethodName
predictions O
. O

We O
use O
the O
NLPAUG3package O
to O
get O
similar O
words O
based O
on O
contextual O
embeddings O
. O

From O
the O
Hugging O
Face O
Repository O
, O
we O
use O
two O
different O
pretrained O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
models O
for O
German O
bert O
- O
base- O
german O
- O
dbmdz O
- O
cased O
& O
bert O
- O
base O
- O
german O
- O
dbmdz- O
uncased O
andbert O
- O
base O
- O
fr O
- O
cased O
for O
French O
. O

We O
upsample O
German O
to O
1.13 O
M O
examples O
and O
French O
to O
1.38 O
M O
examples O
. O

Additional O
Data O
: O
We O
also O
use O
additional O
data O
to O
avoid O
overfitting O
. O

For O
German O
we O
use O
the O
Newscrawl B-DatasetName
( O
WMT19 B-DatasetName
) O
data O
which O
includes O
58 O
M O
examples O
. O

For O
French O
, O
we O
use O
Common B-DatasetName
Crawl I-DatasetName
and I-DatasetName
Europarl I-DatasetName
to O
augment O
4 O
M O
extra O
training O
examples O
. O

We O
observe O
that O
both O
upsampling O
and O
data O
aug- O
mentation O
help O
us O
to O
reduce O
the O
overfitting O
on O
the O
MuST-C B-DatasetName
dev O
set O
. O

B.3 O
Token O
Prediction O
For O
each O
output O
token O
, O
the O
LM B-MethodName
prediction O
is O
ob- O
tained O
by O
feeding O
the O
prefix O
upto O
that O
token O
to O
the O
LM B-MethodName
model O
. O

These O
predictions O
are O
pre O
- O
computed O
for O
training O
and O
validation O
sets O
. O

This O
ensures O
par- O
allelization O
and O
avoids O
the O
overhead O
to O
run O
the O
LM B-MethodName
simultaneously O
during O
the O
training O
process O
. O

During O
inference O
, O
the O
LM B-MethodName
model O
is O
called O
every O
time O
a O
new O
output O
token O
is O
written O
. O

C O
Dataset O

The O
MuST-C B-DatasetName
dataset O
comprises O
of O
English O
TED O
talks O
, O
the O
translations O
and O
transcriptions O
have O
been O
aligned O
with O
the O
speech O
at O
sentence O
level O
. O

Dataset O
statistics O
have O
been O
provided O
in O
the O
Table O
1 O
. O

D O
Effect O
of O
LM B-MethodName
Size O
on O
Latency O
- O
Quality O
We O
train O
several O
SLM B-MethodName
models O
with O
varying O
sizes O
in O
our O
experiments O
and O
choose O
the O
best O
model O
based O
on O
the O
top-1 O
accuracy B-MetricName
. O

As O
we O
increase O
the O
number O
of O
layers O
in O
the O
LM B-MethodName
model O
from O
2 B-HyperparameterValue
to O
4 B-HyperparameterValue
to O
6 B-HyperparameterValue
layers B-HyperparameterName
, O
the O
SLM B-MethodName
and O
the O
proposed O
MMA B-MethodName
with O
future O
infor- O
mation O
models O
have O
shown O
performance O
improve- O
ments O
. O

However O
, O
increasing O
the O
number O
of O
layers O
greater O
than O
6 O
does O
not O
yield O
any O
performance O
im- O
provements O
. O

We O
also O
notice O
this O
degradation O
of O
performance O
with O
the O
XLM B-MethodName
model O
while O
varying O
the O
number O
of O
hidden O
layers O
in O
the O
LM B-MethodName
head O
. O

E O
Training O
Details O

We O
follow O
the O
training O
process O
similar O
to O
Ma O
et O
al O
. O

( O
2020 O
) O
training O
process O
. O

We O
train O
an O
English O
ASR O
model O
using O
the O
source O
speech O
data O
. O

Next O
, O
we O
train O
a O
simultaneous O
model O
without O
the O
latency O
loss O
( O
setting O
λlatency B-HyperparameterName
= O
0 B-HyperparameterValue
) O
after O
initializing O
the O
encoder O
from O
the O
English O
ASR O
model O
. O

After O
this O
step O
, O
we O
finetune O
the O
simultaneous O
model O
for O
different O
λs B-HyperparameterName
. O

This O
training O
process O
is O
repeated O
for O
all O
the O
reported O
models O
and O
for O
each O
task O
. O

The O
details O
regarding O
the O
hyperparameters O
for O
the O
model O
have O
been O
provided O
in O
Table O
2 O
. O

F O
BLEU B-HyperparameterName
- I-HyperparameterName
AL I-HyperparameterName
Numbers O
As O
mentioned O
in O
the O
results O
section O
of O
the O
main O
pa- O
per O
, O
we O
vary O
the O
latency B-HyperparameterName
weight I-HyperparameterName
hyperparameter O
( O
λ B-HyperparameterName
) O
to O
train O
different O
models O
to O
obtain O
different O
latency O
regimes O
. O

We O
also O
vary O
the O
step B-HyperparameterName
- I-HyperparameterName
size I-HyperparameterName
/ I-HyperparameterName
speech I-HyperparameterName
seg- I-HyperparameterName
ment I-HyperparameterName
size I-HyperparameterName
during O
inference O
. O

In O
total O
, O
we O
obtain O
18 O
different O
data O
points O
corresponding O
to O
each O
model O
. O

In O
Table O
3 O
, O
we O
compare O
the O
results O
obtained O
using O
MMA B-MethodName
, O
MMA B-MethodName
- I-MethodName
XLM I-MethodName
and O
MMA B-MethodName
- I-MethodName
SLM I-MethodName
under O
similar O
hyperparameter O
settings O
. O

It O
will O
help O
the O
reader O
to O
quantify O
the O
benefits O
obtained O
from O
our O
proposed O
approach O
. O
