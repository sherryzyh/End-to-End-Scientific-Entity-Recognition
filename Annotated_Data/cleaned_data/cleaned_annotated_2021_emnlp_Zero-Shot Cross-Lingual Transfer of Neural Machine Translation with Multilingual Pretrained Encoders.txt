Zero B-TaskName
- I-TaskName
Shot I-TaskName
Cross I-TaskName
- I-TaskName
Lingual I-TaskName
Transfer I-TaskName
of O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName
with O
Multilingual B-MethodName
Pretrained I-MethodName
Encoders I-MethodName

Guanhua O
Chen1 O
, O
Shuming O
Ma2 O
, O
Yun O
Chen3y O
, O
Li O
Dong2 O
Dongdong O
Zhang2 O
, O
Jia O
Pan1 O
, O
Wenping O
Wang4;1 O
, O
Furu O
Wei2 O

1The O
University O
of O
Hong O
Kong;2Microsoft O
Research O
3Shanghai O
University O
of O
Finance O
and O
Economics;4Texas O
A&M O
University O
{ O
ghchen,jpan,wenping}@cs.hku.hk O
, O
yunchen@sufe.edu.cn O
, O
{ O
shumma O
, O
lidong1 O
, O
dozhang O
, O

fuwei}@microsoft.com O
Abstract O
Previous O
work O
mainly O
focuses O
on O
improving O
cross O
- O
lingual O
transfer O
for O
NLU B-TaskName
tasks O
with O
a O
multilingual O
pretrained O
encoder O
( O
MPE O
) O
, O
or O
im- O
proving O
the O
performance O
on O
supervised O
ma- O
chine O
translation O
with O
BERT O
. O

However O
, O
it O
is O
under O
- O
explored O
that O
whether O
the O
MPE O
can O
help O
to O
facilitate O
the O
cross O
- O
lingual O
transferability O
of O
NMT B-TaskName
model O
. O

In O
this O
paper O
, O
we O
focus O
on O
a O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
task O
in O
NMT B-TaskName
. O

In O
this O
task O
, O
the O
NMT O
model O
is O
trained O
with O
parallel O
dataset O
of O
only O
one O
language O
pair O
and O
an O
off O
- O
the O
- O
shelf O
MPE O
, O
then O
it O
is O
directly O
tested O
on O
zero O
- O
shot O
language O
pairs O
. O

We O
pro- O
pose O
SixT B-MethodName
, O
a O
simple O
yet O
effective O
model O
for O
this O
task O
. O

SixT B-MethodName
leverages O
the O
MPE O
with O
a O
two O
- O
stage O
training O
schedule O
and O
gets O
further O
improvement O
with O
a O
position O
disentangled O
en- O
coder O
and O
a O
capacity O
- O
enhanced O
decoder O
. O

Using O
this O
method O
, O
SixT B-MethodName
signiﬁcantly O
outperforms O
mBART B-MethodName
, O
a O
pretrained O
multilingual O
encoder- O
decoder O
model O
explicitly O
designed O
for O
NMT B-TaskName
, O
with O
an O
average O
improvement O
of O
7.1 B-MetricValue
BLEU B-MetricName
on O
zero O
- O
shot O
any O
- O
to O
- O
English O
test O
sets O
across O
14 O
source O
languages O
. O

Furthermore O
, O
with O
much O
less O
training O
computation O
cost O
and O
training O
data O
, O
our O
model O
achieves O
better O
performance O
on O
15 O
any O
- O
to O
- O
English O
test O
sets O
than O
CRISS B-MethodName
and O
m2m-100 B-MethodName
, O
two O
strong O
multilingual O
NMT B-TaskName
base- O
lines O
. O

1 O
Introduction O
Multilingual O
pretrained O
encoders O
( O
MPE O
) O
such O
as O
mBERT B-MethodName
( O
Wu O
and O
Dredze O
, O
2019 O
) O
, O
XLM B-MethodName
( O
Con- O
neau O
and O
Lample O
, O
2019 O
) O
, O
and O
XLM B-MethodName
- I-MethodName
R I-MethodName
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
have O
shown O
remarkably O
strong O
re- O
sults O
on O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
mainly O
for O
natural B-TaskName
language I-TaskName
understanding I-TaskName
( O
NLU B-TaskName
) O
tasks O
, O
including O
named B-TaskName
entity I-TaskName
recognition I-TaskName
( O
NER B-TaskName
) O
, O
ques- B-TaskName
tion I-TaskName
answering I-TaskName
( O
QA B-TaskName
) O
and O
natural B-TaskName
language I-TaskName
infer- I-TaskName
ence I-TaskName
( O
NLI B-TaskName
) O
. O

These O
methods O
jointly O
train O
a O
Trans- O
former O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
encoder O
to O
perform O
masked O
language O
modeling O
task O
in O
multiple O
lan- O
guages O
. O

The O
pretrained O
model O
is O
then O
ﬁne O
- O
tuned O
on O
a O
downstream O
NLU B-TaskName
task O
using O
labeled O
data O
in O
a O
single O
language O
and O
evaluated O
on O
the O
same O
task O
in O
other O
languages O
. O

With O
this O
pretraining O
and O
ﬁne- O
tuning O
approach O
, O
the O
MPE O
is O
able O
to O
generalize O
to O
other O
languages O
that O
even O
do O
not O
have O
labeled O
data O
. O

Given O
that O
MPE O
has O
achieved O
great O
success O
in O
cross- B-TaskName
lingual I-TaskName
NLU I-TaskName
tasks O
, O
a O
question O
worthy O
of O
research O
is O
how O
to O
perform O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
in O
the O
NMT B-TaskName
task O
by O
leveraging O
the O
MPE B-MethodName
. O

Some O
work O
( O
Zhu O
et O
al O
. O
, O
2020 O
; O
Yang O
et O
al O
. O
, O
2020 O
; O
Weng O
et O

al O
. O
, O
2020 O
; O
Imamura O
and O
Sumita O
, O
2019 O
) O
explores O
approaches O
to O
improve O
NMT O
performance O
by O
in- O
corporating O
monolingual O
pretrained O
Transformer O
encoder O
such O
as O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

How- O
ever O
, O
simply O
replacing O
the O
monolingual O
pretrained O
encoder O
in O
previous O
studies O
with O
MPE O
does O
not O
work O
well O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
of O
NMT B-TaskName
( O
see O
baselines O
in O
Table O
2 O
) O
. O

Others O
propose O
to O
ﬁne O
- O
tune O
the O
encoder O
- O
decoder O
- O
based O
multilingual O
pretrained O
model O
for O
cross O
- O
lingual O
transfer O
of O
NMT O
( O
Liu O
et O
al O
. O
, O
2020 O
; O
Lin O
et O
al O
. O
, O
2020 O
) O
. O

It O
is O
still O
unclear O
how O
to O
conduct O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
for O
NMT B-TaskName
model O
with O
existing O
multilingual O
pretrained O
encoders O
such O
as O
XLM B-MethodName
- I-MethodName
R. I-MethodName

In O
this O
paper O
, O
we O
focus O
on O
a O
Zero B-TaskName
- I-TaskName
shot I-TaskName
cross- I-TaskName
lingual I-TaskName
( I-TaskName
X I-TaskName
) I-TaskName
NMT I-TaskName
Transfer I-TaskName
task O
( O
ZeXT B-TaskName
, O
see O
Fig- O
ure O
1 O
) O
, O
which O
aims O
at O
translating O
multiple O
unseen O
languages O
by O
leveraging O
an O
MPE O
. O

Different O
from O
unsupervised O
or O
multilingual O
NMT O
, O
only O
an O
MPE O
and O
parallel O
dataset O
of O
one O
language O
pair O
such O
as O
German O
- O
English O
are O
available O
in O
this O
task O
. O

The O
trained O
model O
is O
directly O
tested O
on O
many O
- O
to O
- O
one O
test O
sets O
in O
a O
zero O
- O
shot O
manner O
. O

We O
propose O
a O
Simple B-MethodName
cross I-MethodName
- I-MethodName
lingual I-MethodName
( I-MethodName
X) I-MethodName
Transfer I-MethodName
NMT I-MethodName
model I-MethodName
( O
SixT B-MethodName
) O
which O
can O
directly O
translates O
languages O
unseen O
during O
supervised O
training O
. O

We O
initialize O
the O
encoder O
and O
decoder O
embeddings O
of O
SixT B-MethodName
with O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
propose O
a O
two O
- O
stage O
training O
schedule O
that O
trades O
off O
between O
super- O
vised O
performance O
and O
transferability O
. O

At O
the O
ﬁrst O
stage O
, O
we O
only O
train O
the O
decoder O
layers O
, O
while O
at O
the O
second O
stage O
, O
all O
model O
parameters O
are O
jointly O
optimized O
except O
the O
encoder O
embedding O
. O

We O
fur- O
ther O
improve O
the O
model O
by O
introducing O
a O
position O
disentangled O
encoder O
and O
a O
capacity O
- O
enhanced O
de- O
coder O
. O

The O
position O
disentangled O
encoder O
enhances O
cross O
- O
lingual O
transferability O
by O
removing O
residual O
connection O
in O
one O
of O
the O
encoder O
layers O
and O
mak- O
ing O
the O
encoder O
outputs O
more O
language O
- O
agnostic O
. O

The O
capacity O
- O
enhanced O
decoder O
leverages O
a O
bigger O
decoder O
than O
vanilla O
Transformer O
base O
model O
to O
fully O
utilize O
the O
labelled O
dataset O
. O

Although O
trained O
with O
only O
one O
language O
pair O
, O
the O
SixT B-MethodName
model O
alle- O
viates O
the O
effect O
of O
‘ O
catastrophic O
forgetting O
’ O
( O
Serra O
et O
al O
. O
, O
2018 O
) O
and O
can O
be O
transferred O
to O
unseen O
lan- O
guages O
. O

SixT B-MethodName
signiﬁcantly O
outperforms O
mBART B-MethodName
with O
an O
average O
improvement O
of O
7.1 B-MetricValue
BLEU B-MetricName
on O
zero- B-TaskName
shot I-TaskName
any I-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
English I-TaskName
translation I-TaskName
across O
14 O
source O
languages O
. O

Furthermore O
, O
with O
much O
less O
training O
computation O
cost O
and O
training O
data O
, O
the O
SixT B-MethodName
model O
gets O
better O
performance O
on O
15 O
any O
- O
to O
- O
English O
test O
sets O
than O
CRISS B-MethodName
and O
m2m-100 B-MethodName
, O
two O
strong O
multi- O
lingual O
NMT O
baselines.1 O
2 O
Problem O
Statement O

The O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
NMT I-TaskName
transfer I-TaskName
task O
( O
ZeXT B-TaskName
) O
explores O
approaches O
to O
enhance O
the O
cross- O
lingual O
transferability O
of O
NMT O
model O
. O

Given O
an O
MPE O
and O
parallel O
dataset O
of O
a O
language O
pair O
ls O
- O
to- O
lt O
, O
where O
ls O
and O
lt O
are O
supported O
by O
the O
MPE O
, O
we O
aim O
to O
train O
an O
NMT O
model O
that O
can O
be O
transferred O
to O
multiple O
unseen O
language O
pairs O
li O
z O
- O
to O
- O
lt O
, O
where O
li O
z6 O
= O
lsandli O
zis O
supported O
by O
the O
MPE O
. O

The O
learned O
NMT O
model O
is O
directly O
tested O
between O
the O
un- O
seen O
language O
pairs O
li O
z O
- O
to O
- O
ltin O
a O
zero O
- O
shot O
manner O
. O

Different O
from O
multilingual B-TaskName
NMT I-TaskName
( O
Johnson O
et O
al O
. O
, O
2017 O
) O
, O
unsupervised B-TaskName
NMT I-TaskName
( O
Lample O
et O
al O
. O
, O
2018 O
) O
or O
zero B-TaskName
- I-TaskName
resource I-TaskName
NMT I-TaskName
through O
pivoting O
( O
Chen O
et O
al O
. O
, O
2017 O
, O
2018 O
) O
, O
neither O
the O
parallel O
nor O
monolingual O
data O
in O
the O
language O
li O
zis O
directly O
accessible O
in O
the O
ZeXT B-TaskName
task O
. O

The O
model O
has O
to O
rely O
on O
the O
off- O
the O
- O
shelf O
MPE O
to O
translate O
from O
language O
li O
z. O

The O
challenge O
to O
this O
task O
is O
how O
to O
leverage O
an O
MPE O
for O
machine O
translation O
while O
preserving O
its O
cross- O
lingual O
transferability O
. O

In O
this O
paper O
, O
we O
utilize O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
which O
is O
jointly O
trained O
on O
100languages O
, O
as O
the O
off O
- O
the O
- O
shelf O
MPE O
. O

The O
ZeXT B-TaskName
task O
calls O
for O
approaches O
to O
efﬁciently O
build O
a O
many O
- O
to O
- O
one O
NMT O
model O
that O
can O
translate O
from O
100languages O
supported O
by O
XLM B-MethodName
- I-MethodName
R I-MethodName
with O
par- O
allel O
dataset O
of O
only O
one O
language O
pair O
. O

The O
trained O
model O
could O
be O
useful O
for O
translating O
resource O
- O
poor O
languages O
. O

It O
can O
further O
extend O
to O
scenarios O
where O
datasets O
of O
more O
language O
pairs O
are O
available O
. O

In O
addition O
, O
while O
currently O
the O
cross O
- O
lingual O
transfer- O
ability O
of O
different O
MPEs O
is O
mainly O
evaluated O
on O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NLU O
tasks O
, O
the O
ZeXT B-TaskName
task O
provides O
a O
new O
perspective O
for O
the O
evaluation O
, O
which O
can O
hopefully O
facilitate O
the O
research O
on O
MPEs O
. O

3 O
Approach O
3.1 O
Initialization O
and O
Fine O
- O
tuning O
Strategy O
For O
downstream O
tasks O
like O
cross O
- O
lingual O
NLI O
/ O
QA O
, O
only O
an O
output O
layer O
is O
added O
to O
the O
pretrained O
en- O
coder O
at O
the O
ﬁne O
- O
tuning O
stage O
. O

In O
contrast O
, O
an O
en- O
tire O
decoder O
is O
added O
on O
top O
of O
the O
MPE O
when O
the O
model O
is O
adapted O
to O
NMT O
task O
. O

The O
conven- O
tional O
strategy O
that O
ﬁne O
- O
tunes O
all O
parameters O
re- O
duces O
the O
cross O
- O
lingual O
transferability O
in O
the O
pre- O
trained O
encoder O
due O
to O
the O
catastrophic O
forgetting O
effect O
. O

Therefore O
, O
we O
make O
an O
empirical O
explo- O
ration O
on O
how O
to O
initialize O
and O
ﬁne O
- O
tune O
the O
NMT O
model O
with O
an O
MPE O
. O

The O
NMT O
model O
can O
be O
di- O
vided O
into O
four O
parts O
in O
our O
method O
: O
encoder O
em- O
bedding O
, O
encoder O
layers O
, O
decoder O
embedding O
, O
and O
decoder O
layers O
. O

With O
an O
MPE O
, O
each O
part O
can O
be O
trained O
with O
one O
of O
the O
following O
methods O
, O
namely O
, O
Rand O
: O
randomly O
initialized O
and O
trained O
; O
Fix O
: O
initialized O
from O
the O
MPE O
and O
ﬁxed O
; O
FT O
: O
initialized O
from O
the O
MPE O
and O
trained O
. O

We O
compare O
different O
ﬁne O
- O
tuning O
strategies O
for O
these O
modules O
in O
a O
greedy O
manner O
. O

Starting O
from O
vanilla O
Transformer O
where O
all O
parts O
are O
randomly O
initialized O
, O
we O
explore O
the O
best O
training O
method O
for O
the O
encoder O
embedding O
, O
the O
encoder O
layers O
, O
the O
decoder O
embedding O
, O
and O
the O
decoder O
layers O
, O
se- O
quentially O
. O

The O
details O
of O
experimental O
settings O
are O
in O
the O
Section O
4.1 O
. O

From O
the O
results O
shown O
in O
Ta- O
ble O
1 O
, O
we O
observe O
that O
it O
is O
the O
best O
to O
initialize O
the O
encoder O
embedding O
, O
the O
encoder O
layers O
and O
the O
decoder O
embedding O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
keep O
their O
parameters O
frozen O
, O
while O
randomly O
initializing O
the O
decoder O
layers O
( O
see O
Figure O
2 O
) O
. O

More O
discussions O
are O
in O
the O
Section O
4.2 O
. O

Two O
- O
stage O
training O
Since O
we O
freeze O
the O
encoder O
and O
only O
train O
the O
decoder O
layers O
, O
the O
model O
is O
able O
to O
perform O
translation O
while O
preserving O
the O
trans- O
ferability O
of O
the O
encoder O
. O

However O
, O
freezing O
most O
of O
the O
parameters O
limits O
the O
capacity O
of O
the O
NMT O
model O
, O
especially O
when O
the O
training O
data O
goes O
large O
. O

Therefore O
, O
we O
propose O
a O
second O
training O
stage O
to O
further O
improve O
the O
translation O
performance O
by O
jointly O
ﬁne O
- O
tuning O
all O
parameters O
except O
encoder O
embedding O
of O
the O
NMT.2Since O
the O
decoder O
has O
been O
well O
adapted O
to O
the O
encoder O
at O
the O
ﬁrst O
stage O
, O
we O
expect O
the O
model O
can O
be O
slightly O
ﬁne O
- O
tuned O
to O
improve O
the O
translation O
capacity O
without O
losing O
the O
2According O
to O
our O
preliminary O
experiment O
, O
the O
average O
BLEU O
is O
0.2 O
lower O
when O
the O
encoder O
embedding O
is O
also O
learned O
at O
the O
second O
stage O
. O

Besides O
, O
freezing O
encoder O
embed- O
ding O
leads O
to O
higher O
computational O
efﬁciency.transferability O
of O
the O
encoder O
. O

3.2 O
Model O
The O
training O
strategy O
and O
generalization O
objective O
of O
our O
model O
are O
different O
from O
vanilla O
Transformer O
. O

This O
motivates O
us O
to O
propose O
a O
new O
model O
that O
can O
further O
improve O
on O
zero B-TaskName
- I-TaskName
shot I-TaskName
translations I-TaskName
. O

The O
proposed O
model O
consists O
of O
a O
position O
disentangled O
encoder O
and O
a O
capacity O
- O
enhanced O
decoder O
, O
which O
aims O
at O
enhancing O
the O
cross O
- O
lingual O
transferability O
of O
the O
encoder O
and O
fully O
utilizing O
the O
labelled O
data O
, O
respectively O
. O

Position O
disentangled O
encoder O

The O
representa- O
tions O
from O
XLM B-MethodName
- I-MethodName
R I-MethodName
initialized O
encoder O
have O
a O
strong O
positional O
correspondence O
to O
the O
source O
sentence O
. O

The O
word O
order O
information O
inside O
is O
language O
- O
speciﬁc O
and O
may O
hinder O
the O
cross O
- O
lingual O
transfer O
from O
supervised O
source O
language O
to O
unseen O
languages O
. O

Inspired O
by O
Liu O
et O
al O
. O
( O
2021 O
) O
, O
we O
pro- O
pose O
to O
relax O
this O
structural O
constraint O
and O
make O
the O
encoder O
outputs O
less O
position- O
and O
language- O
speciﬁc O
. O

More O
speciﬁcally O
, O
at O
the O
second O
stage O
, O
we O
remove O
the O
residual O
connection O
after O
the O
self- O
attention O
sublayer O
in O
one O
of O
the O
encoder O
layers O
i O
during O
training O
and O
inference. O

The O
other O
encoder O
layers O
remain O
the O
same O
. O

The O
hidden O
states O
in O
this O
ithencoder O
layer O
are O
calculated O
as O
the O
following O
pseudo O
code O
: O
1h[i O
] O
= O
SelfAttn(h[i-1 O
] O
) O
2h[i O
] O
= O
LayerNorm(h[i O
] O
) O

# O
No O
residual O
connection O
here O
3h[i O
] O
= O
h[i O
] O
+ O
LayerNorm(FFN(h[i O
] O
) O
) O

where O
SelfAttn O
is O
the O
encoder O
self O
- O
attention O
sublayer O
, O
FFN O
is O
the O
feed O
- O
forward O
sublayer O
and O
LayerNorm O
is O
the O
layer O
normalization O
. O

Liu O
et O
al O
. O

( O
2021 O
) O
aim O
at O
training O
a O
language O
- O
agnostic O
encoder O
for O
NMT O
using O
parallel O
corpus O
from O
scratch O
. O

Com- O
pared O
with O
them O
, O
our O
method O
shows O
that O
it O
’s O
pos- O
sible O
to O
make O
a O
pretrained O
multilingual O
encoder O
more O
language O
- O
agnostic O
by O
relaxing O
the O
position O
constraint O
during O
ﬁne O
- O
tuning O
. O

Capacity O
- O
enhanced O
decoder O
Some O
previous O
work O
( O
Zhu O
et O
al O
. O
, O
2020 O
; O
Yang O
et O
al O
. O
, O
2020 O
) O
incorpo- O
rates O
BERT O
into O
NMT O
and O
conﬁgures O
the O
decoder O
size O
as O
Vaswani O
et O

al O
. O
( O
2017 O
) O
. O

For O
example O
, O
to O
train O
an O
NMT O
on O
Europarl B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
training O
dataset O
, O
the O
default O
decoder O
conﬁguration O
is O
Transformer O
base O
( O
Gu O
et O
al O
. O
, O
2018 O
; O
Currey O
et O
al O
. O
, O
2020 O
) O
. O

However O
, O
our O
model O
relies O
more O
on O
the O
decoder O
to O
learn O
from O
the O
labeled O
data O
, O
as O
the O
encoder O
is O
mainly O
responsi- O
ble O
for O
cross O
- O
lingual O
transfer O
. O

This O
is O
also O
reﬂected O
in O
our O
training O
strategy O
: O
at O
the O
ﬁrst O
stage O
only O
the O
decoder O
parameters O
are O
optimized O
, O
while O
at O
the O
sec- O
ond O
stage O
the O
encoder O
is O
only O
slightly O
ﬁne O
- O
tuned O
to O
preserve O
its O
transferability O
. O

Therefore O
, O
the O
model O
ca- O
pacity O
of O
SixT B-MethodName
is O
smaller O
than O
vanilla O
Transformer O
with O
the O
same O
size O
. O

We O
propose O
to O
apply O
a O
capacity- O
enhanced O
decoder O
that O
has O
larger O
dimension O
of O
feed O
forward O
network O
, O
more O
layers O
and O
more O
attention O
heads O
at O
both O
the O
ﬁrst O
and O
second O
training O
stages O
. O

The O
improvement O
brought O
by O
the O
big O
decoder O
is O
not O
simply O
because O
of O
more O
model O
parameters O
. O

More O
discussions O
are O
in O
the O
Section O
4.2 O
. O

4 O
Experiments O
4.1 O
Setup O
Dataset O
We O
focus O
on O
the O
any O
- O
to O
- O
English O
transla- O
tions O
for O
the O
ZeXT B-TaskName
task O
. O

The O
Europarl B-DatasetName
- I-DatasetName
v7 I-DatasetName
German I-DatasetName
and I-DatasetName
English I-DatasetName
is O
used O
as O
training O
set O
. O

We O
evaluate O
the O
cross O
- O
lingual O
transfer O
abilities O
of O
NMT O
mod- O
els O
on O
a O
variety O
of O
languages O
from O
different O
lan- O
guage O
groups4 O
: O
German O
group O
( O
De O
, O
Nl O
) O
, O
Romance O
group O
( O
Es O
, O
It O
, O
Ro O
) O
, O
Uralic O
and O
Baltic O
group O
( O
Et O
, O
Fi O
, O
Lv O
) O
, O
Indo O
- O
Aryan O
group O
( O
Hi O
, O
Ne O
) O
and O
Chinese O
( O
Zh O
) O
. O

A O
concatenation O
of O
Fr O
- O
En O
and O
Cs O
- O
En O
validation O
dataset O
which O
are O
from O
different O
language O
groups O
is O
used O
as O
validation O
dataset O
for O
all O
any O
- O
to O
- O
English O
translation O
tasks O
. O

The O
details O
of O
the O
datasets O
are O
in O
the O
appendix O
. O

Note O
that O
none O
of O
the O
monolingual O
dataset O
of O
the O
tested O
source O
languages O
is O
available O
in O
all O
experiments O
. O

Model O
settings O
We O
use O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
base O
model O
as O
the O
off O
- O
the O
- O
shelf O
MPE O
. O

The O
model O
is O
imple- O
mented O
on O
fairseq O
toolkit O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
. O

We O
set O
Transformer O
encoder O
the O
same O
size O
as O
the O
XLM- B-MethodName
R I-MethodName
base O
model O
. O

For O
the O
decoder O
, O
we O
use O
the O
same O
hyper O
- O
parameter O
setting O
as O
the O
encoder O
. O

We O
de- O
note O
model O
with O
such O
conﬁguration O
as O
SixT B-MethodName
and O
use O
this O
conﬁguration O
for O
our O
NMT O
models O
through O
the O
paper O
unless O
otherwise O
stated O
. O

The O
encoder- O
decoder O
attention O
modules O
are O
randomly O
initialized O
. O

We O
remove O
the O
residual O
connection O
at O
the O
11 O
- O
th O
( O
penultimate O
) O
encoder O
layer O
, O
which O
is O
selected O
on O
the O
validation O
dataset O
. O

For O
the O
empirical O
exploration O
in O
Table O
1 O
, O
we O
use O
two O
model O
conﬁgurations O
. O

For O
Strategy O
( O
1)–(7 O
) O
where O
decoder O
layers O
are O
trained O
from O
scratch O
, O
we O
use O
a O
smaller O
decoder O
denoted O
as O
BaseDec B-MethodName
. O

This O
model O
conﬁguration O
is O
denoted O
as O
SixT B-MethodName
small O
. O

For O
the O
rest O
strategies O
, O
we O
follow O
the O
conﬁguration O
of O
SixT B-MethodName
and O
denote O
its O
decoder O
as O
BigDec B-MethodName
. O

Table O
12 O
in O
Appendix O
presents O
the O
details O
of O
different O
model O
conﬁgurations O
. O

Training O
and O
evaluation O
The O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
1= O
0.9 B-HyperparameterValue
and O
2= O
0.98 B-HyperparameterValue
is O
used O
for O
training O
. O

We O
use O
label B-HyperparameterName
smoothing I-HyperparameterName
with O
value O
0.1 B-HyperparameterValue
. O

The O
learning B-HyperparameterName
rate I-HyperparameterName
is O
0.0005 B-HyperparameterValue
and O
warmup B-HyperparameterName
step I-HyperparameterName
is O
4000 B-HyperparameterValue
at O
the O
ﬁrst O
stage O
. O

For O
the O
second O
stage O
, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
as O
0.0001 B-HyperparameterValue
and O
do O
not O
use O
warmup O
. O

All O
the O
drop B-HyperparameterName
- I-HyperparameterName
out I-HyperparameterName
probabilities I-HyperparameterName
are O
set O
to O
0.3 B-HyperparameterValue
. O

We O
use O
eight O
GPUs O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
set O
as O
4096 B-HyperparameterValue
tokens O
per O
GPU O
. O

Maximum B-HyperparameterName
updates I-HyperparameterName
number I-HyperparameterName
is O
200k B-HyperparameterValue
for O
the O
ﬁrst O
stage O
and O
30k B-HyperparameterValue
for O
the O
second O
stage O
. O

We O
use O
beam O
search O
( O
beam B-HyperparameterName
size I-HyperparameterName
is O
5 B-HyperparameterValue
) O
and O
do O
not O
tune O
length O
penalty O
. O

We O
evaluate O
the O
results O
with O
sacrebleu B-MetricName
. O

If O
not O
speciﬁed O
, O
the O
best O
checkpoint O
is O
selected O
by O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
performance O
on O
the O
validation O
set O
for O
all O
experiments O
. O

We O
refer O
the O
reader O
to O
Section O
B O
in O
Appendix O
for O
more O
training O
details O
. O

Baselines O
We O
compare O
our O
model O
with O
vanilla B-MethodName
Transformer I-MethodName
and O
ﬁve O
conventional O
methods O
to O
ap- O
ply O
pretrained O
Transformer O
encoder O
on O
NMT O
task O
. O

The O
pretrained O
encoders O
in O
these O
methods O
are O
re- O
placed O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
base O
for O
fair O
comparison O
. O

Vanilla B-MethodName
Transformer I-MethodName
. O
The O
encoder O
is O
with O
the O
same O
size O
of O
XLM B-MethodName
- I-MethodName
R I-MethodName
base O
, O
the O
decoder O
uses O
the O
size O
of O
BaseDec B-MethodName
. O

All O
model O
parameters O
are O
randomly O
initialized O
. O

+XLM B-MethodName
- I-MethodName
R I-MethodName
ﬁne I-MethodName
- I-MethodName
tune I-MethodName
encoder I-MethodName
( O
Conneau O
and O
Lample O
, O
2019 O
) O
. O
The O
encoder O
is O
initialized O
with O
XLM B-MethodName
- I-MethodName
R. I-MethodName
All O
parameters O
are O
trained O
. O

+XLM B-MethodName
- I-MethodName
R I-MethodName
ﬁne I-MethodName
- I-MethodName
tune I-MethodName
all I-MethodName
( O
Conneau O
and O
Lample O
, O
2019 O
) O
. O
All O
parameters O
except O
those O
of O
cross O
at- O
tention O
module O
are O
initialized O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
and I-MethodName
directly O
ﬁne O
- O
tuned O
. O

+XLM B-MethodName
- I-MethodName
R I-MethodName
as I-MethodName
encoder I-MethodName
embedding I-MethodName
( O
Zhu O
et O
al O
. O
, O
2020 O
) O
. O
The O
XLM B-MethodName
- I-MethodName
R I-MethodName
output O
is O
leveraged O
as O
the O
en- O
coder O
input O
of O
the O
NMT O
. O
The O
XLM B-MethodName
- I-MethodName
R I-MethodName
model O
is O
ﬁxed O
during O
training O
. O

+Recycle B-MethodName
XLM I-MethodName
- I-MethodName
R I-MethodName
for I-MethodName
NMT I-MethodName
( O
Imamura O
and O
Sumita O
, O
2019 O
) O
. O
The O
method O
initializes O
the O
encoder O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
only O
trains O
decoder O
at O
the O
ﬁrst O
step O
. O
Then O
all O
are O
trained O
at O
the O
second O
step O
. O

XLM B-MethodName
- I-MethodName
R I-MethodName
fused I-MethodName
model I-MethodName
( O
Zhu O
et O
al O
. O
, O
2020 O
) O
. O
The O
XLM B-MethodName
- I-MethodName
R I-MethodName
output O
is O
fused O
into O
encoder O
and O
decoder O
separately O
with O
attention O
mechanism O
. O

The O
encoder O
embedding O
is O
initialized O
from O
XLM B-MethodName
- I-MethodName
R I-MethodName
to I-MethodName
facilitate O
transfer O
. O

The O
parameters O
of O
XLM B-MethodName
- I-MethodName
R I-MethodName
are I-MethodName
frozen O
during O
training O
. O

4.2 O
Results O
The O
results O
of O
the O
empirical O
exploration O
in O
the O
Sec- O
tion O
3.1 O
are O
shown O
in O
Table O
1 O
. O

Since O
Strategy O
( O
8) O
– O
( O
9 O
) O
use O
a O
larger O
decoder O
than O
the O
rest O
ones O
, O
we O
add O
Strategy O
( O
10 O
) O
whose O
decoder O
size O
is O
the O
same O
as O
Strategy O
( O
8)–(9 O
) O
for O
fair O
comparison O
. O

Overall O
, O
we O
observe O
that O
it O
is O
best O
to O
use O
a O
big O
decoder O
and O
initialize O
the O
decoder O
embedding O
and O
all O
encoder O
parameters O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
and O
to O
train O
the O
decoder O
layers O
from O
scratch O
( O
Strategy O
( O
10 O
) O
) O
. O

To O
verify O
the O
effect O
of O
a O
capacity O
enhanced O
de- O
coder O
in O
the O
ZeXT B-TaskName
task O
, O
we O
train O
vanilla B-MethodName
Trans- I-MethodName
former I-MethodName
with O
the O
same O
size O
of O
Strategy O
( O
7 O
) O
( O
with O
BaseDec B-MethodName
) O
and O
Strategy O
( O
10 O
) O
( O
with O
BigDec B-MethodName
) O
using O
the O
same O
training O
corpus.6The O
vanilla O
Transformer O
model O
with O
BaseDec B-MethodName
and O
BigDec B-MethodName
obtains O
a O
BLEU B-MetricName
score O
of O
23.5 B-MetricValue
and O
22.9 B-MetricValue
on O
the O
De B-DatasetName
- I-DatasetName
En I-DatasetName
test O
set O
, O
respec- O
tively O
. O

The O
big O
decoder O
improves O
the O
performance O
of O
SixT B-MethodName
, O
but O
fails O
to O
improve O
that O
of O
vanilla B-MethodName
Trans- I-MethodName
former I-MethodName
. O

This O
proves O
the O
effectiveness O
of O
BigDec B-MethodName
to O
improve O
the O
zero B-TaskName
- I-TaskName
shot I-TaskName
translation I-TaskName
performance O
of O
our O
model O
. O

Table O
2 O
illustrates O
the O
performance O
of O
the O
pro- O
posed O
SixT B-MethodName
comparing O
with O
the O
baselines O
. O

SixT B-MethodName
gets O
18.3 B-HyperparameterValue
average O
BLEU B-MetricValue
and O
improves O
over O
the O
best O
baseline O
by O
5.4 B-HyperparameterValue
average O
BLEU B-MetricName
, O
showing O
that O
SixT B-MethodName
successfully O
learns O
to O
translate O
while O
preserv- O
ing O
the O
cross O
- O
lingual O
transferability O
of O
XLM B-MethodName
- I-MethodName
R. I-MethodName

For O
all O
language O
pairs O
, O
SixT B-MethodName
obtains O
better O
transferring O
scores O
. O

In O
contrast O
, O
vanilla B-MethodName
Transformer I-MethodName
can O
hardly O
transfer O
and O
the O
other O
baselines O
do O
not O
well O
transfer O
to O
the O
distant O
languages O
. O

In O
addition O
to O
zero O
- O
shot O
performance O
, O
SixT B-MethodName
also O
achieves O
the O
best O
result O
on O
De O
- O
En O
test O
set O
. O

Note O
that O
the O
best O
checkpoint O
is O
se- O
lected O
with O
zero O
- O
shot O
validation O
set O
for O
all O
methods O
. O

Previous O
work O
( O
Conneau O
et O
al O
. O
, O
2020 O
; O
Hu O
et O
al O
. O
, O
2020 O
) O
mainly O
uses O
XLM B-MethodName
- I-MethodName
R I-MethodName
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
trans- I-TaskName
fer O
on O
NLU O
tasks O
. O

The O
experiments O
demonstrate O
that O
XLM B-MethodName
- I-MethodName
R I-MethodName
can O
be O
also O
utilized O
for O
zero B-TaskName
- I-TaskName
shot I-TaskName
neu- I-TaskName
ral I-TaskName
machine I-TaskName
translation I-TaskName
if O
it O
is O
ﬁne O
- O
tuned O
properly O
. O

We O
leave O
the O
exploration O
of O
cross O
- O
lingual O
transfer O
using O
XLM O
- O
R O
for O
other O
NLG O
tasks O
as O
the O
future O
work O
. O

4.3 O
Ablation O
Study O
We O
conduct O
an O
ablation O
study O
with O
the O
proposed O
SixT B-MethodName
on O
the O
Europarl B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
training O
set O
, O
as O
shown O
in O
Table O
3 O
. O

Overall O
, O
SixT B-MethodName
obtains O
the O
best O
zero B-TaskName
- I-TaskName
shot I-TaskName
translation I-TaskName
results O
, O
demonstrating O
the O
importance O
of O
all O
three O
components O
. O

From O
the O
results O
of O
( O
1 O
) O
to O
( O
3 O
) O
, O
TwoStage B-MethodName
and O
BigDec B-MethodName
along O
improve O
the O
zero- B-TaskName
shot I-TaskName
translation I-TaskName
performance O
by O
0.8 B-MetricValue
and O
0.4 B-MetricValue
average O
BLEU B-MetricName
over O
( O
1 O
) O
, O
respectively O
. O

However O
, O
combining O
them O
together O
brings O
a O
signiﬁcant O
improvement O
of O
2.6 B-MetricValue
average O
BLEU B-MetricName
over O
( O
1 O
) O
. O

This O
indicates O
that O
TwoStage B-MethodName
and O
BigDec O
are O
complementary O
to O
each O
other O
, O
thus O
it O
is O
important O
to O
use O
them O
together O
. O

The O
results O
of O
( O
6)!(5 O
) O
conﬁrms O
our O
claim O
: O
without O
using O
BigDec O
, O
the O
performance O
of O
SixT B-MethodName
drops O
by O
1.8 B-MetricValue
average O
BLEU B-MetricName
. O

We O
also O
observe O
that O
the O
super- O
vised O
task O
( O
De B-DatasetName
- I-DatasetName
En I-DatasetName
) O
improves O
with O
TwoStage B-MethodName
and O
BigDec B-MethodName
( O
from O
results O
of O
( O
1 O
) O
to O
( O
4 O
) O
) O
while O
degrades O
with O
Resdrop B-MethodName
( O
see O
results O
of O
( O
2 O
) O
! O
( O
5 O
) O
and O
( O
4)!(6 O
) O
) O
. O

This O
is O
expected O
since O
Resdrop B-MethodName
helps O
to O
build O
a O
more O
language O
- O
agnostic O
encoder O
. O

Although O
Res- B-MethodName
drop I-MethodName
degrades O
supervised O
performance O
, O
it O
improves O
zero B-TaskName
- I-TaskName
shot I-TaskName
translation I-TaskName
. O

The O
zero O
- O
shot O
performance O
is O
related O
with O
both O
supervised O
performance O
and O
model O
transferability O
. O

By O
either O
enhancing O
the O
su- O
pervised O
performance O
( O
with O
TwoStage B-MethodName
and O
BigDec B-MethodName
) O
or O
the O
model O
transferability O
( O
with O
Resdrop B-MethodName
) O
, O
the O
overall O
performance O
of O
zero O
- O
shot O
translation O
can O
be O
improved. O

5 O
Analysis O
Comparison O
with O
multilingual O
NMT O
In O
this O
part O
, O
we O
compare O
SixT B-MethodName
with O
mBART B-MethodName
( O
Liu O
et O
al O
. O
, O
2020 O
) O
, O
CRISS B-MethodName
( O
Tran O
et O
al O
. O
, O
2020 O
) O
and O
m2m-100 B-MethodName
( O
Fan O
et O
al O
. O
, O
2020 O
) O
on O
any B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
English I-TaskName
test O
sets O
. O

mBART B-MethodName
is O
a O
strong O
pretrained O
multilingual O
encoder- O
decoder O
based O
Transformer B-MethodName
explicitly O
designed O
for O
NMT B-TaskName
. O

We O
follow O
their O
setting O
and O
directly O
ﬁne O
- O
tune O
all O
model O
parameters O
on O
WMT19 B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
training O
set O
. O

CRISS B-MethodName
and O
m2m-100 B-MethodName
are O
the O
state O
- O
of O
- O
the O
- O
art O
unsupervised O
and O
supervised O
multilingual O
NMT O
models O
, O
respectively O
. O

The O
CRISS B-MethodName
model O
is O
initial- O
ized O
with O
the O
mBART B-MethodName
model O
and O
iteratively O
ﬁne- O
tuned O
on O
1.8 O
billion O
sentences O
covering O
90 O
language O
pairs O
. O

m2m-100 B-MethodName
is O
trained O
with O
7.5 O
billion O
parallel O
sentences O
across O
2200 O
translation O
directions O
. O

The O
results O
of O
CRISS B-MethodName
and O
m2m-100 B-MethodName
are O
listed O
as O
ref- O
erence O
, O
because O
CRISS B-MethodName
and O
m2m-100 B-MethodName
are O
many- O
to O
- O
many O
NMT O
models O
whose O
performance O
may O
degrade O
due O
to O
the O
competitions O
among O
different O
target O
languages O
( O
Aharoni O
et O
al O
. O
, O
2019 O
; O
Zhang O
et O
al O
. O
, O
2020 O
) O
, O
while O
SixT B-MethodName
is O
a O
many B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
one I-TaskName
NMT I-TaskName
model O
. O

The O
ofﬁcial O
m2m-100 B-MethodName
model O
has O
three O
sizes O
: O
small O
( O
418 B-HyperparameterValue
M I-HyperparameterValue
parameters B-HyperparameterName
) O
, O
base O
( O
1.2B B-HyperparameterValue
parameters B-HyperparameterName
) O
and O
large O
( O
12B B-HyperparameterValue
parameters B-HyperparameterName
) O
. O

The O
results O
of O
m2m-100 B-MethodName
( I-MethodName
small I-MethodName
) I-MethodName
model O
are O
reported O
. O

To O
compare O
with O
these O
models O
, O
we O
train O
a O
many- O
to O
- O
one O
SixT B-MethodName
large I-MethodName
model O
with O
WMT19 B-DatasetName
German- I-DatasetName
English I-DatasetName
training O
data O
, O
which O
only O
consists O
of O
41 O
million O
sentences O
pairs O
. O

It O
only O
requires O
a O
pre- O
trained O
XLM B-MethodName
- I-MethodName
R I-MethodName
large I-MethodName
model I-MethodName
and O
do O
not O
contain O
any O
data O
in O
other O
languages O
. O

We O
remove O
the O
residual O
connection O
after O
the O
self O
- O
attention O
sublayer O
of O
the O
23 B-HyperparameterValue
- I-HyperparameterValue
th I-HyperparameterValue
( O
penultimate O
) O
encoder O
layer O
. O

The O
dataset O
and O
model O
conﬁguration O
details O
are O
in O
Table O
9 O
and O
12 O
in O
the O
appendix O
. O

From O
the O
results O
in O
Table O
4 O
, O
the O
SixT B-MethodName
large I-MethodName
model O
is O
signiﬁcantly O
better O
than O
mBART B-MethodName
and O
slightly O
bet- O
ter O
than O
CRISS B-MethodName
and O
m2m-100 B-MethodName
. O

The O
averaged O
BLEU B-MetricName
across O
all O
languages O
is O
7.1 B-MetricValue
, O
0.5 B-MetricValue
and O
1.4 B-MetricValue
higher O
than O
mBART B-MethodName
, O
CRISS B-MethodName
and O
m2m-1007 B-MethodName
, O
respectively O
. O

The O
SixT B-MethodName
model O
has O
larger O
model O
size O
, O
nevertheless O
, O
the O
results O
of O
SixT B-MethodName
are O
impressive O
given O
that O
SixT B-MethodName
does O
not O
use O
any O
monolingual O
or O
parallel O
texts O
ex- O
cept O
German B-DatasetName
- I-DatasetName
English I-DatasetName
training O
data O
. O

The O
perfor- O
mance O
gain O
over O
mBART B-MethodName
shows O
that O
with O
proper O
ﬁne O
- O
tuning O
strategy O
, O
the O
pretrained O
multilingual O
en- O
coder O
has O
better O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
ability O
on O
NMT O
tasks O
. O

In O
addition O
, O
with O
large O
- O
scale O
German- O
English O
parallel O
data O
, O
the O
SixT B-MethodName
model O
transfers O
well O
to O
distant O
resource O
- O
poor O
languages O
like O
Ne O
and O
Si O
, O
which O
indicates O
a O
promising O
approach O
to O
translate O
resource O
- O
poor O
languages O
. O

The O
SixT B-MethodName
performance O
might O
be O
further O
improved O
with O
the O
data O
of O
more O
languages O
pairs O
. O

We O
leave O
this O
as O
future O
work O
. O

Language O
transfer O
v.s. O
language O
distance O
In O
this O
part O
, O
we O
explore O
the O
relationship O
between O
the O
cross O
- O
lingual O
transfer O
performance O
and O
the O
lan- O
guage O
distance O
. O

We O
train O
the O
SixT B-MethodName
models O
on O
dif- O
ferent O
supervised O
language O
pairs O
including O
De O
- O
En O
, O
Es O
- O
En O
, O
Fi O
- O
En O
, O
Hi O
- O
En O
and O
Zh O
- O
En O
, O
and O
then O
directly O
apply O
them O
to O
all O
test O
sets O
, O
as O
seen O
in O
Table O
5. O

We O
observe O
that O
the O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
gener- O
ally O
works O
better O
when O
the O
SixT B-MethodName
model O
is O
trained O
on O
source O
languages O
in O
the O
same O
language O
family O
. O

The O
performance O
on O
Ko B-DatasetName
- I-DatasetName
En I-DatasetName
is O
one O
exception O
, O
where O
Hi B-DatasetName
- I-DatasetName
En I-DatasetName
achieves O
the O
best O
transfer O
performance O
. O

We O
also O
notice O
that O
the O
vocabulary O
overlapping O
( O
even O
character O
overlapping O
) O
between O
Hindi O
and O
Korean O
is O
low O
, O
showing O
that O
signiﬁcant O
vocabulary O
sharing O
is O
not O
a O
requirement O
for O
effective O
transfer O
. O

When O
trained O
on O
3.5 O
million O
Hi O
- O
En O
sentence O
pairs O
, O
SixT B-MethodName
obtains O
promising O
results O
on O
the O
Ne O
- O
En O
and O
Si O
- O
En O
translation O
, O
with O
a O
BLEU B-MetricName
score O
of O
16.7 B-MetricValue
and O
9.6 B-MetricValue
, O
respectively O
. O

As O
comparison O
, O
The O
vanilla B-MethodName
Trans- I-MethodName
former I-MethodName
supervised O
with O
FLoRes B-DatasetName
training O
set O
only O
receives O
14.5 B-MetricValue
and O
7.2 B-MetricValue
BLEU B-MetricName
score O
( O
Liu O
et O
al O
. O
, O
2020 O
) O
on O
the O
same O
test O
sets O
. O

Therefore O
, O
another O
approach O
to O
translate O
resource O
- O
poor O
languages O
is O
to O
train O
SixT B-MethodName
on O
similar O
high O
- O
resource O
language O
pairs O
. O

As O
a O
comparison O
, O
we O
train O
vanilla B-MethodName
Transformer I-MethodName
conﬁgured O
as O
Transformer B-MethodName 
big I-MethodName
without O
MPE O
ini- O
tialization O
with O
the O
same O
training O
sets O
and O
valida- O
tion O
sets O
. O

The O
poor O
zero O
- O
shot O
cross O
- O
lingual O
perfor- O
mance O
of O
vanilla B-MethodName
Transformer I-MethodName
indicates O
that O
the O
XLM O
- O
R O
initialized O
encoder O
is O
essential O
and O
can O
pro- O
duce O
language O
- O
agnostic O
representations O
. O

Performance O
on O
the O
supervised O
language O
pair O
To O
study O
whether O
the O
SixT B-MethodName
model O
gains O
the O
cross- O
lingual O
transfer O
ability O
at O
the O
cost O
of O
performance O
degradation O
on O
the O
supervised O
language O
pair O
, O
we O
compare O
the O
vanilla B-MethodName
Transformer I-MethodName
big I-MethodName
model O
and O
SixT B-MethodName
model O
on O
the O
supervised O
translation O
task O
. O

The O
performance O
of O
SixT B-MethodName
is O
lower O
than O
that O
of O
vanilla B-MethodName
Transformer I-MethodName
when O
more O
than O
20 O
M O
parallel O
sentences O
are O
available O
, O
but O
it O
gets O
better O
perfor- O
mance O
with O
fewer O
parallel O
sentences O
. O

The O
Hindi- O
to O
- O
English O
is O
an O
exception O
where O
SixT B-MethodName
has O
lower O
BLEU B-MetricName
. O

When O
large O
amount O
of O
bi O
- O
text O
data O
is O
given O
, O
the O
SixT B-MethodName
model O
size O
is O
expected O
to O
be O
increased O
to O
fully O
digest O
the O
bi O
- O
text O
. O

For O
example O
, O
if O
we O
re- O
place O
SixT B-MethodName
with O
SixT B-MethodName
large I-MethodName
and O
train O
SixT B-MethodName
large I-MethodName
on O
WMT19 B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
, O
we O
get O
33.8 B-MetricValue
BLEU B-MetricName
on O
De B-DatasetName
- I-DatasetName
En I-DatasetName
test O
set O
( O
see O
Table O
4 O
) O
, O
which O
is O
comparable O
of O
33.7 B-MetricValue
BLEU B-MetricName
obtained O
by O
vanilla B-MethodName
Transformer I-MethodName
. O

Performance O
vs. O
training O
corpus O
size O
To O
exam- O
ine O
the O
relationship O
between O
cross O
- O
lingual O
transfer O
ability O
and O
training O
data O
size O
, O
we O
compare O
the O
zero- O
shot O
BLEU B-MetricName
scores O
of O
SixT O
models B-MethodName
trained O
on O
Eu- B-DatasetName
roparl I-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
and O
WMT19 B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
. O

The O
results O
are O
shown O
in O
Table O
7 O
. O

It O
shows O
that O
increasing O
train- O
ing O
data O
size O
can O
consistently O
improve O
the O
zero- B-TaskName
shot I-TaskName
translation I-TaskName
performance O
. O

For O
instance O
, O
SixT B-MethodName
trained O
with O
WMT19 B-DatasetName
improves O
over O
SixT B-MethodName
trained O
with O
Europarl B-DatasetName
- I-DatasetName
v7 I-DatasetName
by O
3.4 B-MetricValue
average O
BLEU B-MetricName
. O

Performance O
with O
other O
target O
language O
To O
build O
many O
- O
to O
- O
one O
NMT O
model O
with O
other O
target O
language O
, O
we O
train O
two O
SixT B-MethodName
models O
on O
WMT16 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
and O
WMT19 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
, O
respectively O
. O

We O
use O
Fi O
- O
De O
as O
validation O
language O
pair O
and O
Fr O
/ O
Cs O
/ O
Ru O
/ O
Nl- O
De O
as O
test O
language O
pairs O
. O

From O
the O
results O
shown O
in O
Table O
8 O
, O
SixT B-MethodName
can O
obtain O
reasonable O
transferring O
scores O
to O
unseen O
source O
languages O
when O
target O
lan- O
guage O
is O
not O
English O
. O

Again O
, O
the O
results O
conﬁrm O
that O
the O
cross O
- O
lingual O
transfer O
ability O
improves O
with O
larger O
training O
data O
. O

6 O
Related O
Work O
Zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
learning I-TaskName

Mul- O
tilingual O
pretrained O
models O
, O
such O
as O
mBERT B-MethodName
( O
Wu O
and O
Dredze O
, O
2019 O
) O
, O
XLM B-MethodName
- I-MethodName
R I-MethodName
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
, O
mBART B-MethodName
( O
Liu O
et O
al O
. O
, O
2020 O
) O
, O
and O
mT5 B-MethodName
( O
Xue O
et O
al O
. O
, O
2021 O
) O
, O
have O
achieved O
success O
on O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross- I-TaskName
lingual I-TaskName
transfer I-TaskName
for O
various O
NLP O
tasks O
. O

The O
models O
are O
pretrained O
on O
large O
- O
scale O
multilingual O
corpora O
with O
a O
shared O
vocabulary O
. O

After O
pretrained O
, O
it O
is O
ﬁne O
- O
tuned O
on O
labeled O
data O
of O
downstream O
tasks O
in O
one O
language O
and O
directly O
tested O
in O
other O
lan- O
guages O
in O
a O
zero O
- O
shot O
manner O
. O

While O
multilingual O
pretrained O
models O
with O
encoder O
- O
decoder O
- O
based O
ar- O
chitecture O
( O
Liu O
et O
al O
. O
, O
2020 O
; O
Chi O
et O
al O
. O
, O
2020 O
) O
work O
well O
on O
cross O
- O
lingual O
transfer O
for O
NLG O
tasks O
, O
multi- O
lingual O
pretrained O
encoders O
( O
Wu O
and O
Dredze O
, O
2019 O
; O
Conneau O
and O
Lample O
, O
2019 O
; O
Conneau O
et O
al O
. O
, O
2020 O
) O
are O
mainly O
applied O
to O
cross O
- O
lingual O
NLU O
tasks O
( O
Hu O
et O
al O
. O
, O
2020 O
) O
. O

In O
this O
work O
, O
we O
explore O
how O
to O
ﬁne O
- O
tune O
an O
off O
- O
the O
- O
shelf O
multilingual O
pretrained O
encoder O
for O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
in O
neu- O
ral O
machine O
translation O
, O
a O
typical O
NLG O
task O
. O

Pretrained O
models O
for O
NMT O
Some O
previous O
works O
( O
Imamura O
and O
Sumita O
, O
2019 O
; O
Conneau O
and O
Lample O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2020 O
; O
Weng O
et O

al O
. O
, O
2020 O
; O
Ma O
et O
al O
. O
, O
2020 O
; O
Zhu O
et O
al O
. O
, O
2020 O
) O
explore O
meth- O
ods O
to O
integrate O
pretrained O
language O
encoders O
into O
the O
NMT O
model O
to O
improve O
supervised O
translation O
performance O
. O

For O
instance O
, O
Zhu O
et O
al O
. O

( O
2020 O
) O
pro- O
pose O
BERT O
- O
fused O
model O
, O
in O
which O
they O
ﬁrst O
use O
BERT O
to O
extract O
representations O
for O
an O
input O
sen- O
tence O
, O
and O
then O
fuses O
the O
representations O
into O
both O
the O
encoder O
and O
decoder O
via O
the O
attention O
mech- O
anism O
. O

Another O
line O
of O
works O
( O
Liu O
et O
al O
. O
, O
2020 O
; O
Song O
et O
al O
. O
, O
2019 O
; O
Lin O
et O
al O
. O
, O
2020 O
) O
propose O
novel O
encoder O
- O
decoder O
- O
based O
multilingual O
pretrained O
lan- O
guage O
models O
and O
ﬁne O
- O
tune O
such O
models O
for O
NMT O
. O

For O
example O
, O
Liu O
et O
al O
. O
( O
2020 O
) O
propose O
mBART B-MethodName
, O
an O
encoder O
- O
decoder O
- O
based O
Transformer O
explicitly O
de- O
signs O
for O
NMT O
and O
demonstrate O
that O
mBART B-MethodName
can O
be O
ﬁne O
- O
tuned O
for O
supervised O
and O
zero B-TaskName
- I-TaskName
shot I-TaskName
NMT I-TaskName
. O

Different O
from O
them O
, O
we O
leverage O
MPE O
for O
zero- B-TaskName
shot I-TaskName
translation I-TaskName
instead O
of O
supervised O
translation O
. O

Among O
the O
previous O
works O
, O
Wei O
et O
al O
. O
( O
2021 O
) O
is O
the O
most O
similar O
with O
ours O
. O

They O
ﬁne O
- O
tune O
their O
MPE O
on O
NMT O
with O
a O
two O
- O
stage O
strategy O
. O

However O
, O
their O
work O
focuses O
on O
improving O
the O
MPE O
for O
a O
more O
universal O
representation O
across O
languages O
and O
lacks O
in O
- O
depth O
study O
of O
cross O
- O
lingual O
NMT O
. O

In O
contrast O
, O
we O
aim O
at O
leveraging O
an O
MPE O
for O
machine O
trans- O
lation O
while O
preserving O
its O
ability O
of O
cross O
- O
lingual O
transfer O
. O

7 O
Conclusion O
In O
this O
paper O
, O
we O
focus O
on O
the O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross- I-TaskName
lingual I-TaskName
NMT I-TaskName
transfer I-TaskName
( O
ZeXT B-TaskName
) O
task O
which O
aims O
at O
leveraging O
an O
MPE O
for O
machine O
translation O
while O
preserving O
its O
ability O
of O
cross O
- O
lingual O
transfer O
. O

In O
this O
task O
, O
only O
a O
multilingual O
pretrained O
encoder O
such O
as O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
one O
parallel O
dataset O
such O
asGerman O
- O
English O
are O
available O
. O

We O
propose O
SixT B-MethodName
for O
this O
task O
, O
which O
enables O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
for I-TaskName
NMT I-TaskName
by O
making O
full O
use O
of O
the O
la- O
belled O
data O
and O
enhancing O
the O
transferability O
of O
XLM B-MethodName
- I-MethodName
R. I-MethodName

Extensive O
experiments O
demonstrate O
the O
effectiveness O
of O
SixT. O
In O
particular O
, O
SixT B-MethodName
outper- O
forms O
mBART B-MethodName
, O
a O
pretrained O
encoder O
- O
decoder O
- O
based O
model O
explicitly O
designed O
for O
NMT O
. O

It O
also O
gets O
bet- O
ter O
performance O
than O
CRISS B-MethodName
and O
m2m-100 B-MethodName
, O
two O
strong O
multilingual O
NMT O
models O
, O
on O
15 O
any O
- O
to- O

English O
test O
sets O
with O
less O
training O
data O
and O
training O
computation O
cost O
. O

Acknowledgements O
This O
project O
was O
supported O
by O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No O
. O
62106138 O
) O
and O
Shanghai O
Sailing O
Program O
( O
No O
. O
21YF1412100 O
) O
. O

Wenping O
Wang O
and O
Jia O
Pan O
acknowledge O
the O
sup- O
port O
from O
Centre O
for O
Transformative O
Garment O
Pro- O
duction O
. O

We O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
feedbacks O
on O
this O
work O
. O

References O
Roee O
Aharoni O
, O
Melvin O
Johnson O
, O
and O
Orhan O
Firat O
. O
2019 O
. O
Massively O
multilingual O
neural O
machine O
translation O
. O
InProceedings O
of O
NAACL O
, O
pages O
3874–3884 O
. O

Yun O
Chen O
, O
Yang O
Liu O
, O
Yong O
Cheng O
, O
and O
Victor O
OK O
Li O
. O
2017 O
. O
A O
teacher O
- O
student O
framework O
for O
zero- O
resource O
neural O
machine O
translation O
. O
In O
Proceedings O
of O
ACL O
, O
pages O
1925–1935 O
. O

Yun O
Chen O
, O
Yang O
Liu O
, O
and O
Victor O
OK O
Li O
. O
2018 O
. O
Zero- O
resource O
neural O
machine O
translation O
with O
multi- O
agent O
communication O
game O
. O
In O
Thirty O
- O
Second O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
. O

Zewen O
Chi O
, O
Li O
Dong O
, O
Furu O
Wei O
, O
Wenhui O
Wang O
, O
Xian- O
Ling O
Mao O
, O
and O
Heyan O
Huang O
. O
2020 O
. O
Cross O
- O
lingual O
natural O
language O
generation O
via O
pre O
- O
training O
. O
In O
Pro- O
ceedings O
of O
the O
AAAI O
Conference O
on O
Artiﬁcial O
Intel- O
ligence O
, O
volume O
34 O
, O
pages O
7570–7577 O
. O

Alexis O
Conneau O
, O
Kartikay O
Khandelwal O
, O
Naman O
Goyal O
, O
Vishrav O
Chaudhary O
, O
Guillaume O
Wenzek O
, O
Francisco O
Guzmán O
, O
Edouard O
Grave O
, O
Myle O
Ott O
, O
Luke O
Zettle- O
moyer O
, O
and O
Veselin O
Stoyanov O
. O

2020 O
. O

Unsupervised O
cross O
- O
lingual O
representation O
learning O
at O
scale O
. O

In O
Proceedings O
of O
ACL O
, O
pages O
8440–8451 O
, O
Online O
. O

Alexis O
Conneau O
and O
Guillaume O
Lample O
. O

2019 O
. O
Cross- O

lingual O
language O
model O
pretraining O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
pages O
7059–7069 O
. O

Anna O
Currey O
, O
Prashant O
Mathur O
, O
and O
Georgiana O
Dinu O
. O
2020 O
. O

Distilling O
multiple O
domains O
for O
neural O
ma- O
chine O
translation O
. O

In O
Proceedings O
of O
EMNLP O
, O
pages O
4500–4511 O
. O

24Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
under- O
standing O
. O

In O
Proceedings O
of O
NAACL O
, O
pages O
4171 O
– O
4186 O
. O

Angela O
Fan O
, O
Shruti O
Bhosale O
, O
Holger O
Schwenk O
, O
Zhiyi O
Ma O
, O
Ahmed O
El O
- O
Kishky O
, O
Siddharth O
Goyal O
, O
Man- O
deep O
Baines O
, O
Onur O
Celebi O
, O
Guillaume O
Wenzek O
, O
Vishrav O
Chaudhary O
, O
Naman O
Goyal O
, O
Tom O
Birch O
, O
Vi- O
taliy O
Liptchinsky O
, O
Sergey O
Edunov O
, O
Edouard O
Grave O
, O
Michael O
Auli O
, O
and O
Armand O
Joulin O
. O

2020 O
. O

Be- O

yond O
english O
- O
centric O
multilingual O
machine O
transla- O
tion O
. O

arXiv O
preprint O
arXiv:2010.11125 O
. O

Jiatao O
Gu O
, O
Yong O
Wang O
, O
Yun O
Chen O
, O
Victor O
O. O
K. O
Li O
, O
and O
Kyunghyun O
Cho O
. O
2018 O
. O

Meta O
- O
learning O
for O
low- O
resource O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
EMNLP O
, O
pages O
3622–3631 O
. O

Junjie O
Hu O
, O
Sebastian O
Ruder O
, O
Aditya O
Siddhant O
, O
Gra- O
ham O
Neubig O
, O
Orhan O
Firat O
, O
and O
Melvin O
Johnson O
. O

2020 O
. O

XTREME O
: O

A O
massively O
multilingual O
multi- O
task O
benchmark O
for O
evaluating O
cross O
- O
lingual O
gen- O
eralisation O
. O

In O
Proceedings O
of O
the O
37th O
Interna- O
tional O
Conference O
on O
Machine O
Learning O
, O
volume O
119 O
, O
pages O
4411–4421 O
. O

Kenji O
Imamura O
and O
Eiichiro O
Sumita O
. O

2019 O
. O

Recycling O
a O
pre O
- O
trained O
BERT O
encoder O
for O
neural O
machine O
trans- O
lation O
. O

In O
Proceedings O
of O
the O
3rd O
Workshop O
on O
Neu- O
ral O
Generation O
and O
Translation O
, O
pages O
23–31 O
. O

Melvin O
Johnson O
, O
Mike O
Schuster O
, O
Quoc O
V O
Le O
, O
Maxim O
Krikun O
, O
Yonghui O
Wu O
, O
Zhifeng O
Chen O
, O
Nikhil O
Thorat O
, O
Fernanda O
Viégas O
, O
Martin O
Wattenberg O
, O
Greg O
Corrado O
, O
et O
al O
. O
2017 O
. O

Google O
’s O
multilingual O
neural O
machine O
translation O
system O
: O
Enabling O
zero O
- O
shot O
translation O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
5:339–351 O
. O

Diederik O
P. O
Kingma O
and O
Jimmy O
Ba O
. O
2015 O
. O

Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O

In O
Proceedings O
of O
ICLR O
. O

Taku O
Kudo O
. O

2018 O
. O

Subword O
regularization O
: O
Improving O
neural O
network O
translation O
models O
with O
multiple O
sub- O
word O
candidates O
. O

In O
Proceedings O
of O
ACL O
, O
pages O
66 O
– O
75 O
. O

Guillaume O
Lample O
, O
Myle O
Ott O
, O
Alexis O
Conneau O
, O
Lu- O
dovic O
Denoyer O
, O
and O
Marc’Aurelio O
Ranzato O
. O

2018 O
. O

Phrase O
- O
based O
& O
neural O
unsupervised O
machine O
trans- O
lation O
. O

In O
Proceedings O
of O
EMNLP O
. O

Zehui O
Lin O
, O
Xiao O
Pan O
, O
Mingxuan O
Wang O
, O
Xipeng O
Qiu O
, O
Jiangtao O
Feng O
, O
Hao O
Zhou O
, O
and O
Lei O
Li O
. O
2020 O
. O

Pre- O
training O
multilingual O
neural O
machine O
translation O
by O
leveraging O
alignment O
information O
. O

In O
Proceed- O
ings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
2649–2663 O
. O

Danni O
Liu O
, O
Jan O
Niehues O
, O
James O
Cross O
, O
Francisco O
Guzmán O
, O
and O
Xian O
Li O
. O
2021 O
. O

Improving O
zero O
- O
shot O
translation O
by O
disentangling O
positional O
information O
. O

InProceedings O
of O
ACL O
, O
pages O
1259–1273.Yinhan O

Liu O
, O
Jiatao O
Gu O
, O
Naman O
Goyal O
, O
Xian O
Li O
, O
Sergey O
Edunov O
, O
Marjan O
Ghazvininejad O
, O
Mike O
Lewis O
, O
and O
Luke O
Zettlemoyer O
. O

2020 O
. O

Multilingual O
denoising O
pre O
- O
training O
for O
neural O
machine O
translation O
. O

arXiv O
preprint O
arXiv:2001.08210 O
. O

Shuming O
Ma O
, O
Jian O
Yang O
, O
Haoyang O
Huang O
, O
Zewen O
Chi O
, O
Li O
Dong O
, O
Dongdong O
Zhang O
, O
Hany O
Hassan O
Awadalla O
, O
Alexandre O
Muzio O
, O
Akiko O
Eriguchi O
, O
Saksham O
Sing- O
hal O
, O
Xia O
Song O
, O
Arul O
Menezes O
, O
and O
Furu O
Wei O
. O
2020 O
. O

Xlm O
- O
t O
: O
Scaling O
up O
multilingual O
machine O
translation O
with O
pretrained O
cross O
- O
lingual O
transformer O
encoders O
. O

arXiv O
preprint O
arXiv:2012.15547 O
. O

Myle O
Ott O
, O
Sergey O
Edunov O
, O
Alexei O
Baevski O
, O
Angela O
Fan O
, O
Sam O
Gross O
, O
Nathan O
Ng O
, O
David O
Grangier O
, O
and O
Michael O
Auli O
. O

2019 O
. O

fairseq O
: O

A O
fast O
, O
extensible O
toolkit O
for O
sequence O
modeling O
. O

In O
Proceedings O
of O
NAACL O
. O

Joan O
Serra O
, O
Didac O
Suris O
, O
Marius O
Miron O
, O
and O
Alexandros O
Karatzoglou O
. O

2018 O
. O

Overcoming O
catastrophic O
forget- O
ting O
with O
hard O
attention O
to O
the O
task O
. O

In O
Proceedings O
of O
ICML O
, O
volume O
80 O
, O
pages O
4548–4557 O
. O

Kaitao O
Song O
, O
Xu O
Tan O
, O
Tao O
Qin O
, O
Jianfeng O
Lu O
, O
and O
Tie- O
Yan O
Liu O
. O
2019 O
. O

Mass O
: O

Masked O
sequence O
to O
se- O
quence O
pre O
- O
training O
for O
language O
generation O
. O

In O
In- O
ternational O
Conference O
on O
Machine O
Learning O
, O
pages O
5926–5936 O
. O

Chau O
Tran O
, O
Yuqing O
Tang O
, O
Xian O
Li O
, O
and O
Jiatao O
Gu O
. O
2020 O
. O

Cross O
- O
lingual O
retrieval O
for O
iterative O
self O
- O
supervised O
training O
. O

In O
Proceedings O
of O
NeurIPS O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
Lukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
NeurIPS O
, O
pages O
5998–6008 O
. O

Xiangpeng O
Wei O
, O
Yue O
Hu O
, O
Rongxiang O
Weng O
, O
Luxi O
Xing O
, O
Heng O
Yu O
, O
and O
Weihua O
Luo O
. O
2021 O
. O

On O
learning O
uni- O
versal O
representations O
across O
languages O
. O

Proceed- O
ings O
of O
ICLR O
. O

Rongxiang O
Weng O
, O
Heng O
Yu O
, O
Shujian O
Huang O
, O
Shanbo O
Cheng O
, O
and O
Weihua O
Luo O
. O
2020 O
. O

Acquiring O
knowl- O
edge O
from O
pre O
- O
trained O
model O
to O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
AAAI O
, O
pages O
9266 O
– O
9273 O
. O

Shijie O
Wu O
and O
Mark O
Dredze O
. O

2019 O
. O

Beto O
, O
bentz O
, O
becas O
: O
The O
surprising O
cross O
- O
lingual O
effectiveness O
of O
bert O
. O

In O
Proceedings O
of O
EMNLP O
- O
IJCNLP O
, O
pages O
833–844 O
. O

Linting O
Xue O
, O
Noah O
Constant O
, O
Adam O
Roberts O
, O
Mi- O
hir O
Kale O
, O
Rami O
Al O
- O
Rfou O
, O
Aditya O
Siddhant O
, O
Aditya O
Barua O
, O
and O
Colin O
Raffel O
. O
2021 O
. O

mT5 O
: O

A O
massively O
multilingual O
pre O
- O
trained O
text O
- O
to O
- O
text O
transformer O
. O

In O
Proceedings O
of O
NAACL O
, O
pages O
483–498 O
. O

Jiacheng O
Yang O
, O
Mingxuan O
Wang O
, O
Hao O
Zhou O
, O
Chengqi O
Zhao O
, O
Weinan O
Zhang O
, O
Yong O
Yu O
, O
and O
Lei O
Li O
. O
2020 O
. O

Towards O
making O
the O
most O
of O
bert O
in O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
AAAI O
, O
pages O
9378 O
– O
9385 O
. O

25Biao O
Zhang O
, O
Philip O
Williams O
, O
Ivan O
Titov O
, O
and O
Rico O
Sen- O
nrich O
. O

2020 O
. O

Improving O
massively O
multilingual O
neu- O
ral O
machine O
translation O
and O
zero O
- O
shot O
translation O
. O

In O
Proceedings O
of O
ACL O
, O
pages O
1628–1639 O
. O

Jinhua O
Zhu O
, O
Yingce O
Xia O
, O
Lijun O
Wu O
, O
Di O
He O
, O
Tao O
Qin O
, O
Wengang O
Zhou O
, O
Houqiang O
Li O
, O
and O
Tieyan O
Liu O
. O
2020 O
. O

Incorporating O
bert O
into O
neural O
machine O
translation O
. O

InProceedings O
of O
ICLR O
. O

A O
Dataset O

The O
dataset O
is O
from O
WMT O
translation O
task O
, O
CCAligned O
corpus11 O
, O
WAT21 O
translation O
task12 O
, O
Flores O
test O
set13and O
Tatoeba O
test O
sets14 O
. O

We O
use O
the O
ﬁrst O
20 O
M O
sentence O
pairs O
in O
the O
Es O
- O
En O
CCAligned O
corpus O
as O
training O
set O
. O

For O
experiments O
of O
Table O
5 O
, O
the O
validation O
set O
for O
De O
- O
En O
, O
Es O
- O
En O
and O
Fi O
- O
En O
are O
the O
concatnation O
of O
Fr O
- O
En O
and O
Cs O
- O
En O
validation O
set O
. O

We O
use O
Ta O
- O
En O
and O
Zh O
- O
En O
as O
the O
validation O
set O
for O
Hi O
- O
En O
and O
Zh O
- O
En O
, O
respectively O
. O

More O
details O
are O
in O
Table O
9 O
to O
Table O
11 O
. O

To O
be O
compatible O
with O
XLM O
- O
R O
model O
, O
all O
texts O
are O
tokenized O
with O
the O
same O
XLM O
- O
R O
sentencepiece O
( O
Kudo O
, O
2018 O
) O
model O
. O

The O
< O
bos O
> O
token O
is O
added O
at O
the O
beginning O
of O
each O
source O
sentence O
while O
< O
eos O
> O
token O
is O
appended O
at O
the O
end O
when O
the O
NMT O
model O
initializes O
encoder O
with O
XLM O
- O
R. O

The O
source O
sentence O
length O
is O
limited O
within O
512 O
tokens O
. O

B O
Model O
and O
Training O
Details O

The O
encoder O
of O
SixT B-MethodName
is O
the O
same O
size O
of O
XLM O
- O
R O
model O
. O

We O
compare O
models O
with O
different O
decoder O
conﬁgurations O
in O
the O
paper O
, O
the O
details O
are O
in O
the O
Table O
12 O
. O

For O
all O
models O
, O
the O
dimension O
of O
decoder O
hidden O
states O
equals O
that O
of O
encoder O
hidden O
states O
. O

The O
number O
of O
attentionO O
heads O
is O
set O
as O
16 O
for O
the O
decoder O
of O
SixT O
large O
model O
, O
so O
that O
the O
dimension O
of O
hidden O
states O
can O
be O
divided O
by O
the O
number O
of O
attention O
heads O
. O

We O
use O
separate O
encoder O
and O
de- O
coder O
embeddings O
. O

We O
tie O
the O
decoder O
input O
and O
output O
embeddings O
. O

The O
source O
vocabulary O
uses O
the O
same O
250k O
vocabulary O
of O
XLM O
- O
R O
, O
while O
the O
target O
vocabulary O
is O
generated O
from O
the O
training O
corpus O
. O

All O
experiments O
are O
done O
with O
8 O
GPUs O
. O

We O
compare O
SixT O
large O
with O
CRISS O
, O
m2m-100 O
and O
mBART O
in O
the O
Table O
4 O
. O

We O
use O
the O
ofﬁcial O
model O
checkpoints O
of O
mBART15 O
(611 O
M O
parame- O
ters O
) O
, O
CRISS16O O
(680 O
M O
parameters O
) O
and O
m2m-100 O

