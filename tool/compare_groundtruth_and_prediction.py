import argparse
import os

if __name__ == '__main__':
    '''
    This script takes your model outputs (in CoNLL format) on
    sentence-level data (generated by paragraph_to_sentence.py), and
    removes the extra newlines to match the paragraph-segmented outputs
    that we expect.
    '''

    parser = argparse.ArgumentParser()
    parser.add_argument('-i', '--insystem', default='baseline_bert_train_unweighted_everysent',
        help='Your model outputs on the sentence-segmented test set in CoNLL format')
    parser.add_argument('-d', '--dummyfile', default='Dataset/val_data_ground_truth/validation_data_ground_truth.conll',
        help='The provided empty CoNLL file, used to determine the original paragraph boundaries')
    parser.add_argument('-r', '--reference_dummyfile', default='Dataset/val_data_ground_truth/output_validation_data_ground_truth.conll',
        help='The provided empty CoNLL file, used to determine the original paragraph boundaries')
    args = parser.parse_args()

    input_lines = []
    dummy_lines = []
    output_lines = []

    infile_list = os.path.join("predictions", args.insystem)
    for file in os.listdir(infile_list):
        if file[:4] == "pred":
            infile = os.path.join("predictions", args.insystem, file)
            break
    # print("infile: ", infile)
    with open(infile) as f:
        input_lines = f.readlines()

    with open(args.dummyfile) as f:
        dummy_lines = f.readlines()


    # make sure all tokens match our dummy reference CoNLL file
    # print(f"total lines {len(dummy_lines)} | in lines {len(input_lines)}")
    assert(len(dummy_lines) == len(input_lines))
    # print("*" * 20)

    for i in range(len(input_lines)):
        dummy_lines[i] = dummy_lines[i].strip()
        input_lines[i] = input_lines[i].strip()

        dummy_token = dummy_lines[i].split(' ')[0]
        input_token = input_lines[i].split(' ')[0]
        if dummy_token != input_token:
            print("dummy line:", dummy_lines[i])
            print("input line:", input_lines[i])
            print(f"line {i} | dummy {len(dummy_token)} {dummy_token} | input {len(input_token)} {input_token}")

        assert(dummy_token == input_token)

    outfile = os.path.join("predictions", args.insystem, f"output_{args.insystem}.conll")
    # print("outfile:", outfile)
    with open(outfile, 'w', encoding="utf-8") as f:
        f.write('\n'.join(input_lines))

    with open(args.reference_dummyfile, 'w', encoding="utf-8") as f:
        f.write('\n'.join(dummy_lines))
