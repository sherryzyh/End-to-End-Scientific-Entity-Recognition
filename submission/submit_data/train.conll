Addressing O
the O
Rare O
Word O
Problem O
in O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName
Minh O
- O
Thang O
Luong† O
∗ O
Stanford O
lmthang@stanford.edu O

Ilya O
Sutskever† O
GoogleQuoc O
V O
. O

Le† O
Google O
{ O
ilyasu O
, O
qvl O
, O
vinyals O
} O
@google.comOriol O
Vinyals O
GoogleWojciech O
Zaremba∗ O

New O
York O
University O
woj.zaremba@gmail.com O

Abstract O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName
( O
NMT B-TaskName
) O
is O
a O
new O
approach O
to O
machine B-TaskName
translation I-TaskName
that O
has O
shown O
promising O
results O
that O
are O
com- O
parable O
to O
traditional O
approaches O
. O

A O
sig- O
niﬁcant O
weakness O
in O
conventional O
NMT O
systems O
is O
their O
inability O
to O
correctly O
trans- O
late O
very O
rare O
words O
: O
end O
- O
to O
- O
end O
NMTs O
tend O
to O
have O
relatively O
small O
vocabularies O
with O
a O
single O
unk O
symbol O
that O
represents O
every O
possible O
out O
- O
of O
- O
vocabulary O
( O
OOV O
) O
word O
. O

In O
this O
paper O
, O
we O
propose O
and O
im- O
plement O
an O
effective O
technique O
to O
address O
this O
problem O
. O

We O
train O
an O
NMT O
system O
on O
data O
that O
is O
augmented O
by O
the O
output O
of O
a O
word O
alignment O
algorithm O
, O
allowing O
the O
NMT O
system O
to O
emit O
, O
for O
each O
OOV O
word O
in O
the O
target O
sentence O
, O
the O
position O
of O
its O
corresponding O
word O
in O
the O
source O
sen- O
tence O
. O

This O
information O
is O
later O
utilized O
in O
a O
post O
- O
processing O
step O
that O
translates O
every O
OOV O
word O
using O
a O
dictionary O
. O

Our O
exper- O
iments O
on O
the O
WMT’14 B-DatasetName
English I-DatasetName
to I-DatasetName
French I-DatasetName
translation O
task O
show O
that O
this O
method O
pro- O
vides O
a O
substantial O
improvement O
of O
up O
to O
2.8 B-MetricValue
BLEU B-MetricName
points O
over O
an O
equivalent O
NMT O
system O
that O
does O
not O
use O
this O
technique O
. O

With O
37.5 B-MetricValue
BLEU B-MetricName
points O
, O
our O
NMT O
sys- O
tem O
is O
the O
ﬁrst O
to O
surpass O
the O
best O
result O
achieved O
on O
a O
WMT’14 B-DatasetName
contest O
task O
. O

1 O
Introduction O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName
( O
NMT B-TaskName
) O
is O
a O
novel O
ap- O
proach O
to O
MT B-TaskName
that O
has O
achieved O
promising O
results O
( O
Kalchbrenner O
and O
Blunsom O
, O
2013 O
; O

Sutskever O
et O
al O
. O
, O
2014 O
; O
Cho O
et O
al O
. O
, O
2014 O
; O
Bahdanau O
et O
al O
. O
, O
2015 O
; O
Jean O
et O
al O
. O
, O
2015 O
) O
. O

An O
NMT O
system O
is O
a O
conceptu- O
ally O
simple O
large O
neural O
network O
that O
reads O
the O
en- O
∗Work O
done O
while O
the O
authors O
were O
in O
Google O
. O

†indicates O
equal O
contribution.tire O
source O
sentence O
and O
produces O
an O
output O
trans- O
lation O
one O
word O
at O
a O
time O
. O

NMT O
systems O
are O
ap- O
pealing O
because O
they O
use O
minimal O
domain O
knowl- O
edge O
which O
makes O
them O
well O
- O
suited O
to O
any O
prob- O
lem O
that O
can O
be O
formulated O
as O
mapping O
an O
input O
sequence O
to O
an O
output O
sequence O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
. O

In O
addition O
, O
the O
natural O
ability O
of O
neural O
networks O
to O
generalize O
implies O
that O
NMT O
systems O
will O
also O
generalize O
to O
novel O
word O
phrases O
and O
sen- O
tences O
that O
do O
not O
occur O
in O
the O
training O
set O
. O

In O
addi- O
tion O
, O
NMT O
systems O
potentially O
remove O
the O
need O
to O
store O
explicit O
phrase O
tables O
and O
language O
models O
which O
are O
used O
in O
conventional O
systems O
. O

Finally O
, O
the O
decoder O
of O
an O
NMT O
system O
is O
easy O
to O
imple- O
ment O
, O
unlike O
the O
highly O
intricate O
decoders O
used O
by O
phrase O
- O
based O
systems O
( O
Koehn O
et O
al O
. O
, O
2003 O
) O
. O

Despite O
these O
advantages O
, O
conventional O
NMT O
systems O
are O
incapable O
of O
translating O
rare O
words O
be- O

cause O
they O
have O
a O
ﬁxed O
modest O
- O
sized O
vocabulary1 O
which O
forces O
them O
to O
use O
the O
unk O
symbol O
to O
repre- O
sent O
the O
large O
number O
of O
out O
- O
of O
- O
vocabulary O
( O
OOV O
) O
words O
, O
as O
illustrated O
in O
Figure O
1 O
. O

Unsurpris- O
ingly O
, O
both O
Sutskever O
et O
al O
. O

( O
2014 O
) O
and O
Bahdanau O
et O

al O
. O

( O
2015 O
) O
have O
observed O
that O
sentences O
with O
many O
rare O
words O
tend O
to O
be O
translated O
much O
more O
poorly O
than O
sentences O
containing O
mainly O
frequent O
words O
. O

Standard O
phrase O
- O
based O
systems O
( O
Koehn O
et O
al O
. O
, O
2007 O
; O
Chiang O
, O
2007 O
; O
Cer O
et O
al O
. O
, O
2010 O
; O
Dyer O
et O
al O
. O
, O
2010 O
) O
, O
on O
the O
other O
hand O
, O
do O
not O
suffer O
from O
the O
rare O
word O
problem O
to O
the O
same O
extent O
because O
they O
can O
support O
a O
much O
larger O
vocabulary O
, O
and O
because O
their O
use O
of O
explicit O
alignments O
and O
phrase O
tables O
allows O
them O
to O
memorize O
the O
translations O
of O
even O
extremely O
rare O
words O
. O

Motivated O
by O
the O
strengths O
of O
standard O
phrase- O
based O
system O
, O
we O
propose O
and O
implement O
a O
novel O
approach O
to O
address O
the O
rare O
word O
problem O
of O
NMTs B-TaskName
. O

Our O
approach O
annotates O
the O
training O
cor- O
pus O
with O
explicit O
alignment O
information O
that O
en- O
ables O
the O
NMT O
system O
to O
emit O
, O
for O
each O
OOV O
word O
, O
a O
“ O
pointer O
” O
to O
its O
corresponding O
word O
in O
the O
source O
sentence O
. O

This O
information O
is O
later O
utilized O
in O
a O
post O
- O
processing O
step O
that O
translates O
the O
OOV O
words O
using O
a O
dictionary O
or O
with O
the O
identity O
trans- O
lation O
, O
if O
no O
translation O
is O
found O
. O

Our O
experiments O
conﬁrm O
that O
this O
approach O
is O
effective O
. O

On O
the O
English B-DatasetName
to I-DatasetName
French I-DatasetName
WMT’14 I-DatasetName
translation O
task O
, O
this O
approach O
provides O
an O
im- O
provement O
of O
up O
to O
2.8 B-MetricValue
( O
if O
the O
vocabulary O
is O
rel- O
atively O
small O
) O
BLEU B-MetricName
points O
over O
an O
equivalent O
NMT O
system O
that O
does O
not O
use O
this O
technique O
. O

Moreover O
, O
our O
system O
is O
the O
ﬁrst O
NMT O
that O
out- O
performs O
the O
winner O
of O
a O
WMT’14 B-DatasetName
task O
. O

2 O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName
A O
neural O
machine O
translation O
system O
is O
any O
neural O
network O
that O
maps O
a O
source O
sentence O
, O
s1 O
, O
. O
. O
. O

, O
s O
n O
, O
to O
a O
target O
sentence O
, O
t1 O
, O
. O

. O
. O

, O
t O
m O
, O
where O
all O
sen- O
tences O
are O
assumed O
to O
terminate O
with O
a O
special O
“ O
end O
- O
of O
- O
sentence O
” O
token O
< O
eos O
> O
. O

More O
con- O
cretely O
, O
an O
NMT O
system O
uses O
a O
neural O
network O
to O
parameterize O
the O
conditional O
distributions O
p(tj|t O
< O
j O
, O
s≤n O
) O
( O
1 O
) O
for1≤j≤m O
. O

By O
doing O
so O
, O
it O
becomes O
pos- O
sible O
to O
compute O
and O
therefore O
maximize O
the O
log O
probability O
of O
the O
target O
sentence O
given O
the O
source O
sentence O
logp(t|s O
) O

= O
m O
/ O
summationdisplay O
j=1logp(tj|t O
< O
j O
, O
s≤n O
) O
( O
2 O
) O
There O
are O
many O
ways O
to O
parameterize O
these O
con- O
ditional O
distributions O
. O

For O
example O
, O
Kalchbrennerand O
Blunsom O
( O
2013 O
) O
used O
a O
combination O
of O
a O
con- O
volutional O
neural O
network O
and O
a O
recurrent O
neural O
network O
, O
Sutskever O
et O
al O
. O

( O
2014 O
) O
used O
a O
deep O
Long O
Short O
- O
Term O
Memory O
( O
LSTM O
) O
model O
, O
Cho O
et O
al O
. O

( O
2014 O
) O
used O
an O
architecture O
similar O
to O
the O
LSTM O
, O
and O
Bahdanau O
et O

al O
. O

( O
2015 O
) O
used O
a O
more O
elabo- O
rate O
neural O
network O
architecture O
that O
uses O
an O
atten- O
tional O
mechanism O
over O
the O
input O
sequence O
, O
similar O
to O
Graves O
( O
2013 O
) O
and O
Graves O
et O

al O
. O
( O
2014 O
) O
. O

In O
this O
work O
, O
we O
use O
the O
model O
of O
Sutskever O
et O
al O
. O

( O
2014 O
) O
, O
which O
uses O
a O
deep O
LSTM O
to O
encode O
the O
input O
sequence O
and O
a O
separate O
deep O
LSTM O
to O
out- O
put O
the O
translation O
. O

The O
encoder O
reads O
the O
source O
sentence O
, O
one O
word O
at O
a O
time O
, O
and O
produces O
a O
large O
vector O
that O
represents O
the O
entire O
source O
sentence O
. O

The O
decoder O
is O
initialized O
with O
this O
vector O
and O
gen- O
erates O
a O
translation O
, O
one O
word O
at O
a O
time O
, O
until O
it O
emits O
the O
end O
- O
of O
- O
sentence O
symbol O
< O
eos O
> O
. O

None O
the O
early O
work O
in O
neural O
machine O
transla- O
tion O
systems O
has O
addressed O
the O
rare O
word O
problem O
, O
but O
the O
recent O
work O
of O
Jean O
et O
al O
. O
( O
2015 O
) O
has O
tack- O
led O
it O
with O
an O
efﬁcient O
approximation O
to O
the O
soft- O
max O
to O
accommodate O
for O
a O
very O
large O
vocabulary O
( O
500 O
K O
words O
) O
. O

However O
, O
even O
with O
a O
large O
vocab- O
ulary O
, O
the O
problem O
with O
rare O
words O
, O
e.g. O
, O
names O
, O
numbers O
, O
etc O
. O
, O
still O
persists O
, O
and O
Jean O
et O
al O
. O

( O
2015 O
) O
found O
that O
using O
techniques O
similar O
to O
ours O
are O
beneﬁcial O
and O
complementary O
to O
their O
approach O
. O

3 O
Rare O
Word O
Models O
Despite O
the O
relatively O
large O
amount O
of O
work O
done O
on O
pure O
neural O
machine O
translation O
systems O
, O
there O
has O
been O
no O
work O
addressing O
the O
OOV O
problem O
in O
NMT O
systems O
, O
with O
the O
notable O
exception O
of O
Jean O
et O
al O
. O

( O
2015 O
) O
’s O
work O
mentioned O
earlier O
. O

We O
propose O
to O
address O
the O
rare O
word O
problem O
by O
training O
the O
NMT O
system O
to O
track O
the O
origins O
of O
the O
unknown O
words O
in O
the O
target O
sentences O
. O

If O
we O
knew O
the O
source O
word O
responsible O
for O
each O
un- O
known O
target O
word O
, O
we O
could O
introduce O
a O
post- O
processing O
step O
that O
would O
replace O
each O
unk O
in O
the O
system O
’s O
output O
with O
a O
translation O
of O
its O
source O
word O
, O
using O
either O
a O
dictionary O
or O
the O
identity O
translation O
. O

For O
example O
, O
in O
Figure O
1 O
, O
if O
the O
model O
knows O
that O
the O
second O
unknown O
token O
in O
the O
NMT O
( O
line O
nn O
) O
originates O
from O
the O
source O
word O
ecotax O
, O
it O
can O
perform O
a O
word O
dictionary O
lookup O
to O
replace O
that O
unknown O
token O
by O
´ O
ecotaxe O
. O

Sim- O

ilarly O
, O
an O
identity O
translation O
of O
the O
source O
word O
Pont O
- O
de O
- O
Buis O
can O
be O
applied O
to O
the O
third O
un- O
known O
token O
. O

We O
present O
three O
annotation O
strategies O
that O
can O
easily O
be O
applied O
to O
any O
NMT O
system O
( O
Kalchbren- O
ner O
and O
Blunsom O
, O
2013 O
; O
Sutskever O
et O
al O
. O
, O
2014 O
; O
Cho O
et O
al O
. O
, O
2014 O
) O
. O

We O
treat O
the O
NMT O
system O
as O
a O
black O
box O
and O
train O
it O
on O
a O
corpus O
annotated O
by O
one O
of O
the O
models O
below O
. O

First O
, O
the O
alignments O
are O
produced O
with O
an O
unsupervised O
aligner O
. O

Next O
, O
we O
use O
the O
alignment O
links O
to O
construct O
a O
word O
dictio- O

nary O
that O
will O
be O
used O
for O
the O
word O
translations O
in O
the O
post O
- O
processing O
step.2If O
a O
word O
does O
not O
ap- O
pear O
in O
our O
dictionary O
, O
then O
we O
apply O
the O
identity O
translation O
. O

The O
ﬁrst O
few O
words O
of O
the O
sentence O
pair O
in O
Fig- O
ure O
1 O
( O
lines O
enandfr O
) O
illustrate O
our O
models O
. O

3.1 O
Copyable B-MethodName
Model O
In O
this O
approach O
, O
we O
introduce O
multiple O
tokens O
to O
represent O
the O
various O
unknown O
words O
in O
the O
source O
and O
in O
the O
target O
language O
, O
as O
opposed O
to O
using O
only O
one O
unk O
token O
. O

We O
annotate O
the O
OOV O
words O
in O
the O
source O
sentence O
with O
unk O
1,unk O
2 O
, O
unk O
3 O
, O
in O
that O
order O
, O
while O
assigning O
repeating O
un- O
known O
words O
identical O
tokens O
. O

The O
annotation O
of O
the O
unknown O
words O
in O
the O
target O
language O
is O
slightly O
more O
elaborate O
: O
( O
a O
) O
each O
unknown O
target O
word O
that O
is O
aligned O
to O
an O
unknown O
source O
word O
is O
assigned O
the O
same O
unknown O
token O
( O
hence O
, O
the O
“ O
copy O
” O
model O
) O
and O
( O
b O
) O
an O
unknown O
target O
word O
that O
has O
no O
alignment O
or O
that O
is O
aligned O
with O
a O
known O
word O
uses O
the O
special O
null O
token O
unk O
∅. O

See O
Figure O
2 O
for O
an O
example O
. O

This O
annotation O
enables O
us O
to O
translate O
every O
non O
- O
null O
unknown O
token O
. O

3.2 O
Positional B-MethodName
All I-MethodName
Model O
( O
PosAll B-MethodName
) O
The O
copyable B-MethodName
model O
is O
limited O
by O
its O
inability O
to O
translate O
unknown O
target O
words O
that O
are O
aligned O
toknown O
words O
in O
the O
source O
sentence O
, O
such O
as O
the O
pair O
of O
words O
, O
“ O
portico O
” O
and O
“ O
portique O
” O
, O
in O
our O
running O
example O
. O

The O
former O
word O
is O
known O
on O
the O
source O
sentence O
; O
whereas O
latter O
is O
not O
, O
so O
it O
is O
labelled O
with O
unk O
∅. O

This O
happens O
often O
since O
the O
source O
vocabularies O
of O
our O
models O
tend O
to O
be O
much O
larger O
than O
the O
target O
vocabulary O
since O
a O
large O
source O
vocabulary O
is O
cheap O
. O

This O
limita- O
tion O
motivated O
us O
to O
develop O
an O
annotation O
model O
that O
includes O
the O
complete O
alignments O
between O
the O
source O
and O
the O
target O
sentences O
, O
which O
is O
straight- O
forward O
to O
obtain O
since O
the O
complete O
alignments O
are O
available O
at O
training O
time O
. O

Speciﬁcally O
, O
we O
return O
to O
using O
only O
a O
single O
universal O
unk O
token O
. O

However O
, O
on O
the O
target O
side O
, O
we O
insert O
a O
positional O
token O
pdafter O
ev- O
ery O
word O
. O

Here O
, O
dindicates O
a O
relative O
position O
( O
d=−7 O
, O
. O
. O
. O

, O
−1,0,1 O
, O
. O
. O

. O
, O
7 O
) O
to O
denote O
that O
a O
tar- O
get O
word O
at O
position O
jis O
aligned O
to O
a O
source O
word O
at O
position O
i O
= O
j−d O
. O

Aligned O
words O
that O
are O
too O
far O
apart O
are O
considered O
unaligned O
, O
and O
unaligned O
words O

rae O
annotated O
with O
a O
null O
token O
pn O
. O

Our O
an- O
notation O
is O
illustrated O
in O
Figure O
3 O
. O
3.3 O
Positional B-MethodName
Unknown I-MethodName
Model O
( O
PosUnk B-MethodName
) O

The O
main O
weakness O
of O
the O
PosAll B-MethodName
model O
is O
that O
it O
doubles O
the O
length O
of O
the O
target O
sentence O
. O

This O
makes O
learning O
more O
difﬁcult O
and O
slows O
the O
speed O
of O
parameter O
updates O
by O
a O
factor O
of O
two O
. O

How- O
ever O
, O
given O
that O
our O
post O
- O
processing O
step O
is O
con- O
cerned O
only O
with O
the O
alignments O
of O
the O
unknown O
words O
, O
so O
it O
is O
more O
sensible O
to O
only O
annotate O
the O
unknown O
words O
. O

This O
motivates O
our O
positional B-MethodName
un- I-MethodName
known I-MethodName
model O
which O
uses O
unkpos O
dtokens O
( O
for O
d O
in−7 O
, O
. O
. O
. O

, O
7or∅ O
) O
to O
simultaneously O
denote O
( O
a)13 O

the O
fact O
that O
a O
word O
is O
unknown O
and O
( O
b O
) O
its O
rela- O
tive O
position O
dwith O
respect O
to O
its O
aligned O
source O
word O
. O

Like O
the O
PosAll O
model O
, O
we O
use O
the O
symbol O
unkpos O
∅for O
unknown O
target O
words O
that O
do O
not O
have O
an O
alignment O
. O

We O
use O
the O
universal O
unk O
for O
all O
unknown O
tokens O
in O
the O
source O
language O
. O

See O
Figure O
4 O
for O
an O
annotated O
example O
. O

It O
is O
possible O
that O
despite O
its O
slower O
speed O
, O
the O
PosAll B-MethodName
model O
will O
learn O
better O
alignments O
because O
it O
is O
trained O
on O
many O
more O
examples O
of O
words O
and O
their O
alignments O
. O

However O
, O
we O
show O
that O
this O
is O
not O
the O
case O
( O
see O
§ O
5.2 O
) O
. O

4 O
Experiments O
We O
evaluate O
the O
effectiveness O
of O
our O
OOV O
mod- O
els O
on O
the O
WMT’14 B-DatasetName
English I-DatasetName
- I-DatasetName
to I-DatasetName
- I-DatasetName
French I-DatasetName
translation O
task O
. O

Translation O
quality O
is O
measured O
with O
the O
BLEU B-MetricName
metric O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
on O
the O
new- B-DatasetName
stest2014 I-DatasetName
test O
set O
( O
which O
has O
3003 O
sentences O
) O
. O

4.1 O
Training O
Data O
To O
be O
comparable O
with O
the O
results O
reported O
by O
pre- O
vious O
work O
on O
neural O
machine O
translation O
systems O
( O
Sutskever O
et O
al O
. O
, O
2014 O
; O
Cho O
et O
al O
. O
, O
2014 O
; O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
, O
we O
train O
our O
models O
on O
the O
same O
training O
data O
of O
12 O
M O
parallel O
sentences O
( O
348 O
M O
French O
and O
304 O
M O
English O
words O
) O
, O
obtained O
from O
( O
Schwenk O
, O
2014 O
) O
. O

The O
12 O
M O
subset O
was O
selected O
from O
the O
full O
WMT’14 B-DatasetName
parallel O
corpora O
using O
the O
method O
proposed O
in O
Axelrod O
et O
al O
. O

( O
2011 O
) O
. O

Due O
to O
the O
computationally O
intensive O
nature O
of O
the O
naive O
softmax O
, O
we O
limit O
the O
French O
vocabulary O
( O
thetarget O
language O
) O
to O
the O
either O
the O
40 O
K O
or O
the O
80 O
K O
most O
frequent O
French O
words O
. O

On O
the O
source O
side O
, O
we O
can O
afford O
a O
much O
larger O
vocabulary O
, O
so O
we O
use O
the O
200 O
K O
most O
frequent O
English O
words O
. O

The O
model O
treats O
all O
other O
words O
as O
unknowns.3 O
We O
annotate O
our O
training O
data O
using O
the O
three O
schemes O
described O
in O
the O
previous O
section O
. O

The O
alignment O
is O
computed O
with O
the O
Berkeley O
aligner O
( O
Liang O
et O
al O
. O
, O
2006 O
) O
using O
its O
default O
settings O
. O

We O
discard O
sentence O
pairs O
in O
which O
the O
source O
or O
the O
target O
sentence O
exceed O
100 O
tokens O
. O

4.2 O
Training O
Details O
Our O
training O
procedure O
and O
hyperparameter O
choices O
are O
similar O
to O
those O
used O
by O
Sutskever O
et O
al O
. O

( O
2014 O
) O
. O

In O
more O
details O
, O
we O
train O
multi O
- O
layer O
deep O
LSTMs O
, O
each O
of O
which O
has O
1000 B-HyperparameterValue
cells O
, O
with O
1000 B-HyperparameterValue
dimensional O
embeddings O
. O

Like O
Sutskever O
et O
al O
. O

( O
2014 O
) O
, O
we O
reverse O
the O
words O
in O
the O
source O
sen- O
tences O
which O
has O
been O
shown O
to O
improve O
LSTM O
memory O
utilization O
and O
results O
in O
better O
transla- O
tions O
of O
long O
sentences O
. O

Our O
hyperparameters O
can O
be O
summarized O
as O
follows O
: O
( O
a O
) O
the O
parameters B-HyperparameterName
are O
initialized O
uniformly O
in O
[ B-HyperparameterValue
-0.08 I-HyperparameterValue
, I-HyperparameterValue
0.08 I-HyperparameterValue
] I-HyperparameterValue
for O
4 B-HyperparameterValue
- O
layer O
models O
and O
[ B-HyperparameterValue
-0.06 I-HyperparameterValue
, I-HyperparameterValue
0.06 I-HyperparameterValue
] I-HyperparameterValue
for O
6 B-HyperparameterValue
- O
layer O
models O
, O
( O
b O
) O
SGD O
has O
a O
ﬁxed O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0.7 B-HyperparameterValue
, O
( O
c O
) O
we O
train O
for O
8 O
epochs O
( O
after O
5 O
epochs O
, O
we O
begin O
to O
halve O
the O
learning O
rate O
every O
0.5 O
epoch O
) O
, O
( O
d O
) O
the O
size B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
mini I-HyperparameterName
- I-HyperparameterName
batch I-HyperparameterName
is O
128 B-HyperparameterValue
, O
and O
( O
e O
) O
we O
rescale O
the O
normalized O
gradient O
to O
ensure O
that O
its O
norm B-HyperparameterName
does O
not O
exceed O
5 B-HyperparameterValue
( O
Pascanu O
et O
al O
. O
, O
2012 O
) O
. O

We O
also O
follow O
the O
GPU O
parallelization O
scheme O
proposed O
in O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
, O
allowing O
us O
to O
reach O
a O
training O
speed O
of O
5.4 O
K O
words O
per O
sec- O
ond O
to O
train O
a O
depth-6 O
model O
with O
200 O
K O
source O
and O
80 O
K O
target O
vocabularies O
; O
whereas O
Sutskever O
et O
al O
. O

( O
2014 O
) O
achieved O
6.3 O
K O
words O
per O
second O
for O
a O
depth-4 O
models O
with O
80 O
K O
source O
and O
target O
vo- O
cabularies O
. O

Training O
takes O
about O
10 O
- O
14 O
days O
on O
an O
8 O
- O
GPU O
machine O
. O

4.3 O
A O
note O
on O
BLEU B-MetricName
scores O
We O
report O
BLEU B-MetricName
scores O
based O
on O
both O
: O
( O
a O
) O
detok- O
enized O
translations O
, O
i.e. O
, O
WMT’14 O
style O
, O
to O
be O
com- O
parable O
with O
results O
reported O
on O
the O
WMT O
web- O
site4and O

( O
b O
) O
tokenized O
translations O
, O
so O
as O
to O
be O
consistent O
with O
previous O
work O
( O
Cho O
et O
al O
. O
, O
2014 O
; O
Bahdanau O
et O
al O
. O
, O
2015 O
; O
Schwenk O
, O
2014 O
; O
Sutskever O
et O
al O
. O
, O
2014 O
; O
Jean O
et O
al O
. O
, O
2015).5 O

The O
existing O
WMT’14 O
state O
- O
of O
- O
the O
- O
art O
system O
( O
Durrani O
et O
al O
. O
, O
2014 O
) O
achieves O
a O
detokenized B-MetricName
BLEU I-MetricName
score O
of O
35.8 B-MetricValue
on O
the O
newstest2014 B-DatasetName
test O
set O
for O
English O
to O
French O
language O
pair O
( O
see O
Table O
2 O
) O
. O

In O
terms O
of O
the O
tokenized B-MetricName
BLEU I-MetricName
, O
its O
performance O
is O
37.0 B-MetricValue
points O
( O
see O
Table O
1 O
) O
. O

4.4 O
Main O
Results O
We O
compare O
our O
systems O
to O
others O
, O
including O
the O
current O
state O
- O
of O
- O
the O
- O
art O
MT O
system O
( O
Durrani O
et O
al O
. O
, O
2014 O
) O
, O
recent O
end O
- O
to O
- O
end O
neural O
systems O
, O
as O
well O
as O
phrase O
- O
based O
baselines O
with O
neural O
com- O
ponents O
. O

The O
results O
shown O
in O
Table O
1 O
demonstrate O
that O
our O
unknown O
word O
translation O
technique O
( O
in O
par- O
ticular O
, O
the O
PosUnk B-MethodName
model O
) O
signiﬁcantly O
improves O
the O
translation O
quality O
for O
both O
the O
individual O
( O
non- O
ensemble O
) O
LSTM O
models O
and O
the O
ensemble O
mod O
- O
els.6For O
40K O
- O
word O
vocabularies O
, O
the O
performance O
gains O
are O
in O
the O
range O
of O
2.3 B-MetricValue
- I-MetricValue
2.8 I-MetricValue
BLEU B-MetricName
points O
. O

With O
larger O
vocabularies O
( O
80 O
K O
) O
, O
the O
performance O
gains O
are O
diminished O
, O
but O
our O
technique O
can O
still O
provide O
a O
nontrivial O
gains O
of O
1.6 B-MetricValue
- I-MetricValue
1.9 I-MetricValue
BLEU B-MetricName
points O
. O

It O
is O
interesting O
to O
observe O
that O
our O
approach O
is O
more O
useful O
for O
ensemble O
models O
as O
compared O
to O
the O
individual O
ones O
. O

This O
is O
because O
the O
useful- O
ness O
of O
the O
PosUnk B-MethodName
model O
directly O
depends O
on O
the O
ability O
of O
the O
NMT O
to O
correctly O
locate O
, O
for O
a O
given O
OOV O
target O
word O
, O
its O
corresponding O
word O
in O
the O
source O
sentence O
. O

An O
ensemble O
of O
large O
models O
identiﬁes O
these O
source O
words O
with O
greater O
accu- O
racy O
. O

This O
is O
why O
for O
the O
same O
vocabulary O
size O
, O
better O
models O
obtain O
a O
greater O
performance O
gain O
our O
post O
- O
processing O
step O
. O

e O

Except O
for O
the O
very O
re- O
cent O
work O
of O
Jean O
et O
al O
. O

( O
2015 O
) O
that O
employs O
a O
sim- O
ilar O
unknown O
treatment O
strategy7as O
ours O
, O
our O
best O
result O
of O
37.5 B-MetricValue
BLEU B-MetricName
outperforms O
all O
other O
NMT O
systems O
by O
a O
arge O
margin O
, O
and O
more O
importanly O
, O
our O
system O
has O
established O
a O
new O
record O
on O
the O
WMT’14 B-DatasetName
English I-DatasetName
to I-DatasetName
French I-DatasetName
translation O
. O

5 O
Analysis O
We O
analyze O
and O
quantify O
the O
improvement O
ob- O
tained O
by O
our O
rare O
word O
translation O
approach O
and O
provide O
a O
detailed O
comparison O
of O
the O
different O
rare O
word O
techniques O
proposed O
in O
Section O
3 O
. O

We O
also O
examine O
the O
effect O
of O
depth O
on O
the O
LSTM O
architectures O
and O
demonstrate O
a O
strong O
correla- O
tion O
between O
perplexities O
and O
BLEU B-MetricName
scores O
. O

We O
also O
highlight O
a O
few O
translation O
examples O
where O
our O
models O
succeed O
in O
correctly O
translating O
OOV O
words O
, O
and O
present O
several O
failures O
. O

5.1 O
Rare O
Word O
Analysis O
To O
analyze O
the O
effect O
of O
rare O
words O
on O
translation O
quality O
, O
we O
follow O
Sutskever O
et O
al O
. O

( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
and O
sort O
sentences O
in O
newstest2014 B-DatasetName
by O
the O
average O
inverse O
frequency O
of O
their O
words O
. O

We O
split O
the O
test O
sentences O
into O
groups O
where O
the O
sentences O
within O
each O
group O
have O
a O
comparable O
number O
of O
rare O
words O
and O
evaluate O
each O
group O
independently O
. O

We O
evaluate O
our O
systems O
before O
and O
after O
translat- O

ing O
the O
OOV O
words O
and O
compare O
with O
the O
stan- O
dard O
MT O
systems O
– O
we O
use O
the O
best O
system O
from O
the O
WMT’14 O
contest O
( O
Durrani O
et O
al O
. O
, O
2014 O
) O
, O
and O
neural O
MT O
systems O
– O
we O
use O
the O
ensemble O
systems O
described O
in O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
and O
Section O
4 O
. O

Rare B-TaskName
word I-TaskName
translation I-TaskName
is O
challenging O
for O
neural O
machine O
translation O
systems O
as O
shown O
in O
Figure O
5 O
. O

Speciﬁcally O
, O
the O
translation O
quality O
of O
our O
model O
before O
applying O
the O
postprocessing O
step O
is O
shown O
by O
the O
green O
curve O
, O
and O
the O
current O
best O
NMT O
sys- O
tem O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
is O
the O
purple O
curve O
. O

While O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
produces O
better O
translations O
for O
sentences O
with O
frequent O
words O
( O
the O
left O
part O
of O
the O
graph O
) O
, O
they O
are O
worse O
than O
best O
system O
( O
red O
curve O
) O
on O
sentences O
with O
many O
rare O
words O
( O
the O
right O
side O
of O
the O
graph O
) O
. O

When O
applying O
our O
unknown O
word O
translation O
technique O
( O
purple O
curve O
) O
, O
we O
signiﬁcantly O
improve O
the O
translation O
quality O
of O
our O
NMT O
: O
for O
the O
last O
group O
of O
500 O
sen- O
tences O
which O
have O
the O
greatest O
proportion O
of O
OOV O
words O
in O
the O
test O
set O
, O
we O
increase O
the O
BLEU B-MetricName
score O
of O
our O
system O
by O
4.8 B-MetricValue
BLEU B-MetricName
points O
. O

Overall O
, O
our O
rare O
word O
translation O
model O
interpolates O
between O
the O
SOTA O
system O
and O
the O
system O
of O
Sutskever O
et O
al O
. O

( O
2014 O
) O
, O
which O
allows O
us O
to O
outperform O
the O
win- O
ning O
entry O
of O
WMT’14 O
on O
sentences O
that O
consist O
predominantly O
of O
frequent O
words O
and O
approach O
its O
performance O
on O
sentences O
with O
many O
OOV O
words O
. O

5.2 O
Rare O
Word O
Models O
We O
examine O
the O
effect O
of O
the O
different O
rare O
word O
models O
presented O
in O
Section O
3 O
, O
namely O
: O
( O
a O
) O
Copy- B-MethodName
able I-MethodName
– O
which O
aligns O
the O
unknown O
words O
on O
both O
the O
input O
and O
the O
target O
side O
by O
learning O
to O
copy O
in- O
dices O
, O
( O
b O
) O
the O
Positional B-MethodName
All I-MethodName
( O
PosAll B-MethodName
) O
– O
which O
pre- O
dicts O
the O
aligned O
source O
positions O
for O
every O
target O
word O
, O
and O
( O
c O
) O
the O
Positional B-MethodName
Unknown I-MethodName
( O
PosUnk B-MethodName
) O
– O
which O
predicts O
the O
aligned O
source O
positions O
for O
only O
the O
unknown O
target O
words.8It O
is O
also O
interest- O
ing O
to O
measure O
the O
improvement O
obtained O
when O
no O
alignment O
information O
is O
used O
during O
training O
. O

As O
such O
, O
we O
include O
a O
baseline O
model O
with O
no O
align- O
ment O
knowledge O
( O
NoAlign B-MethodName
) O
in O
which O
we O
simply O
as- O
sume O
that O
the O
ithunknown O
word O
on O
the O
target O
sen- O
tence O
is O
aligned O
to O
the O
ithunknown O
word O
in O
the O
source O
sentence O
. O

From O
the O
results O
in O
Figure O
6 O
, O
a O
simple O
mono- O
tone O
alignment O
assumption O
for O
the O
NoAlign B-MethodName
model O
yields O
a O
modest O
gain O
of O
0.8 B-MetricValue
BLEU B-MetricName
points O
. O

If O
we O
train O
the O
model O
to O
predict O
the O
alignment O
, O
then O
the O
Copyable B-MethodName
model O
offers O
a O
slightly O
better O
gain O
of O
1.0 B-MetricValue
BLEU B-MetricName
. O

Note O
, O
however O
, O
that O
English O
and O
French O
have O
similar O
word O
order O
structure O
, O
so O
it O
would O
be O
interesting O
to O
experiment O
with O
other O
language O
pairs O
, O
such O
as O
English O
and O
Chinese O
, O
in O
which O
the O
word O
order O
is O
not O
as O
monotonic O
. O

These O
harder O
lan- O
guage O
pairs O
potentially O
imply O
a O
smaller O
gain O
for O
the O
NoAlign B-MethodName
model O
and O
a O
larger O
gain O
for O
the O
Copyable B-MethodName
model O
. O

We O
leave O
it O
for O
future O
work O
. O

The O
positional O
models O
( O
PosAll B-MethodName
and O
PosUnk B-MethodName
) O
im- O
prove O
translation O
performance O
by O
more O
than O
2 B-MetricValue
BLEU B-MetricName
points O
. O

This O
proves O
that O
the O
limitation O
of O
the O
copyable B-MethodName
model O
, O
which O
forces O
it O
to O
align O
each O
un- O
known O
output O
word O
with O
an O
unknown O
input O
word O
, O
is O
considerable O
. O

In O
contrast O
, O
the O
positional O
mod- O
els O
can O
align O
the O
unknown O
target O
words O
with O
any O
source O
word O
, O
and O
as O
a O
result O
, O
post O
- O
processing O
has O
a O
much O
stronger O
effect O
. O

The O
PosUnk B-MethodName
model O
achieves O
better O
translation O
results O
than O
the O
PosAll B-MethodName
model O
which O
suggests O
that O
it O
is O
easier O
to O
train O
the O
LSTM O
on O
shorter O
sequences O
. O

5.3 O
Other O
Effects O
Deep O
LSTM O
architecture O
– O
We O
compare O
PosUnk B-MethodName
models O
trained O
with O
different O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
( O
3 B-HyperparameterValue
, O
4 B-HyperparameterValue
, O
and O
6 B-HyperparameterValue
) O
. O

We O
observe O
that O
the O
gain O
obtained O
by O
the O
PosUnk B-MethodName
model O
increases O
in O
tandem O
with O
the O
overall O
accuracy O
of O
the O
model O
, O
which O
is O
consistent O
with O
the O
idea O
that O
larger O
models O
can O
point O
to O
the O
ap- O
propriate O
source O
word O
more O
accurately O
. O

Addition- O
ally O
, O
we O
observe O
that O
on O
average O
, O
each O
extra O
LSTM O
layer O
provides O
roughly O
1.0 B-MetricValue
BLEU B-MetricName
point O
improve- O
ment O
as O
demonstrated O
in O
Figure O
7 O
. O

Perplexity O
and O
BLEU B-MetricName
– O

Lastly O
, O
we O
ﬁnd O
it O
inter- O
esting O
to O
observe O
a O
strong O
correlation O
between O
the O
perplexity O
( O
our O
training O
objective O
) O
and O
the O
transla- O
tion O
quality O
as O
measured O
by O
BLEU B-MetricName
. O

Figure O
8 O
shows O
the O
performance O
of O
a O
4 O
- O
layer O
LSTM O
, O
in O
which O
we O
compute O
both O
perplexity O
and O
BLEU B-MetricName
scores O
at O
dif- O
ferent O
points O
during O
training O
. O

We O
ﬁnd O
that O
on O
aver- O
age O
, O
a O
reduction O
of O
0.5 O
perplexity O
gives O
us O
roughly O
1.0 B-MetricValue
BLEU B-MetricName
point O
improvement. O

5.4 O
Sample O
Translations O
We O
present O
three O
sample O
translations O
of O
our O
best O
system O
( O
with O
37.5 B-MetricValue
BLEU B-MetricName
) O
in O
Table O
3 O
. O

In O
our O
ﬁrst O
example O
, O
the O
model O
translates O
all O
the O
un- O
known O
words O
correctly O
: O
2600 O
, O
orthop O
´ O
ediques O
, O
and O
cataracte O
. O

It O
is O
interesting O
to O
observe O
that O
the O
model O
can O
accurately O
predict O
an O
alignment O
of O
dis- O
tances O
of O
5 O
and O
6 O
words O
. O

The O
second O
example O
highlights O
the O
fact O
that O
our O
model O
can O
translate O
long O
sentences O
reasonably O
well O
and O
that O
it O
was O
able O
to O
correctly O
translate O
the O
unknown O
word O
for O
JP- O
Morgan O
at O
the O
very O
far O
end O
of O
the O
source O
sentence O
. O

Lastly O
, O
our O
examples O
also O
reveal O
several O
penalties O
incurred O
by O
our O
model O
: O
( O
a O
) O
incorrect O
entries O
in O
the O
word O
dictionary O
, O
as O
with O
n´egociateur O
vs.trader O
in O
the O
second O
example O
, O
and O
( O
b O
) O
incorrect O
alignment O
prediction O
, O
such O
as O
when O
unkpos O
3is O
incorrectlyaligned O
with O
the O
source O
word O
was O
and O
not O
with O
abandoning O
, O
which O
resulted O
in O
an O
incorrect O
trans- O
lation O
in O
the O
third O
sentence O
. O

6 O
Conclusion O
We O
have O
shown O
that O
a O
simple O
alignment O
- O
based O
technique O
can O
mitigate O
and O
even O
overcome O
one O
of O
the O
main O
weaknesses O
of O
current O
NMT O
systems O
, O
which O
is O
their O
inability O
to O
translate O
words O
that O
are O
not O
in O
their O
vocabulary O
. O

A O
key O
advantage O
of O
our O
technique O
is O
the O
fact O
that O
it O
is O
applicable O
to O
any O
NMT O
system O
and O
not O
only O
to O
the O
deep O
LSTM O
model O
of O
Sutskever O
et O
al O
. O

( O
2014 O
) O
. O

A O
technique O
like O
ours O
is O
likely O
necessary O
if O
an O
NMT O
system O
is O
to O
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
machine O
translation O
. O

We O
have O
demonstrated O
empirically O
that O
on O
the18 O

WMT’14 B-DatasetName
English I-DatasetName
- I-DatasetName
French I-DatasetName
translation O
task O
, O
our O
technique O
yields O
a O
consistent O
and O
substantial O
im- O
provement O
of O
up O
to O
2.8 B-MetricValue
BLEU B-MetricName
points O
over O
various O
NMT O
systems O
of O
different O
architectures O
. O

Most O
im- O
portantly O
, O
with O
37.5 B-MetricValue
BLEU B-MetricName
points O
, O
we O
have O
estab- O
lished O
the O
ﬁrst O
NMT O
system O
that O
outperformed O
the O
best O
MT O
system O
on O
a O
WMT’14 B-DatasetName
contest O
dataset O
. O

Acknowledgments O
We O
thank O
members O
of O
the O
Google O
Brain O
team O
for O
thoughtful O
discussions O
and O
insights O
. O

The O
ﬁrst O
author O
especially O
thanks O
Chris O
Manning O
and O
the O
Stanford O
NLP O
group O
for O
helpful O
comments O
on O
the O
early O
drafts O
of O
the O
paper O
. O

Lastly O
, O
we O
thank O
the O
an- O
nonymous O
reviewers O
for O
their O
valuable O
feedback O
. O

References O
Amittai O
Axelrod O
, O
Xiaodong O
He O
, O
and O
Jianfeng O
Gao O
. O
2011 O
. O

Domain O
adaptation O
via O
pseudo O
in O
- O
domain O
data O
selection O
. O

In O
EMNLP O
. O

D. O
Bahdanau O
, O
K. O
Cho O
, O
and O
Y O
. O

Bengio O
. O

2015 O
. O

Neural O
machine O
translation O
by O
jointly O
learning O
to O
align O
and O
translate O
. O

In O
ICLR O
. O

D. O
Cer O
, O
M. O
Galley O
, O
D. O
Jurafsky O
, O
and O
C. O
D. O
Manning O
. O

2010 O
. O

Phrasal O
: O
A O
statistical O
machine O
translation O
toolkit O
for O
exploring O
new O
model O
features O
. O

In O
ACL O
, O
Demonstration O
Session O
. O

David O
Chiang O
. O

2007 O
. O

Hierarchical O
phrase O
- O
based O
trans- O
lation O
. O

Computational O
Linguistics O
, O
33(2):201–228 O
. O

Kyunghyun O
Cho O
, O
Bart O
van O
Merrienboer O
, O
Caglar O
Gul- O
cehre O
, O
Fethi O
Bougares O
, O
Holger O
Schwenk O
, O
and O
Yoshua O
Bengio O
. O

2014 O
. O

Learning O
phrase O
representations O
using O
rnn O
encoder O
- O
decoder O
for O
statistical O
machine O
translation O
. O

In O
EMNLP O
. O

Nadir O
Durrani O
, O
Barry O
Haddow O
, O
Philipp O
Koehn O
, O
and O
Kenneth O
Heaﬁeld O
. O

2014 O
. O

Edinburgh O
’s O
phrase O
- O
based O
machine O
translation O
systems O
for O
WMT-14 O
. O

In O
WMT O
. O

Chris O
Dyer O
, O
Jonathan O
Weese O
, O
Hendra O
Setiawan O
, O
Adam O
Lopez O
, O
Ferhan O
Ture O
, O
Vladimir O
Eidelman O
, O
Juri O
Gan- O
itkevitch O
, O
Phil O
Blunsom O
, O
and O
Philip O
Resnik O
. O

2010 O
. O

cdec O
: O
A O
decoder O
, O
alignment O
, O
and O
learning O
framework O
for O
ﬁnite O
- O
state O
and O
context O
- O
free O
translation O
models O
. O

InACL O
, O
Demonstration O
Session O
. O

A. O
Graves O
, O
G. O
Wayne O
, O
and O
I. O
Danihelka O
. O

2014 O
. O

Neural O
turing O
machines O
. O

arXiv O
preprint O
arXiv:1410.5401 O
. O

A. O
Graves O
. O

2013 O
. O

Generating O
sequences O
with O
recurrent O
neural O
networks O
. O

In O
Arxiv O
preprint O
arXiv:1308.0850 O
. O

S O
´ O
ebastien O
Jean O
, O
Kyunghyun O
Cho O
, O
Roland O
Memisevic O
, O
and O
Yoshua O
Bengio O
. O

2015 O
. O

On O
using O
very O
large O
tar- O
get O
vocabulary O
for O
neural O
machine O
translation O
. O

In O
ACL.N. O
Kalchbrenner O
and O
P. O
Blunsom O
. O

2013 O
. O

Recurrent O
continuous O
translation O
models O
. O

In O
EMNLP O
. O

Philipp O
Koehn O
, O
Franz O
Josef O
Och O
, O
and O
Daniel O
Marcu O
. O

2003 O
. O

Statistical O
phrase O
- O
based O
translation O
. O

In O
NAACL O
. O

Philipp O
Koehn O
, O
Hieu O
Hoang O
, O
Alexandra O
Birch O
, O
Chris O
Callison O
- O
Burch O
, O
Marcello O
Federico O
, O
Nicola O
Bertoldi O
, O
Brooke O
Cowan O
, O
Wade O
Shen O
, O
Christine O
Moran O
, O
Richard O
Zens O
, O
et O
al O
. O
2007 O
. O

Moses O
: O
Open O
source O
toolkit O
for O
statistical O
machine O
translation O
. O

In O
ACL O
, O
Demonstration O
Session O
. O

P. O
Liang O
, O
B. O
Taskar O
, O
and O
D. O
Klein O
. O

2006 O
. O

Alignment O
by O
agreement O
. O

In O
NAACL O
. O

Kishore O
Papineni O
, O
Salim O
Roukos O
, O
Todd O
Ward O
, O
and O
Wei O
jing O
Zhu O
. O
2002 O
. O

BLEU O
: O
a O
method O
for O
automatic O
evaluation O
of O
machine O
translation O
. O

In O
ACL O
. O

R. O
Pascanu O
, O
T. O
Mikolov O
, O
and O
Y O
. O

Bengio O
. O

2012 O
. O

On O
the O
difﬁculty O
of O
training O
recurrent O
neural O
networks O
. O

arXiv O
preprint O
arXiv:1211.5063 O
. O

H. O
Schwenk O
. O

2014 O
. O

University O
le O
mans O
. O

http://www-lium.univ-lemans.fr/ O
˜schwenk O
/ O
cslm_joint_paper/ O
. O

[ O
Online O
; O
accessed O
03 O
- O
September-2014 O
] O
. O

I. O
Sutskever O
, O
O. O
Vinyals O
, O
and O
Q. O
V O
. O

Le O
. O
2014 O
. O

Sequence O
to O
sequence O
learning O
with O
neural O
networks O
. O

In O
NIPS O
. O

Wojciech O
Zaremba O
, O
Ilya O
Sutskever O
, O
and O
Oriol O
Vinyals O
. O
2015 O
. O

Recurrent O
neural O
network O
regularization O
. O

In O
ICLR O
.19 O

Neural B-TaskName
Relation I-TaskName
Extraction I-TaskName
with O
Multi O
- O
lingual O
Attention O
Yankai O
Lin1 O
, O
Zhiyuan O
Liu1∗ O
, O
Maosong O
Sun1,2 O
1Department O
of O
Computer O
Science O
and O
Technology O
, O
State O
Key O
Lab O
on O
Intelligent O
Technology O
and O
Systems O
, O
National O
Lab O
for O
Information O
Science O
and O
Technology O
, O
Tsinghua O
University O
, O
Beijing O
, O
China O
2Jiangsu O
Collaborative O
Innovation O
Center O
for O
Language O
Competence O
, O
Jiangsu O
, O
China O
Abstract O
Relation B-TaskName
extraction I-TaskName
has O
been O
widely O
used O
for O
ﬁnding O
unknown O
relational O
facts O
from O
the O
plain O
text O
. O

Most O
existing O
methods O
fo- O
cus O
on O
exploiting O
mono O
- O
lingual O
data O
for O
relation O
extraction O
, O
ignoring O
massive O
in- O
formation O
from O
the O
texts O
in O
various O
lan- O
guages O
. O

To O
address O
this O
issue O
, O
we O
intro- O
duce O
a O
multi O
- O
lingual O
neural O
relation O
ex- O
traction O
framework O
, O
which O
employs O
mono- O
lingual O
attention O
to O
utilize O
the O
information O
within O
mono O
- O
lingual O
texts O
and O

further O
pro- O
poses O
cross O
- O
lingual O
attention O
to O
consider O
the O
information O
consistency O
and O
comple- O
mentarity O
among O
cross O
- O
lingual O
texts O
. O

Ex- O
perimental O
results O
on O
real O
- O
world O
datasets O
show O
that O
our O
model O
can O
take O
advan- O
tage O
of O
multi O
- O
lingual O
texts O
and O
consistently O
achieve O
signiﬁcant O
improvements O
on O
re- B-TaskName
lation I-TaskName
extraction I-TaskName
as O
compared O
with O
base- O
lines O
. O

The O
source O
code O
of O
this O
paper O
can O
be O
obtained O
from O
https://github O
. O
com O
/ O
thunlp O
/ O
MNRE O
1 O
Introduction O
People O
build O
many O
large O
- O
scale O
knowledge O
bases O
( O
KBs O
) O
to O
store O
structured O
knowledge O
about O
the O
real O
world O
, O
such O
as O
Wikidata1and O
DBpedia2 O
. O

KBs O
are O
playing O
an O
important O
role O
in O
many O
AI O
and O
NLP O
applications O
such O
as O
information O
retrieval O
and O
question O
answering O
. O

The O
facts O
in O
KBs O
are O
typically O
organized O
in O
the O
form O
of O
triplets O
, O
e.g. O
, O
( O
New O
York O
, O
CityOf O
, O
United O
States O
) O
. O

Since O
ex- O
isting O
KBs O
are O
far O
from O
complete O
and O
new O
facts O
are O
growing O
inﬁnitely O
, O
meanwhile O
manual O
anno- O
tation O
of O
these O
knowledge O
is O
time O
- O
consuming O
and O
human-intensive O
, O
many O
works O
have O
been O
devoted O
to O
automated O
extraction O
of O
novel O
facts O
from O
vari- O
ous O
Web O
resources O
, O
where O
relation B-TaskName
extraction I-TaskName
( O
RE B-TaskName
) O
from O
plain O
texts O
is O
one O
the O
most O
important O
knowl- O
edge O
sources O
. O

Among O
various O
methods O
for O
relation B-TaskName
extraction I-TaskName
, O
distant O
supervision O
is O
the O
most O
promising O
approach O
( O
Mintz O
et O
al O
. O
, O
2009 O
; O
Riedel O
et O
al O
. O
, O
2010 O
; O
Hoffmann O
et O
al O
. O
, O
2011 O
; O
Surdeanu O
et O
al O
. O
, O
2012 O
) O
, O
which O
can O
au- O
tomatically O
generate O
training O
instances O
via O
aligning O
KBs O
and O
texts O
to O
address O
the O
issue O
of O
lacking O
super- O
vised O
data O
. O

As O
the O
development O
of O
deep O
learning O
, O
Zeng O
et O
al O
. O
( O
2015 O
) O
introduce O
neural O
networks O
to O
ex- O
tract O
relations O
with O
automatically O
learned O
features O
from O
training O
instances O
. O

To O
address O
the O
wrong O
labelling O
issue O
of O
distant O
supervision O
data O
, O
Lin O
et O
al O
. O

( O
2016 O
) O
further O
employ O
sentence O
- O
level O
atten- O
tion O
mechanism O
in O
neural B-TaskName
relation I-TaskName
extraction I-TaskName
, O
and O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
. O

However O
, O
most O
RE O
systems O
concentrate O
on O
ex- O
tracting O
relational O
facts O
from O
mono O
- O
lingual O
data O
. O

In O
fact O
, O
people O
describe O
knowledge O
about O
the O
world O
using O
various O
languages O
. O

And O
people O
speaking O
different O
languages O
also O
share O
similar O
knowledge O
about O
the O
world O
due O
to O
the O
similarities O
of O
human O
experiences O
and O
human O
cognitive O
systems O
. O

For O
in- O
stance O
, O
though O
New O
York O
andUnited O
States O
are O
ex- O
pressed O
as O
纽约 O
and美国 O
respectively O
in O
Chinese O
, O
both O
Americans O
and O
Chinese O
share O
the O
fact O
that O
“ O
New O
York O
is O
a O
city O
of O
USA O
. O
” O

It O
is O
straightforward O
to O
build O
mono O
- O
lingual O
RE O
systems O
separately O
for O
each O
single O
language O
. O

But O
if O
so O
, O
it O
wo O
n’t O
be O
able O
to O
take O
full O
advantage O
of O
di- O
verse O
information O
hidden O
in O
the O
data O
of O
various O
lan- O
guages O
. O

Multi O
- O
lingual O
data O
will O
beneﬁt O
relation O
ex- O

traction O
for O
the O
following O
two O
reasons O
: O
1 O
. O
Consis- O

tency O
. O

According O
to O
the O
distant O
supervision O
data O
in O
our O
experiments3 O
, O
we O
ﬁnd O
that O
over O
half O
of O
Chinese O
and O
English O
sentences O
are O
longer O
than O
20words O
, O
in O
which O
only O
several O
words O
are O
related O
to O
the O
re- O
lational O
facts O
. O

Take O
Table O
1for O
example O
. O

The O
ﬁrst O
Chinese O
sentence O
has O
over O
20words O
, O
in O
which O
only O
“ O
纽约 O
” O
( O
New O
York O
) O
and O
“ O
ᱥ美国ㅢжཝค O
ᐸ O
” O
( O
is O
the O
biggest O
city O
in O
the O
United O
States O
) O
ac- O
tually O
directly O
reﬂect O
the O
relational O
fact O
CityOf O
. O

It O
is O
thus O
non O
- O
trivial O
to O
locate O
and O
learn O
these O
rela- O
tional O
patterns O
from O
complicated O
sentences O
for O
re- B-TaskName
lation I-TaskName
extraction I-TaskName
. O

Fortunately O
, O
a O
relational O
fact O
is O
usually O
expressed O
with O
certain O
patterns O
in O
various O
languages O
, O
and O
the O
correspondence O
of O
these O
pat- O
terns O
among O
languages O
is O
substantially O
consistent O
. O

The O
pattern O
consistency O
among O
languages O
provides O
us O
augmented O
clues O
to O
enhance O
relational O
pattern O
learning O
for O
relation B-TaskName
extraction I-TaskName
. O

2 O
. O
Complementarity O
. O

From O
our O
experiment O
data O
, O
we O
also O
ﬁnd O
that O
42.2 O
% O
relational O
facts O
in O
English O
data O
and O
41.6 O
% O
ones O
in O
Chinese O
data O
are O
unique O
. O

Moreover O
, O
for O
nearly O
half O
of O
relations O
, O
the O
number O
of O
sentences O
expressing O
relational O
facts O
of O
these O
relations O
varies O
a O
lot O
in O
different O
languages O
. O

It O
is O
thus O
straightforward O
that O
the O
texts O
in O
differ- O
ent O
languages O
can O
be O
complementary O
to O
each O
other O
, O
especially O
from O
those O
resource O
- O
rich O
languages O
to O
resource O
- O
poor O
languages O
, O
and O
improve O
the O
overall O
performance O
of O
relation O
extraction O
. O

To O
take O
full O
consideration O
of O
these O
issues O
, O
we O
propose O
Multi B-MethodName 
- I-MethodName
lingual I-MethodName
Attention I-MethodName
- I-MethodName
based I-MethodName
Neural I-MethodName
Relation I-MethodName
Extraction I-MethodName
( O
MNRE B-MethodName
) O
. O

We O
ﬁrst O
employ O
a O
convolutional O
neural O
network O
( O
CNN O
) O
to O
embed O
the O
relational O
patterns O
in O
sentences O
into O
real O
- O
valued O
vectors O
. O

Afterwards O
, O
to O
consider O
the O
complemen- O
tarity O
of O
all O
informative O
sentences O
in O
various O
lan- O
guages O
and O
capture O
the O
consistency O
of O
relational O
patterns O
, O
we O
apply O
mono O
- O
lingual O
attention O
to O
select O
the O
informative O
sentences O
within O
each O
language O
and O
propose O
cross O
- O
lingual O
attention O
to O
take O
advan- O
tages O
of O
pattern O
consistency O
and O
complementarity O
among O
languages O
. O

Finally O
, O
we O
classify O
relations O
according O
to O
the O
global O
vector O
aggregated O
from O
all O
sentence O
vectors O
weighted O
by O
mono O
- O
lingual O
atten- O
tion O
and O
cross O
- O
lingual O
attention O
. O

In O
experiments O
, O
we O
build O
training O
instances O
via O
distant O
supervision O
by O
aligning O
Wikidata O
with O
Chi- O
nese O
Baidu O
Baike O
and O
English O
Wikipedia O
articles O
θ O
and O
evaluate O
the O
performance O
of O
relation O
extraction O
in O
both O
English O
and O
Chinese O
. O

The O
experimental O
results O
show O
that O
our O
framework O
achieves O
signif- O
icant O
improvement O
for O
relation O
extraction O
as O
com- O
pared O
to O
all O
baseline O
methods O
including O
both O
mono- O
lingual O
and O
multi O
- O
lingual O
ones O
. O

It O
indicates O
that O
our O
framework O
can O
take O
full O
advantages O
of O
sentences O
in O
different O
languages O
and O
better O
capture O
sophisti- O
cated O
patterns O
expressing O
relations O
. O

2 O
Related O
Work O
Recent O
years O
KBs O
have O
been O
widely O
used O
on O
var- O
ious O
AI O
and O
NLP O
applications O
. O

As O
an O
impor- O
tant O
approach O
to O
enrich O
KBs O
, O
relation B-TaskName
extraction I-TaskName
from O
plain O
text O
has O
attracted O
many O
research O
in- O
terests O
. O

Relation B-TaskName
extraction I-TaskName
typically O
classiﬁes O
each O
entity O
pair O
into O
various O
relation O
types O
ac- O

cording O
to O
supporting O
sentences O
that O
the O
both O
enti- O
ties O
appear O
, O
which O
needs O
human O
- O
labelled O
relation- O
speciﬁc O
training O
instances O
. O

Many O
works O
have O
been O
invested O
to O
relation B-TaskName
extraction I-TaskName
including O
kernel- O
based O
model O
( O
Zelenko O
et O
al O
. O
, O
2003 O
) O
, O
embedding- O
based O
model O
( O
Gormley O
et O

al O
. O
, O
2015 O
) O
, O
CNN O
- O
based O
models O
( O
Zeng O
et O
al O
. O
, O
2014 O
; O
dos O
Santos O
et O
al O
. O
, O
2015 O
) O
, O
and O
RNN O
- O
based O
model O
( O
Socher O
et O
al O
. O
, O
2012 O
) O
. O

Nevertheless O
, O
these O
RE O
systems O
are O
insufﬁ- O
cient O
due O
to O
the O
lack O
of O
training O
data O
. O

To O
ad- O
dress O
this O
issue O
, O
Mintz O
et O
al O
. O

( O
2009 O
) O
align O
plain O
text O
with O
Freebase O
to O
automatically O
generate O
train- O
ing O
instances O
following O
the O
distant O
supervision O
assumption O
. O

To O
further O
alleviate O
the O
wrong O
la- O
belling O
problem O
, O
Riedel O
et O
al O
. O

( O
2010 O
) O
model O
dis- O
tant O
supervision O
for O
relation B-TaskName
extraction I-TaskName
as O
a O
multi- O
instance O
single O
- O
label O
learning O
problem O
, O
and O
Hoff- O
mann O
et O

al O
. O

( O
2011 O
) O
; O
Surdeanu O
et O
al O
. O

( O
2012 O
) O
regard O
it O
as O
a O
multi O
- O
instance O
multi O
- O
label O
learning O
problem O
. O

Recently O
, O
Zeng O
et O
al O
. O
( O
2015 O
) O
attempt O
to O
connect O
neural O
networks O
with O
distant O
supervision O
follow- O
ing O
the O
expressed O
- O
at O
- O
least O
- O
once O
assumption O
. O

Lin O
et O
al O
. O

( O
2016 O
) O
further O
utilize O
sentence O
- O
level O
attention O
mechanism O
to O
consider O
all O
informative O
sentences O
jointly O
. O

Most O
existing O
RE O
systems O
are O
absorbed O
in O
ex- O
tracting O
relations O
from O
mono O
- O
lingual O
data O
, O
ignor- O
ing O
massive O
information O
lying O
in O
texts O
from O
mul- O
tiple O
languages O
. O

In O
this O
area O
, O
Faruqui O
and O
Kumar O
( O
2015 O
) O
present O
a O
language O
independent O
open O
do- O
main O
relation O
extraction O
system O
, O
and O
Verga O
et O

al O
. O

( O
2015 O
) O
further O
employ O
Universal O
Schema O
to O
com- O
bine O
OpenIE O
and O
link O
- O
prediction O
perspective O
for O
multi O
- O
lingual O
relation O
extraction O
. O

Both O
the O
works O
focus O
on O
multi O
- O
lingual O
transfer O
learning O
and O
learn O
a O
predictive O
model O
on O
a O
new O
language O
for O
existing O
KBs O
, O
by O
leveraging O
uniﬁed O
representation O
learn- O
ing O
for O
cross O
- O
lingual O
entities O
. O

Different O
from O
these O
works O
, O
our O
framework O
aims O
to O
jointly O
model O
the O
texts O
in O
multiple O
languages O
to O
enhance O
relation O
ex- O

traction O
with O
distant O
supervision O
. O

To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
ﬁrst O
effort O
to O
multi B-TaskName
- I-TaskName
lingual I-TaskName
neural I-TaskName
relation I-TaskName
extraction I-TaskName
. O

The O
scope O
of O
multi O
- O
lingual O
analysis O
has O
been O
widely O
considered O
in O
many O
tasks O
besides O
relation B-TaskName
extraction I-TaskName
, O
such O
as O
sentiment O
analysis O
( O
Boiy O
and O
Moens O
, O
2009 O
) O
, O
cross O
- O
lingual O
document O
summa- O
rization O
( O
Boudin O
et O
al O
. O
, O
2011 O
) O
, O
information O
retrieval O
in O
Web O
search O
( O
Dong O
et O
al O
. O
, O
2014 O
) O
and O
so O
on O
. O

3 O
Methodology O

In O
this O
section O
, O
we O
describe O
our O
proposed O
MNRE B-MethodName
framework O
in O
detail O
. O

The O
key O
motivation O
of O
MNRE B-MethodName
is O
that O
, O
for O
each O
relational O
fact O
, O
the O
relation O
pat- O
terns O
in O
sentences O
of O
different O
languages O
should O
be O
substantially O
consistent O
, O
and O
MNRE B-MethodName
can O
utilize O
the O
pattern O
consistency O
and O
complementarity O
amonglanguages O
to O
achieve O
better O
results O
for O
relation B-TaskName
ex- I-TaskName
traction I-TaskName
. O

Formally O
, O
given O
two O
entities O
, O
their O
correspond- O
ing O
sentences O
in O
mdifferent O
languages O
are O
de- O
ﬁned O
as O
T={S1 O
, O
S2 O
, O
. O
. O
. O

, O
S O
m O
} O
, O
where O
Sj= O
{ O
x1 O
j O
, O
x2 O
j O
, O
. O
. O
. O

, O
xnj O
j}corresponds O
to O
the O
sentence O
set O
in O
the O
jth O
language O
with O
njsentences O
. O

Our O
model O
measures O
a O
score O
f(T O
, O
r)for O
each O
relation O
r O
, O
which O
is O
expected O
to O
be O
high O
when O
ris O
the O
valid O
one O
, O
oth- O
erwise O
low O
. O

The O
MNRE B-MethodName
framework O
contains O
two O
main O
components O
: O
1 O
. O
Sentence O
Encoder O
. O

Given O
a O
sentence O
xand O
two O
target O
entities O
, O
we O
employ O
CNN O
to O
encode O
re- O
lation O
patterns O
in O
xinto O
a O
distributed O
representation O
x. O

The O
sentence O
encoder O
can O
also O
be O
implemented O
with O
GRU O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
or O
LSTM O
( O
Hochre- O
iter O
and O
Schmidhuber O
, O
1997 O
) O
. O

In O
experiments O
, O
we O
ﬁnd O
CNN O
can O
achieve O
a O
better O
trade O
- O
off O
between O
computational O
efﬁciency O
and O
performance O
effec- O
tiveness O
. O

Thus O
, O
in O
this O
paper O
, O
we O
focus O
on O
CNN O
as O
the O
sentence O
encoder O
. O

2 O
. O
Multi O
- O
lingual O
Attention O
. O

With O
all O
sentences O
in O
various O
languages O
encoded O
into O
distributed O
vec- O
tor O
representations O
, O
we O
apply O
mono O
- O
lingual O
and O
cross O
- O
lingual O
attentions O
to O
capture O
those O
infor- O
mative O
sentences O
with O
accurate O
relation O
patterns O
. O

MNRE B-MethodName
further O
aggregates O
these O
sentence O
vectors O
with O
weighted O
attentions O
into O
global O
representa- O
tions O
for O
relation O
prediction O
. O

We O
introduce O
the O
two O
components O
in O
detail O
as O
follows O
. O

3.1 O
Sentence O
Encoder O
The O
sentence O
encoder O
aims O
to O
transform O
a O
sentence O
xinto O
its O
distributed O
representation O
xvia O
CNN O
. O

First O
, O
it O
embeds O
the O
words O
in O
the O
input O
sentence36 O

into O
dense O
real O
- O
valued O
vectors O
. O

Next O
, O
it O
employs O
convolutional O
, O
max O
- O
pooling O
and O
non O
- O
linear O
trans- O
formation O
layers O
to O
construct O
the O
distributed O
repre- O
sentation O
of O
the O
sentence O
, O
i.e. O
, O
x. O
3.1.1 O
Input O
Representation O
Following O
( O
Zeng O
et O
al O
. O
, O
2014 O
) O
, O
we O
transform O
each O
input O
word O
into O
the O
concatenation O
of O
two O
kinds O
of O
representations O
: O
( O
1 O
) O
a O
word O
embedding O
which O
cap- O
tures O
syntactic O
and O
semantic O
meanings O
of O
the O
word O
, O
and O
( O
2 O
) O
a O
position O
embedding O
which O
speciﬁes O
the O
position O
information O
of O
this O
word O
with O
respect O
to O
two O
target O
entities O
. O

In O
this O
way O
, O
we O
can O
repre- O
sent O
the O
input O
sentence O
as O
a O
vector O
sequence O
w= O
{ O
w1,w2 O
, O
. O
. O

.}with O

wi∈Rd O
, O
where O
d O
= O
da+db×2 O
. O

( O
daanddbare O
the O
dimensions O
of O
word O
embeddings O
and O
position O
embeddings O
respectively O
) O
3.1.2 O
Convolution O
, O
Max O
- O
pooling O
and O
Non O
- O
linear O
Layers O
After O
encoding O
the O
input O
sentence O
, O
we O
use O
a O
con- O
volutional O
layer O
to O
extract O
the O
local O
features O
, O
max- O
pooling O
, O
and O
non O
- O
linear O
layers O
to O
merge O
all O
local O
features O
into O
a O
global O
representation O
. O

First O
, O
the O
convolutional O
layer O
extracts O
local O
fea- O
tures O
by O
sliding O
a O
window O
of O
length O
l B-HyperparameterName
over O
the O
sen- O
tence O
and O
perform O
a O
convolution O
within O
each O
slid- O
ing O
window O
. O

Formally O
, O
the O
output O
of O
convolutional O
layer O
for O
the O
ith O
sliding O
window O
is O
computed O
as O
: O

pi O
= O
Ww O
i−l+1 O
: O
i+b O
, O
( O
1 O
) O
where O
wi−l+1 O
: O
iindicates O
the O
concatenation O
of O
l O
word O
embeddings O
within O
the O
i O
- O
th O
window O
, O
W∈ O
Rdc×(l×d)is O
the O
convolution O
matrix O
and O
b∈Rdc O
is O
the O
bias O
vector O
. O

( O
dcis O
the O
dimension O
of O
output O
embeddings O
of O
the O
convolution O
layer O
) O

After O
that O
, O
we O
combines O
all O
local O
features O
via O
a O
max O
- O
pooling O
operation O
and O
apply O
a O
hyperbolic O
tan- O
gent O
function O
to O
obtain O
a O
ﬁxed O
- O
sized O
sentence O
vec- O
tor O
for O
the O
input O
sentence O
. O

Formally O
, O
the O
ith O
ele- O
ment O
of O
the O
output O
vector O
x∈Rdcis O
calculated O
as O
: O
[ O
x]j O
= O
tanh O
( O
max O
i(pij O
) O
) O
. O

( O
2 O
) O
The O
ﬁnal O
vector O
xis O
expected O
to O
efﬁciently O
en- O
code O
relation O
patterns O
about O
target O
entities O
from O
the O
input O
sentence O
. O

Here O
, O
instead O
of O
max O
pooling O
operation O
, O
we O
can O
use O
piecewise O
max O
pooling O
operation O
adopted O
by O
PCNN O
( O
Zeng O
et O
al O
. O
, O
2015 O
) O
which O
is O
a O
variation O
of O
CNN O
to O
better O
capture O
the O
relation O
patterns O
in O
the O
input O
sentence.3.2 O
Multi O
- O
lingual O
Attention O
To O
exploit O
the O
information O
of O
the O
sentences O
from O
all O
languages O
, O
our O
model O
adopts O
two O
kinds O
of O
at- O
tention O
mechanisms O
for O
multi B-TaskName
- I-TaskName
lingual I-TaskName
relation I-TaskName
ex- I-TaskName
traction I-TaskName
, O
including O
: O
( O
1 O
) O
the O
mono O
- O
lingual O
atten- O
tion O
which O
selects O
the O
informative O
sentences O
within O
one O
language O
and O
( O
2 O
) O
the O
cross O
- O
lingual O
attention O
which O
measures O
the O
pattern O
consistency O
among O
languages O
. O

3.2.1 O
Mono O
- O
lingual O
Attention O
To O
address O
the O
wrong O
- O
labelling O
issue O
in O
distant O
su- O
pervision O
, O
we O
follow O
the O
idea O
of O
sentence O
- O
level O
at- O
tention O
( O
Lin O
et O
al O
. O
, O
2016 O
) O
and O
set O
mono O
- O
lingual O
at- O
tention O
for O
MNRE B-MethodName
. O

It O
is O
intuitive O
that O
each O
hu- O
man O
language O
has O
its O
own O
characteristics O
. O

Hence O
we O
adopt O
different O
mono O
- O
lingual O
attentions O
to O
de- O
emphasize O
those O
noisy O
sentences O
within O
each O
lan- O
guage O
. O

More O
speciﬁcally O
, O
for O
the O
j O
- O
th O
language O
and O
the O
sentence O
set O
Sj O
, O
we O
aim O
to O
aggregate O
all O
sentence O
vectors O
into O
a O
real O
- O
valued O
vector O
Sjfor O
relation O
pre- O
diction O
. O

The O
mono O
- O
lingual O
vector O
Sjis O
computed O
as O
a O
weighted O
sum O
of O
those O
sentence O
vectors O
xi O

j O
: O
Sj=∑ O
iαi O
jxi O
j O
, O
( O
3 O
) O
where O
αi O
jis O
the O
attention O
score O
of O
each O
sentence O
vector O
xi O
j O
, O
deﬁned O
as O
: O
αi O
j O
= O
exp(ei O
j O
) O
∑ O
kexp(ek O
j O
) O
, O
( O
4 O
) O
where O
ei O
jis O
referred O
as O
a O
query O
- O
based O
function O
which O
scores O
how O
well O
the O
input O
sentence O
xi O
jre- O
ﬂects O
its O
labelled O
relation O
r. O

There O
are O
many O
ways O
to O
obtain O
ei O
j O
, O
and O
here O
we O
simply O
compute O
eias O
the O
inner O
product O
: O
ei O
j O
= O
xi O
j·rj O
. O

( O
5 O
) O
Here O
rjis O
the O
query O
vector O
of O
the O
relation O
rwith O
respect O
to O
the O
j O
- O
th O
language O
. O

3.2.2 O
Cross O
- O
lingual O
Attention O
Besides O
mono O
- O
lingual O
attention O
, O
we O
propose O
cross- O
lingual O
attention O
for O
neural B-TaskName
relation I-TaskName
extraction I-TaskName
to O
better O
take O
advantages O
of O
multi O
- O
lingual O
data O
. O

The O
key O
idea O
of O
cross O
- O
lingual O
attention O
is O
to O
em- O
phasize O
those O
sentences O
which O
have O
strong O
con- O
sistency O
among O
different O
languages O
. O

On O
the O
basis O
of O
mono O
- O
lingual O
attention O
, O
cross O
- O
lingual O
attention37 O

is O
capable O
of O
further O
removing O
unlikely O
sentences O
and O
resulting O
in O
more O
concentrated O
and O
informa- O
tive O
sentences O
, O
with O
the O
factor O
of O
consistent O
cor- O
respondence O
of O
relation O
patterns O
among O
different O
languages O
. O

Cross O
- O
lingual O
attention O
works O
similar O
to O
mono- O
lingual O
attention O
. O

Suppose O
jindicates O
a O
language O
andkis O
a O
another O
language O
( O
k̸=j O
) O
. O

Formally O
, O
the O
cross O
- O
lingual O
representation O
Sjkis O
deﬁned O
as O
a O
weighted O
sum O
of O
those O
sentence O
vectors O
xi O
jin O
the O
jth O
language O
: O
Sjk=∑ O
iαi O
jkxi O
j O
, O
( O
6 O
) O
where O
αi O
jkis O
the O
cross O
- O
lingual O
attention O
score O
of O
each O
sentence O
vector O
xi O
jwith O
respect O
to O
the O
kth O
lan- O
guage O
. O

The O
cross O
- O
lingual O
attention O
αi O
jkis O
deﬁned O
as O
: O
αi O
jk O
= O
exp(ei O
jk O
) O
∑ O
kexp(ek O
jk O
) O
, O
( O
7 O
) O
where O
ei O
jkis O
referred O
as O
a O
query O
- O
based O
function O
which O
scores O
the O
consistency O
between O
the O
input O
sentence O
xi O
jin O
the O
jth O
language O
and O
the O
relation O
patterns O
in O
the O
kth O
language O
for O
expressing O
the O
se- O
mantic O
meanings O
of O
the O
labelled O
relation O
r. O
Similar O
to O
the O
mono O
- O
lingual O
attention O
, O
we O
compute O
ei O
jkas O
follows O
: O
ei O
jk O
= O
xi O
j·rk O
, O
( O
8) O
where O
rkis O
the O
query O
vector O
of O
the O
relation O
rwith O
respect O
to O
the O
kth O
language O
. O

Note O
that O
, O
for O
convenience O
, O
we O
denote O
those O
mono O
- O
lingual O
attention O
vectors O
SjasSjjin O
the O
re- O
mainder O
of O
this O
paper O
. O

3.3 O
Prediction O
For O
each O
entity O
pair O
and O
its O
corresponding O
sentence O
setTinmlanguages O
, O
we O
can O
obtain O
m×mvec- O
tors{Sjk|j O
, O
k∈ O
{ O
1 O
, O
. O
. O
. O

, O
m O
} O
} O
from O
the O
neural O
net- O
works O
with O
multi O
- O
lingual O
attention O
. O

Those O
vectors O
with O
j O
= O
kare O
mono O
- O
lingual O
attention O
vectors O
, O
and O
those O
with O
j̸=kare O
cross O
- O
lingual O
attention O
vec- O
tors O
. O

We O
take O
all O
vectors O
{ O
Sjk}together O
and O
deﬁne O
the O
overall O
score O
function O
f(T O
, O
r)as O
follows O
: O
f(T O
, O
r O
) O
= O
∑ O
j O
, O
k∈{1, O
... O
,m}logp(r|Sjk O
, O
θ),(9 O
) O
where O
p(r|Sjk O
, O
θ)is O
the O
probability O
of O
predicting O
the O
relation O
rconditional O
on O
Sjk O
, O
computed O
usinga O
softmax O
layer O
as O
follows O
: O
p(r|Sjk O
, O
θ O
) O

= O
softmax O
( O
MS O
jk+d O
) O
, O
( O
10 O
) O
where O
d∈Rnris O
a O
bias O
vector O
, O
nris O
the O
number O
of O
relation O
types O
and O
M∈Rnr×Rcis O
a O
global O
relation O
matrix O
initialized O
randomly O
. O

To O
better O
consider O
the O
characteristics O
of O
each O
hu- O
man O
language O
, O
we O
further O
introduce O
Rkas O
the O
spe- O
ciﬁc O
relation O
matrix O
of O
the O
kth O
language O
. O

Here O
we O
simply O
deﬁne O
Rkas O
composed O
by O
rkin O
Eq O
. O

( O
8) O
. O

Hence O
, O
Eq O
. O

( O
10 O
) O
can O
be O
extended O
to O
: O
p(r|Sjk O
, O
θ O
) O
= O
softmax O

[ O
( O
Rk+M)Sjk+d],(11 O
) O
where O
Mencodes O
global O
patterns O
for O
predicting O
relations O
and O
Rkencodes O
those O
language O
- O
speciﬁc O
characteristics O
. O

Note O
that O
, O
in O
the O
training O
phase O
, O
the O
vectors O
{ O
Sjk}are O
constructed O
using O
Eq O
. O

( O
3 O
) O
and O
( O
6 O
) O
using O
the O
labelled O
relation O
. O

In O
the O
testing O
phase O
, O
since O
the O
relation O
is O
not O
known O
in O
advance O
, O
we O
will O
construct O
different O
vectors O
{ O
Sjk}for O
each O
possible O
relation O
r O
to O
compute O
f(T O
, O
r)for O
relation O
prediction O
. O

3.4 O
Optimization O
Here O
we O
introduce O
the O
learning O
and O
optimization O
details O
of O
our O
MNRE B-MethodName
framework O
. O

We O
deﬁne O
the O
objective O
function O
as O
follows O
: O
J(θ O
) O
= O
s∑ O
i=1f(Ti O
, O
ri O
) O
, O
( O
12 O
) O
where O
sindicates O
the O
number O
of O
all O
entity O
pairs O
with O
each O
corresponding O
to O
a O
sentence O
set O
in O
dif- O
ferent O
languages O
, O
and O
θindicates O
all O
parameters O
of O
our O
framework O
. O

To O
solve O
the O
optimization O
problem O
, O
we O
adopt O
mini O
- O
batch O
stochastic O
gradient O
descent O
( O
SGD O
) O
to O
minimize O
the O
objective O
function O
. O

For O
learning O
, O
we O
iterate O
by O
randomly O
selecting O
a O
mini O
- O
batch O
from O
the O
training O
set O
until O
converge O
. O

4 O
Experiments O
We O
ﬁrst O
introduce O
the O
datasets O
and O
evaluation O
met- O
rics O
used O
in O
the O
experiments O
. O

Next O
, O
we O
use O
a O
vali- O
dation O
set O
to O
determine O
the O
best O
model O
parameters O
and O
choose O
the O
best O
model O
via O
early O
stopping O
. O

Af- O
terwards O
, O
we O
show O
the O
effectiveness O
of O
our O
frame- O
work O
of O
considering O
pattern O
complementarity O
and O
consistency O
for O
multi O
- O
lingual O
relation O
extraction O
by O
quantitative O
and O
qualitative O
analysis O
. O

Finally O
, O
we O
compare O
the O
effect O
of O
two O
kinds O
of O
relation O
matri- O
ces O
in O
Eq O
. O

( O
11 O
) O
used O
for O
prediction.38 O

4.1 O
Datasets O
and O
Evaluation O
Metrics O
We O
generate O
a O
new O
multi O
- O
lingual O
relation O
extrac- O
tion O
dataset O
to O
evaluate O
our O
MNRE B-MethodName
framework O
. O

Without O
loss O
of O
generality O
, O
the O
experiments O
fo- O
cus O
on O
relation B-TaskName
extraction I-TaskName
from O
two O
languages O
in- O

cluding O
English O
and O
Chinese O
. O

In O
this O
dataset O
, O
the O
Chinese O
instances O
are O
generated O
by O
aligning O
Chinese O
Baidu O
Baike O
with O
Wikidata O
, O
and O
the O
En- O
glish O
instances O
are O
generated O
by O
aligning O
English O
Wikipedia O
articles O
with O
Wikidata O
. O

The O
relational O
facts O
of O
Wikidata O
in O
this O
dataset O
are O
divided O
into O
three O
parts O
for O
training O
, O
validation O
and O
testing O
re- O
spectively O
. O

There O
are O
176relations O
including O
a O
spe- O
cial O
relation O
NA O
indicating O
there O
is O
no O
relation O
be- O
tween O
entities O
. O

And O
we O
set O
both O
validation O
and O
test- O
ing O
sets O
for O
Chinese O
and O
English O
parts O
contain O
the O
same O
facts O
. O

We O
list O
the O
statistics O
about O
the O
dataset O
in O
Table O
2 O
. O

Dataset O
# O
Rel O
# O
Sent O
# O
Fact O
Train O
1,022,239 O
47,638 O
English O
Valid O
176 O
80,191 O
2,192 O
Test O
162,018 O
4,326 O
Train O
940,595 O
42,536 O
Chinese O
Valid O

176 O
82,699 O
2,192 O
Test O
167,224 O
4,326 O
Table O
2 O
: O
Statistics O
of O
the O
dataset O
. O

We O
follow O
previous O
works O
( O
Mintz O
et O
al O
. O
, O
2009 O
) O
and O
investigate O
the O
performance O
of O
RE O
systems O
us- O

ing O
the O
held O
- O
out O
evaluation O
, O
by O
comparing O
the O
re- O
lational O
facts O
discovered O
by O
RE O
systems O
from O
the O
testing O
set O
with O
those O
facts O
in O
KB O
. O

The O
evaluation O
method O
assumes O
that O
if O
a O
RE O
system O
accurately O
ﬁnds O
more O
relational O
facts O
in O
KBs O
from O
the O
test- O
ing O
set O
, O
it O
will O
achieve O
better O
performance O
for O
rela- B-TaskName
tion I-TaskName
extraction I-TaskName
. O

The O
held O
- O
out O
evaluation O
provides O
an O
approximate O
measure O
of O
RE B-TaskName
performance O
with- O
out O
time O
- O
consuming O
human O
evaluation O
. O

In O
experi- O
ments O
, O
we O
report O
the O
precision B-MetricName
/ O
recall B-MetricName
curves O
as O
the O
evaluation O
metric O
. O

4.2 O
Experimental O
Settings O
We O
tune O
the O
parameters O
of O
our O
MNRE B-MethodName
framework O
by O
grid O
searching O
using O
validation O
set O
. O

For O
train- O
ing O
, O
we O
set O
the O
iteration B-HyperparameterName
number O
over O
all O
the O
train- O
ing O
data O
as O
15 B-HyperparameterValue
. O

The O
best O
models O
were O
selected O
by O
early O
stopping O
using O
the O
evaluation O
results O
on O
the O
validation O
set O
. O

In O
Table O
3we O
show O
the O
best O
setting O
of O
all O
parameters O
used O
in O
our O
experiments O
. O

4.3 O
Effectiveness O
of O
Consistency O
To O
demonstrate O
the O
effectiveness O
of O
considering O
pattern O
consistency O
among O
languages O
, O
we O
empir- O
ically O
compare O
different O
methods O
through O
held O
- O
out O
evaluation O
. O

We O
select O
CNN O
proposed O
in O
( O
Zeng O
et O
al O
. O
, O
2014 O
) O
as O
our O
sentence O
encoder O
and O
imple- O
ment O
it O
by O
ourselves O
which O
achieves O
comparable O
results O
as O
the O
authors O
reported O
on O
their O
experimen- O
tal O
dataset O
NYT104 B-DatasetName
. O

And O
we O
compare O
the O
perfor- O
mance O
of O
our O
framework O
with O
the O
[ B-MethodName
P]CNN I-MethodName
model O
trained O
with O
only O
English O
data O
( O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
) O
, O
only O
Chinese O
data O
( O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
) O
, O
a O
joint O
model O
( O
[ B-MethodName
P]CNN+joint I-MethodName
) O
which O
predicts O
using O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
and O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
jointly O
, O
and O
another O
joint O
model O
with O
shared O
embeddings O
( O
[ B-MethodName
P]CNN+share I-MethodName
) O
which O
trains O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
and O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
with O
common O
relation O
embedding O
matrices O
. O

From O
Fig O
. O

2 O
, O
we O
have O
the O
following O
observa- O
tions O
: O
( O
1 O
) O
Both O
[ B-MethodName
P]CNN+joint I-MethodName
and O
[ B-MethodName
P]CNN+share I-MethodName
achieve O
better O
performances O
as O
compared O
to O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
and O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
. O

It O
indicates O
that O
uti- O
lizing O
Chinese O
and O
English O
sentences O
jointly O
is O
beneﬁcial O
to O
extracting O
novel O
relational O
facts O
. O

The O
reason O
is O
that O
those O
relational O
facts O
that O
are O
discov- O
ered O
from O
multiple O
languages O
are O
more O
reliable O
to O
be O
true O
. O

( O
2 O
) O
CNN+share B-MethodName
only O
has O
similar O
performance O
as O
compared O
to O
CNN+joint B-MethodName
, O
even O
through O
a O
bit O
worse O
when O
recall B-MetricName
ranges O
from O
0.1 B-MetricValue
to O
0.2 B-MetricValue
. O

Besides O
, O
PCNN+share B-MethodName
performs O
worse O
than O
PCNN+joint B-MethodName
nearly O
over O
the O
entire O
range O
of O
recall B-MetricName
. O

It O
demon- O
strates O
that O
a O
simple O
combination O
of O
multiple O
lan- O
guages O
by O
sharing O
relation O
embedding O
matrices O
can O
not O
further O
capture O
more O
implicit O
correlations O
among O
various O
languages O
. O

( O
3 O
) O
Our O
MNRE B-MethodName
model O
achieves O
the O
highest O
pre- O
cision O
over O
the O
entire O
range O
of O
recall O
as O
com- O
pared O
to O
other O
methods O
including O
[ B-MethodName
P]CNN+joint I-MethodName
and O
[ B-MethodName
P]CNN+share I-MethodName
models O
. O

By O
grid O
searching O
of O
parameters O
for O
these O
baseline O
models O
, O
we O
can O
ob- O
serve O
that O
both O
[ B-MethodName
P]CNN+joint I-MethodName
and O
[ B-MethodName
P]CNN+share I-MethodName
can O
not O
achieve O
competitive O
results O
compared O
to O
MNRE B-MethodName
even O
when O
increasing O
the O
size O
of O
the O
output O
layer O
. O

This O
indicates O
that O
no O
more O
useful O
informa- O
tion O
can O
be O
captured O
by O
simply O
increasing O
model O
size O
. O

On O
the O
contrary O
, O
our O
proposed O
MNRE B-MethodName
model O
can O
successfully O
improve O
multi B-TaskName
- I-TaskName
lingual I-TaskName
relation I-TaskName
ex- I-TaskName
traction I-TaskName
by O
considering O
pattern O
consistency O
among O
languages O
. O

We O
further O
give O
an O
example O
of O
cross O
- O
lingual O
at O
- O
tention O
in O
Table O
4 O
. O

It O
shows O
four O
sentences O
hav- O
ing O
the O
highest O
and O
lowest O
Chinese O
- O
to O
- O
English O
and O
English O
- O
to O
- O
Chinese O
attention O
weights O
respectively O
with O
respect O
to O
the O
relation O
PlaceOfBirth O
in O
MNRE B-MethodName
. O

We O
highlight O
the O
entity O
pairs O
in O
bold O
face O
. O

For O
comparison O
, O
we O
also O
show O
their O
attention O
weights O
from O
CNN+Zh B-MethodName
and O
CNN+En B-MethodName
. O

From O
the O
table O
we O
ﬁnd O
that O
, O
although O
all O
of O
the O
four O
sentences O
actually O
express O
the O
fact O
that O
Barzun O
was O
born O
in O
France O
, O
the O
ﬁrst O
and O
third O
sentences O
contain O
much O
more O
noisy O
information O
that O
may O
confuse O
RE B-TaskName
sys- O
tems O
. O

By O
considering O
pattern O
consistency O
between O
sentences O
in O
two O
languages O
with O
cross O
- O
lingual O
at- O
tention O
, O
MNRE B-MethodName
can O
identify O
the O
second O
and O
fourth O
sentences O
that O
unambiguously O
express O
the O
relation O
PlaceOfBirth O
with O
higher O
attention O
as O
com- O
pared O
to O
CNN+Zh B-MethodName
and O
CNN+En B-MethodName
. O

4.4 O
Effectiveness O
of O
Complementarity O
To O
demonstrate O
the O
effectiveness O
of O
consider- O
ing O
pattern O
complementarity O
among O
languages O
, O
we O
empirically O
compare O
the O
following O
methods O
through O
held O
- O
out O
evaluation O
: O
MNRE B-MethodName
for I-MethodName
English I-MethodName
( O
MNRE B-MethodName
- I-MethodName
En I-MethodName
) O
and O
MNRE B-MethodName
for I-MethodName
Chinese I-MethodName
( O
MNRE B-MethodName
- I-MethodName
Zh I-MethodName
) O
which O
only O
use O
the O
mono O
- O
lingual O
vectors O
to O
predict O
relations O
, O
and O
[ O
P]CNN O
- O
En O
and O
[ O
P]CNN O
- O
Zh O
mod- O
els O
. O

Fig O
. O

3shows O
the O
aggregated O
precision O
/ O
recall O
curves O
of O
the O
four O
models O
for O
both O
CNN O
and O
PCNN O
. O

From O
the O
ﬁgure O
, O
we O
ﬁnd O
that O
: O
( O
1 O
) O
MNRE B-MethodName
- I-MethodName
En I-MethodName
and O
MNRE B-MethodName
- I-MethodName
Zh I-MethodName
outperform O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
and O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
almost O
in O
entire O
range O
of O
recall B-MetricName
. O

It O
indicates O
that O
by O
jointly O
training O
with O
multi O
- O
lingual O
attention O
, O
both O
Chinese O
and O
English O
relation O
extractors O
are O
beneﬁcial O
from O
those O
sentences O
from O
the O
other O
language O
. O

( O
2 O
) O
Although O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
underperforms O
as O
compared O
to O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
, O
MNRE B-MethodName
- I-MethodName
En I-MethodName
is O
compara- O
ble O
to O
MNRE B-MethodName
- I-MethodName
Zh I-MethodName
by O
jointly O
training O
through O
multi- O
lingual O
attention O
. O

It O
demonstrates O
that O
both O
Chi- O
nese O
and O
English O
relation O
extractors O
can O
take O
full O
advantages O
of O
texts O
in O
both O
languages O
via O
our O
pro- O
pose O
multi O
- O
lingual O
attention O
scheme O
. O

Table O
5shows O
the O
detailed O
results O
( O
in O
preci- B-MetricName
sion I-MetricName
@ O
1 B-MetricValue
) O
of O
some O
speciﬁc O
relations O
of O
which O
the O
training O
instances O
are O
un O
- O
balanced O
on O
English O
and O
Chinese O
sides O
. O

From O
the O
table O
, O
we O
can O
see O
that O
: O
( O
1 O
) O
For O
the O
relation O
Contains O
of O
which O
the O
number O
of O
English O
training O
instances O
is O
only O
1/7 O
of O
Chinese O
ones O
, O
CNN B-MethodName
- I-MethodName
En I-MethodName
gets O
much O
worse O
per- O
formance O
as O
compared O
to O
CNN B-MethodName
- I-MethodName
Zh I-MethodName
due O
to O
the O
lack O
of O
training O
data O
. O

Nevertheless O
, O
by O
jointly O
training O
through O
multi O
- O
lingual O
attention O
, O
MNRE(CNN)- B-MethodName
En I-MethodName
is O
comparable O
to O
and O
slightly O
better O
than O
MNRE(CNN)-Zh B-MethodName
. O

( O
2 O
) O
For O
the O
relation O
HeadquartersLoca- O
tion O
of O
which O
the O
number O
of O
Chinese O
training O
in- O
stances O
is O
only O
1/9of O
English O
ones O
, O
CNN B-MethodName
- I-MethodName
Zh I-MethodName
even O
can O
not O
predict O
any O
correct O
results O
. O

The O
reason O
is O
perhaps O
that O
, O
CNN B-MethodName
- I-MethodName
Zh I-MethodName
of O
the O
relation O
is O
not O
suf- O
ﬁciently O
trained O
because O
there O
are O
only O
210Chi- O
nese O
training O
instances O
for O
this O
relation O
. O

Simi- O
larly O
, O
by O
jointly O
training O
through O
multi O
- O
lingual O
at- O
tention O
, O
MNRE(CNN)-En B-MethodName
and O
MNRE(CNN)-Zh B-MethodName
both O
achieve O
promising O
results O
. O

( O
3 O
) O
For O
the O
relations O
Father O
andCountry- O
OfCitizenship O
of O
which O
the O
sentence O
number O
in O
English O
and O
Chinese O
are O
not O
so O
un O
- O
balanced O
, O
our O
MNRE B-MethodName
can O
still O
improve O
the O
performance O
of O
rela- O
tion O
extraction O
on O
both O
English O
and O
Chinese O
sides O
. O

4.5 O
Comparison O
of O
Relation O
Matrix O
For O
relation O
prediction O
, O
we O
use O
two O
kinds O
of O
re- O
lation O
matrices O
including O
: O
Mthat O
considers O
the O
global O
consistency O
of O
relations O
, O
and O
Rthat O
consid- O
ers O
the O
speciﬁc O
characteristics O
of O
relations O
for O
each O
language O
. O

To O
measure O
the O
effect O
of O
the O
two O
relation O
matrices O
, O
we O
compare O
the O
performance O
of O
MNRE B-MethodName
using O
the O
both O
matrices O
with O
those O
only O
using O
M O
( O
MNRE B-MethodName
- I-MethodName
M I-MethodName
) O
and O
only O
using O
R( O
MNRE B-MethodName
- I-MethodName
R I-MethodName
) O
. O

Fig O
. O

4shows O
the O
precision O
- O
recall O
curves O
for O
each O
method O
. O

From O
the O
ﬁgure O
, O
we O
observe O
that O
: O
t O
( O
1 O
) O
The O
performance O
of O
MNRE B-MethodName
- I-MethodName
M I-MethodName
is O
much O
worse O
than O
both O
MNRE B-MethodName
- I-MethodName
R I-MethodName
and O
MNRE B-MethodName
. O

It O
indicates O
that O
we O
can O
not O
just O
use O
global O
relation O
matrix O
for O
relation O
prediction O
. O

The O
reason O
is O
that O
each O
lan- O
guage O
has O
its O
own O
speciﬁc O
characteristics O
to O
ex- O
press O
relation O
patterns O
, O
which O
can O
not O
be O
well O
in- O
tegrated O
into O
a O
single O
relation O
matrix O
. O

( O
2 O
) O
MNRE(CNN)-R B-MethodName
has O
similar O
performance O
as O
compared O
to O
MNRE(CNN B-MethodName
) I-MethodName
when O
the O
recall O
is O
low O
. O

However O
, O
it O
has O
a O
sharp O
decline O
when O
the O
recall B-MetricName
reaches O
0.25 B-MetricValue
. O

It O
suggests O
there O
also O
exists O
global O
consistency O
of O
relation O
patterns O
among O
languages O
which O
can O
not O
be O
neglected O
. O

Hence O
, O
we O
should O
combine O
both O
MandRtogether O
for O
multi B-TaskName
- I-TaskName
lingual I-TaskName
relation I-TaskName
extraction I-TaskName
, O
as O
proposed O
in O
our O
MNRE B-MethodName
framework O
. O

5 O
Conclusion O
In O
this O
paper O
, O
we O
introduce O
a O
neural B-TaskName
relation I-TaskName
extrac- I-TaskName
tion I-TaskName
framework O
with O
multi O
- O
lingual O
attention O
to O
take O
pattern O
consistency O
and O
complementarity O
among O
multiple O
languages O
into O
consideration O
. O

We O
evalu- O
ate O
our O
framework O
on O
multi B-TaskName
- I-TaskName
lingual I-TaskName
relation I-TaskName
extrac- I-TaskName
tion I-TaskName
task O
, O
and O
the O
results O
show O
that O
our O
framework O
can O
effectively O
model O
relation O
patterns O
among O
lan- O
guages O
and O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
. O

We O
will O
explore O
the O
following O
directions O
as O
fu- O
ture O
work O
: O
( O
1 O
) O
In O
this O
paper O
, O
we O
only O
consider O
sentence O
- O
level O
multi O
- O
lingual O
attention O
for O
relation B-TaskName
extraction I-TaskName
. O

In O
fact O
, O
we O
ﬁnd O
that O
the O
word O
alignment O
information O
may O
be O
also O
helpful O
for O
capturing O
rela- O
tion O
patterns O
. O

Hence O
, O
the O
word O
- O
level O
multi O
- O
lingual O
attention O
, O
which O
may O
discover O
implicit O
alignments O
between O
words O
in O
multiple O
languages O
, O
will O
fur- O
ther O
improve O
multi B-TaskName
- I-TaskName
lingual I-TaskName
relation I-TaskName
extraction I-TaskName
. O

We O
will O
explore O
the O
effectiveness O
of O
word O
- O
level O
multi- O
lingual O
attention O
for O
relation B-TaskName
extraction I-TaskName
as O
our O
fu O
- O
ture O
work O
. O

( O
2 O
) O
MNRE B-MethodName
can O
be O
ﬂexibly O
implemented O
in O
the O
scenario O
of O
multiple O
languages O
, O
and O
this O
pa- O
per O
focuses O
on O
two O
languages O
of O
English O
and O
Chi- O
nese O
. O

In O
future O
, O
we O
will O
extend O
MNRE B-MethodName
to O
more O
lan- O
guages O
and O
explore O
its O
signiﬁcance O
. O

Acknowledgments O
This O
work O
is O
supported O
by O
the O
973 O
Program O
( O
No O
. O
2014CB340501 O
) O
, O
the O
National O
Natu- O
ral O
Science O
Foundation O
of O
China O
( O
NSFC O
No O
. O
61572273 O
, O
61532010 O
) O
, O
and O
the O
Key O
Technologies O
Research O
and O
Development O
Program O
of O
China O
( O
No O
. O
2014BAK04B03 O
) O
. O

This O
work O
is O
also O
funded O
by O
the O
Natural O
Science O
Foundation O
of O
China O
( O
NSFC O
) O
and O
the O
German O
Research O
Foundation(DFG O
) O
in O
Project O
Crossmodal O
Learning O
, O
NSFC O
61621136008 O
/ O
DFC O
TRR-169 O
. O

References O
Erik O
Boiy O
and O
Marie O
- O
Francine O
Moens O
. O

2009 O
. O

A O
machine O
learning O
approach O
to O
sentiment O
analysis O
in O
multilingual O
web O
texts O
. O

Information O
retrieval O
12(5):526–558 O
. O

Florian O
Boudin O
, O
Stéphane O
Huet O
, O
and O
Juan O
- O
Manuel O
Torres O
- O
Moreno O
. O
2011 O
. O

A O
graph O
- O
based O
approach O
to O
cross O
- O
language O
multi O
- O
document O
summarization O
. O

Polibits O
( O
43):113–118 O
. O
Kyunghyun O
Cho O
, O
Bart O
Van O
Merriënboer O
, O
Dzmitry O
Bah- O
danau O
, O
and O
Yoshua O
Bengio O
. O

2014 O
. O

On O
the O
properties O
of O
neural O
machine O
translation O
: O
Encoder O
- O
decoder O
ap- O
proaches O
. O

arXiv O
preprint O
arXiv:1409.1259 O
. O

Meiping O
Dong O
, O
Yong O
Cheng O
, O
Yang O
Liu O
, O
Jia O
Xu O
, O
Maosong O
Sun O
, O
Tatsuya O
Izuha O
, O
and O
Jie O
Hao O
. O
2014 O
. O

Query O
lattice O
for O
translation O
retrieval O
. O

In O
Proceed- O
ings O
of O
COLING O
. O

pages O
2031–2041 O
. O

Cıcero O
Nogueira O
dos O
Santos O
, O
Bing O
Xiang O
, O
and O
Bowen O
Zhou O
. O

2015 O
. O

Classifying O
relations O
by O
ranking O
with O
convolutional O
neural O
networks O
. O

In O
Proceedings O
of O
ACL O
. O

volume O
1 O
, O
pages O
626–634 O
. O

Manaal O
Faruqui O
and O
Shankar O
Kumar O
. O
2015 O
. O

Multilin- O
gual O
open O
relation O
extraction O
using O
cross O
- O
lingual O
pro- O
jection O
. O

arXiv O
preprint O
arXiv:1503.06450 O
. O

Matthew O
R. O
Gormley O
, O
Mo O
Yu O
, O
and O
Mark O
Dredze O
. O
2015 O
. O

Improved O
relation O
extraction O
with O
feature O
- O
rich O
com- O
positional O
embedding O
models O
. O

In O
Proceedings O
of O
EMNLP O
. O

pages O
1774–1784 O
. O

Sepp O
Hochreiter O
and O
Jürgen O
Schmidhuber O
. O

1997 O
. O

Long O
short O
- O
term O
memory O
. O

Neural O
Computation O
pages O
1735–1780.42 O

Raphael O
Hoffmann O
, O
Congle O
Zhang O
, O
Xiao O
Ling O
, O
Luke O
Zettlemoyer O
, O
and O
Daniel O
S O
Weld O
. O

2011 O
. O

Knowledge- O
based O
weak O
supervision O
for O
information O
extraction O
of O
overlapping O
relations O
. O

In O
Proceedings O
of O
ACL O
- O
HLT O
. O

pages O
541–550 O
. O

Yankai O
Lin O
, O
Shiqi O
Shen O
, O
Zhiyuan O
Liu O
, O
Huanbo O
Luan O
, O
and O
Maosong O
Sun O
. O
2016 O
. O

Neural O
relation O
extraction O
with O
selective O
attention O
over O
instances O
. O

In O
Proceed- O
ings O
of O
ACL O
. O

volume O
1 O
, O
pages O
2124–2133 O
. O

Mike O
Mintz O
, O
Steven O
Bills O
, O
Rion O
Snow O
, O
and O
Dan O
Juraf- O
sky O
. O

2009 O
. O

Distant O
supervision O
for O
relation O
extrac- O
tion O
without O
labeled O
data O
. O

In O
Proceedings O
of O
ACL- O
IJCNLP O
. O

pages O
1003–1011 O
. O

Sebastian O
Riedel O
, O
Limin O
Yao O
, O
and O
Andrew O
McCallum O
. O

2010 O
. O

Modeling O
relations O
and O
their O
mentions O
without O
labeled O
text O
. O

In O
Proceedings O
of O
ECML O
- O
PKDD O
. O

pages O
148–163 O
. O

Richard O
Socher O
, O
Brody O
Huval O
, O
Christopher O
D O
Manning O
, O
and O
Andrew O
Y O
Ng O
. O
2012 O
. O

Semantic O
compositionality O
through O
recursive O
matrix O
- O
vector O
spaces O
. O

In O
Proceed- O
ings O
of O
EMNLP O
- O
CoNLL O
. O
pages O
1201–1211 O
. O

Nitish O
Srivastava O
, O
Geoffrey O
Hinton O
, O
Alex O
Krizhevsky O
, O
Ilya O
Sutskever O
, O
and O
Ruslan O
Salakhutdinov O
. O

2014 O
. O

Dropout O
: O
A O
simple O
way O
to O
prevent O
neural O
networks O
from O
overﬁtting O
. O

JMLR O
15(1):1929–1958 O
. O

Mihai O
Surdeanu O
, O
Julie O
Tibshirani O
, O
Ramesh O
Nallapati O
, O
and O
Christopher O
D O
Manning O
. O

2012 O
. O

Multi O
- O
instance O
multi O
- O
label O
learning O
for O
relation O
extraction O
. O

In O
Pro- O
ceedings O
of O
EMNLP O
. O
pages O
455–465 O
. O

Patrick O
Verga O
, O
David O
Belanger O
, O
Emma O
Strubell O
, O
Ben- O
jamin O
Roth O
, O
and O
Andrew O
McCallum O
. O
2015 O
. O

Multi- O
lingual O
relation O
extraction O
using O
compositional O
uni- O
versal O
schema O
. O

arXiv O
preprint O
arXiv:1511.06396 O
. O

Dmitry O
Zelenko O
, O
Chinatsu O
Aone O
, O
and O
Anthony O
Richardella O
. O
2003 O
. O

Kernel O
methods O
for O
relation O
extraction O
. O

JMLR O
3(Feb):1083–1106 O
. O

Daojian O
Zeng O
, O
Kang O
Liu O
, O
Yubo O
Chen O
, O
and O
Jun O
Zhao O
. O
2015 O
. O

Distant O
supervision O
for O
relation O
extraction O
via O
piecewise O
convolutional O
neural O
networks O
. O

In O
Pro- O
ceedings O
of O
EMNLP O
. O

Daojian O
Zeng O
, O
Kang O
Liu O
, O
Siwei O
Lai O
, O
Guangyou O
Zhou O
, O
and O
Jun O
Zhao O
. O

2014 O
. O

Relation O
classiﬁcation O
via O
con- O
volutional O
deep O
neural O
network O
. O

In O
Proceedings O
of O
COLING O
. O

pages O
2335–2344.43 O

Probabilistic B-MethodName
FastText I-MethodName
for O
Multi O
- O
Sense O
Word O
Embeddings O
Ben O
Athiwaratkun O
Cornell O
University O
pa338@cornell.eduAndrew O
Gordon O
Wilson O
Cornell O
University O
andrew@cornell.eduAnima O
Anandkumar O
AWS O
& O
Caltech O
anima@amazon.com O

Abstract O
We O
introduce O
Probabilistic B-MethodName
FastText I-MethodName
, O
a O
new O
model O
for O
word O
embeddings O
that O
can O
cap- O
ture O
multiple O
word O
senses O
, O
sub O
- O
word O
struc- O
ture O
, O
and O
uncertainty O
information O
. O

In O
particular O
, O
we O
represent O
each O
word O
with O
a O
Gaussian O
mixture O
density O
, O
where O
the O
mean O
of O
a O
mixture O
component O
is O
given O
by O
the O
sum O
of O
n O
- O
grams O
. O

This O
represen- O
tation O
allows O
the O
model O
to O
share O
statis- O
tical O
strength O
across O
sub O
- O
word O
structures O
( O
e.g. O
Latin O
roots O
) O
, O
producing O
accurate O
rep- O
resentations O
of O
rare O
, O
misspelt O
, O
or O
even O
un- O
seen O
words O
. O

Moreover O
, O
each O
component O
of O
the O
mixture O
can O
capture O
a O
different O
word O
sense O
. O

Probabilistic B-MethodName
FastText I-MethodName
out- O
performs O
both O
F B-MethodName
ASTTEXT I-MethodName
, O
which O
has O
no O
probabilistic O
model O
, O
and O
dictionary B-MethodName
- I-MethodName
level I-MethodName
probabilistic I-MethodName
embeddings I-MethodName
, O
which O
do O
not O
incorporate O
subword O
structures O
, O
on O
sev- O
eral O
word O
- O
similarity O
benchmarks O
, O
includ- O
ing O
English B-DatasetName
RareWord I-DatasetName
and O
foreign O
lan- O
guage O
datasets O
. O

We O
also O
achieve O
state O
- O
of- O
art O
performance O
on O
benchmarks O
that O
mea- O
sure O
ability O
to O
discern O
different O
meanings O
. O

Thus O
, O
the O
proposed O
model O
is O
the O
ﬁrst O
to O
achieve O
multi O
- O
sense O
representations O
while O
having O
enriched O
semantics O
on O
rare O
words O
. O

1 O
Introduction O
Word O
embeddings O
are O
foundational O
to O
natural O
language O
processing O
. O

In O
order O
to O
model O
lan- O
guage O
, O
we O
need O
word O
representations O
to O
contain O
as O
much O
semantic O
information O
as O
possible O
. O

Most O
re- O

search O
has O
focused O
on O
vector O
word O
embeddings O
, O
such O
as O
W B-MethodName
ORD2VEC I-MethodName
(Mikolov O
et O
al O
. O
, O
2013a O
) O
, O
where O
words O
with O
similar O
meanings O
are O
mapped O
to O
nearby O
points O
in O
a O
vector O
space O
. O

Following O
the O
seminal O
work O
of O
Mikolov O
et O
al O
. O

( O
2013a O
) O
, O
there O
have O
been O
numerous O
works O
looking O
to O
learn O
efﬁcient O
word O
embeddings O
. O

One O
shortcoming O
with O
the O
above O
approaches O
to O
word O
embedding O
that O
are O
based O
on O
a O
prede- O
ﬁned O
dictionary O
( O
termed O
as O
dictionary O
- O
based O
em- O
beddings O
) O
is O
their O
inability O
to O
learn O
representa- O
tions O
of O
rare O
words O
. O

To O
overcome O
this O
limitation O
, O
character O
- O
level O
word O
embeddings O
have O
been O
pro- O
posed O
. O

F B-MethodName
ASTTEXT I-MethodName
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
is O
the O
state O
- O
of O
- O
the O
- O
art O
character O
- O
level O
approach O
to O
em- O
beddings O
. O

In O
F B-MethodName
ASTTEXT I-MethodName
, O
each O
word O
is O
modeled O
by O
a O
sum O
of O
vectors O
, O
with O
each O
vector O
represent- O
ing O
an O
n O
- O
gram O
. O

The O
beneﬁt O
of O
this O
approach O
is O
that O
the O
training O
process O
can O
then O
share O
strength O
across O
words O
composed O
of O
common O
roots O
. O

For O
exam- O
ple O
, O
with O
individual O
representations O
for O
“ O
circum O
” O
and O
“ O
navigation O
” O
, O
we O
can O
construct O
an O
informa- O
tive O
representation O
for O
“ O
circumnavigation O
” O
, O
which O
would O
otherwise O
appear O
too O
infrequently O
to O
learn O
a O
dictionary O
- O
level O
embedding O
. O

In O
addition O
to O
effec- O
tively O
modelling O
rare O
words O
, O
character O
- O
level O
em- O
beddings O
can O
also O
represent O
slang O
or O
misspelled O
words O
, O
such O
as O
“ O
dogz O
” O
, O
and O
can O
share O
strength O
across O
different O
languages O
that O
share O
roots O
, O
e.g. O
Romance O
languages O
share O
latent O
roots O
. O

A O
different O
promising O
direction O
involves O
repre- O
senting O
words O
with O
probability O
distributions O
, O
in- O
stead O
of O
point O
vectors O
. O

For O
example O
, O
Vilnis O
and O
McCallum O
( O
2014 O
) O
represents O
words O
with O
Gaussian O
distributions O
, O
which O
can O
capture O
uncertainty O
infor- O
mation O
. O

Athiwaratkun O
and O
Wilson O
( O
2017 O
) O
gen- O
eralizes O
this O
approach O
to O
multimodal O
probability O
distributions O
, O
which O
can O
naturally O
represent O
words O
with O
different O
meanings O
. O

For O
example O
, O
the O
distri- O
bution O
for O
“ O
rock O
” O
could O
have O
mass O
near O
the O
word O
“ O
jazz O
” O
and O
“ O
pop O
” O
, O
but O
also O
“ O
stone O
” O
and O
“ O
basalt O
” O
. O

Athiwaratkun O
and O
Wilson O
( O
2018 O
) O
further O
devel- O
oped O
this O
approach O
to O
learn O
hierarchical O
word O
rep- O
resentations O
: O
for O
example O
, O
the O
word O
“ O
music O
” O
can O

2be O
learned O
to O
have O
a O
broad O
distribution O
, O
which O
en- O
capsulates O
the O
distributions O
for O
“ O
jazz O
” O
and O
“ O
rock O
” O
. O

In O
this O
paper O
, O
we O
propose O
Probabilistic B-MethodName
Fast- I-MethodName
Text I-MethodName
( O
PFT B-MethodName
) O
, O
which O
provides O
probabilistic O
character- O
level O
representations O
of O
words O
. O

The O
resulting O
word O
embeddings O
are O
highly O
expressive O
, O
yet O
straightfor- O
ward O
and O
interpretable O
, O
with O
simple O
, O
efﬁcient O
, O
and O
intuitive O
training O
procedures O
. O

PFT B-MethodName
can O
model O
rare O
words O
, O
uncertainty O
information O
, O
hierarchical O
rep- O
resentations O
, O
and O
multiple O
word O
senses O
. O

In O
partic- O
ular O
, O
we O
represent O
each O
word O
with O
a O
Gaussian O
or O
a O
Gaussian O
mixture O
density O
, O
which O
we O
name O
PFT B-MethodName
- I-MethodName
G I-MethodName
and O
PFT B-MethodName
- I-MethodName
GM I-MethodName
respectively O
. O

Each O
component O
of O
the O
mixture O
can O
represent O
different O
word O
senses O
, O
and O
the O
mean O
vectors O
of O
each O
component O
decompose O
into O
vectors O
of O
n O
- O
grams O
, O
to O
capture O
character O
- O
level O
information O
. O

We O
also O
derive O
an O
efﬁcient O
energy- O
based O
max O
- O
margin O
training O
procedure O
for O
PFT B-MethodName
. O

We O
perform O
comparison O
with O
F B-MethodName
ASTTEXT I-MethodName
as O
well O
as O
existing O
density O
word O
embeddings O
W2 B-MethodName
G I-MethodName
( O
Gaussian O
) O
and O
W2GM B-MethodName
(Gaussian O
mixture O
) O
. O

Our O
models O
extract O
high O
- O
quality O
semantics O
based O
on O
multiple O
word O
- O
similarity O
benchmarks O
, O
including O
the O
rare B-DatasetName
word I-DatasetName
dataset O
. O

We O
obtain O
an O
average O
weighted O
improvement O
of O
3.7% B-MetricValue
over O
F B-MethodName
ASTTEXT I-MethodName
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
and O
3.1 B-MetricValue
% I-MetricValue
over O
the O
dictionary B-MethodName
- I-MethodName
level I-MethodName
density I-MethodName
- I-MethodName
based I-MethodName
models I-MethodName
. O

We O
also O
observe O
meaningful O
nearest O
neighbors O
, O
particu- O
larly O
in O
the O
multimodal O
density O
case O
, O
where O
each O
mode O
captures O
a O
distinct O
meaning O
. O

Our O
models O
are O
also O
directly O
portable O
to O
foreign O
languages O
with- O
out O
any O
hyperparameter O
modiﬁcation O
, O
where O
we O
observe O
strong O
performance O
, O
outperforming O
F B-MethodName
AST- I-MethodName
TEXT I-MethodName
on O
many O
foreign O
word O
similarity O
datasets O
. O

Our O
multimodal O
word O
representation O
can O
also O
dis- O
entangle O
meanings O
, O
and O
is O
able O
to O
separate O
differ- O
ent O
senses O
in O
foreign O
polysemies O
. O

In O
particular O
, O
our O
models O
attain O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
SCWS B-DatasetName
, O
a O
benchmark O
to O
measure O
the O
ability O
to O
sep- O
arate O
different O
word O
meanings O
, O
achieving O
1.0% B-MetricValue
im- O
provement O
over O
a O
recent O
density O
embedding O
model O
W2GM B-MethodName
(Athiwaratkun O
and O
Wilson O
, O
2017 O
) O
. O

To O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
ﬁrst O
to O
develop O
multi O
- O
sense O
embeddings O
with O
high O
se- O
mantic O
quality O
for O
rare O
words O
. O

Our O
code O
and O
em- O
beddings O
are O
publicly O
available.1 O
2 O
Related O
Work O
Early O
word O
embeddings O
which O
capture O
semantic O
information O
include O
Bengio O

et O
al O
. O

( O
2003 O
) O
, O
Col- O
lobert O
and O
Weston O
( O
2008 O
) O
, O
and O
Mikolov O

et O

al O
. O
( O
2011 O
) O
. O

Later O
, O
Mikolov O
et O
al O
. O

( O
2013a O
) O
developed O
the O
popular O
W B-MethodName
ORD2VEC I-MethodName
method O
, O
which O
proposes O
a O
log O
- O
linear O
model O
and O
negative O
sampling O
ap- O
proach O
that O
efﬁciently O
extracts O
rich O
semantics O
from O
text O
. O

Another O
popular O
approach O
G B-MethodName
LOVE I-MethodName
learns O
word O
embeddings O
by O
factorizing O
co O
- O
occurrence O
matrices O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

Recently O
there O
has O
been O
a O
surge O
of O
interest O
in O
making O
dictionary O
- O
based O
word O
embeddings O
more O
ﬂexible O
. O

This O
ﬂexibility O
has O
valuable O
applica- O

tions O
in O
many O
end O
- O
tasks O
such O
as O
language O
mod- O
eling O
( O
Kim O
et O
al O
. O
, O
2016 O
) O
, O
named O
entity O
recogni- O
tion O
( O
Kuru O
et O
al O
. O
, O
2016 O
) O
, O
and O
machine O
translation O
( O
Zhao O
and O
Zhang O
, O
2016 O
; O
Lee O
et O
al O
. O
, O
2017 O
) O
, O
where O
unseen O
words O
are O
frequent O
and O
proper O
handling O
of O
these O
words O
can O
greatly O
improve O
the O
performance O
. O

These O
works O
focus O
on O
modeling O
subword O
informa- O
tion O
in O
neural O
networks O
for O
tasks O
such O
as O
language O
modeling O
. O

Besides O
vector O
embeddings O
, O
there O
is O
recent O
work O
on O
multi O
- O
prototype O
embeddings O
where O
each O
word O
is O
represented O
by O
multiple O
vectors O
. O

The O
learn- O
ing O
approach O
involves O
using O
a O
cluster O
centroid O
of O
context O
vectors O
( O
Huang O
et O
al O
. O
, O
2012 O
) O
, O
or O
adapt- O
ing O
the O
skip O
- O
gram O
model O
to O
learn O
multiple O
latent O
representations O
( O
Tian O
et O
al O
. O
, O
2014 O
) O
. O

Neelakan- O
tan O
et O
al O
. O

( O
2014 O
) O
furthers O
adapts O
skip O
- O
gram O
with O
a O
non O
- O
parametric O
approach O
to O
learn O
the O
embed- O
dings O
with O
an O
arbitrary O
number O
of O
senses O
per O
word O
. O

Chen O
et O
al O
. O

( O
2014 O
) O
incorporates O
an O
external O
dataset O
WORDNET B-DatasetName
to O
learn O
sense O
vectors O
. O

We O
compare O
these O
models O
with O
our O
multimodal O
embeddings O
in O
Section O
4 O
. O
3 O
Probabilistic B-MethodName
FastText I-MethodName
We O
introduce O
Probabilistic B-MethodName
FastText I-MethodName
, O
which O
com- O
bines O
a O
probabilistic O
word O
representation O
with O
the O
ability O
to O
capture O
subword O
structure O
. O

We O
describe O
the O
probabilistic O
subword O
representation O
in O
Sec- O
tion O
3.1 O
. O

We O
then O
describe O
the O
similarity O
measure O
and O
the O
loss O
function O
used O
to O
train O
the O
embeddings O
in O
Sections O
3.2 O
and O
3.3 O
. O

We O
conclude O
by O
brieﬂy O
presenting O
a O
simpliﬁed O
version O
of O
the O
energy O
func- O
tion O
for O
isotropic O
Gaussian O
representations O
( O
Sec- O
tion O
3.4 O
) O
, O
and O
the O
negative O
sampling O
scheme O
we O
use O
in O
training O
( O
Section O
3.5 O
) O
. O

3.1 O
Probabilistic O
Subword O
Representation O
We O
represent O
each O
word O
with O
a O
Gaussian O
mixture O
with O
K B-HyperparameterName
Gaussian O
components O
. O

That O
is O
, O
a O
word O
wis O
associated O
with O
a O
density O
function O
f(x O
) O

= O
PK O
i=1pw;iN(x;~ O
w;i;w;i)wherefw;igK O
k=1are O
the O
mean O
vectors O
and O
fw;igare O
the O
covariance O
matrices O
, O
andfpw;igK O
k=1are O
the O
component O
prob- O
abilities O
which O
sum O
to O
1 O
. O

The O
mean O
vectors O
of O
Gaussian O
components O
hold O
much O
of O
the O
semantic O
information O
in O
density O
em- O

beddings O
. O

While O
these O
models O
are O
successful O
based O
on O
word O
similarity O
and O
entailment O
bench- O
marks O
( O
Vilnis O
and O
McCallum O
, O
2014 O
; O
Athiwaratkun O
and O
Wilson O
, O
2017 O
) O
, O
the O
mean O
vectors O
are O
often O
dictionary O
- O
level O
, O
which O
can O
lead O
to O
poor O
semantic O
estimates O
for O
rare O
words O
, O
or O
the O
inability O
to O
handle O
words O
outside O
the O
training O
corpus O
. O

We O
propose O
us- O
ing O
subword O
structures O
to O
estimate O
the O
mean O
vec- O
tors O
. O

We O
outline O
the O
formulation O
below O
. O

For O
wordw O
, O
we O
estimate O
the O
mean O
vector O
w O
with O
the O
average O
over O
n O
- O
gram O
vectors O
and O
its O
dictionary O
- O
level O
vector O
. O

That O
is O
, O
w=1 O
jNGwj+ O
10 O
@vw+X O
g2NGwzg1 O
A O
( O
1 O
) O
wherezgis O
a O
vector O
associated O
with O
an O
n O
- O
gram O
g O
, O
vwis O
the O
dictionary O
representation O
of O
word O
w O
, O
and O
NGwis O
a O
set O
ofn O
- O
grams O
of O
word O
w. O
Examples O
of O
3,4 O
- O
grams O
for O
a O
word O
“ O
beautiful O
” O
, O
including O
thebeginning O
- O
of O
- O
word O
character O
‘ O
h O
’ O
and O
end O
- O
of O
- O
word O
character O
‘ O
i O
’ O
, O
are O
: O
3 O
- O
grams O
: O
hbe O
, O
bea O
, O
eau O
, O
aut O
, O
uti O
, O
tif O
, O
ful O
, O
ul O
i O
4 O
- O
grams O
: O
hbea O
, O
beau O
.. O
, O
iful O
, O
ful O

i O
This O
structure O
is O
similar O
to O
that O
of O
F B-MethodName
ASTTEXT I-MethodName
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
; O
however O
, O
we O
note O
that O
F B-MethodName
ASTTEXT I-MethodName
uses O
single O
- O
prototype O
determinis- O
tic O
embeddings O
as O
well O
as O
a O
training O
approach O
that O
maximizes O
the O
negative O
log O
- O
likelihood O
, O
whereas O
we O
use O
a O
multi O
- O
prototype O
probabilistic O
embedding O
and O
for O
training O
we O
maximize O
the O
similarity O
be- O
tween O
the O
words O
’ O
probability O
densities O
, O
as O
de- O
scribed O
in O
Sections O
3.2 O
and O
3.3 O
Figure O
1a O
depicts O
the O
subword O
structure O
for O
the O
mean O
vector O
. O

Figure O
1b O
and O
1c O
depict O
our O
models O
, O
Gaussian B-MethodName
probabilistic I-MethodName
F I-MethodName
ASTTEXT I-MethodName
( O
PFT- B-MethodName
G I-MethodName
) O
and O
Gaussian B-MethodName
mixture I-MethodName
probabilistic I-MethodName
F I-MethodName
ASTTEXT I-MethodName
( O
PFT B-MethodName
- I-MethodName
GM I-MethodName
) O
. O

In O
the O
Gaussian O
case O
, O
we O
represent O
each O
mean O
vector O
with O
a O
subword O
estimation O
. O

For O
the O
Gaussian O
mixture O
case O
, O
we O
represent O
one O
Gaus- O
sian O
component O
’s O
mean O
vector O
with O
the O
subword O
structure O
whereas O
other O
components O
’ O
mean O
vec- O
tors O
are O
dictionary O
- O
based O
. O

This O
model O
choice O
to O
use O
dictionary O
- O
based O
mean O
vectors O
for O
other O
com- O
ponents O
is O
to O
reduce O
to O
constraint O
imposed O
by O
the O
subword O
structure O
and O
promote O
independence O
for O
meaning O
discovery O
. O

3.2 O
Similarity O
Measure O
between O
Words O
Traditionally O
, O
if O
words O
are O
represented O
by O
vec- O
tors O
, O
a O
common O
similarity O
metric O
is O
a O
dot O
prod- O
uct O
. O

In O
the O
case O
where O
words O
are O
represented O
by O
distribution O
functions O
, O
we O
use O
the O
general- O
ized O
dot O
product O
in O
Hilbert O
space O
h;iL2 O
, O
which O
is O
called O
the O
expected B-MetricName
likelihood I-MetricName
kernel I-MetricName
( O
Jebara O
et O
al O
. O
, O
2004 O
) O
. O

We O
deﬁne O
the O
energy O
E(f;g O
) O
between O
two O
words O
fandgto O
beE(f;g O
) O

= O
loghf;giL2= O

logR O
f(x)g(x)dx O
. O

With O
Gaussian O
mixturesf(x O
) O
= O
PK O
i=1piN(x;~ O
f;i;f;i)and O
g(x O
) O
= O
PK O
i=1qiN(x;~ O
g;i;g;i),PK O
i=1pi= O
1 O
, O
andPK O
i=1qi= O
1 O
, O
the O
energy O
has O
a O
closed O
form O
: O
E(f;g O
) O
= O

logKX O
j=1KX O
i=1piqjei;j(2 O
) O
wherej;jis O
the O
partial O
energy O
which O
corresponds O
to O
the O
similarity O
between O
component O
iof O
the O
ﬁrst O

4wordfand O
component O
jof O
the O
second O
word O
g.2 O
i;jlogN(0;~ O
f;i ~ O
g;j;f;i+ O
g;j O
) O

= O
 1 O
2log O
det( O
f;i+ O
g;j) D O
2log(2 O
) O
 1 O

2(~ O
f;i ~ O
g;j)>(f;i+ O
g;j) 1(~ O
f;i ~ O
g;j O
) O
( O
3 O
) O
Figure O
2 O
demonstrates O
the O
partial O
energies O
among O
the O
Gaussian O
components O
of O
two O
words O
. O

3.3 O
Loss O
Function O
The O
model O
parameters O
that O
we O
seek O
to O
learn O
are O
vw O
for O
each O
word O
wandzgfor O
each O
n O
- O
gram O
g. O
We O
train O
the O
model O
by O
pushing O
the O
energy O
of O
a O
true O
context O
pair O
wandcto O
be O
higher O
than O
the O
nega- O
tive O
context O
pair O
wandnby O

a O
margin O
m. O
We O
use O
Adagrad O
( O
Duchi O
et O
al O
. O
, O
2011 O
) O
to O
minimize O
the O
fol- O
lowing O
loss O
to O
achieve O
this O
outcome O
: O
L(f;g O
) O
= O
max O
[ O
0;m E(f;g O
) O
+ O
E(f;n)]:(4 O
) O
We O
describe O
how O
to O
sample O
words O
as O
well O
as O
its O
positive O
and O
negative O
contexts O
in O
Section O
3.5 O
. O

This O
loss O
function O
together O
with O
the O
Gaussian O
mixture O
model O
with O
K B-HyperparameterName
> B-HyperparameterValue
1 I-HyperparameterValue
has O
the O
ability O
to O
extract O
multiple O
senses O
of O
words O
. O

That O
is O
, O
for O
a O
word O
with O
multiple O
meanings O
, O
we O
can O
observe O
each O
mode O
to O
represent O
a O
distinct O
meaning O
. O

For O
in- O
stance O
, O
one O
density O
mode O
of O
“ O
star O
” O
is O
close O
to O
the O
densities O
of O
“ O
celebrity O
” O
and O
“ O
hollywood O
” O
whereas O
another O
mode O
of O
“ O
star O
” O
is O
near O
the O
densities O
of O
“ O
constellation O
” O
and O
“ O
galaxy O
” O
. O

3.4 O
Energy O
Simpliﬁcation O
In O
theory O
, O
it O
can O
be O
beneﬁcial O
to O
have O
covari- O
ance O
matrices O
as O
learnable O
parameters O
. O

In O
prac- O
tice O
, O
Athiwaratkun O
and O
Wilson O
( O
2017 O
) O
observe O
that O
spherical O
covariances O
often O
perform O
on O
par O
with O
diagonal O
covariances O
with O
much O
less O
computa- O
tional O
resources O
. O

Using O
spherical O
covariances O
for O
each O
component O
, O
we O
can O
further O
simplify O
the O
en- O
ergy O
function O
as O
follows O
: O
i;j=  O

  O
2jjf;i g;jjj2 O
; O
( O
5 O
) O
where O
the O
hyperparameter O

is O
the O
scale B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
in- I-HyperparameterName
verse I-HyperparameterName
covariance I-HyperparameterName
term O
in O
Equation O
3 O
. O

We O
note O
that O
Equation O
5 O
is O
equivalent O
to O
Equation O
3 O
up O
to O
an O
ad- O
ditive O
constant O
given O
that O
the O
covariance O
matrices O
are O
spherical O
and O
the O
same O
for O
all O
components O
. O

3.5 O
Word O
Sampling O
To O
generate O
a O
context O
word O
cof O
a O
given O
word O
w O
, O
we O
pick O
a O
nearby O
word O
within O
a O
context O
window O
of O
a O
ﬁxed B-HyperparameterName
length I-HyperparameterName
` O
. O

We O
also O
use O
a O
word O
sampling O
technique O
similar O
to O
Mikolov O
et O

al O
. O
( O
2013b O
) O
. O

This O
subsampling O
procedure O
selects O
words O
for O
training O
with O
lower O
probabilities O
if O
they O
appear O
frequently O
. O

This O
technique O
has O
an O
effect O
of O
reducing O
the O
impor- O
tance O
of O
words O
such O
as O
‘ O
the O
’ O
, O
‘ O
a O
’ O
, O
‘ O
to O
’ O
which O
can O
be O
predominant O
in O
a O
text O
corpus O
but O
are O
not O
as O
mean- O
ingful O
as O
other O
less O
frequent O
words O
such O
as O
‘ O
city O
’ O
, O
‘ O
capital O
’ O
, O
‘ O
animal O
’ O
, O
etc O
. O

In O
particular O
, O
word O
whas O
probabilityP(w O
) O

= O
1 p O
t O
= O
f(w)wheref(w)is O
the O
frequency O
of O
word O
win O
the O
corpus O
and O
tis O
the O
frequency O
threshold O
. O

A O
negative O
context O
word O
is O
selected O
using O
a O
dis- O
tributionPn(w)/U(w)3=4whereU(w)is O
a O
un- O
igram O
probability O
of O
word O

w. O

The O
exponent O
3=4 O
also O
diminishes O
the O
importance O
of O
frequent O
words O
and O
shifts O
the O
training O
focus O
to O
other O
less O
frequent O
words O
. O

4 O
Experiments O
We O
have O
proposed O
a O
probabilistic B-MethodName
F I-MethodName
ASTTEXT I-MethodName
model O
which O
combines O
the O
ﬂexibility O
of O
subword O
structure O
with O
the O
density O
embedding O
approach O
. O

In O
this O
section O
, O
we O
show O
that O
our O
probabilistic O
representation O
with O
subword O
mean O
vectors O
with O
the O
simpliﬁed O
energy O
function O
outperforms O
many O
word O
similarity O
baselines O
and O
provides O
disentan- O
gled O
meanings O
for O
polysemies O
. O

First O
, O
we O
describe O
the O
training O
details O
in O
Section O
4.1 O
. O

We O
provide O
qualitative O
evaluation O
in O
Section O

54.2 O
, O
showing O
meaningful O
nearest O
neighbors O
for O
the O
Gaussian O
embeddings O
, O
as O
well O
as O
the O
ability O
to O
capture O
multiple O
meanings O
by O
Gaussian O
mixtures O
. O

Our O
quantitative O
evaluation O
in O
Section O
4.3 O
demon- O
strates O
strong O
performance O
against O
the O
baseline O
models O
F B-MethodName
ASTTEXT I-MethodName
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
and O
the O
dictionary O
- O
level O
Gaussian O
( O
W2 O
G O
) O
( O
Vilnis O
and O
McCallum O
, O
2014 O
) O
and O
Gaussian O
mixture O
embed- O
dings O
( O
Athiwaratkun O
and O
Wilson O
, O
2017 O
) O
( O
W2GM O
) O
. O

We O
train O
our O
models O
on O
foreign O
language O
corpuses O
and O
show O
competitive O
results O
on O
foreign O
word O
sim- O
ilarity O
benchmarks O
in O
Section O
4.4 O
. O

Finally O
, O
we O
ex- O
plain O
the O
importance O
of O
the O
n O
- O
gram O
structures O
for O
semantic O
sharing O
in O
Section O
4.5 O
. O

4.1 O
Training O
Details O
We O
train O
our O
models O
on O
both O
English O
and O
for- O
eign O
language O
datasets O
. O

For O
English O
, O
we O
use O
the O
concatenation O
of O
UKWAC B-DatasetName
and O
W B-DatasetName
ACKY I-DatasetName
PEDIA I-DatasetName
( O
Ba- O
roni O
et O
al O
. O
, O
2009 O
) O
which O
consists O
of O
3:376billion O
words O
. O

We O
ﬁlter O
out O
word O
types O
that O
occur O
fewer O
than5times O
which O
results O
in O
a O
vocabulary O
size O
of O
2,677,466 O
. O

For O
foreign O
languages O
, O
we O
demonstrate O
the O
training O
of O
our O
model O
on O
French O
, O
German O
, O
and O
Ital- O
ian O
text O
corpuses O
. O

We O
note O
that O
our O
model O
should O
be O
applicable O
for O
other O
languages O
as O
well O
. O

We O
use O
F B-DatasetName
RWAC I-DatasetName
(French O
) O
, O
D B-DatasetName
EWAC I-DatasetName
(German O
) O
, O
I B-DatasetName
TWAC I-DatasetName
( O
Italian O
) O
datasets O
( O
Baroni O
et O
al O
. O
, O
2009 O
) O
for O
text O
cor- O
puses O
, O
consisting O
of O
1:634,1:716and1:955billion O
words O
respectively O
. O

We O
use O
the O
same O
threshold O
, O
ﬁltering O
out O
words O
that O
occur O
less O
than O
5times O
in O
each O
corpus O
. O

We O
have O
dictionary O
sizes O
of O
1:3,2:7 O
, O
and1:4million O
words O
for O
F B-DatasetName
RWAC I-DatasetName
, O
DEWAC B-DatasetName
, O
and O
ITWAC B-DatasetName
. O

We O
adjust O
the O
hyperparameters O
on O
the O
English O
corpus O
and O
use O
them O
for O
foreign O
languages O
. O

Note O
that O
the O
adjustable O
parameters O
for O
our O
models O
are O
the O
loss B-HyperparameterName
margin I-HyperparameterName
m B-HyperparameterName
in O
Equation O
4 O
and O
the O
scale B-HyperparameterName

  O
in O
Equation O
5 O
. O

We O
search O
for O
the O
optimal O
hyperpa- O
rameters O
in O
a O
grid O
m B-HyperparameterName
2f O
0.01 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
1 B-HyperparameterValue
, O
10 B-HyperparameterValue
, O
100 B-HyperparameterValue
, O
g O
and O

2f1 O

5*10-3/1 B-HyperparameterValue
, O
10-3/1 B-HyperparameterValue
, O
2*10-4/1 B-HyperparameterValue
, O
1*10-4/1 B-HyperparameterValue
gon O
our O
En- O
glish O
corpus O
. O

The O
hyperpameter O

affects O
the O
scale O
of O
the O
loss O
function O
; O
therefore O
, O
we O
adjust O
the O
learn- B-HyperparameterName
ing I-HyperparameterName
rate I-HyperparameterName
appropriately O
for O
each O

. O

In O
particular O
, O
the O
learning B-HyperparameterName
rates I-HyperparameterName
used O
are O

= O
f
10-4 B-HyperparameterValue
, O
10-5 B-HyperparameterValue
, O
10-6 B-HyperparameterValue
g O
for O
the O
respective O

values O
. O

Other O
ﬁxed O
hyperparameters O
include O
the O
num- B-HyperparameterName
ber I-HyperparameterName
of I-HyperparameterName
Gaussian I-HyperparameterName
components I-HyperparameterName
K B-HyperparameterName
= O
2 B-HyperparameterValue
, O
the O
con- B-HyperparameterName
text I-HyperparameterName
window I-HyperparameterName
length I-HyperparameterName
` O
= O
10 B-HyperparameterValue
and O
the O
subsampling B-HyperparameterName
threshold I-HyperparameterName
t B-HyperparameterName
= O
10-5 B-HyperparameterValue
. O

Similar O
to O
the O
setup O
in O
F B-MethodName
AST I-MethodName
- I-MethodName
TEXT I-MethodName
, O
we O
use O
n O
- O
grams O
where O
n B-HyperparameterName
= O

3 B-HyperparameterValue
, O
4 B-HyperparameterValue
, O
5 B-HyperparameterValue
, O
6 B-HyperparameterValue
to O
es- O
timate O
the O
mean O
vectors O
. O

4.2 O
Qualitative O
Evaluation O
- O
Nearest O
neighbors O
We O
show O
that O
our O
embeddings O
learn O
the O
word O
se- O
mantics O
well O
by O
demonstrating O
meaningful O
nearest O
neighbors O
. O

Table O
1 O
shows O
examples O
of O
polysemous O
words O
such O
as O
rock O
, O
star O
, O
andcell O
. O

Table O
1 O
shows O
the O
nearest O
neighbors O
of O
polyse- O
mous O
words O
. O

We O
note O
that O
subword O
embeddings O
prefer O
words O
with O
overlapping O
characters O
as O
near- O
est O
neighbors O
. O

For O
instance O
, O
“ O
rock O
- O
y O
” O
, O
“ O
rockn O
” O
, O
and O
“ O
rock O
” O
are O
both O
close O
to O
the O
word O
“ O
rock O
” O
. O

For O
the O
purpose O
of O
demonstration O
, O
we O
only O
show O
words O
with O
meaningful O
variations O
and O
omit O
words O
with O
small O
character O
- O
based O
variations O
previously O
men- O
tioned O
. O

However O
, O
all O
words O
shown O
are O
in O
the O
top- O
100 O
nearest O
words O
. O

We O
observe O
the O
separation O
in O
meanings O
for O
the O
multi O
- O
component O
case O
; O
for O
instance O
, O
one O
compo- O
nent O
of O
the O
word O
“ O
bank O
” O
corresponds O
to O
a O
ﬁnancial O
bank O
whereas O
the O
other O
component O
corresponds O
to O
a O
river O
bank O
. O

The O
single O
- O
component O
case O
also O
has O
interesting O
behavior O
. O

We O
observe O
that O
the O
subword O
embeddings O
of O
polysemous O
words O
can O
represent O
both O
meanings O
. O

For O
instance O
, O
both O
“ O
lava O
- O
rock O
” O
and O
“ O
rock O
- O
pop O
” O
are O
among O
the O
closest O
words O
to O
“ O
rock O
” O
. O

4.3 O
Word O
Similarity O
Evaluation O
We O
evaluate O
our O
embeddings O
on O
several O
standard O
word O
similarity O
datasets O
, O
namely O
, O
SL-999 B-DatasetName
( O
Hill O
et O
al O
. O
, O
2014 O
) O
, O
WS-353 B-DatasetName
( O
Finkelstein O
et O
al O
. O
, O
2002 O
) O
, O
MEN-3k B-DatasetName
( O
Bruni O
et O
al O
. O
, O
2014 O
) O
, O
MC-30 B-DatasetName
( O
Miller O
and O
Charles O
, O
1991 O
) O
, O
RG-65 B-DatasetName
( O
Rubenstein O
and O
Goode- O
nough O
, O
1965 O
) O
, O
YP-130 B-DatasetName
( O
Yang O
and O
Powers O
, O
2006 O
) O
, O
MTurk(-287,-771 B-DatasetName
) I-DatasetName
( O
Radinsky O
et O
al O
. O
, O
2011 O
; O
Halawi O
et O
al O
. O
, O
2012 O
) O
, O
and O
RW-2k B-DatasetName
( O
Luong O
et O
al O
. O
, O
2013 O
) O
. O

Each O
dataset O
contains O
a O
list O
of O
word O
pairs O
with O
a O
human O
score O
of O
how O
related O
or O
similar O
the O
two O
words O
are O
. O

We O
use O
the O
notation O
DATASET O
-NUM O
to O
denote O
the O
number O
of O
word O
pairs O
NUM O
in O
each O
evaluation O
set O
. O

We O
note O
that O
the O
dataset O
RW B-DatasetName
fo- O
cuses O
more O
on O
infrequent O
words O
and O
SimLex-999 B-DatasetName
focuses O
on O
the O
similarity O
of O
words O
rather O
than O
re- O
latedness O
. O

We O
also O
compare O
PFT B-MethodName
- I-MethodName
GM I-MethodName
with O
other O
multi O
- O
prototype O
embeddings O
in O
the O
literature O
us- O
ing O

SCWS B-DatasetName
( O
Huang O
et O
al O
. O
, O
2012 O
) O
, O
a O
word O
similar- O

ity O
dataset O
that O
is O
aimed O
to O
measure O
the O
ability O
of O
embeddings O
to O
discern O
multiple O
meanings O
. O

We O
calculate O
the O
Spearman B-MetricName
correlation I-MetricName
( O
Spear- O
man O
, O
1904 O
) O
between O
the O
labels O
and O
our O
scores O
gen- O
erated O
by O
the O
embeddings O
. O

The O
Spearman B-MetricName
corre- I-MetricName
lation I-MetricName
is O
a O
rank O
- O
based O
correlation O
measure O
that O
as- O
sesses O
how O
well O
the O
scores O
describe O
the O
true O
labels O
. O

The O
scores O
we O
use O
are O
cosine O
- O
similarity O
scores O
be- O
tween O
the O
mean O
vectors O
. O

In O
the O
case O
of O
Gaussian O
mixtures O
, O
we O
use O
the O
pairwise O
maximum O
score O
: O
s(f;g O
) O

= O
max O
i21;:::;Kmax O
j21;:::;Kf;ig;j O
jjf;ijjjjg;jjj:(6 O
) O

The O
pair O
( O
i;j)that O
achieves O
the O
maximum O
cosine O
similarity O
corresponds O
to O
the O
Gaussian O
component O
pair O
that O
is O
the O
closest O
in O
meanings O
. O

Therefore O
, O
this O
similarity O
score O
yields O
the O
most O
related O
senses O
of O
a O
given O
word O
pair O
. O

This O
score O
reduces O
to O
a O
cosine O
similarity O
in O
the O
Gaussian O
case O
( O
K B-HyperparameterName
= O
1 B-HyperparameterValue
) O
.4.3.1 O

Comparison O
Against O
Dictionary B-MethodName
- I-MethodName
Level I-MethodName
Density I-MethodName
Embeddings I-MethodName
and O
FASTTEXT B-MethodName
We O
compare O
our O
models O
against O
the O
dictionary- B-MethodName
level I-MethodName
Gaussian I-MethodName
and I-MethodName
Gaussian I-MethodName
mixture I-MethodName
embed- I-MethodName
dings I-MethodName
in O
Table O
2 O
, O
with O
50 O
- O
dimensional O
and O
300- O
dimensional O
mean O
vectors O
. O

The O
50 O
- O
dimensional O
results O
for O
W2G B-MethodName
and O
W2GM B-MethodName
are O
obtained O
directly O
from O
Athiwaratkun O
and O
Wilson O
( O
2017 O
) O
. O

For O
com- O
parison O
, O
we O
use O
the O
public O
code3to O
train O
the O
300- O
dimensional O
W2G B-MethodName
and O
W2GM B-MethodName
models O
and O
the O
pub- O
licly O
available O
F B-MethodName
ASTTEXT I-MethodName
model4 O
. O

We O
calculate O
Spearman B-MetricName
’s I-MetricName
correlations I-MetricName
for O
each O
of O
the O
word O
similarity O
datasets O
. O

These O
datasets O
vary O
greatly O
in O
the O
number O
of O
word O
pairs O
; O
there- O
fore O
, O
we O
mark O
each O
dataset O
with O
its O
size O
for O
visibil- O
ity O
. O

For O
a O
fair O
and O
objective O
comparison O
, O
we O
cal- O
culate O
a O
weighted O
average O
of O
the O
correlation O
scores O
for O
each O
model O
. O

Our O
PFT B-MethodName
- I-MethodName
GM I-MethodName
achieves O
the O
highest O
average O
score O
among O
all O
competing O
models O
, O
outperforming O
both O
FASTTEXT B-MethodName
and O
the O
dictionary O
- O
level O
embeddings O
W2G B-MethodName
and O
W2GM B-MethodName
. O

Our O
unimodal O
model O
PFT B-MethodName
- I-MethodName
G I-MethodName
also O
outperforms O
the O
dictionary O
- O
level O
counterpart O
W2 B-MethodName
G I-MethodName
and O
F B-MethodName
ASTTEXT I-MethodName
. O

We O
note O
that O
the O
model O
W2GM B-MethodName
appears O
quite O
strong O
according O
to O
Table O
2 O
, O
beating O
PFT B-MethodName
- I-MethodName
GM I-MethodName
on O
many O
word O
similarity O
datasets O
. O

How- O
ever O
, O
the O
datasets O
that O
W2GM B-MethodName
performs O
better O
than O
PFT B-MethodName
- I-MethodName
GM I-MethodName
often O
have O
small O
sizes O
such O
as O
MC-30 B-DatasetName
or O
RG-65 B-DatasetName
, O
where O
the O
Spearman B-MetricName
’s I-MetricName
correlations I-MetricName
are O
more O
subject O
to O
noise O
. O

Overall O
, O
PFT B-MethodName
- I-MethodName
GM I-MethodName
outper- O
forms O
W2GM B-DatasetName
by O
3.1% B-MetricValue
and O
8.7% B-MetricValue
in O
300 O
and O 
50di- O
mensional O
models O
. O

In O
addition O
, O
PFT B-MethodName
- I-MethodName
G I-MethodName
and O
PFT B-MethodName
- I-MethodName
GM I-MethodName
also O
outperform O
F B-MethodName
ASTTEXT I-MethodName
by O
1.2% B-MetricValue
and O
3.7% B-MetricValue
re- O
spectively O
. O

4.3.2 O
Comparison O
Against O
Multi O
- O
Prototype O
Models O
In O
Table O
3 O
, O
we O
compare O
50and300dimensional O
PFT B-MethodName
- I-MethodName
GM I-MethodName
models O
against O
the O
multi O
- O
prototype O
em- O
beddings O
described O
in O
Section O
2 O
and O
the O
existing O
multimodal O
density O
embeddings O
W2GM B-MethodName
. O

We O
use O
the O
word O
similarity O
dataset O
SCWS B-DatasetName
( O
Huang O
et O
al O
. O
, O
2012 O
) O
which O
contains O
words O
with O
potentially O
many O
meanings O
, O
and O
is O
a O
benchmark O
for O
distinguishing O
senses O
. O

We O
use O
the O
maximum B-MetricName
similarity I-MetricName
score O
( O
Equation O
6 O
) O
, O
denoted O
as O
M B-MetricName
AXSIM I-MetricName
. O

AVESIM B-MetricName
de- O
notes O
the O
average B-MetricName
of I-MetricName
the I-MetricName
similarity I-MetricName
scores I-MetricName
, O
rather O
than O
the O
maximum O
. O

We O
outperform O
the O
dictionary O
- O
based O
density O
embeddings O
W2GM B-MethodName
in O
both O
50and300 O
dimen- O
sions O
, O
demonstrating O
the O
beneﬁts O
of O
subword O
in- O

formation O
. O

Our O
model O
achieves O
state O
- O
of O
- O
the O
- O
art O
re- O
sults O
, O
similar O
to O
that O
of O
Neelakantan O
et O
al O
. O

( O
2014 O
) O
. O

4.4 O
Evaluation O
on O
Foreign O
Language O
Embeddings O
We O
evaluate O
the O
foreign O
- O
language O
embeddings O
on O
word O
similarity O
datasets O
in O
respective O
lan- O
guages O
. O

We O
use O
Italian B-DatasetName
W I-DatasetName
ORDSIM353 I-DatasetName
and O
Ital- B-DatasetName
ian I-DatasetName
S I-DatasetName
IMLEX-999 I-DatasetName
( O
Leviant O
and O
Reichart O
, O
2015 O
) O
for O
Italian O
models O
, O
GUR350 B-DatasetName
and O
GUR65 B-DatasetName
( O
Gurevych O
, O
2005 O
) O
for O
German O
models O
, O
and O
French O
W B-DatasetName
ORD- I-DatasetName
SIM353 I-DatasetName
( O
Finkelstein O
et O
al O
. O
, O
2002 O
) O
for O
French O
mod- O
els O
. O

For O
datasets O
GUR350 B-DatasetName
and O
GUR65 B-DatasetName
, O
we O
use O
the O
results O
reported O
in O
the O
F B-MethodName
ASTTEXT I-MethodName
publication O
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
. O

For O
other O
datasets O
, O
we O
train O
F B-MethodName
ASTTEXT I-MethodName
models O
for O
comparison O
using O
the O
public O
code5on O
our O
text O
corpuses O
. O

We O
also O
train O
dictionary O
- O
level O
models O
W2 B-MethodName
G I-MethodName
, O
and O
W2GM B-MethodName
for O
com- O
parison O
. O

Table O
4 O
shows O
the O
Spearman B-MetricName
’s I-MetricName
correlation I-MetricName
re- O
sults O
of O
our O
models O
. O

We O
outperform O
F B-MethodName
ASTTEXT I-MethodName
on O
many O
word O
similarity O
benchmarks O
. O

Our O
results O
are O
also O
signiﬁcantly O
better O
than O
the O
dictionary O
- O
based O
models O
, O
W2G B-MethodName
and O
W2GM B-MethodName
. O

We O
hypothesize O
that O
W2G B-MethodName
and O
W2GM B-MethodName
can O
perform O
better O
than O
the O
cur- O
rent O
reported O
results O
given O
proper O
pre O
- O
processing O
of O
words O
due O
to O
special O
characters O
such O
as O
accents O
. O

We O
investigate O
the O
nearest O
neighbors O
of O
poly- O
semies O
in O
foreign O
languages O
and O
also O
observe O
clear O
sense O
separation O
. O

For O
example O
, O
piano O
in O
Italian O
can O
mean O
“ O
ﬂoor O
” O
or O
“ O
slow O
” O
. O

These O
two O
meanings O
are O
reﬂected O
in O
the O
nearest O
neighbors O
where O
one O
component O
is O
close O
to O
piano O
- O
piano O
, O
pianod O
which O
mean O
“ O
slowly O
” O
whereas O
the O
other O
component O
is O
close O
to O
piani O
( O
ﬂoors O
) O
, O
istrutturazione O
( O
renovation O
) O
orinfrastruttre O
( O
infrastructure O
) O
. O

Table O
5 O
shows O
ad- O
ditional O
results O
, O
demonstrating O
that O
the O
disentan- O
gled O
semantics O
can O
be O
observed O
in O
multiple O
lan- O
guages O
. O

4.5 O
Qualitative O
Evaluation O
- O
Subword O
Decomposition O
One O
of O
the O
motivations O
for O
using O
subword O
infor- O
mation O
is O
the O
ability O
to O
handle O
out O
- O
of O
- O
vocabulary O
words O
. O

Another O
beneﬁt O
is O
the O
ability O
to O
help O
im- O
prove O
the O
semantics O
of O
rare O
words O
via O
subword O
sharing O
. O

Due O
to O
an O
observation O
that O
text O
corpuses O
follow O
Zipf O
’s O
power O
law O
( O
Zipf O
, O
1949 O
) O
, O
words O
at O
the O
tail O
of O
the O
occurrence O
distribution O
appears O
much O
less O
frequently O
. O

Training O
these O
words O
to O
have O
a O
good O
semantic O
representation O
is O
challenging O
if O
done O
at O
the O
word O
level O
alone O
. O

However O
, O
an O
n- O
gram O
such O
as O
‘ O
abnorm O
’ O
is O
trained O
during O
both O
oc- O
currences O
of O
“ O
abnormal O
” O
and O
“ O
abnormality O
” O
in O
the O
corpus O
, O
hence O
further O
augments O
both O
words O
’s O
se- O
mantics O
. O

Figure O
3 O
shows O
the O
contribution O
of O
n O
- O
grams O
to O
the O
ﬁnal O
representation O
. O

We O
ﬁlter O
out O
to O
show O
only O
the O
n O
- O
grams O
with O
the O
top-5 O
and O
bottom-5 O
similarity O
scores O
. O

We O
observe O
that O
the O
ﬁnal O
representations O
of O
both O
words O
align O
with O
n O
- O
grams O
“ O
abno O
” O
, O
“ O
bnor O
” O
, O
“ O
abnorm O
” O
, O
“ O
anbnor O
” O
, O
“ O
< O
abn O
” O
. O

In O
fact O
, O
both O
“ O
ab- O
normal O
” O
and O
“ O
abnormality O
” O
share O
the O
same O
top-5 O
n O
- O
grams O
. O

Due O
to O
the O
fact O
that O
many O
rare O
words O
such O
as O
“ O
autobiographer O
” O
, O
“ O
circumnavigations O
” O
, O
or O
“ O
hypersensitivity O
” O
are O
composed O
from O
many O
com- O
mon O
sub O
- O
words O
, O
the O
n O
- O
gram O
structure O
can O
help O
im- O
prove O
the O
representation O
quality O
. O

5 O
Numbers O
of O
Components O
It O
is O
possible O
to O
train O
our O
approach O
with O
K B-HyperparameterName
> B-HyperparameterValue
2 I-HyperparameterValue
mixture O
components O
; O
however O
, O
Athiwaratkun O
and O
Wilson O
( O
2017 O
) O
observe O
that O
dictionary O
- O
level O
Gaus- O
sian O
mixtures O
with O
K B-HyperparameterName
= O
3 B-HyperparameterValue
do O
not O
overall O
im- O
prove O
word O
similarity O
results O
, O
even O
though O
these O
mixtures O
can O
discover O
3distinct O
senses O
for O
certain O
words O
. O

Indeed O
, O
while O
K B-HyperparameterName
> B-HyperparameterValue
2 I-HyperparameterValue
in O
principle O
allows O
for O
greater O
ﬂexibility O
than O
K B-HyperparameterName
= O
2 B-HyperparameterValue
, O
most O
words O
can O
be O
very O
ﬂexibly O
modelled O
with O
a O
mixture O
of O
two O
Gaussians O
, O
leading O
to O
K B-HyperparameterName
= O
2 B-HyperparameterValue
representing O
a O
good O
balance O
between O
ﬂexibility O
and O
Occam O
’s O
razor O
. O

Even O
for O
words O
with O
single O
meanings O
, O
our O
PFT B-MethodName
model O
with O
K B-HyperparameterName
= O
2 B-HyperparameterValue
often O
learns O
richer O
repre- O
sentations O
than O
a O
K B-HyperparameterName
= O
1 B-HyperparameterValue
model O
. O

For O
example O
, O
the O
two O
mixture O
components O
can O
learn O
to O
cluster O
to- O

9gether O
to O
form O
a O
more O
heavy O
tailed O
unimodal O
distri- O
bution O
which O
captures O
a O
word O
with O
one O
dominant O
meaning O
but O
with O
close O
relationships O
to O
a O
wide O
range O
of O
other O
words O
. O

In O
addition O
, O
we O
observe O
that O
our O
model O
with O
K B-HyperparameterName
components O
can O
capture O
more O
than O
K B-HyperparameterName
meanings O
. O

For O
instance O
, O
in O
K B-HyperparameterName
= O
1 B-HyperparameterValue
model O
, O
the O
word O
pairs O
( O
“ O
cell O
” O
, O
“ O
jail O
” O
) O
and O
( O
“ O
cell O
” O
, O
“ O
biology O
” O
) O
and O
( O
“ O
cell O
” O
, O
“ O
phone O
” O
) O
will O
all O
have O
positive O
similarity O
scores O
based O
on O
K B-HyperparameterName
= O
1 B-HyperparameterValue
model O
. O

In O
general O
, O
if O
a O
word O
has O
multiple O
meanings O
, O
these O
meanings O
are O
usually O
compressed O
into O
the O
linear O
substructure O
of O
the O
em- O
beddings O
( O
Arora O
et O
al O
. O
, O
2016 O
) O
. O

However O
, O
the O
pairs O
of O
non O
- O
dominant O
words O
often O
have O
lower O
similar- O
ity O
scores O
, O
which O
might O
not O
accurately O
reﬂect O
their O
true O
similarities O
. O

6 O
Conclusion O
and O
Future O
Work O
We O
have O
proposed O
models O
for O
probabilistic O
word O
representations O
equipped O
with O
ﬂexible O
sub O
- O
word O
structures O
, O
suitable O
for O
rare O
and O
out O
- O
of O
- O
vocabulary O
words O
. O

The O
proposed O
probabilistic O
formulation O
in- O
corporates O
uncertainty O
information O
and O
naturally O
allows O
one O
to O
uncover O
multiple O
meanings O
with O
multimodal O
density O
representations O
. O

Our O
models O
offer O
better O
semantic O
quality O
, O
outperforming O
com- O
peting O
models O
on O
word O
similarity O
benchmarks O
. O

Moreover O
, O
our O
multimodal O
density O
models O
can O
provide O
interpretable O
and O
disentangled O
representa- O
tions O
, O
and O
are O
the O
ﬁrst O
multi O
- O
prototype O
embeddings O
that O
can O
handle O
rare O
words O
. O

Future O
work O
includes O
an O
investigation O
into O
the O
trade O
- O
off O
between O
learning O
full O
covariance O
ma- O

trices O
for O
each O
word O
distribution O
, O
computational O
complexity O
, O
and O
performance O
. O

This O
direction O
can O
potentially O
have O
a O
great O
impact O
on O
tasks O
where O
the O
variance O
information O
is O
crucial O
, O
such O
as O
for O
hi- O
erarchical O
modeling O
with O
probability O
distributions O
( O
Athiwaratkun O
and O
Wilson O
, O
2018 O
) O
. O

Other O
future O
work O
involves O
co O
- O
training O
PFT B-MethodName
on O
many O
languages O
. O

Currently O
, O
existing O
work O
on O
multi O
- O
lingual O
embeddings O
align O
the O
word O
seman- O
tics O
on O
pre O
- O
trained O
vectors O
( O
Smith O
et O
al O
. O
, O
2017 O
) O
, O
which O
can O
be O
suboptimal O
due O
to O
polysemies O
. O

We O
envision O
that O
the O
multi O
- O
prototype O
nature O
can O
help O
disambiguate O
words O
with O
multiple O
meanings O
and O
facilitate O
semantic O
alignment O
. O

References O
Sanjeev O
Arora O
, O
Yuanzhi O
Li O
, O
Yingyu O
Liang O
, O
Tengyu O
Ma O
, O
and O
Andrej O
Risteski O
. O

2016 O
. O

Linear O
al O
- O
gebraic O
structure O
of O
word O
senses O
, O
with O
appli- O
cations O
to O
polysemy O
. O

CoRR O
abs/1601.03764 O
. O

http://arxiv.org/abs/1601.03764 O
. O

Ben O
Athiwaratkun O
and O
Andrew O
Gordon O
Wilson O
. O
2017 O
. O

Multimodal O
word O
distributions O
. O

In O
ACL O
. O
https://arxiv.org/abs/1704.08424 O
. O

Ben O
Athiwaratkun O
and O
Andrew O
Gordon O
Wilson O
. O

2018 O
. O

On O
modeling O
hierarchical O
data O
via O
probabilistic O
or- O
der O
embeddings O
. O

ICLR O
. O

Marco O
Baroni O
, O
Silvia O
Bernardini O
, O
Adriano O
Fer- O
raresi O
, O
and O
Eros O
Zanchetta O
. O

2009 O
. O

The O
wacky O
wide O
web O
: O
a O
collection O
of O
very O
large O
linguis- O
tically O
processed O
web O
- O
crawled O
corpora O
. O

Lan- O
guage O
Resources O
and O
Evaluation O
43(3):209–226 O
. O

https://doi.org/10.1007/s10579-009-9081-4 O
. O

Yoshua O
Bengio O
, O
R O
´ O
ejean O
Ducharme O
, O
Pascal O
Vin- O
cent O
, O
and O
Christian O
Janvin O
. O

2003 O
. O

A O
neu- O
ral O
probabilistic O
language O
model O
. O

Journal O
of O
Machine O
Learning O
Research O
3:1137–1155 O
. O
http://www.jmlr.org/papers/v3/bengio03a.html O
. O

Piotr O
Bojanowski O
, O
Edouard O
Grave O
, O
Armand O
Joulin O
, O
and O
Tomas O
Mikolov O
. O

2016 O
. O

Enriching O
word O
vectors O
with O
subword O
information O
. O

CoRR O
abs/1607.04606 O
. O

http://arxiv.org/abs/1607.04606 O
. O

Elia O
Bruni O
, O
Nam O
Khanh O
Tran O
, O
and O
Marco O
Ba- O
roni O
. O

2014 O
. O

Multimodal O
distributional O
se- O
mantics O
. O

J. O
Artif O
. O

Int O
. O

Res O
. O

49(1):1–47 O
. O

http://dl.acm.org/citation.cfm?id=2655713.2655714 O
. O

Xinxiong O
Chen O
, O
Zhiyuan O
Liu O
, O
and O
Maosong O
Sun O
. O

2014 O
. O

A O
uniﬁed O
model O
for O
word O
sense O
represen- O
tation O
and O
disambiguation O
. O

In O
Proceedings O
of O
the O
2014 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
EMNLP O
2014 O
, O
October O
25- O
29 O
, O
2014 O
, O
Doha O
, O
Qatar O
, O
A O
meeting O
of O
SIGDAT O
, O
a O
Spe- O
cial O
Interest O
Group O
of O
the O
ACL O
. O

pages O
1025–1035 O
. O

http://aclweb.org/anthology/D/D14/D14-1110.pdf O
. O

Ronan O
Collobert O
and O
Jason O
Weston O
. O

2008 O
. O

A O
uni- O
ﬁed O
architecture O
for O
natural O
language O
processing O
: O
deep O
neural O
networks O
with O
multitask O
learning O
. O

In O
Machine O
Learning O
, O
Proceedings O
of O
the O
Twenty- O
Fifth O
International O
Conference O
( O
ICML O
2008 O
) O
, O
Helsinki O
, O
Finland O
, O
June O
5 O
- O
9 O
, O
2008 O
. O

pages O
160–167 O
. O

http://doi.acm.org/10.1145/1390156.1390177 O
. O

John O
C. O
Duchi O
, O
Elad O
Hazan O
, O
and O
Yoram O
Singer O
. O

2011 O
. O

Adaptive O
subgradient O
methods O
for O
on- O
line O
learning O
and O
stochastic O
optimization O
. O

Jour- O
nal O
of O
Machine O
Learning O
Research O
12:2121–2159 O
. O

http://dl.acm.org/citation.cfm?id=2021068 O
. O

Lev O
Finkelstein O
, O
Evgeniy O
Gabrilovich O
, O
Yossi O
Matias O
, O
Ehud O
Rivlin O
, O
Zach O
Solan O
, O
Gadi O
Wolfman O
, O
and O
Eytan O
Ruppin O
. O

2002 O
. O

Placing O
search O
in O
context O
: O
the O
con- O
cept O
revisited O
. O

ACM O
Trans O
. O

Inf O
. O
Syst O
. O

20(1):116–131 O
. O

http://doi.acm.org/10.1145/503104.503110 O
. O

10Iryna O
Gurevych O
. O
2005 O
. O

Using O
the O
structure O
of O
a O
concep- O
tual O
network O
in O
computing O
semantic O
relatedness O
. O

In O
Natural O
Language O
Processing O
- O
IJCNLP O
2005 O
, O
Sec- O
ond O
International O
Joint O
Conference O
, O
Jeju O
Island O
, O
Ko- O
rea O
, O
October O
11 O
- O
13 O
, O
2005 O
, O
Proceedings O
. O

pages O
767 O
– O
778 O
. O

Guy O
Halawi O
, O
Gideon O
Dror O
, O
Evgeniy O
Gabrilovich O
, O
and O
Yehuda O
Koren O
. O

2012 O
. O

Large O
- O
scale O
learning O
of O
word O
relatedness O
with O
constraints O
. O

In O
The O
18th O
ACM O
SIGKDD O
International O
Conference O
on O
Knowl- O
edge O
Discovery O
and O
Data O
Mining O
, O
KDD O
’ O
12 O
, O
Bei- O
jing O
, O
China O
, O
August O
12 O
- O
16 O
, O
2012 O
. O

pages O
1406–1414 O
. O

http://doi.acm.org/10.1145/2339530.2339751 O
. O

Felix O
Hill O
, O
Roi O
Reichart O
, O
and O
Anna O
Korhonen O
. O

2014 O
. O

Simlex-999 O
: O
Evaluating O
semantic O
models O
with O
( O
gen- O
uine O
) O
similarity O
estimation O
. O

CoRR O
abs/1408.3456 O
. O

http://arxiv.org/abs/1408.3456 O
. O

Eric O
H. O
Huang O
, O
Richard O
Socher O
, O
Christopher O
D. O
Man- O
ning O
, O
and O
Andrew O
Y O
. O

Ng O
. O
2012 O
. O

Improving O
word O
representations O
via O
global O
context O
and O
multiple O
word O
prototypes O
. O

In O
The O
50th O
Annual O
Meeting O
of O
the O
As- O
sociation O
for O
Computational O
Linguistics O
, O
Proceed- O
ings O
of O
the O
Conference O
, O
July O
8 O
- O
14 O
, O
2012 O
, O
Jeju O
Island O
, O
Korea O
- O
Volume O
1 O
: O
Long O
Papers O
. O

pages O
873–882 O
. O

http://www.aclweb.org/anthology/P12-1092 O
. O

Tony O
Jebara O
, O
Risi O
Kondor O
, O
and O
Andrew O
Howard O
. O

2004 O
. O

Probability O
product O
kernels O
. O

Journal O
of O
Machine O
Learning O
Research O
5:819–844 O
. O

Yoon O
Kim O
, O
Yacine O
Jernite O
, O
David O
Sontag O
, O
and O
Alexan- O
der O
M. O
Rush O
. O

2016 O
. O

Character O
- O
aware O
neural O
lan- O
guage O
models O
. O

In O
Proceedings O
of O
the O
Thirtieth O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
February O
12- O
17 O
, O
2016 O
, O
Phoenix O
, O
Arizona O
, O
USA O
. O
. O

pages O
2741 O
– O
2749 O
. O

Onur O
Kuru O
, O
Ozan O
Arkan O
Can O
, O
and O
Deniz O
Yuret O
. O
2016 O
. O

Charner O
: O
Character O
- O
level O
named O
entity O
recogni- O
tion O
. O

In O
COLING O
2016 O
, O
26th O
International O
Con- O
ference O
on O
Computational O
Linguistics O
, O
Proceed- O
ings O
of O
the O
Conference O
: O
Technical O
Papers O
, O
Decem- O
ber O
11 O
- O
16 O
, O
2016 O
, O
Osaka O
, O
Japan O
. O

pages O
911–921 O
. O

http://aclweb.org/anthology/C/C16/C16-1087.pdf O
. O

Jason O
Lee O
, O
Kyunghyun O
Cho O
, O
and O
Thomas O
Hofmann O
. O
2017 O
. O

Fully O
character O
- O
level O
neural O
machine O
translation O
without O
ex- O
plicit O
segmentation O
. O

TACL O
5:365–378 O
. O

https://transacl.org/ojs/index.php/tacl/article/view/1051 O
. O

Ira O
Leviant O
and O
Roi O
Reichart O
. O

2015 O
. O

Judgment O
lan- O
guage O
matters O
: O
Multilingual O
vector O
space O
models O
for O
judgment O
language O
aware O
lexical O
semantics O
. O

CoRR O
abs/1508.00106 O
. O

http://arxiv.org/abs/1508.00106 O
. O

Minh O
- O
Thang O
Luong O
, O
Richard O
Socher O
, O
and O
Christo- O
pher O
D. O
Manning O
. O

2013 O
. O

Better O
word O
representations O
with O
recursive O
neural O
networks O
for O
morphology O
. O

In O
CoNLL O
. O

Soﬁa O
, O
Bulgaria O
. O

Tomas O
Mikolov O
, O
Kai O
Chen O
, O
Greg O
Corrado O
, O
and O
Jeffrey O
Dean O
. O

2013a O
. O

Efﬁcient O
estimation O
of O
word O
repre- O
sentations O
in O
vector O
space O
. O

CoRR O
abs/1301.3781 O
. O

http://arxiv.org/abs/1301.3781 O
. O

Tomas O
Mikolov O
, O
Kai O
Chen O
, O
Greg O
Corrado O
, O
and O
Jeffrey O
Dean O
. O

2013b O
. O

Efﬁcient O
estimation O
of O
word O
repre- O
sentations O
in O
vector O
space O
. O

CoRR O
abs/1301.3781 O
. O

http://arxiv.org/abs/1301.3781 O
. O

Tomas O
Mikolov O
, O
Stefan O
Kombrink O
, O
Luk O
´ O
as O
Burget O
, O
Jan O
Cernock O
´ O
y O
, O
and O
Sanjeev O
Khudanpur O
. O

2011 O
. O

Exten- O
sions O
of O
recurrent O
neural O
network O
language O
model O
. O

InProceedings O
of O
the O
IEEE O
International O
Confer- O

ence O
on O
Acoustics O
, O
Speech O
, O
and O
Signal O
Processing O
, O
ICASSP O
2011 O
, O
May O
22 O
- O
27 O
, O
2011 O
, O
Prague O
Congress O
Center O
, O
Prague O
, O
Czech O
Republic O
. O

pages O
5528–5531 O
. O
https://doi.org/10.1109/ICASSP.2011.5947611 O
. O

George O
A. O
Miller O
and O
Walter O
G. O
Charles O
. O

1991 O
. O

Contextual O
Correlates O
of O
Semantic O
Similarity O
. O

Language O
& O
Cognitive O
Processes O
6(1):1–28 O
. O

https://doi.org/10.1080/01690969108406936 O
. O

Arvind O
Neelakantan O
, O
Jeevan O
Shankar O
, O
Alexandre O
Pas- O
sos O
, O
and O
Andrew O
McCallum O
. O

2014 O
. O

Efﬁcient O
non- O
parametric O
estimation O
of O
multiple O
embeddings O
per O
word O
in O
vector O
space O
. O

In O
Proceedings O
of O
the O
2014 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Lan- O
guage O
Processing O
, O
EMNLP O
2014 O
, O
October O
25 O
- O
29 O
, O
2014 O
, O
Doha O
, O
Qatar O
, O
A O
meeting O
of O
SIGDAT O
, O
a O
Spe- O
cial O
Interest O
Group O
of O
the O
ACL O
. O

pages O
1059–1069 O
. O

http://aclweb.org/anthology/D/D14/D14-1113.pdf O
. O

Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christo- O
pher O
D. O
Manning O
. O

2014 O
. O

Glove O
: O
Global O
vectors O
for O
word O
representation O
. O

In O
Proceedings O
of O
the O
2014 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Lan- O
guage O
Processing O
, O
EMNLP O
2014 O
, O
October O
25 O
- O
29 O
, O
2014 O
, O
Doha O
, O
Qatar O
, O
A O
meeting O
of O
SIGDAT O
, O
a O
Spe- O
cial O
Interest O
Group O
of O
the O
ACL O
. O

pages O
1532–1543 O
. O

http://aclweb.org/anthology/D/D14/D14-1162.pdf O
. O
Kira O
Radinsky O
, O
Eugene O
Agichtein O
, O
Evgeniy O
Gabrilovich O
, O
and O
Shaul O
Markovitch O
. O
2011 O
. O

A O
word O
at O
a O
time O
: O

Computing O
word O
relatedness O
using O
temporal O
semantic O
analysis O
. O

In O
Proceed- O
ings O
of O
the O
20th O
International O
Conference O
on O
World O
Wide O
Web O
. O

WWW O
’ O
11 O
, O
pages O
337–346 O
. O

http://doi.acm.org/10.1145/1963405.1963455 O
. O

Herbert O
Rubenstein O
and O
John O
B. O
Goode- O
nough O
. O

1965 O
. O

Contextual O
correlates O
of O
syn- O
onymy O
. O

Commun O
. O

ACM O
8(10):627–633 O
. O

http://doi.acm.org/10.1145/365628.365657 O
. O

Samuel O
L. O
Smith O
, O
David O
H. O
P. O
Turban O
, O
Steven O
Ham- O
blin O
, O
and O
Nils O
Y O
. O

Hammerla O
. O
2017 O
. O

Ofﬂine O
bilin- O

gual O
word O
vectors O
, O
orthogonal O
transformations O
and O
the O
inverted O
softmax O
. O

CoRR O
abs/1702.03859 O
. O

http://arxiv.org/abs/1702.03859 O
. O

C. O
Spearman O
. O

1904 O
. O

The O
proof O
and O
measurement O
of O
association O
between O
two O
things O
. O

American O
Journal O
of O
Psychology O
15:88–103 O
. O

11Fei O
Tian O
, O
Hanjun O
Dai O
, O
Jiang O
Bian O
, O
Bin O
Gao O
, O
Rui O
Zhang O
, O
Enhong O
Chen O
, O
and O
Tie O
- O
Yan O
Liu O
. O

2014 O
. O

A O
prob- O
abilistic O
model O
for O
learning O
multi O
- O
prototype O
word O
embeddings O
. O

In O
COLING O
2014 O
, O
25th O
International O
Conference O
on O
Computational O
Linguistics O
, O
Proceed- O
ings O
of O
the O
Conference O
: O
Technical O
Papers O
, O
Au- O

gust O
23 O
- O
29 O
, O
2014 O
, O
Dublin O
, O
Ireland O
. O

pages O
151–160 O
. O

http://aclweb.org/anthology/C/C14/C14-1016.pdf O
. O
Luke O
Vilnis O
and O
Andrew O
McCallum O
. O

2014 O
. O

Word O
representations O
via O
gaussian O
embedding O
. O

CoRR O
abs/1412.6623 O
. O

http://arxiv.org/abs/1412.6623 O
. O

Dongqiang O
Yang O
and O
David O
M. O
W. O
Powers O
. O

2006 O
. O

Verb O
similarity O
on O
the O
taxonomy O
of O
wordnet O
. O

In O
In O
the O
3rd O
International O
WordNet O
Conference O
( O
GWC-06 O
) O
, O
Jeju O
Island O
, O
Korea O
. O

Shenjian O
Zhao O
and O
Zhihua O
Zhang O
. O

2016 O
. O

An O
efﬁcient O
character O
- O
level O
neural O
machine O
translation O
. O

CoRR O
abs/1608.04738 O
. O
http://arxiv.org/abs/1608.04738 O
. O

G.K. O
Zipf O
. O

1949 O
. O

Human O
behavior O
and O
the O
principle O
of O
least O
effort O
: O
an O
introduction O
to O
human O
ecology O
. O

Addison O
- O
Wesley O
Press O
. O

https://books.google.com/books?id=1tx9AAAAIAAJ O
. O

Proceedings O
of O
NAACL O
- O
HLT O
2018 O
, O
pages O
37–46 O
New O
Orleans O
, O
Louisiana O
, O
June O
1 O
- O
6 O
, O
2018 O
. O

c O

2018 O
Association O
for O
Computational O
Linguistics O
A O
Deep B-MethodName
Generative I-MethodName
Model I-MethodName
of O
Vowel O
Formant O
Typology O

Ryan O
Cotterell O
andJason O
Eisner O
Department O
of O
Computer O
Science O
Johns O
Hopkins O
University O
, O
Baltimore O
MD O
, O
21218 O
{ O
ryan.cotterell O
, O
eisner O
} O
@jhu.edu O

Abstract O
What O
makes O
some O
types O
of O
languages O
more O
probable O
than O
others O
? O

For O
instance O
, O
we O
know O
that O
almost O
all O
spoken O
languages O
contain O
the O
vowel O
phoneme O
/i/ O
; O
why O
should O
that O
be O
? O

The O
ﬁeld O
of O
linguistic O
typology O
seeks O
to O
answer O
these O
questions O
and O
, O
thereby O
, O
divine O
the O
mech- O
anisms O
that O
underlie O
human O
language O
. O

In O
our O
work O
, O
we O
tackle O
the O
problem O
of O
vowel O
system O
typology O
, O
i.e. O
, O
we O
propose O
a O
generative B-MethodName
proba-bility I-MethodName
model I-MethodName
of O
which O
vowels O
a O
language O
con- O
tains O
. O

In O
contrast O
to O
previous O
work O
, O
we O
work O
di- O
rectly O
with O
the O
acoustic O
information O
— O
the O
ﬁrst O
two O
formant O
values O
— O
rather O
than O
modeling O
dis- O
crete O
sets O
of O
phonemic O
symbols O
( O
IPA O
) O
. O

We O
de- O
velop O

a O
novel O
generative B-MethodName
probability I-MethodName
model I-MethodName
and O
report O
results O
based O
on O
a O
corpus O
of O
233 O
lan- O
guages O
. O

1 O
Introduction O
Human O
languages O
are O
far O
from O
arbitrary O
; O
cross- O
linguistically O
, O
they O
exhibit O
surprising O
similarity O
in O
many O
respects O
and O
many O
properties O
appear O
to O
be O
universally O
true O
. O

The O
ﬁeld O
of O
linguistic O
typology O
seeks O
to O
investigate O
, O
describe O
and O
quantify O
the O
axes O
along O
which O
languages O
vary O
. O

One O
facet O
of O
language O
that O
has O
been O
the O
subject O
of O
heavy O
investigation O
is O
the O
nature O
of O
vowel O
inventories O
, O
i.e. O
, O
which O
vowels O
a O
language O
contains O
. O

It O
is O
a O
cross O
- O
linguistic O
univer- O
sal O
that O
all O
spoken O
languages O
have O
vowels O
( O
Gordon O
, O
2016 O
) O
, O
and O
the O
underlying O
principles O
guiding O
vowel O
selection O
are O
understood O
: O
vowels O
must O
be O
both O
easily O
recognizable O
and O
well O
- O
dispersed O
( O
Schwartz O
et O
al O
. O
, O
2005 O
) O
. O

In O
this O
work O
, O
we O
offer O
a O
more O
formal O
treatment O
of O
the O
subject O
, O
deriving O
a O
generative B-MethodName
prob-ability I-MethodName
model I-MethodName
of O
vowel O
inventory O
typology O
. O

Our O
work O
builds O
on O
( O
Cotterell O
and O
Eisner O
, O
2017 O
) O
by O
in- O

vestigating O
not O
just O
discrete O
IPA O
inventories O
but O
the O
cross O
- O
linguistic O
variation O
in O
acoustic O
formants O
. O

The O
philosophy O
behind O
our O
approach O
is O
that O
lin- O
guistic O
typology O
should O
be O
treated O
probabilisticallyand O
its O
goal O
should O
be O
the O
construction O
of O
a O
univer- O
sal O
prior O
over O
potential O
languages O
. O

A O
probabilistic O
approach O
does O
not O
rule O
out O
linguistic O
systems O
com- O
pletely O
( O
as O
long O
as O
one O
’s O
theoretical O
formalism O
can O
describe O
them O
at O
all O
) O
, O
but O
it O
can O
position O
phenomena O
on O
a O
scale O
from O
very O
common O
to O
very O
improbable O
. O

Probabilistic O
modeling O
also O
provides O
a O
discipline O
for O
drawing O
conclusions O
from O
sparse O
data O
. O

While O
we O
know O
of O
over O
7000 O
human O
languages O
, O
we O
have O
some O
sort O
of O
linguistic O
analysis O
for O
only O
2300 O
of O
them O
( O
Comrie O
et O
al O
. O
, O
2013 O
) O
, O
and O
the O
dataset O
used O
in O
this O
paper O
( O
Becker O
- O
Kristal O
, O
2010 O
) O
provides O
simple O
vowel O
data O
for O
fewer O
than O
250 O
languages O
. O

Formants O
are O
the O
resonant O
frequencies O
of O
the O
hu- O
man O
vocal O
tract O
during O
the O
production O
of O
speech O
sounds O
. O

We O
propose O
a O
Bayesian O
generative B-MethodName
model I-MethodName
of O
vowel O
inventories O
, O
where O
each O
language O
’s O
inven- O

tory O
is O
a O
ﬁnite O
subset O
of O
acoustic O
vowels O
represented O
as O
points O
( O
F1,F2)∈R2 O
. O

We O
deploy O
tools O
from O
the O
neural O
- O
network O
and O
point O
- O
process O
literatures O
and O
experiment O
on O
a O
dataset O
with O
233 O
distinct O
languages O
. O

We O
show O
that O
our O
most O
complicated O
model O
outper- O
forms O
simpler O
models O
. O

2 O
Acoustic O
Phonetics O
and O
Formants O
Much O
of O
human O
communication O
takes O
place O
through O
speech O
: O
one O
conversant O
emits O
a O
sound O
wave O
to O
be O
comprehended O
by O
a O
second O
. O

In O
this O
work O
, O
we O
consider O
the O
nature O
of O
the O
portions O
of O
such O
sound O
waves O
that O
correspond O
to O
vowels O
. O

We O
brieﬂy O
review O
the O
relevant O
bits O
of O
acoustic O
phonetics O
so O
as O
to O
give O
an O
overview O
of O
the O
data O
we O
are O
actually O
modeling O
and O
develop O
our O
notation O
. O

The O
anatomy O
of O
a O
sound O
wave O
. O

The O
sound O
wave O
that O
carries O
spoken O
language O
is O
a O
function O
from O
time O
to O
amplitude O
, O
describing O
sound O
pressure O
vari- O
ation O
in O
the O
air O
. O

To O
distinguish O
vowels O
, O
it O
is O
help- O
ful O
to O
transform O
this O
function O
into O
a O
spectrogram O
( O
Fig O
. O
1 O
) O
by O
using O
a O
short O
- O
time O
Fourier O
transform37 O

Example O
spectrogram O
of O
the O
three O
English O
vowels O
: O
/i/ O
, O
/u/ O

and O
/ O
A/. O
Thex O
- O
axis O
is O
time O
and O
y O
- O
axis O
is O
frequency O
. O

The O
ﬁrst O
two O
formants O
F1andF2are O
marked O
in O
with O
arrows O
for O
each O
vowel O
. O

The O
ﬁgure O
was O
made O
with O
Praat O
( O
Boersma O
et O
al O
. O
, O
2002 O
) O
. O

( O
Deng O
and O
O’Shaughnessy O
, O
2003 O
, O
Chapter O
1 O
) O
to O
de- O
compose O
each O
short O
interval O
of O
the O
wave O
function O
into O
a O
weighted O
sum O
of O
sinusoidal O
waves O
of O
differ- O
ent O
frequencies O
( O
measured O
in O
Hz O
) O
. O

At O
each O
interval O
, O
the O
variable O
darkness O
of O
the O
spectrogram O
indicates O
the O
weights O
of O
the O
different O
frequencies O
. O

In O
pho- O
netic O
analysis O
, O
a O
common O
quantity O
to O
consider O
is O
aformant O
— O
a O
local O
maximum O
of O
the O
( O
smoothed O
) O
frequency O
spectrum O
. O

The O
fundamental O
frequency O
F0determines O
the O
pitch O
of O
the O
sound O
. O

The O
formants O
F1andF2determine O
the O
quality O
of O
the O
vowel O
. O

Two O
is O
all O
you O
need O
( O
and O
what O
we O
left O
out O
) O
. O

In O
terms O
of O
vowel O
recognition O
, O
it O
is O
widely O
speculated O
that O
humans O
rely O
almost O
exclusively O
on O
the O
ﬁrst O
two O
formants O
of O
the O
sound O
wave O
( O
Ladefoged O
, O
2001 O
, O
Chapter O
5 O
) O
. O

The O
two O
- O
formant O
assumption O
breaks O
down O
in O
edge O
cases O
: O
e.g. O
, O
the O
third O
formant O
F3 O
helps O
to O
distinguish O
the O
roundness O
of O
the O
vowel O
( O
Ladefoged O
, O
2001 O
, O
Chapter O
5 O
) O
. O

Other O
non O
- O
formant O
features O
may O
also O
play O
a O
role O
. O

For O
example O
, O
in O
tonal O
languages O
, O
the O
same O
vowel O
may O
be O
realized O
with O
different O
tones O
( O
which O
are O
signaled O
using O
F0 O
): O

Mandarin O
Chinese O
makes O
a O
distinction O
between O
m O
ˇ O
a O
( O
horse O
) O
and O
m O
´ O
a O
( O
hemp O
) O
without O
modifying O
the O
qual- O
ity O
of O
the O
vowel O
/a/. O

Other O
features O
, O
such O
as O
creaky O
voice O
, O
can O
play O
a O
role O
in O
distinguishing O
phonemes O
. O

We O
do O
not O
explicitly O
model O
any O
of O
these O
aspects O
of O
vowel O
space O
, O
limiting O
ourselves O
to O
( O
F1,F2)as O
in O
previous O
work O
( O
Liljencrants O
and O
Lindblom O
, O
1972 O
) O
. O

However O
, O
it O
would O
be O
easy O
to O
extend O
all O
the O
models O
we O
will O
propose O
here O
to O
incorporate O
such O
informa- O
tion O
, O
given O
appropriate O
datasets O
. O

3 O
The O
Phonology O
of O
Vowel O
Systems O
The O
vowel O
inventories O
of O
the O
world O
’s O
languages O
display O
clear O
structure O
and O
appear O
to O
obey O
several O
underlying O
principles O
. O

The O
most O
prevalent O
of O
theseprinciples O
are O
focalization O
anddispersion O
. O

Focalization O
. O

The O
notion O
of O
focalization O
grew O
out O
of O
quantal O
vowel O
theory O
( O
Stevens O
, O
1989 O
) O
. O

Quan- O
tal O
vowels O
are O
those O
that O
are O
phonetically O
“ O
better O
” O
than O
others O
. O

They O
tend O
to O
display O
certain O
proper- O
ties O
, O
e.g. O
, O
the O
formants O
tend O
to O
be O
closer O
together O
( O
Stevens O
, O
1987 O
) O
. O

Cross O
- O
linguistically O
, O
quantal O
vow- O
els O
are O
the O
most O
frequently O
attested O
vowels O
, O
e.g. O
, O
the O
cross O
- O
linguistically O
common O
vowel O
/i/ O
is O
considered O
quantal O
, O
but O
less O
common O
/y/ O
is O
not O
. O

Dispersion O
. O

The O
second O
core O
principle O
of O
vowel O
system O
organization O
is O
known O
as O
dispersion O
. O

As O
the O
name O
would O
imply O
, O
the O
principle O
states O
that O
the O
vowels O
in O
“ O
good O
” O
vowel O
systems O
tend O
to O
be O
spread O
out O
. O

The O
motivation O
for O
such O
a O
principle O
is O
clear O
— O
a O
well O
- O
dispersed O
set O
of O
vowels O
reduces O
a O
listener O
’s O
potential O
confusion O
over O
which O
vowel O
is O
being O
pronounced O
. O

See O
Schwartz O
et O
al O
. O

( O
1997 O
) O
for O
a O
review O
of O
dispersion O
in O
vowel O
system O
typology O
and O
its O
interaction O
with O
focalization O
, O
which O
has O
led O
to O
the O
joint O
dispersion O
- O
focalization O
theory O
. O

Notation O
. O

We O
will O
denote O
the O
universal O
set O
of O
international O
phonetic O
alphabet O
( O
IPA O
) O
symbols O
asV. O

The O
observed O
vowel O
inventory O
for O
lan- O
guage O
/ O
lscripthas O
sizen O
/ O
lscriptand O
is O
denoted O
V O
/ O
lscript= O
{ O
( O
v O
/ O
lscript O
1,v O
/ O
lscript O
1 O
) O
, O
... O
, O
( O
v O
/ O
lscript O
n O
/ O
lscript O
, O
v O
/ O
lscript O
n O
/ O
lscript)}⊆V× O
Rd O
, O
where O
for O
eachk∈[1,n O
/ O
lscript],v O
/ O
lscript O
k∈V O
is O
an O
IPA O
symbol O
assigned O
by O
a O
linguist O
and O
v O
/ O
lscript O
k∈Rdis O
a O
vector O
of O
dmeasur- O
able O
phonetic O
quantities O
. O

In O
short O
, O
the O
IPA O
symbol O
v O
/ O
lscript O
kwas O
assigned O
as O
a O
label O
for O
a O
phoneme O
with O
pro- O
nunciation O
v O
/ O
lscript O
k. O

The O
ordering O
of O
the O
elements O
within O
V O
/ O
lscriptis O
arbitrary O
. O

Goals O
. O

This O
framework O
recognizes O
that O
the O
same O
IPA O
symbolv(such O
as O
/u/ O
) O
may O
represent O
a O
slightly O
different O
sound O
vin O
one O
language O
than O
in O
another O
, O
although O
they O
are O
transcribed O
identically O
. O

We O
are O
speciﬁcally O
interested O
in O
how O
the O
vowels O
in O
a O
lan- O
guage O
inﬂuence O
one O
another O
’s O
ﬁne O
- O
grained O
pro- O
nunciation O
in O
Rd O
. O

In O
general O
, O
there O
is O
no O
reason O
to O
suspect O
that O
speakers O
of O
two O
languages O
, O
whose O
phonological O
systems O
contain O
the O
same O
IPA O
symbol O
, O
should O
produce O
that O
vowel O
with O
identical O
formants O
. O

Data O
. O

For O
the O
remainder O
of O
the O
paper O
, O
we O
will O
taked= O
2 O
so O
that O
each O
v= O
( O
F1,F2)∈R2 O
, O
the O
vector O
consisting O
of O
the O
ﬁrst O
two O
formant O
values O
, O
as O
compiled O
from O
the O
ﬁeld O
literature O
by O
Becker- O
Kristal O
( O
2006 O
) O
. O

This O
dataset O
provides O
inventories O
V O
/ O
lscriptin O
the O
form O
above O
. O

Thus O
, O
we O
do O
not O
consider O
further O
variation O
of O
the O
vowel O
pronunciation O
that38 O

may O
occur O
within O
the O
language O
( O
between O
speakers O
, O
between O
tokens O
of O
the O
vowel O
, O
or O
between O
earlier O
and O
later O
intervals O
within O
a O
token O
) O
. O

4 O
Phonemes O
versus O
Phones O
Previous O
work O
( O
Cotterell O
and O
Eisner O
, O
2017 O
) O
has O
placed O
a O
distribution O
over O
discrete O
phonemes O
, O
ignor- O

ing O
the O
variation O
across O
languages O
in O
the O
pronuncia- O
tionof O
each O
phoneme O
. O

In O
this O
paper O
, O
we O
crack O
open O
the O
phoneme O
abstraction O
, O
moving O
to O
a O
learned O
set O
of O
ﬁner O
- O
grained O
phones O
. O

Cotterell O
and O
Eisner O
( O
2017 O
) O
proposed O
( O
among O
other O
options O
) O
using O
a O
determinantal O
point O
process O
( O
DPP B-MethodName
) O
over O
a O
universal O
inventory O
Vof O
53 O
sym- O
bolic O
( O
IPA O
) O
vowels O
. O

A O
draw O
from O
such O
a O
DPP B-MethodName
is O
a O
language O
- O
speciﬁc O
inventory O
of O
vowel O
phonemes O
, O
V⊆V. O

In O
this O
paper O
, O
we O
say O
that O
a O
language O
in- O
stead O
draws O
its O
inventory O
from O
a O
larger O
set O
¯V O
, O
again O
using O
a O
DPP B-MethodName
. O

In O
both O
cases O
, O
the O
reason O
to O
use O
a O
DPP B-MethodName
is O
that O
it O
prefers O
relatively O
diverse O
inventories O
whose O
individual O
elements O
are O
relatively O
quantal O
. O

While O
we O
could O
in O
principle O
identify O
¯VwithRd O
, O
for O
convenience O
we O
still O
take O
it O
to O
be O
a O
( O
large O
) O
dis- O
crete O
ﬁnite O
set O
¯V={¯v1 O
, O
... O
, O
¯vN O
} O
, O
whose O
elements O
we O
call O
phones O
.¯Vis O

a O
learned O
cross O
- O
linguistic O
pa- O
rameter O
of O
our O
model O
; O
thus O
, O
its O
elements O
— O
the O
“ O
uni- O
versal O
phones”—may O
or O
may O
not O
correspond O
to O
phonetic O
categories O
traditionally O
used O
by O
linguists O
. O

We O
presume O
that O
language O
/lscriptdraws O
from O
the O
DPP B-MethodName
a O
subset O
¯V O
/ O
lscript⊆¯V O
, O
whose O
size O
we O
call O
n O
/ O
lscript O
. O

For O
each O
universal O
phone O
¯vithat O
appears O
in O
this O
inventory O
¯V O
/ O
lscript O
, O
the O
language O
then O
draws O
an O
observable O
language- O
speciﬁc O
pronunciation O
v O
/ O
lscript O
i∼N O
/ O
parenleftbig O
µi O
, O
σ2I O
/ O
parenrightbig O
from O
a O
distribution O
associated O
cross O
- O
linguistically O
with O
the O
universal O
phone O
¯vi O
. O

We O
now O
have O
an O
inventory O
of O
pronunciations O
. O

As O
a O
ﬁnal O
step O
in O
generating O
the O
vowel O
inventory O
, O
we O
could O
model O
IPA O
labels O
. O

For O
each O
¯vi∈¯V O
/ O
lscript O
, O
a O
ﬁeld O
linguist O
presumably O
draws O
the O
IPA O
label O
v O
/ O
lscript O
i O
conditioned O
on O
all O
the O
pronunciations O
{ O
v O
/ O
lscript O
i∈Rd O
: O
¯vi∈¯V O
/ O
lscript}in O
the O
inventory O
( O
and O
perhaps O
also O
on O
their O
underlying O
phones O
¯vi∈¯V O
/ O
lscript O
) O
. O

This O
labeling O
process O
may O
be O
complex O
. O

While O
each O
pronuncia- O
tion O
in O
Rd(or O
each O
underlying O
phone O
in O
¯V O
) O
may O
have O
a O
preference O
for O
certain O
IPA O
labels O
in O
V O
, O
the O
n O
/ O
lscriptlabels O
must O
be O
drawn O
jointly O
because O
the O
lin- O
guist O
will O
take O
care O
not O
to O
use O
the O
same O
label O
for O
two O
phones O
, O
and O
also O
because O
the O
linguist O
may O
like O
to O
describe O
the O
inventory O
using O
a O
small O
number O
of O
distinct O
IPA O
features O
, O
which O
will O
tend O
to O
favor O
fac- O
torial O
grids O
of O
symbols O
. O

The O
linguist O
’s O
use O
of O
IPAfeatures O
may O
also O
be O
informed O
by O
phonological O
and O
phonetic O
processes O
in O
the O
language O
. O

We O
leave O
mod- O
eling O
of O
this O
step O
to O
future O
work O
; O
so O
our O
current O
likelihood O
term O
ignores O
the O
evidence O
contributed O
by O
the O
IPA O
labels O
in O
the O
dataset O
, O
considering O
only O
the O
pronunciations O
in O
Rd O
. O

The O
overall O
idea O
is O
that O
human O
languages O
/lscriptdraw O
their O
inventories O
from O
some O
universal O
prior O
, O
which O
we O
are O
attempting O
to O
reconstruct O
. O

A O
caveat O
is O
that O
we O
will O
train O
our O
method O
by O
maximum O
- O
likelihood O
, O
which O
does O
not O
quantify O
our O
uncertainty O
about O
the O
reconstructed O
parameters O
. O

An O
additional O
caveat O
is O
that O
some O
languages O
in O
our O
dataset O
are O
related O
to O
one O
another O
, O
which O
belies O
the O
idea O
that O
they O
were O
drawn O
independently O
. O

Ideally O
, O
one O
ought O
to O
capture O
these O
relationships O
using O
hierarchical O
or O
evolution- O
ary O
modeling O
techniques O
. O

5 O
Determinantal O
Point O
Processes O
Before O
delving O
into O
our O
generative B-MethodName
model I-MethodName
, O
we O
brieﬂy O
review O
technical O
background O
used O
by O
Cot- O
terell O
and O
Eisner O
( O
2017 O
) O
. O

A O
DPP B-MethodName
is O
a O
probability O
distribution O
over O
the O
subsets O
of O
a O
ﬁxed O
ground O
set O
of O
sizeN O
— O
in O
our O
case O
, O
the O
set O
of O
phones O
¯V. O
The O
DPP B-MethodName
is O
usually O
given O
as O
an O
L O
- O
ensemble O
( O
Borodin O
and O
Rains O
, O
2005 O
) O
, O
meaning O
that O
it O
is O
parameterized O
by O
a O
positive O
semi O
- O
deﬁnite O
matrix O
L∈RN×N. O
Given O
a O
discrete O
base O
set O
¯Vof O
phones O
, O
the O
probability O
of O
a O
subset O
¯V⊆¯Vis O
given O
by O
p(¯V)∝det(L¯V O
) O
, O
( O
1 O
) O
whereL¯Vis O
the O
submatrix O
of O
Lcorresponding O
to O
the O
rows O
and O
columns O
associated O
with O
the O
subset O
¯V⊆¯V. O
The O
entryLij O
, O
wherei O
/ O
negationslash O
= O
j O
, O
has O
the O
effect O
of O
describing O
the O
similarity O
between O
the O
elements O
¯viand¯vj(both O
in O
¯V)—an O
ingredient O
needed O
to O
model O
dispersion O
. O

And O
, O
the O
entry O
Liidescribes O
the O
quality O
— O
focalization O
— O
of O
the O
vowel O
¯vi O
, O
i.e. O
, O
how O
much O
the O
model O
wants O
to O
have O
¯viin O
a O
sampled O
set O
independent O
of O
the O
other O
members O
. O

5.1 O
Probability O
Kernel O
In O
this O
work O
, O
each O
phone O
¯vi∈¯Vis O
associated O
with O
a O
probability O
density O
over O
the O
space O
of O
possible O
pro- O
nunciations O
R2 O
. O

Our O
measure O
of O
phone O
similarity O
will O
consider O
the O
“ O
overlap O
” O
between O
the O
densities O
associated O
with O
two O
phones O
. O

This O
works O
as O
follows O
: O
Given O
two O
densities O
f(x O
, O
y)andf O
/ O
prime(x O
, O
y)overR2 O
, O
we O
deﬁne O
the O
kernel O
( O
Jebara O
et O
al O
. O
, O
2004 O
) O
as O
K(f O
, O
f O
/ O
prime;ρ O

) O

= O
/integraldisplay O
x O
/ O
integraldisplay O
yf(x O
, O
y)ρf O
/ O
prime(x O
, O
y)ρdxdy O
, O
( O
3)39 O

M O
/ O
productdisplay O
/lscript=1 O
/ O
bracketleftBig O
p(v O
/ O
lscript,1, O
... O
,v O
/ O
lscript O
, O
n O
/ O
lscript|µ1, O
... O
,µN O
, O
N)/bracketrightBig O
p(µ1, O
... O
µN|N)p(N O
) O
( O
2 O
) O
= O
M O
/ O
productdisplay O
/lscript=1 O
/ O
bracketleftBigg O
/ O
summationdisplay O
a O
/ O
lscript∈A(n O
/ O
lscript O
, O
N)/parenleftBiggn O
/ O
lscript O
/ O
productdisplay O
k=1p(v O
/ O
lscript O
, O
k|µa O
/ O
lscript O
k O
) O
/bracehtipupleft O
/ O
bracehtipdownright O
/ O
bracehtipdownleft O
/ O
bracehtipupright O
4 O
/ O
parenrightBigg O
p(¯V(a O
/ O
lscript)|µ1, O
... O
,µN O
, O
N)/bracehtipupleft O
/ O
bracehtipdownright O
/ O
bracehtipdownleft O
/ O
bracehtipupright O
3 O
/ O
bracketrightBigg O

p(µ1, O
... O
µN|N)/bracehtipupleft O
/ O
bracehtipdownright O
/ O
bracehtipdownleft O
/ O
bracehtipupright O
2p(N)/bracehtipupleft O
/ O
bracehtipdownright O
/ O
bracehtipdownleft O
/ O
bracehtipupright O
1 O
Figure O
2 O
: O
Joint O
likelihood O
of O
Mvowel O
systems O
under O
our O
deep O
generative B-MethodName
probability I-MethodName
model I-MethodName
for O
continuous O
- O
space O
vowel O
inventories O
. O

Here O
language O
/lscripthas O
an O
observed O
inventory O
of O
pronunciations O
{ O
v O
/ O
lscript O
, O
k O
: O
1≤k≤n O
/ O
lscript O
} O
, O
anda O
/ O
lscript O
k∈[1,N]denotes O
a O
phone O
that O
might O
be O
responsible O
for O
the O
pronunciation O
v O
/ O
lscript O
, O
k. O

Thus O
, O
a O
/ O
lscriptdenotes O
some O
way O
to O
jointly O
label O
all O
n O
/ O
lscriptpronunciations O
with O
distinct O
phones O
. O

We O
must O
sum O
over O
all O
/ O
parenleftbigN O
n O
/ O
lscript O
/ O
parenrightbig O
such O
labelings O
a O
/ O
lscript∈A(n O
/ O
lscript O
, O
N)since O
the O
true O
labeling O
is O
not O
observed O
. O

In O
other O
words O
, O
we O
sum O
over O
all O
ways O
a O
/ O
lscriptof O
completing O
the O
data O
for O
language O
/lscript O
. O

Within O
each O
summand O
, O
the O
product O
of O
factors O
3 O
and O
4 O
is O
the O
probability O
of O
the O
completed O
data O
, O
i.e. O
, O
the O
joint O
probability O
of O
generating O
the O
inventory O
¯V(a O
/ O
lscript)of O
phones O
used O
in O
the O
labeling O
and O
their O
associated O
pronunciations O
. O

Factor O
3 O
considers O
the O
prior O
probability O
of O
¯V(a O
/ O
lscript)under O
the O
DPP B-MethodName
, O
and O
factor O
4 O
is O
a O
likelihood O
term O
that O
considers O
the O
probability O
of O
the O
associated O
pronunciations O
. O

with O
inverse O
temperature O
parameter O
ρ O
. O

In O
our O
setting O
, O
f O
, O
f O
/ O
primewill O
both O
be O
Gaussian O
dis- O
tributions O
with O
means O
µandµ/primethat O
share O
a O
ﬁxed O
spherical O
covariance O
matrix O
σ2I. O

Then O
eq O
. O

( O
3 O
) O
and O
indeed O
its O
generalization O
to O
any O
Rdhas O
a O
closed- O
form O
solution O
( O
Jebara O
et O
al O
. O
, O
2004 O
, O
§ O
3.1 O
): O
K(f O
, O
f O
/ O
prime;ρ O
) O
= O
( O
4 O
) O
( O
2ρ)d O
2 O
/ O
parenleftbig O
2πσ2 O
/ O
parenrightbig(1−2ρ)d O
2exp O
/ O
parenleftbigg O
−ρ||µ−µ/prime||2 O
4σ2 O
/ O
parenrightbigg O
. O

Notice O
that O
making O
ρsmall O
( O
i.e. O
, O
high O
temperature O
) O
has O
an O
effect O
on O
( O
4)similar O
to O
scaling O
the O
variance O
σ2by O
the O
temperature O
, O
but O
it O
also O
results O
in O
chang- O
ing O
the O
scale O
ofK O
, O
which O
affects O
the O
balance O
be- O
tween O
dispersion O
and O
focalization O
in O
( O
6 O
) O
below O
. O

5.2 O
Focalization O
Score O
The O
probability O
kernel O
given O
in O
eq O
. O

( O
3 O
) O
naturally O
handles O
the O
linguistic O
notion O
of O
dispersion O
. O

What O
about O
focalization O
? O

We O
say O
that O
a O
phone O
is O
focal O
to O
the O
extent O
that O
it O
has O
a O
high O
score O
F(µ O
) O
= O
exp O
( O
U2tanh(U1µ+b1 O
) O
+ O
b2)>0 O
( O
5 O
) O
where O
µis O
the O
mean O
of O
its O
density O
. O

To O
learn O
the O
parameters O
of O
this O
neural O
network O
from O
data O
is O
to O
learn O
which O
phones O
are O
focal O
. O

We O
use O
a O
neural O
net- O
work O
since O
the O
focal O
regions O
of O
R2are O
distributed O
in O
a O
complex O
way O
. O

5.3 O
TheLMatrix O
Iffi O
= O
N(µi O
, O
σ2I)is O
the O
density O
associated O
with O
the O
phone O
¯vi O
, O
we O
may O
populate O
an O
N×NrealAlgorithm O
1 O
Generative O
Process O
1 O
: O
N∼Poisson O
( O
λ O
) O
( O
∈N O
) O
1 O
2 O
: O
fori= O
1toN O
: O
3:µi∼N O
( O
0,I O
) O
( O
∈R2 O
) O
2 O
4 O
: O
deﬁneL∈RN×Nvia O
( O
6 O
) O
5 O
: O
for O
/ O
lscript= O
1toM O
: O
6 O
: O
¯V O
/ O
lscript∼DPP(L O
) O
( O
⊆[1,N O
] O
) O
; O
letn O
/ O
lscript=|¯V O
/ O
lscript|3 O
7 O
: O
fori∈¯V O
/ O
lscript O
: O
8 O
: O
˜v O
/ O
lscript O
i∼N O
/ O
parenleftbig O
µi O
, O
σ2I O
/ O
parenrightbig O
4 O
9 O
: O
v O
/ O
lscript O
i O
= O
νθ O
/ O
parenleftbig˜v O
/ O
lscript O
i O
/ O
parenrightbig O
4 O
matrixLwhere O
Lij=/braceleftBigg O
K(fi O
, O
fj;ρ O
) O
ifi O
/ O
negationslash O
= O
j O
K(fi O
, O
fj;ρ O
) O

+ O
F(µi)ifi O
= O
j(6 O
) O
SinceLis O
the O
sum O
of O
two O
positive O
deﬁnite O
ma- O
trices O
( O
the O
ﬁrst O
specializes O
a O
known O
kernel O
and O
the O
second O
is O
diagonal O
and O
positive O
) O
, O
it O
is O
also O
positive O
deﬁnite O
. O

As O
a O
result O
, O
it O
can O
be O
used O
to O
parameterize O
a O
DPP B-MethodName
over O
¯V. O
Indeed O
, O
since O
Lis O
positive O
deﬁnite O
and O
not O
merely O
positive O
semideﬁnite O
, O
it O
will O
assign O
positive O
probability O
to O
anysubset O
of O
¯V. O
As O
previously O
noted O
, O
this O
DPP B-MethodName
does O
not O
deﬁne O
a O
distribution O
over O
an O
inﬁnite O
set O
, O
e.g. O
, O
the O
pow- O
erset O
of O
R2 O
, O
as O
does O
recent O
work O
on O
continuous O
DPPs O
( O
Affandi O
et O
al O
. O
, O
2013 O
) O
. O

Rather O
, O
it O
deﬁnes O
a O
distribution O
over O
the O
powerset O
of O
a O
set O
of O
densities O
with O
ﬁnite O
cardinality O
. O

Once O
we O
have O
sampled O
a O
subset O
of O
densities O
, O
a O
real O
- O
valued O
quantity O
may O
be O
additionally O
sampled O
from O
each O
sampled O
density O
. O

6 O
A O
Deep O
Generative B-MethodName
Model I-MethodName
We O
are O
now O
in O
a O
position O
to O
expound O
our O
generative B-MethodName
model I-MethodName
of O
continuous O
- O
space O
vowel O
typology O
. O

We40 O

generate O
a O
set O
of O
formant O
pairs O
for O
Mlanguages O
in O
a O
four O
step O
process O
. O

Note O
that O
throughout O
this O
exposition O
, O
language O
- O
speciﬁc O
quantities O
with O
be O
superscripted O
with O
an O
integral O
language O
marker O
/lscript O
, O
whereas O
universal O
quantities O
are O
left O
unsuper- O
scripted O
. O

The O
generative O
process O
is O
written O
in O
al- O
gorithmic O
form O
in O
Alg O
. O

1 O
. O
Note O
that O
each O
step O
is O
numbered O
and O
color O
- O
coded O
for O
ease O
of O
comparison O
with O
the O
full O
joint O
likelihood O
in O
Fig O
. O

2 O
. O
Step O
1 O
: O
p(N).We O
sample O
the O
size O
Nof O
the O
uni- O
versal O
phone O
inventory O
¯Vfrom O
a O
Poisson O
distribu- O
tion O
with O
a O
rate O
parameter O
λ O
, O
i.e. O
, O
N∼Poisson O
( O
λ O
) O
. O

( O
7 O
) O
That O
is O
, O
we O
do O
not O
presuppose O
a O
certain O
number O
of O
phones O
in O
the O
model O
. O

Step O
2 O
: O
p(µ1, O
... O
,µN).Next O
, O
we O
sample O
the O
means O
µiof O
the O
Gaussian O
phones O
. O

In O
the O
model O
presented O
here O
, O
we O
assume O
that O
each O
phone O
is O
generated O
independently O
, O
so O
p(µ1, O
... O
,µN O
) O

= O
/producttextN O
i=1p(µi O
) O
. O

Also O
, O
we O
assume O
a O
standard O
Gaussian O
prior O
over O
the O
means O
, O
µi∼N(0,I O
) O
. O

The O
sampled O
means O
deﬁne O
our O
NGaussian O
phonesN O
/ O
parenleftbig O
µi O
, O
σ2I O
/ O
parenrightbig O
: O
we O
are O
assuming O
for O
simplic- O
ity O
that O
all O
phones O
share O
a O
single O
spherical O
covari- O
ance O
matrix O
, O
deﬁned O
by O
the O
hyperparameter O
σ2 O
. O

The O
dispersion O
and O
focalization O
of O
these O
phones O
deﬁne O
the O
matrix O
Laccording O
to O
equations O
( O
4)–(6 O
) O
, O
whereρin(4)and O

the O
weights O
of O
the O
focalization O
neural O
net O
( O
5 O
) O
are O
also O
hyperparameters O
. O

Step O
3 O
: O
p(¯V O
/ O
lscript|µ1, O
... O
,µN).Next O
, O
for O
each O
lan- O
guage O
/ O
lscript∈[1, O
... O
,M O
] O
, O
we O
sample O
a O
diverse O
subset O
of O
theNphones O
, O
via O
a O
single O
draw O
from O
a O
DPP B-MethodName
parameterized O
by O
matrix O
L O
: O
¯V O
/ O
lscript∼DPP(L O
) O
, O
( O
8) O
where O
¯V O
/ O
lscript⊆[1,N O
] O
. O

Thus O
, O
i∈¯V O
/ O
lscriptmeans O
that O
language O
/ O
lscriptcontains O
phone O
¯vi O
. O
Note O
that O
even O
the O
size O
of O
the O
inventory O
, O
n O
/ O
lscript=|¯V O
/ O
lscript| O
, O
was O
chosen O
by O
the O
DPP B-MethodName
. O

In O
general O
, O
we O
have O
n O
/ O
lscript O
/ O
lessmuchN. O
Step O
4:/producttext O
i∈¯V O
/ O
lscriptp(v O
/ O
lscript O

i|µi)The O
ﬁnal O
step O
in O
our O
generative O
process O
is O
that O
the O
phones O
¯viin O
language O
/lscriptmust O
generate O
the O
pronunciations O
v O
/ O
lscript O
i∈R2(for- O
mant O
vectors O
) O
that O
are O
actually O
observed O
in O
lan- O
guage O
/ O
lscript O
. O

Each O
vector O
takes O
two O
steps O
. O

For O
each O
i∈¯V O
/ O
lscript O
, O
we O
generate O
an O
underlying O
˜vi∈R2from O
the O
corresponding O
Gaussian O
phone O
. O

Then O
, O
we O
runthis O
vector O
through O
a O
feed O
- O
forward O
neural O
network O
νθwith O
parameters O
θ O
. O

In O
short O
: O
˜v O
/ O
lscript O
i∼N(µi O
, O
σ2I O
) O
( O
9 O
) O
v O
/ O
lscript O
i O
= O
νθ(˜v O
/ O
lscript O
i O
) O
, O
( O
10 O
) O
where O
the O
second O
step O
is O
deterministic O
. O

We O
can O
fuse O
these O
two O
steps O
into O
a O
single O
step O
p(vi|µi O
) O
, O
whose O
closed O
- O
form O
density O
is O
given O
in O
eq O
. O

( O
12 O
) O
be- O
low O
. O

In O
effect O
, O
step O
4 O
takes O
a O
Gaussian O
phone O
as O
input O
and O
produces O
the O
observed O
formant O
vector O
with O
an O
underlying O
formant O
vector O
in O
the O
middle O
. O

This O
completes O
our O
generative O
process O
. O

We O
do O
not O
observe O
all O
the O
steps O
, O
but O
only O
the O
ﬁnal O
col- O
lection O
of O
pronunciations O
v O
/ O
lscript O
ifor O
each O
language O
, O
where O
the O
subscripts O
ithat O
indicate O
phone O
identity O
have O
been O
lost O
. O

The O
probability O
of O
this O
incomplete O
dataset O
involves O
summing O
over O
possible O
phones O
for O
each O
pronunciation O
, O
and O
is O
presented O
in O
Fig O
. O

2 O
. O
6.1 O

A O
Neural O
Transformation O
of O
a O
Gaussian O
A O
crucial O
bit O
of O
our O
model O
is O
running O
a O
sample O
from O
a O
Gaussian O
through O
a O
neural O
network O
. O

Under O
certain O
restrictions O
, O
we O
can O
ﬁnd O
a O
closed O
form O
for O
the O
resulting O
density O
; O
we O
discuss O
these O
below O
. O

Let O
νθbe O
a O
depth-2 B-HyperparameterName
multi I-HyperparameterName
- I-HyperparameterName
layer I-HyperparameterName
perceptron O
νθ(˜ O
vi O
) O

= O
W2tanh O
( O
W1˜ O
vi+b1 O
) O
+ O
b2.(11 O
) O

In O
order O
to O
ﬁnd O
a O
closed O
- O
form O
solution O
, O
we O
require O
that O
( O
5)be O
a O
diffeomorphism O
, O
i.e. O
, O
an O
invertible O
mapping O
from O
R2→R2where O
both O
νθand O
its O
inverseν−1 O
θare O
differentiable O
. O

This O
will O
be O
true O
as O
long O
asW1,W2∈R2×2are O
square O
matrices O
of O
full- O
rank O
and O
we O
choose O
a O
smooth O
, O
invertible O
activation O
function O
, O
such O
as O
tanh B-MethodName
. O

Under O
those O
conditions O
, O
we O
may O
apply O
the O
standard O
theorem O
for O
transforming O
a O
random O
variable O
( O
see O
Stark O
and O
Woods O
, O
2011 O
): O
p(vi|µi O
) O

= O
p(ν−1 O
θ(vi)|µi)detJν−1 O
θ(vi O
) O

= O
p(˜vi|µi)detJν−1 O
θ(vi)(12 O
) O

whereJν−1 O
θ(x)is O
the O
Jacobian O
of O
the O
inverse O
of O
the O
neural O
network O
at O
the O
point O
x. O
Recall O
that O
p(˜vi|µi O
) O
is O
Gaussian O
- O
distributed O
. O

7 O
Modeling O
Assumptions O
Imbued O
in O
our O
generative O
story O
are O
a O
number O
of O
assumptions O
about O
the O
linguistic O
processes O
behind O
vowel O
inventories O
. O

We O
brieﬂy O
draw O
connections O
between O
our O
theory O
and O
the O
linguistics O
literature.41 O

Why O
underlying O
phones O
? O

A O
technical O
assump- O
tion O
of O
our O
model O
is O
the O
existence O
of O
a O
universal O
set O
of O
underlying O
phones O
. O

Each O
phone O
is O
equipped O
with O
a O
probability O
distribution O
over O
reported O
acous- O
tic O
measurements O
( O
pronunciations O
) O
, O
to O
allow O
for O
a O
single O
phone O
to O
account O
for O
multiple O
slightly O
differ- O
ent O
pronunciations O
in O
different O
languages O
( O
though O
never O
in O
the O
same O
language O
) O
. O

This O
distribution O
can O
capture O
both O
actual O
interlingual O
variation O
and O
also O
random O
noise O
in O
the O
measurement O
process O
. O

While O
our O
universal O
phones O
may O
seem O
to O
re- O
semble O
the O
universal O
IPA O
symbols O
used O
in O
phono- O
logical O
transcription O
, O
they O
lack O
the O
rich O
featural O
speciﬁcations O
of O
such O
phonemes O
. O

A O
phone O
in O
our O
model O
has O
no O
features O
other O
than O
its O
mean O
position O
, O
which O
wholly O
determines O
its O
behavior O
. O

Our O
univer- O
sal O
phones O
are O
not O
a O
substantive O
linguistic O
hypothe- O
sis O
, O
but O
are O
essentially O
just O
a O
way O
of O
partitioning O
R2 O
into O
ﬁnitely O
many O
small O
regions O
whose O
similarity O
and O
focalization O
can O
be O
precomputed O
. O

This O
techni- O
cal O
trick O
allows O
us O
to O
use O
a O
discrete O
rather O
than O
a O
continuous O
DPP B-MethodName
over O
the O
R2space.1 O
Why O
a O
neural O
network O
? O

Our O
phones O
are O
Gaus- O
sians O
of O
spherical O
variance O
σ2 O
, O
presumed O
to O
be O
scat- O
tered O
with O
variance O
1 O
about O
a O
two O
- O
dimensional O
la- O
tentvowel O
space O
. O

Distances O
in O
this O
latent O
space O
are O
used O
to O
compute O
the O
dissimilarity O
of O
phones O
for O
modeling O
dispersion O
, O
and O
also O
to O
describe O
the O
phone O
’s O
ability O
to O
vary O
across O
languages O
. O

That O
is O
, O
two O
phones O
that O
are O
distant O
in O
the O
latent O
space O
can O
appear O
in O
the O
same O
inventory O
— O
presumably O
they O
are O
easy O
to O
discriminate O
in O
both O
perception O
and O
articulation O
— O
and O
it O
is O
easy O
to O
choose O
which O
one O
better O
explains O
an O
acoustic O
measurement O
, O
thereby O
affecting O
the O
other O
measurements O
that O
may O
appear O
in O
the O
inventory O
. O

We O
relate O
this O
latent O
space O
to O
measurable O
acous- O
tic O
space O
by O
a O
learned O
diffeomorphism O
νθ(Cotterell O
and O
Eisner O
, O
2017 O
) O
. O

ν−1 O
θcan O
be O
regarded O
as O
warping O
the O
acoustic O
distances O
into O
perceptual O
/ O
articulatory O
distances O
. O

In O
some O
“ O
high O
- O
resolution O
” O
regions O
of O
acoustic O
space O
, O
phones O
with O
fairly O
similar O
( O
F1,F2 O
) O
values O
might O
yet O
be O
far O
apart O
in O
the O
latent O
space O
. O

Conversely O
, O
in O
other O
regions O
, O
relatively O
large O
acous- O
1Indeed O
, O
we O
could O
have O
simply O
taken O
our O
universal O
phone O
set O
to O
be O
a O
huge O
set O
of O
tiny O
, O
regularly O
spaced O
overlapping O
Gaus- O
sians O
that O
“ O
covered O
” O
( O
say O
) O
the O
unit O
circle O
. O

As O
a O
computational O
matter O
, O
we O
instead O
opted O
to O
use O
a O
smaller O
set O
of O
Gaussians O
, O
giving O
the O
learner O
the O
freedom O
to O
infer O
their O
positions O
and O
tune O
their O
variance O
σ2 O
. O

Because O
of O
this O
freedom O
, O
this O
set O
should O
not O
be O
too O
large O
, O
or O
a O
MAP O
learner O
may O
overﬁt O
the O
training O
data O
with O
zero O
- O
variance O
Gaussians O
and O
be O
unable O
to O
explain O
the O
test O
languages O
— O
similar O
to O
overﬁtting O
a O
Gaussian O
mixture O
model.tic O
changes O
in O
some O
direction O
might O
not O
prevent O
two O
phones O
from O
acting O
as O
similar O
or O
two O
pronunci- O
ations O
from O
being O
attributed O
to O
the O
same O
phone O
. O

In O
general O
, O
a O
unit O
circle O
of O
radius O
σin O
latent O
space O
may O
be O
mapped O
by O
νθto O
an O
oddly O
shaped O
connected O
re- O
gion O
in O
acoustic O
space O
, O
and O
a O
Gaussian O
in O
latent O
space O
may O
be O
mapped O
to O
a O
multimodal O
distribution O
. O

8 O
Inference O
and O
Learning O
We O
ﬁt O
our O
model O
via O
MAP B-MethodName
- I-MethodName
EM I-MethodName
( O
Dempster O
et O
al O
. O
, O
1977 O
) O
. O

The O
E O
- O
step O
involves O
deciding O
which O
phones O
each O
language O
has O
. O

To O
achieve O
this O
, O
we O
fashion O
a O
Gibbs O
sampler O
( O
Geman O
and O
Geman O
, O
1984 O
) O
, O
yielding O
a O
Markov O
- O
Chain O
Monte O
Carlo O
E O
- O
step O
( O
Levine O
and O
Casella O
, O
2001 O
) O
. O

8.1 O
Inference O
: O
MCMC B-MethodName
E I-MethodName
- I-MethodName
Step I-MethodName
Inference O
in O
our O
model O
is O
intractable O
even O
when O
the O
phones O
µ1, O
... O
,µNare O
ﬁxed O
. O

Given O
a O
language O
withnvowels O
, O
we O
have O
to O
determine O
which O
subset O
of O
theNphones O
best O
explains O
those O
vowels O
. O

As O
discussed O
above O
, O
the O
alignment O
abetween O
the O
n O
vowels O
andnof O
theNphones O
represents O
a O
latent O
variable O
. O

Marginalizing O
it O
out O
is O
# O
P O
- O
hard O
, O
as O
we O
can O
see O
that O
it O
is O
equivalent O
to O
summing O
over O
all O
bipartite O
matchings O
in O
a O
weighted O
graph O
, O
which O
, O
in O
turn O
, O
is O
as O
costly O
as O
computing O
the O
permanent O
of O
a O
matrix O
( O
Valiant O
, O
1979 O
) O
. O

Our O
sampler2is O
an O
approxi- O
mation O
algorithm O
for O
the O
task O
. O

We O
are O
interested O
in O
sampling O
a O
, O
the O
labeling O
of O
observed O
vowels O
with O
universal O
phones O
. O

Note O
that O
this O
implicitly O
sam- O
ples O
the O
language O
’s O
phone O
inventory O
¯V(a O
) O
, O
which O
is O
fully O
determined O
by O
a. O
Speciﬁcally O
, O
we O
employ O
an O
MCMC B-MethodName
method O
closely O
related O
to O
Gibbs O
sampling O
. O

At O
each O
step O
of O
the O
sampler O
, O
we O
update O
our O
vowel O
- O
phone O
align- O
menta O
/ O
lscriptas O
follows O
. O

Choose O
a O
language O
/lscriptand O
a O
vowel O
index O
k∈[1,n O
/ O
lscript O
] O
, O
and O
leti O
= O
a O
/ O
lscript O
k(that O
is O
, O
pronunciation O
v O
/ O
lscript O
, O
kis O
currently O
labeled O
with O
univer- O
sal O
phone O
¯vi O
) O
. O

We O
will O
consider O
changing O
a O
/ O
lscript O
ktoj O
, O
wherejis O
drawn O
from O
the O
( O
N−n O
/ O
lscript)phones O
that O
donotappear O
in O
¯V(a O
/ O
lscript O
) O
, O
heuristically O
choosing O
jin O
proportion O
to O
the O
likelihood O
p(v O
/ O
lscript O
, O
k|µj O
) O
. O

We O
then O
stochastically O
decide O
whether O
to O
keep O
a O
/ O
lscript O
k O
= O
ior O
set O
a O
/ O
lscript O
k O
= O
jin O
proportion O
to O
the O
resulting O
values O
of O
the O
product O
4·3 O
in O
eq O
. O

( O
2 O
) O
. O

For O
a O
single O
E O
- O
step O
, O
the O
Gibbs O
sampler O
“ O
warm- O
starts O
” O
with O
the O
labeling O
from O
the O
end O
of O
the O
pre- O
vious O
iteration O
’s O
E O
- O
step O
. O

It O
sweeps O
S= B-HyperparameterName
5 B-HyperparameterValue
times O
2Taken O
from O
V O
olkovs O
and O
Zemel O
( O
2012 O
, O
3.1).42 O

through O
all O
vowels O
for O
all O
languages O
, O
and O
returns O
S O
sampled O
labelings O
, O
one O
from O
the O
end O
of O
each O
sweep O
. O

We O
are O
also O
interested O
in O
automatically O
choosing O
the O
number O
of O
phones O
N O
, O
for O
which O
we O
take O
the O
Poisson O
’s O
rate O
parameter O
λ= B-HyperparameterName
100 B-HyperparameterValue
. O

To O
this O
end O
, O
we O
employ O
reversible O
- O
jump O
MCMC B-MethodName
( O
Green O
, O
1995 O
) O
, O
resamplingNat O
the O
start O
of O
every O
E O
- O
step O
. O

8.2 O
Learning O
: O
M O
- O
Step O
Given O
the O
set O
of O
sampled O
alignments O
provided O
by O
the O
E O
- O
step O
, O
our O
M O
- O
step O
consists O
of O
optimizing O
the O
log O
- O
likelihood O
of O
the O
now O
- O
complete O
training O
data O
using O
the O
inferred O
latent O
variables O
. O

We O
achieved O
this O
through O
SGD B-MethodName
training O
of O
the O
diffeomorphism O
parameters O
θ O
, O
the O
means O
µiof O
the O
Gaussian O
phones O
, O
and O
the O
parameters O
of O
the O
focalization O
kernel O
F. O
9 O
Experiments O
9.1 O
Data O
Our O
data O
is O
taken O
from O
the O
Becker O
- O
Kristal O
corpus O
( O
Becker O
- O
Kristal O
, O
2006 O
) O
, O
which O
is O
a O
compilation O
of O
various O
phonetic O
studies O
and O
forms O
the O
largest O
multi- O
lingual O
phonetic O
database O
. O

Each O
entry O
in O
the O
corpus O
corresponds O
to O
a O
linguist O
’s O
phonetic O
description O
of O
a O
language O
’s O
vowel O
system O
: O
an O
inventory O
consist- O
ing O
of O
IPA O
symbols O
where O
each O
symbol O
is O
associ- O
ated O
with O
two O
or O
more O
formant O
values O
. O

The O
corpus O
contains O
data O
from O
233 O
distinct O
languages O
. O

When O
multiple O
inventories O
were O
available O
for O
the O
same O
language O
( O
due O
to O
various O
studies O
in O
the O
literature O
) O
, O
we O
selected O
one O
at O
random O
and O
discarded O
the O
others O
. O

9.2 O
Baselines O
Baseline O
# O
1 O
: O
Removing O
dispersion O
. O

The O
key O
technical O
innovation O
in O
our O
work O
lies O
in O
the O
incor- O
poration O
of O
a O
DPP B-MethodName
into O
a O
generative B-MethodName
model I-MethodName
of O
vowel O
formants O
— O
a O
continuous O
- O
valued O
quantity O
. O

The O
role O
of O
the O
DPP B-MethodName
was O
to O
model O
the O
linguistic O
principle O
of O
dispersion O
— O
we O
may O
cripple O
this O
portion O
of O
our O
model O
, O
e.g. O
, O
by O
forcing O
Kto O
be O
a O
diagonal O
kernel O
, O
i.e. O
,Kij= O
0 O
fori O
/ O
negationslash O
= O
j. O

In O
this O
case O
the O
DPP B-MethodName
becomes O
a O
Bernoulli O
Point O
Process O
( O
BPP)—a O
spe- O
cial O
case O
of O
the O
DPP B-MethodName
. O

Since O
dispersion O
is O
widely O
accepted O
to O
be O
an O
important O
principle O
governing O
naturally O
occurring O
vowel O
systems O
, O
we O
expect O
a O
system O
trained O
without O
such O
knowledge O
to O
perform O
worse O
. O

Baseline O
# O
2 O
: O
Removing B-TaskName
the I-TaskName
neural I-TaskName
network I-TaskName
νθ O
. O

Another O
question O
we O
may O
ask O
of O
our O
formulation O
is O
whether O
we O
actually O
need O
a O
fancy O
neural O
mapping O
νθto O
model O
our O
typological O
data O
well O
. O

The O
humanperceptual O
system O
is O
known O
to O
perform O
a O
non O
- O
linear O
transformation O
on O
acoustic O
signals O
, O
starting O
with O
the O
non O
- O
linear O
cochlear O
transform O
that O
is O
physically O
performed O
in O
the O
ear O
. O

While O
ν−1 O
θis O
intended O
as O
loosely O
analogous O
, O
we O
determine O
its O
beneﬁt O
by O
re- O
moving O
eq O
. O

( O
10 O
) O
from O
our O
generative O
story O
, O
i.e. O
, O
we O
take O
the O
observed O
formants O
vkto O
arise O
directly O
from O
the O
Gaussian O
phones O
. O

Baseline O
# O
3 O
: O
Supervised O
phones O
and O
alignments O
. O

A O
ﬁnal O
baseline O
we O
consider O
is O
supervised O
phones O
. O

Linguists O
standardly O
employ O
a O
ﬁnite O
set O
of O
phones O
— O
symbols O
from O
the O
international O
phonetic O
alphabet O
( O
IPA O
) O
. O

In O
phonetic O
annotation O
, O
it O
is O
common O
to O
map O
each O
sound O
in O
a O
language O
back O
to O
this O
universal O
dis- O
crete O
alphabet O
. O

Under O
such O
an O
annotation O
scheme O
, O
it O
is O
easy O
to O
discern O
, O
cross O
- O
linguistically O
, O
which O
vow- O
els O
originate O
from O
the O
same O
phoneme O
: O
an O
/ O
I/ O
in O
German O
may O
be O
roughly O
equated O
with O
an O
/ O
I/ O
in O
En- O
glish O
. O

However O
, O
it O
is O
not O
clear O
how O
consistent O
this O
annotation O
truly O
is O
. O

There O
are O
several O
reasons O
to O
expect O
high O
- O
variance O
in O
the O
cross O
- O
linguistic O
acous- O
tic O
signal O
. O

First O
, O
IPA O
symbols O
are O
primarily O
useful O
for O
interlinked O
phonological O
distinctions O
, O
i.e. O
, O
one O
applies O
the O
symbol O
/ O
I/ O
to O
distinguish O
it O
from O
/i/ O
in O
the O
given O
language O
, O
rather O
than O
to O
associate O
it O
with O
the O
sound O
bearing O
the O
same O
symbol O
in O
a O
second O
language O
. O

Second O
, O
ﬁeld O
linguists O
often O
resort O
to O
the O
closest O
common O
IPA O
symbol O
, O
rather O
than O
an O
exact O
match O
: O
if O
a O
language O
makes O
no O
distinction O
between O
/i/ O
and O
/ O
I/ O
, O
it O
is O
more O
common O
to O
denote O
the O
sound O
with O
a O
/i/. O

Thus O
, O
IPA O
may O
not O
be O
as O
universal O
as O
hoped O
. O

Our O
dataset O
contains O
50 O
IPA O
symbols O
so O
this O
baseline O
is O
only O
reported O
for O
N= B-HyperparameterName
50 B-HyperparameterValue
. O

9.3 O
Evaluation O
Evaluation O
in O
our O
setting O
is O
tricky O
. O

The O
scientiﬁc O
goal O
of O
our O
work O
is O
to O
place O
a O
bit O
of O
linguistic O
the- O
ory O
on O
a O
ﬁrm O
probabilistic O
footing O
, O
rather O
than O
a O
downstream O
engineering O
- O
task O
, O
whose O
performance O
we O
could O
measure O
. O

We O
consider O
three O
metrics O
. O

Cross O
- O
Entropy O
. O

Our O
ﬁrst O
evaluation O
metric O
is O
cross O
- O
entropy O
: O
the O
average O
negative O
log O
- O
probability O
of O
the O
vowel O
systems O
in O
held O
- O
out O
test O
data O
, O
given O
the O
universal O
inventory O
of O
Nphones O
that O
we O
trained O
through O
EM O
. O

We O
ﬁnd O
this O
to O
be O
the O
cleanest O
method O
for O
scientiﬁc O
evaluation O
— O
it O
is O
the O
metric O
of O
opti- O
mization O
and O
has O
a O
clear O
interpretation O
: O
how O
sur- O
prised O
was O
the O
model O
to O
see O
the O
vowel O
systems O
of O
held O
- O
out O
, O
but O
attested O
, O
languages O
? O

The O
cross O
- O
entropy O
is O
the O
negative O
log O
of O
the O
/ O
producttext O
/ O
bracketleftbig O
· O
· O
· O
/bracketrightbig O
expression O
in O
eq O
. O

( O
2 O
) O
, O
with O
/lscriptnow O
rang-43 O

N O
metric O
DPP B-MethodName
+ O
νθBPP+νθDPP−νθSup O
. O

Cross B-MetricName
- I-MetricName
entropy I-MetricName
in O
nats O
per O
language O
( O
lower O
is O
better O
) O
and O
expected O
Euclidean O
- O
distance O
error O
of O
the O
cloze O
prediction O
( O
lower O
is O
better O
) O
. O

The O
overall O
best O
value O
for O
each O
task O
is O
bold- O
faced O
. O

The O
case O
N= B-HyperparameterName
50 B-HyperparameterValue
is O
compared O
against O
our O
supervised O
baseline O
. O

The O
N= B-HyperparameterName
57 B-HyperparameterValue
row O
is O
the O
case O
where O
we O
allowed O
N O
to O
ﬂuctuate O
during O
inference O
using O
reversible O
- O
jump O
MCMC B-MethodName
; O
this O
was O
theNvalue O
selected O
at O
the O
ﬁnal O
EM O
iteration O
. O

ing O
over O
held O
- O
out O
languages.3Wallach O
et O
al O
. O

( O
2009 O
) O
give O
several O
methods O
for O
estimating O
the O
intractable O
sum O
in O
language O
/lscript O
. O

We O
use O
the O
simple O
harmonic O
mean O
estimator O
, O
based O
on O
50 O
samples O
of O
a O
/ O
lscriptdrawn O
with O
our O
Gibbs O
sampler O
( O
warm O
- O
started O
from O
the O
ﬁnal O
E O
- O
step O
of O
training O
) O
. O

Cloze O
Evaluation O
. O

In O
addition O
, O
following O
Cot- O
terell O
and O
Eisner O
( O
2017 O
) O
, O
we O
evaluate O
our O
trained O
model O
’s O
ability O
to O
perform O
a O
cloze O
task O
( O
Taylor O
, O
1953 O
) O
. O

Given O
n O
/ O
lscript−1orn O
/ O
lscript−2of O
the O
vowels O
in O
held- O
out O
language O
/lscript O
, O
can O
we O
predict O
the O
pronunciations O
vkof O
the O
remaining O
1 O
or O
2 O
? O

We O
predict O
vkto O
be O
νθ(µi)wherei O
= O
a O
/ O
lscript O
kis O
the O
phone O
inferred O
by O
the O
sampler O
. O

Note O
that O
the O
sampler O
’s O
inference O
here O
is O
based O
only O
on O
the O
observed O
vowels O
( O
the O
likelihood O
) O
and O
the O
focalization O
- O
dispersion O
preferences O
of O
the O
DPP B-MethodName
( O
the O
prior O
) O
. O

We O
report O
the O
expected O
error O
of O
such O
a O
prediction O
— O
where O
error O
is O
quantiﬁed O
by O
Eu- O
clidean O
distance O
in O
( O
F1,F2)formant O
space O
— O
over O
the O
same O
50 O
samples O
of O
a O
/ O
lscript O
. O

For O
instance O
, O
consider O
a O
previously O
unseen O
vowel O
system O
with O
formant O
values O
{ O
( O
499,2199 O
) O
, O
( O
861,1420 O
) O
, O
( O
571,1079 O
) O
} O
. O

A O
“ O
cloze1 O
” O
evaluation O
would O
aim O
to O
predict O
{ O
( O
499,2199)}as O
the O
missing O
3Since O
that O
expression O
is O
the O
product O
of O
both O
probability O
distributions O
and O
probability O
densities O
, O
our O
“ O
cross O
- O
entropy O
” O
metric O
is O
actually O
the O
sum O
of O
both O
entropy O
terms O
and O
( O
poten- O
tially O
negative O
) O
differential O
entropy O
terms O
. O

Thus O
, O
a O
value O
of O
0 O
has O
no O
special O
signiﬁcance O
. O

A O
graph O
of O
v= O
( O
F1,F2)in O
the O
union O
of O
all O
the O
training O
languages O
’ O
inventories O
, O
color O
- O
coded O
by O
inferred O
phone O
( O
N= B-HyperparameterName
50 B-HyperparameterValue
) O
. O

vowel O
, O
given{(861,1420 O
) O
, O
( O
571,1079 O
) O
} O
, O
and O
the O
fact O
thatn O
/ O
lscript= O
3 O
. O

A O
“ O
cloze12 O
” O
evaluation O
would O
aim O
to O
predict O
two O
missing O
vowels O
. O

9.4 O
Experimental O
Details O
Here O
, O
we O
report O
experimental O
details O
and O
the O
hy- O
perparameters O
that O
we O
use O
to O
achieve O
the O
results O
reported O
. O

We O
consider O
a O
neural O
network O
νθwith O
k∈[1,4]layers O
and O
ﬁnd O
k= O
1 O
the O
best O
per- O
former O
on O
development O
data O
. O

Recall O
that O
our O
dif- O
feomorphism O
constraint O
requires O
that O
each B-HyperparameterName
layer I-HyperparameterName
have O
exactly O
two B-HyperparameterValue
hidden I-HyperparameterValue
units I-HyperparameterValue
, O
the O
same O
as O
the O
number O
of O
observed O
formants O
. O

We O
consider O
N∈ O
{ O
15,25,50,100}phones O
as O
well O
as O
letting O
Nﬂuc- O
tuate O
with O
reversible O
- O
jump O
MCMC B-MethodName
( O
see O
footnote O
1 O
) O
. O

We O
train O
for O
100 B-HyperparameterName
iterations I-HyperparameterName
of O
EM O
, O
taking O
S= O
5 O
samples O
at O
each O
E O
- O
step O
. O

At O
each O
M O
- O
step O
, O
we O
run O
50 B-HyperparameterName
iterations I-HyperparameterName
of O
SGD B-MethodName
for O
the O
focalization O
NN O
and O
also O
for O
the O
diffeomorphism O
NN O
. O

For O
each O
N O
, O
we O
selected O
( O
σ2,ρ)by O
minimizing O
cross O
- O
entropy O
on O
a O
held O
- O
out O
development O
set O
. O

We O
considered O
( O
σ2,ρ)∈{10k}5 O
k=1×{ρk}5 O
k=1 O
. O

9.5 O
Results O
and O
Error O
Analysis O
We O
report O
results O
in O
Tab O
. O

1 O
. O

We O
ﬁnd O
that O
our O
DPP B-MethodName
model O
improves O
over O
the O
baselines O
. O

The O
results O
support O
two O
claims O
: O
( O
i O
) O
dispersion O
plays O
an O
impor- O
tant O
role O
in O
the O
structure O
of O
vowel O
systems O
and O
( O
ii O
) O
learning O
a O
non O
- O
linear O
transformation O
of O
a O
Gaussian O
improves O
our O
ability O
to O
model O
sets O
of O
formant O
- O
pairs O
. O

Also O
, O
we O
observe O
that O
as O
we O
increase O
the O
number O
of O
phones O
, O
the O
role O
of O
the O
DPP B-MethodName
becomes O
more O
impor- O
tant O
. O

We O
visualize O
a O
sample O
of O
the O
trained O
alignment O
in O
Fig O
. O

3.44 O

Frequency O
Encodes O
Dispersion O
. O

Why O
does O
dis- O
persion O
not O
always O
help O
? O

The O
models O
with O
fewer O
phones O
do O
not O
reap O
the O
beneﬁts O
that O
the O
models O
with O
more O
phones O
do O
. O

The O
reason O
lies O
in O
the O
fact O
that O
the O
most O
common O
vowel O
formants O
are O
already O
dispersed O
. O

This O
indicates O
that O
we O
still O
have O
not O
quite O
modeled O
the O
mechanisms O
that O
select O
for O
good O
vowel O
formants O
, O
despite O
our O
work O
at O
the O
phonetic O
level O
; O
further O
research O
is O
needed O
. O

We O
would O
prefer O
a O
model O
that O
explains O
the O
evolutionary O
motivation O
of O
sound O
systems O
as O
communication O
systems O
. O

Number O
of O
Induced O
Phones O
. O

What O
is O
most O
salient O
in O
the O
number O
of O
induced O
phones O
is O
that O
it O
is O
close O
to O
the O
number O
of O
IPA O
phonemes O
in O
the O
data O
. O

However O
, O
the O
performance O
of O
the O
phoneme- O
supervised O
system O
is O
much O
worse O
, O
indicating O
that O
, O
perhaps O
, O
while O
the O
linguists O
have O
the O
right O
idea O
about O
the O
number O
of O
universal O
symbols O
, O
they O
did O
not O
specify O
the O
correct O
IPA O
symbol O
in O
all O
cases O
. O

Our O
data O
analysis O
indicates O
that O
this O
is O
often O
due O
to O
pragmatic O
concerns O
in O
linguistic O
ﬁeld O
analysis O
. O

For O
example O
, O
even O
if O
/ O
I/ O
is O
the O
proper O
IPA O
symbol O
for O
the O
sound O
, O
if O
there O
is O
no O
other O
sound O
in O
the O
vicinity O
the O
annotator O
may O
prefer O
to O
use O
more O
common O
/i/. O
10 O
Related O
Work O
Most O
closely O
related O
to O
our O
work O
is O
the O
classic O
study O
of O
Liljencrants O
and O
Lindblom O
( O
1972 O
) O
, O
who O
provide O
a O
simulation O
- O
based O
account O
of O
vowel O
systems O
. O

They O
argued O
that O
minima O
of O
a O
certain O
objective O
that O
en- O
codes O
dispersion O
should O
correspond O
to O
canonical O
vowel O
systems O
of O
a O
given O
size O
n. O

Our O
tack O
is O
dif- O
ferent O
in O
that O
we O
construct O
a O
generative B-MethodName
probability I-MethodName
model I-MethodName
, O
whose O
parameters O
we O
learn O
from O
data O
. O

How- O
ever O
, O
the O
essence O
of O
modeling O
is O
the O
same O
in O
that O
we O
explain O
formant O
values O
, O
rather O
than O
discrete O
IPA O
symbols O
. O

By O
extension O
, O
our O
work O
is O
also O
closely O
related O
to O
extensions O
of O
this O
theory O
( O
Schwartz O
et O
al O
. O
, O
1997 O
; O
Roark O
, O
2001 O
) O
that O
focused O
on O
incorporating O
the O
notion O
of O
focalization O
into O
the O
experiments O
. O

Our O
present O
paper O
can O
also O
be O
regarded O
as O
a O
con- O
tinuation O
of O
Cotterell O
and O
Eisner O
( O
2017 O
) O
, O
in O
which O
we O
used O
DPPs O
to O
model O
vowel O
inventories O
as O
sets O
of O
discrete O
IPA O
symbols O
. O

That O
paper O
pretended O
that O
each O
IPA O
symbol O
had O
a O
single O
cross O
- O
linguistic O
( O
F1,F2)pair O
, O
an O
idealization O
that O
we O
remove O
in O
this O
paper O
by O
discarding O
the O
IPA O
symbols O
and O
modeling O
formant O
values O
directly.11 O
Conclusion O
Our O
model O
combines O
existing O
techniques O
of O
proba- O
bilistic O
modeling O
and O
inference O
to O
attempt O
to O
ﬁt O
the O
actual O
distribution O
of O
the O
world O
’s O
vowel O
systems O
. O

We O
presented O
a O
generative B-MethodName
probability I-MethodName
model I-MethodName
of O
sets O
of O
measured O
( O
F1,F2)pairs O
. O

We O
view O
this O
as O
a O
necessary O
step O
in O
the O
development O
of O
generative B-MethodName
probability I-MethodName
model I-MethodName
that O
can O
explain O
the O
distribu- O
tion O
of O
the O
world O
’s O
languages O
. O

Previous O
work O
on O
generating O
vowel O
inventories O
has O
focused O
on O
how O
those O
inventories O
were O
transcribed O
into O
IPA O
by O
ﬁeld O
linguists O
, O
whereas O
we O
focus O
on O
the O
ﬁeld O
linguists O
’ O
acoustic O
measurements O
of O
how O
the O
vowels O
are O
actu- O
ally O
pronounced O
. O

Acknowledgments O
We O
would O
like O
to O
acknowledge O
Tim O
Vieira O
, O
Katha- O
rina O
Kann O
, O
Sebastian O
Mielke O
and O
Chu O
- O
Cheng O
Lin O
for O
reading O
many O
early O
drafts O
. O

The O
ﬁrst O
author O
would O
like O
to O
acknowledge O
an O
NDSEG O
grant O
and O
a O
Facebook O
PhD O
fellowship O
. O

This O
material O
is O
also O
based O
upon O
work O
supported O
by O
the O
National O
Sci- O
ence O
Foundation O
under O
Grant O
No O
. O
1718846 O
to O
the O
last O
author O
. O

References O
Raja O
Haﬁz O
Affandi O
, O
Emily O
Fox O
, O
and O
Ben O
Taskar O
. O

2013 O
. O

Approximate O
inference O
in O
continuous O
determinantal O
processes O
. O

In O
Advances O
in O
Neural O
Information O
Pro- O
cessing O
Systems O
, O
pages O
1430–1438 O
. O

Roy O
Becker B-DatasetName
- I-DatasetName
Kristal I-DatasetName
. O

2006 O
. O

Predicting O
vowel O
inven- O
tories O
: O
The O
dispersion O
- O
focalization O
theory O
revisited O
. O

The O
Journal O
of O
the O
Acoustical O
Society O
of O
America O
, O
120(5):3248–3248 O
. O

Roy O
Becker B-DatasetName 
- I-DatasetName
Kristal I-DatasetName
. O

2010 O
. O

Acoustic O
Typology O
of O
Vowel O
Inventories O
and O
Dispersion O
Theory O
: O
Insights O
from O
a O
Large O
Cross O
- O
Linguistic O
Corpus O
. O

Ph.D. O
thesis O
, O
UCLA O
. O

Paulus O
Petrus O
Gerardus O
Boersma O
et O

al O
. O
2002 O
. O

Praat O
, O
a O
system O
for O
doing O
phonetics O
by O
computer O
. O

Glot O
Inter- O
national O
, O
5 O
. O

Alexei O
Borodin O
and O
Eric O
M. O
Rains O
. O

2005 O
. O

Eynard- O
Mehta O
theorem O
, O
Schur O
process O
, O
and O
their O
Pfafﬁan O
analogs O
. O

Journal O
of O
Statistical O
Physics O
, O
121(3- O
4):291–317 O
. O

Bernard O
Comrie O
, O
Matthew O
S. O
Dryer O
, O
David O
Gil O
, O
and O
Martin O
Haspelmath O
. O

2013 O
. O

Introduction O
. O

In O
Matthew O
S. O
Dryer O
and O
Martin O
Haspelmath O
, O
editors O
, O
The O
World O
Atlas O
of O
Language O
Structures O
Online O
. O

Max O
Planck O
Institute O
for O
Evolutionary O
Anthropol- O
ogy O
, O
Leipzig.45 O

Ryan O
Cotterell O
and O
Jason O
Eisner O
. O

2017 O
. O

Probabilistic O
typology O
: O

Deep O
generative B-MethodName
model I-MethodName
of O
vowel O
inven- O
tories O
. O

In O
Proceedings O
of O
the O
55th O
Annual O
Meet- O
ing O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
ACL O
) O
, O
Vancouver O
, O
Canada O
. O

Arthur O
P. O
Dempster O
, O
Nan O
M. O
Laird O
, O
and O
Donald O
B. O
Ru- O
bin O
. O

1977 O
. O

Maximum O
likelihood O
from O
incomplete O
data O
via O
the O
EM O
algorithm O
. O

Journal O
of O
the O
Royal O
Rta- O
tistical O
Society O
, O
Series O
B O
( O
Statistical O
Methodology O
) O
, O
pages O
1–38 O
. O

Li O
Deng O
and O
Douglas O
O’Shaughnessy O
. O

2003 O
. O

Speech O
Processing O
: O
A O
Dynamic O
and O
Optimization O
- O
Oriented O
Approach O
. O

CRC O
Press O
. O

Stuart O
Geman O
and O
Donald O
Geman O
. O

1984 O
. O

Stochas- O

tic O
relaxation O
, O
Gibbs O
distributions O
, O
and O
the O
Bayesian O
restoration O
of O
images O
. O

IEEE O
Transactions O
on O
Pattern O
Analysis O
and O
Machine O
Intelligence O
, O
( O
6):721–741 O
. O

Matthew O
K. O
Gordon O
. O

2016 O
. O

Phonological O
Typology O
. O

Oxford O
. O

Peter O
J. O
Green O
. O

1995 O
. O

Reversible O
jump O
Markov O
chain O
Monte O
Carlo O
computation O
and O
Bayesian O
model O
de- O
termination O
. O

Biometrika O
, O
82(4):711–732 O
. O

Tony O
Jebara O
, O
Risi O
Kondor O
, O
and O
Andrew O
Howard O
. O

2004 O
. O

Probability O
product O
kernels O
. O

Journal O
of O
Machine O
Learning O
Research O
, O
5:819–844 O
. O

Peter O
Ladefoged O
. O

2001 O
. O

Vowels O
and O
Consonants O
: O
An O
Introduction O
to O
the O
Sounds O
of O
Languages O
. O

Wiley- O
Blackwell O
. O

Richard O
A. O
Levine O
and O
George O
Casella O
. O

2001 O
. O

Im- O
plementations O
of O
the O
Monte O
Carlo O
EM O
algorithm O
. O

Journal O
of O
Computational O
and O
Graphical O
Statistics O
, O
10(3):422–439 O
. O

Johan O
Liljencrants O
and O
Bj O
¨orn O
Lindblom O
. O

1972 O
. O

Numer- O
ical O
simulation O
of O
vowel O
quality O
systems O
: O

The O
role O
of O
perceptual O
contrast O
. O

Language O
, O
pages O
839–862 O
. O

Brian O
Roark O
. O

2001 O
. O

Explaining O
vowel O
inventory O
ten- O
dencies O
via O
simulation O
: O
Finding O
a O
role O
for O
quantal O
locations O
and O
formant O
normalization O
. O

In O
North O
East O
Linguistic O
Society O
, O
volume O
31 O
, O
pages O
419–434 O
. O

Jean O
- O
Luc O
Schwartz O
, O
Christian O
Abry O
, O
Louis O
- O
Jean O
Bo O
¨e O
, O
Nathalie O
Vall O
´ O
ee O
, O
and O
Lucie O
M O
´ O
enard O
. O

2005 O
. O

The O
dispersion O
- O
focalization O
theory O
of O
sound O
systems O
. O

The O
Journal O
of O
the O
Acoustical O
Society O
of O
America O
, O
117(4):2422–2422 O
. O
Jean O
- O
Luc O
Schwartz O
, O
Louis O
- O
Jean O
Bo O
¨e O
, O
Nathalie O
Vall O
´ O
ee O
, O
and O
Christian O
Abry O
. O

1997 O
. O

The O
dispersion- O
focalization O
theory O
of O
vowel O
systems O
. O

Journal O
of O
Phonetics O
, O
25(3):255–286 O
. O

Henry O
Stark O
and O
John O
Woods O
. O

2011 O
. O

Probability O
, O
Statis- O
tics O
, O
and O
Random O
Processes O
for O
Engineers O
. O

Pearson O
. O

Kenneth O
N. O
Stevens O
. O
1987 O
. O

Relational O
properties O
as O
per- O
ceptual O
correlates O
of O
phonetic O
features O
. O

In O
Interna- O
tional O
Conference O
of O
Phonetic O
Sciences O
, O
pages O
352 O
– O
355.Kenneth O
N. O
Stevens O
. O
1989 O
. O

On O
the O
quantal O
nature O
of O
speech O
. O

Journal O
of O
Phonetics O
, O
17:3–45 O
. O

Wilson O
L. O
Taylor O
. O

1953 O
. O

Cloze B-MetricName
procedure O
: O
a O
new O
tool O
for O
measuring O
readability O
. O

Journalism O
and O
Mass O
Communication O
Quarterly O
, O
30(4):415 O
. O

Leslie O
G. O
Valiant O
. O

1979 O
. O

The O
complexity O
of O
comput- O
ing O
the O
permanent O
. O

Theoretical O
Computer O
Science O
, O
8(2):189–201 O
. O

Maksims O
V O
olkovs O
and O
Richard O
S. O
Zemel O
. O

2012 O
. O

Efﬁ- O
cient O
sampling O
for O
bipartite O
matching O
problems O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
pages O
1313–1321 O
. O

Hanna O
Wallach O
, O
Ian O
Murray O
, O
Ruslan O
Salakhutdinov O
, O
and O
David O
Mimno O
. O

2009 O
. O

Evaluation O
methods O
for O
topic O
models O
. O

In O
International O
Conference O
on O
Ma- O
chine O
Learning O
( O
ICML O
) O
, O
pages O
1105–1112.46 O

The O
Emergence O
of O
Number O
and O
Syntax O
Units O
in O
LSTM B-TaskName
Language O
Models O
Yair O
Lakretz O
Cognitive O
Neuroimaging O
Unit O
NeuroSpin O
center O
91191 O
, O
Gif O
- O
sur O
- O
Yvette O
, O
France O
yair.lakretz@gmail.comGerman O
Kruszewski O
Facebook O
AI O
Research O
Paris O
, O
France O

germank@gmail.com O

Theo O
Desbordes O
Facebook O
AI O
Research O
Paris O
, O
France O
tdesbordes@fb.comDieuwke O
Hupkes O
ILLC O
, O
University O
of O
Amsterdam O
Amsterdam O
, O
Netherlands O
d.hupkes@uva.nl O
Stanislas O
Dehaene O
Cognitive O
Neuroimaging O
Unit O
NeuroSpin O
center O
91191 O
, O
Gif O
- O
sur O
- O
Yvette O
, O
France O
stanislas.dehaene@gmail.comMarco O
Baroni O
Facebook O
AI O
Research O
Paris O
, O

France O
mbaroni@fb.com O

Abstract O
Recent O
work O
has O
shown O
that O
LSTMs B-TaskName
trained O
on O
a O
generic O
language O
modeling O
objective O
capture O
syntax O
- O
sensitive O
generalizations O
such O
as O
long- O
distance O
number O
agreement O
. O

We O
have O
however O
no O
mechanistic O
understanding O
of O
how O
they O
ac- O
complish O
this O
remarkable O
feat O
. O

Some O
have O
conjectured O
it O
depends O
on O
heuristics O
that O
do O
not O
truly O
take O
hierarchical O
structure O
into O
account O
. O

We O
present O
here O
a O
detailed O
study O
of O
the O
inner O
mechanics O
of O
number O
tracking O
in O
LSTMs B-TaskName
at O
the O
single O
neuron O
level O
. O

We O
discover O
that O
long- O
distance O
number O
information O
is O
largely O
man- O
aged O
by O
two O
“ O
number O
units O
” O
. O

Importantly O
, O
the O
behaviour O
of O
these O
units O
is O
partially O
controlled O
by O
other O
units O
independently O
shown O
to O
track O
syntactic O
structure O
. O

We O
conclude O
that O
LSTMs B-TaskName
are O
, O
to O
some O
extent O
, O
implementing O
genuinely O
syntactic O
processing O
mechanisms O
, O
paving O
the O
way O
to O
a O
more O
general O
understanding O
of O
gram- O
matical O
encoding O
in O
LSTMs B-TaskName
. O

1 O
Introduction O
In O
the O
last O
years O
, O
recurrent O
neural O
networks O
( O
RNNs B-TaskName
) O
, O
and O
particularly O
long O
- O
short O
- O
term O
- O
memory O
( O
LSTM B-TaskName
) O
architectures O
( O
Hochreiter O
and O
Schmidhu O
- O
ber O
, O
1997 O
) O
, O
have O
been O
successfully O
applied O
to O
a O
variety O
of O
NLP B-TaskName
tasks O
. O

This O
has O
spurred O
interest O
in O
whether O
these O
generic O
sequence O
- O
processing O
de- O
vices O
are O
discovering O
genuine O
structural O
properties O
of O
language O
in O
their O
training O
data O
, O
or O
whether O
their O
success O
can O
be O
explained O
by O
opportunistic O
surface- O
pattern O
- O
based O
heuristics O
. O

Until O
now O
, O
this O
debate O
has O
mostly O
relied O
on O
“ O
behavioural O
” O
evidence O
: O
The O
LSTM B-TaskName
had O
been O
treated O
as O
a O
black O
box O
, O
and O
its O
capacities O
had O
been O
indirectly O
inferred O
by O
its O
performance O
on O
linguistic O
tasks O
. O

In O
this O
study O
, O
we O
took O
a O
com- O
plementary O
approach O
inspired O
by O
neuroscience O
: O
We O
thoroughly O
investigated O
the O
inner O
dynamics O
of O
an O
LSTM B-TaskName
language O
model O
performing O
a O
number O
agreement O
task O
, O
striving O
to O
achieve O
a O
mechanis- O
tic O
understanding O
of O
how O
it O
accomplishes O
it O
. O

We O
found O
that O
the O
LSTM B-TaskName
had O
specialized O
two O
“ O
grand- O
mother O
” O
cells O
( O
Bowers O
, O
2009 O
) O
to O
carry O
number O
fea- O
tures O
from O
the O
subject O
to O
the O
verb O
across O
the O
in- O
tervening O
material.1Interestingly O
, O
the O
LSTM B-TaskName
also O
1In O
the O
neuroscientiﬁc O
literature O
, O
“ O
grandmother O
” O
cells O
are O
( O
sets O
of O
) O
neurons O
coding O
for O
speciﬁc O
information O
, O
e.g. O
, O
about O
your O
grandmother O
, O
in O
a O
non O
- O
distributed O
manner O
. O

12possesses O
a O
more O
distributed O
mechanism O
to O
predict O
number O
when O
subject O
and O
verb O
are O
close O
, O
with O
the O
grandmother O
number O
cells O
only O
playing O
a O
crucial O
role O
in O
more O
difﬁcult O
long O
- O
distance O
cases O
. O

Cru- O
cially O
, O
we O
independently O
identiﬁed O
a O
set O
of O
cells O
tracking O
syntactic O
structure O
, O
and O
found O
that O
one O
of O
them O
encodes O
the O
presence O
of O
an O
embedded O
phrase O
separating O
the O
main O
subject O
- O
verb O
depen- O
dency O
, O
and O
has O
strong O
efferent O
connections O
to O
the O
long O
- O
distance O
number O
cells O
, O
suggesting O
that O
the O
network O
relies O
on O
genuine O
syntactic O
information O
to O
regulate O
agreement O
- O
feature O
percolation O
. O

Our O
analysis O
thus O
provides O
direct O
evidence O
for O
the O
claim O
that O
LSTMs B-TaskName
trained O
on O
unannotated O
cor- O
pus O
data O
, O
despite O
lacking O
signiﬁcant O
linguistic O
pri- O
ors O
, O
learn O
to O
perform O
structure O
- O
dependent O
linguis- O
tic O
operations O
. O

In O
turn O
, O
this O
suggests O
that O
raw O
lin- O
guistic O
input O
and O
generic O
memory O
mechanisms O
, O
such O
as O
those O
implemented O
in O
LSTMs B-TaskName
, O
may O
sufﬁce O
to O
trigger O
the O
induction O
of O
non O
- O
trivial O
grammatical O
rules O
. O

2 O
Related O
Work O
Starting O
with O
the O
seminal O
work O
of O
Linzen O
et O
al O
. O

( O
2016 O
) O

, O
a O
long O
- O
distance O
number O
agreement O
task O
has O
emerged O
as O
a O
standard O
way O
to O
probe O
the O
syn- O
tactic O
capabilities O
of O
neural O
language O
models O
. O

In O
the O
number O
agreement O
task O
, O
a O
model O
is O
asked O
to O
predict O
the O
verb O
in O
a O
sentence O
where O
the O
subject O
and O
main O
verb O
are O
separated O
by O
one O
or O
more O
inter- O
vening O
nouns O
( O
“ O
the O
boynear O
the O
carsgreets O
. O
. O
. O
” O
) O
and O
evaluated O
based O
on O
how O
often O
it O
predicts O
the O
right O
verb O
form O
. O

Following O
mixed O
initial O
results O
by O
Linzen O
and O
colleagues O
and O
Bernardy O
and O
Lappin O
( O
2017 O
) O
, O
Gu- O
lordava O
et O
al O
. O

( O
2018 O
) O
and O
Kuncoro O
et O

al O
. O
( O
2018b O
) O
have O
robustly O
established O
that O
LSTM B-TaskName
language O
models O
achieve O
near O
- O
human O
performance O
on O
the O
agreement O
task O
. O

While O
Gulordava O
and O
colleagues O
provided O
some O
evidence O
that O
the O
LSTMs B-TaskName
are O
re- O
lying O
on O
genuine O
syntactic O
generalizations O
, O
Kun- O
coro O
et O
al O
. O
( O
2018a O
) O
and O
Linzen O
and O
Leonard O
( O
2018 O
) O
suggested O
that O
the O
LSTM B-TaskName
achievements O
can O
, O
at O
least O
in O
part O
, O
be O
accounted O
by O
superﬁcial O
heuristics O
( O
e.g. O
, O
“ O
percolate O
the O
number O
of O
the O
ﬁrst O
noun O
in O
a O
sentence O
” O
) O
. O

Other O
recent O
work O
has O
extended O
syn- O
tax O
probing O
to O
other O
phenomena O
such O
as O
negative O
polarity O
items O
and O
island O
constraints O
( O
Chowdhury O
and O
Zamparelli O
, O
2018 O
; O
Jumelet O
and O
Hupkes O
, O
2018 O
; O
Marvin O
and O
Linzen O
, O
2018 O
; O
Wilcox O
et O
al O
. O
, O
2018 O
) O
. O

While O
Linzen O
et O
al O
. O

( O
2016 O
) O
presented O
intrigu O
- O
ing O
qualitative O
data O
showing O
cells O
that O
track O
gram- O
matical O
number O
in O
a O
network O
directly O
trained O
on O
the O
agreement O
task O
, O
most O
of O
the O
following O
work O
focused O
on O
testing O
the O
network O
output O
behaviour O
, O
rather O
than O
on O
understanding O
how O
the O
latter O
fol- O
lows O
from O
the O
inner O
representations O
of O
the O
net- O
work O
. O

Another O
research O
line O
studied O
linguistic O
processing O
in O
neural O
networks O
through O
‘ O
diagnos- O
tic O
classiﬁers O
’ O
, O
that O
is O
, O
classiﬁers O
trained O
to O
predict O
a O
certain O
property O
from O
network O
activations O
( O
e.g. O
, O
Gelderloos O
and O
Chrupała O
, O
2016 O
; O

Adi O
et O
al O
. O
, O
2017 O
; O
Alain O
and O
Bengio O
, O
2017 O
; O
Hupkes O
et O
al O
. O
, O
2018 O
) O
. O

This O
approach O
may O
give O
insight O
into O
which O
infor- O
mation O
is O
encoded O
by O
the O
network O
in O
different O
lay- O
ers O
or O
at O
different O
time O
points O
, O
but O
it O
only O
provides O
indirect O
evidence O
about O
the O
speciﬁc O
mechanics O
of O
linguistic O
processing O
in O
the O
network O
. O

Other O
studies O
are O
closer O
to O
our O
approach O
in O
that O
they O
attempt O
to O
attribute O
function O
to O
spe- O
ciﬁc O
network O
cells O
, O
often O
by O
means O
of O
visual- O
ization O
( O
Karpathy O
et O
al O
. O
, O
2016 O
; O
Li O
et O

al O
. O
, O
2016 O
; O
Tang O
et O
al O
. O
, O
2017 O
) O
. O

Radford O
et O

al O
. O
( O
2017 O
) O
, O
for O
example O
, O
detected O
a O
“ O
sentiment O
” O
grandmother O
cell O
in O
a O
language O
- O
model O
- O
trained O
network O
. O

Ke- O
mentchedjhieva O
and O
Lopez O
( O
2018 O
) O
recently O
found O
a O
character O
- O
level O
RNN B-TaskName
to O
track O
morpheme O
bound- O
aries O
in O
a O
single O
cell O
. O

We O
are O
however O
not O
aware O
of O
others O
studies O
systematically O
characterizing O
the O
processing O
of O
a O
linguistic O
phenomenon O
at O
the O
level O
of O
RNN B-TaskName
cell O
dynamics O
, O
as O
is O
the O
attempt O
in O
the O
study O
hereby O
presented O
. O

3 O
Setup O
Language O
Model O
We O
study O
the O
pretrained O
LSTM B-TaskName
language O
model O
made O
available O
by O
Gu- O
lordava O
et O

al O
. O
( O
2018 O
) O
. O

This O
model O
is O
composed O
of O
a O
650 B-HyperparameterValue
- I-HyperparameterValue
dimensional I-HyperparameterValue
embedding B-HyperparameterName
layer I-HyperparameterName
, O
two O
650 B-HyperparameterValue
- I-HyperparameterValue
dimensional I-HyperparameterValue
hidden B-HyperparameterName
layers I-HyperparameterName
, O
and O
an O
output O
layer O
with O
vocabulary B-HyperparameterName
size I-HyperparameterName
50,000 B-HyperparameterValue
. O

The O
model O
was O
trained O
on O
Wikipedia B-DatasetName
data I-DatasetName
, O
without O
ﬁne O
- O
tuning O
for O
number O
agreement O
, O
and O
obtained O
perplexity O
close O
to O
state O
of O
the O
art O
in O
the O
experiments O
of O
Gulordava O
et O
al.2 O
Number O
- O
Agreement O
Tasks O
We O
complement O
analysis O
of O
the O
naturalistic O
, O
corpus B-DatasetName
- O
derived O
number O
- O
agreement O
test O
set O
of O
Linzen O

et O

al O
. O
( O
2016 O
) O
, O
in O
the O
version O
made O
available O
by O
Gulordava O
et O
al O
. O
( O
2018 O
) O
, O
with O
synthetically O
generated O
data O
- O
sets O
. O

2Key O
ﬁndings O
reported O
below O
were O
also O
replicated O
with O
the O
same O
model O
trained O
with O
different O
initialization O
seeds O
and O
variations O
with O
different O
hyper O
- O
parameters O
. O

13Simple O
theboy O
greets O
the O
guy O
Adv O
theboyprobably O
greets O
the O
guy O
2Adv O
theboymost O
probably O
greets O
the O
guy O
CoAdv B-TaskName
theboyopenly O
and O
deliberately O
greets O
the O
guy O
NamePP O
theboynear O
Pat O
greets O
the O
guy O
NounPP O
theboynear O
the O
car O
greets O
the O
guy O
NounPPAdv O
theboynear O
the O
car O
kindly O
greets O
the O
guy O
Table O
1 O
: O
NA B-TaskName
tasks O
illustrated O
by O
representative O
singular O
sentences O
. O

Each O
synthetic O
number O
- O
agreement O
task O
( O
NA B-TaskName
- O
task O
) O
instantiates O
a O
ﬁxed O
syntactic O
structure O
with O
varied O
lexical O
material O
, O
in O
order O
to O
probe O
subject O
- O
verb O
number O
agreement O
in O
controlled O
and O
increasingly O
challenging O
setups.3The O
different O
structures O
are O
illustrated O
in O
Table O
1 O
, O
where O
all O
forms O
are O
in O
the O
singular O
. O

Distinct O
sentences O
were O
randomly O
generated O
by O
selecting O
words O
from O
pools O
of O
20 O
subject O
/ O
object O
nouns O
, O
15 O
verbs O
, O
10 O
adverbs O
, O
5 O
prepositions O
, O
10 O
proper O
nouns O
and O
10 O
location O
nouns O
. O

The O
items O
were O
selected O
so O
that O
their O
combination O
would O
not O
lead O
to O
semantic O
anoma- O
lies O
. O

For O
each O
NA B-TaskName
- O
task O
, O
we O
generated O
singular O
and O
plural O
versions O
of O
each O
sentence O
. O

We O
refer O
to O
each O
such O
version O
as O
a O
condition O
. O

For O
NA- O
tasks O
that O
have O
other O
nouns O
occurring O
between O
subject O
and O
main O
verb O
, O
we O
also O
systematically O
vary O
their O
number O
, O
resulting O
in O
two O
congruent O
and O
two O
incongruent O
conditions O
. O

For O
example O
, O
the O
NounPP O
sentence O
in O
the O
table O
illustrates O
the O
congruent O
SS O
( O
singular O
- O
singular O
) O
condition O
and O
the O
corresponding O
sentence O
in O
the O
incongruent O
PS O
( O
plural O
- O
singular O
) O
condition O
is O
: O
“ O
the O
boys O
near O
thecar O
greet O
the O
guy O
” O
. O

For O
all O
NA B-TaskName
- O
tasks O
, O
each O
condition O
consisted O
of O
600 O
sentences O
Syntactic O
Depth O
Data O
- O
Set O
We O
probed O
the O
im- O
plicit O
syntax O
- O
parsing O
abilities O
of O
the O
model O
by O
test- O

ing O
whether O
its O
representations O
predict O
the O
syn- O
tactic O
depth O
of O
the O
words O
they O
process O
. O

Follow- O
ing O
Nelson O
et O
al O
. O

( O
2017 O
) O
, O
this O
was O
operational- O
ized O
as O
predicting O
the O
number O
of O
open O
syntactic O
nodes O
at O
each O
word O
, O
given O
the O
canonical O
syntac- O
tic O
parse O
of O
a O
sentence O
. O

We O
generated O
a O
data O
- O
set O
of O
sentences O
with O
unambiguous O
but O
varied O
syntac- O
tic O
structures O
and O
annotated O
them O
with O
the O
number O
of O
open O
nodes O
at O
each O
word O
. O

For O
example O
: O
“ O
Ten O
1 O
really O
2ecstatic O
3cousins O
3of4four O
5teachers O
6are2 O
quickly O
3laughing O
4 O
” O
, O
where O
indexes O
show O
the O
cor- O
3We O
exclude O
, O
for O
the O
time O
being O
, O
agreement O
across O
a O
rel- O
ative O
clause O
, O
as O
it O
comes O
with O
the O
further O
complication O
of O
ac- O
counting O
for O
the O
extra O
agreement O
process O
taking O
place O
inside O
the O
relative O
clause.responding O
number O
of O
open O
nodes O
. O

Since O
syntactic O
depth O
is O
naturally O
correlated O
with O
the O
position O
of O
a O
word O
in O
a O
sentence O
, O
we O
used O
a O
data O
- O
point O
sampling O
strategy O
to O
de O
- O
correlate O
these O
factors O
. O

For O
each O
length O
between O
2 O
and O
25 O
words O
, O
we O
randomly O
gen- O
erated O
300 O
sentences O
. O

From O
this O
set O
, O
we O
randomly O
picked O
examples O
uniformly O
covering O
all O
possible O
position O
- O
depth O
combinations O
within O
the O
7 O
- O
12 O
posi- O
tion O
and O
3 O
- O
8 O
depth O
ranges O
. O

The O
ﬁnal O
data O
- O
set O
con- O
tains O
4,033 O
positions O
from O
1,303 O
sentences.4 O
4 O
Experiments O
To O
successfully O
perform O
the O
NA B-TaskName
- O
task O
, O
the O
LSTM B-TaskName
should O
: O
( O
1 O
) O
encode O
and O
store O
the O
grammatical O
number O
of O
the O
subject O
; O
and O
( O
2 O
) O
track O
the O
main O
subject O
- O
verb O
syntactic O
dependency O
. O

The O
latter O
in- O
formation O
is O
important O
for O
identifying O
the O
time O
period O
during O
which O
subject O
number O
should O
be O
stored O
, O
output O
and O
then O
updated O
by O
the O
network O
. O

This O
section O
describes O
the O
‘ O
neural O
circuit O
’ O
that O
en- O
codes O
and O
processes O
this O
information O
in O
the O
LSTM B-TaskName
. O

4.1 O
Long O
- O
Range O
Number O
Units O
We O
ﬁrst O
tested O
the O
performance O
of O
the O
LSTM B-TaskName
on O
the O
Linzen O
’s O
data O
and O
on O
the O
NA B-TaskName
- O
tasks O
in O
Table O
1 O
. O

Following O
Linzen O
et O
al O
. O

( O
2016 O
) O
and O
later O
work O
, O
we O
computed O
the O
likelihood O
that O
the O
LSTM B-TaskName
assigns O
to O
the O
main O
verb O
of O
each O
sentence O
given O
the O
preced- O
ing O
context O
and O
compared O
it O
to O
the O
likelihood O
it O
as- O
signs O
to O
the O
wrong O
verb O
inﬂection O
. O

Accuracy O
in O
a O
given O
condition O
was O
measured O
as O
the O
proportion O
of O
sentences O
in O
this O
condition O
for O
which O
the O
model O
as- O
signed O
a O
higher O
likelihood O
to O
the O
correct O
verb O
form O
than O
to O
the O
wrong O
one O
. O

Network O
performance O
is O
reported O
in O
Table O
2 O
( O
right O
column O
– O
‘ O
Full O
’ O
) O
. O

We O
ﬁrst O
note O
that O
our O
results O
on O
the O
Linzen O
NA B-TaskName
- O
task O
conﬁrm O
those O
re- O
ported O
in O
Gulordava O
et O
al O
. O
( O
2018 O
) O
. O

For O
the O
other O
NA B-TaskName
- O
tasks O
, O
results O
show O
that O
some O
tasks O
and O
condi- O
tions O
are O
more O
difﬁcult O
than O
others O
. O

For O
example O
, O
performance O
on O
the O
Simple O
( O
0 O
- O
distance O
) O

NA B-TaskName
- O
task O
is O
better O
than O
that O
on O
the O
Co O
- O
Adv O
NA B-TaskName
- O
task O
, O
which O
in O
turn O
is O
better O
than O
that O
of O
the O
nounPP B-TaskName
tasks O
. O

Second O
, O
as O
expected O
, O
incongruent O
conditions O
( O
the O
number O
- O
mismatch O
conditions O
of O
namePP B-TaskName
, O
nounPP B-TaskName
and O
nounPPAdv B-TaskName
) O
reduce O
network O
performance O
. O

4All O
our O
data O
- O
sets O
are O
available O
at O
: O
https O
: O
//github.com O
/ O
FAIRNS O
/ O
Number_and_syntax O
_ O
units_in_LSTM_LMs O
. O

14NA B-TaskName
task O
CAblatedFull776 O
988 O
Simple O
S O
- O
- O
100 O
Adv O
S O
- O
- O
100 O
2Adv O
S O
- O
- O
99.9 O
CoAdv B-TaskName
S O
- O
82 O
98.7 O
namePP B-TaskName
SS O
- O
- O
99.3 O
nounPP B-TaskName
SS O
- O
- O
99.2 O
nounPP B-TaskName
SP O
- O
54.2 O
87.2 O
nounPPAdv B-TaskName
SS O
- O
- O
99.5 O
nounPPAdv B-TaskName
SP O
- O
54.0 O
91.2 O
Simple O
P O
- O
- O
100 O
Adv O
P O
- O
- O
99.6 O
2Adv O
P O
- O
- O
99.3 O
CoAdv B-TaskName
P O
79.2 O
- O
99.3 O
namePP B-TaskName
PS O
39.9 O
- O
68.9 O
nounPP B-TaskName
PS O
48.0 O
- O
92.0 O
nounPP B-TaskName
PP O
78.3 O
- O
99.0 O
nounPPAdv B-TaskName
PS O
63.7 O
- O
99.2 O
nounPPAdv B-TaskName
PP O
- O
- O
99.8 O
Linzen O
- O
75.3 O
- O
93.9 O
Table O
2 O
: O
Ablation O
- O
experiments O
results O
: O

Percentage O
ac- O
curacy O
in O
all O
NA B-TaskName
- O
tasks O
. O

Full O
: O
non O
- O
ablated O
model O
, O
C O
: O
condition O
, O
S O
: O
singular O
, O
P O
: O
plural O
. O

Pink O
( O
dark O
lines O
in O
B&W O
printing O
): O
plural O
subject O
, O
Light O
blue O
: O
singular O
subject O
. O

Performance O
reduction O
less O
than O
10 O
% O
is O
de- O
noted O
by O
‘ O
- O
’ O
. O

Third O
, O
for O
long O
- O
range O
dependencies O
, O
reliably O
en- O
coding O
singular O
subject O
across O
an O
interfering O
noun O
is O
more O
difﬁcult O
than O
a O
plural O
subject O
: O
for O
both O
nounPP B-TaskName
and O
nounPPAdv B-TaskName
, O
PS O
is O
easier O
than O
SP O
. O

A O
possible O
explanation O
for O
this O
ﬁnding O
is O
that O
in O
En- O
glish O
the O
plural O
form O
is O
almost O
always O
more O
fre- O
quent O
than O
the O
singular O
one O
, O
as O
the O
latter O
only O
marks O
third O
person O
singular O
, O
whereas O
the O
former O
is O
identical O
to O
the O
inﬁnitive O
and O
other O
forms O
. O

Thus O
, O
if O
the O
network O
reverts O
to O
unigram O
probabilities O
, O
it O
will O
tend O
to O
prefer O
the O
plural O
. O

Looking O
for O
Number O
Units O
Through O
Ablation O
Number O
information O
may O
be O
stored O
in O
the O
network O
in O
either O
a O
local O
, O
sparse O
, O
or O
a O
distributed O
way O
, O
de- O
pending O
on O
the O
fraction O
of O
active O
units O
that O
carry O
it O
. O

We O
hypothesized O
that O
if O
the O
network O
uses O
a O
local O
or O
sparse O
coding O
, O
meaning O
that O
there O
’s O
a O
small O
set O
of O
units O
that O
encode O
number O
information O
, O

then O
ablat- O
ing O
these O
units O
would O
lead O
to O
a O
drastic O
decrease O
in O
performance O
in O
the O
NA B-TaskName
- O
tasks O
. O

To O
test O
this O
, O
we O
ab- O
lated O
each O
unit O
of O
the O
network O
, O
one O
at O
a O
time O
, O
by O
ﬁxing O
its O
activation O
to O
zero O
, O
and O
tested O
on O
the O
NA B-TaskName
- O
tasks O
. O

Two O
units O
were O
found O
to O
have O
exceptional O
ef- O
fect O
on O
network O
performance O
( O
Table O
2 O
, O
776 O
and O
988 O
columns).5Ablating O
them O
reduced O
network O
performance O
by O
more O
than O
10 B-MetricValue
% I-MetricValue
across O
various O
conditions O
, O
and O
, O
importantly O
, O
they O
were O
the O
only O
units O
whose O
ablation O
consistently O
brought O
network O
performance O
to O
around O
chance O
level O
in O
the O
more O
difﬁcult O
incongruent O
conditions O
of O
the O
namePP B-TaskName
, O
nounPP B-TaskName
and O
nounPPAdv B-TaskName
tasks O
. O

Moreover O
, O
the O
ablation O
effect O
depended O
on O
the O
grammatical O
number O
of O
the O
subject O
: O
ablating O
776 O
signiﬁcantly O
reduced O
network O
performance O
only O
if O
the O
subject O
was O
plural O
( O
P O
, O
PS O
or O
PP O
condi- O
tions O
) O
and O
988 O
only O
if O
the O
subject O
was O
singular O
( O
S O
, O
SP O
or O
SS O
conditions O
) O
. O

In O
what O
follows O
, O
we O
will O
therefore O
refer O
to O
these O
units O
as O
the O
‘ O
plural O
’ O
and O
‘ O
singular O
’ O
units O
, O
respectively O
, O
or O
long O
- O
range O
( O
LR O
) O
number O
units O
when O
referring O
to O
both O
. O

Finally O
, O
we O
note O
that O
although O
the O
Linzen O
NA B-TaskName
- O
task O
contained O
mixed O
stimuli O
from O
many O
types O
of O
conditions O
, O
the O
plural O
unit O
was O
found O
to O
have O
a O
substantial O
effect O
on O
average O
on O
network O
performance O
. O

The O
singu- O
lar O
unit O
did O
n’t O
show O
a O
similar O
effect O
in O
this O
case O
, O
which O
highlights O
the O
importance O
of O
using O
carefully O
crafted O
stimuli O
, O
as O
in O
the O
nounPP B-TaskName
and O
nounPPAdv B-TaskName
tasks O
, O
for O
understanding O
network O
dynamics O
. O

Taken O
together O
, O
these O
results O
suggest O
a O
highly O
local O
cod- O
ing O
scheme O
of O
grammatical O
number O
when O
process- O
ing O
long O
- O
range O
dependencies O
. O

Visualizing O
Gate O
and O
Cell O
- O
State O
Dynamics O
To O
understand O
the O
functioning O
of O
the O
number O
units O
, O
we O
now O
look O
into O
their O
gate O
and O
state O
dynam- O
ics O
during O
sentence O
processing O
. O

We O
focus O
on O
the O
nounPP B-TaskName
NA B-TaskName
- O
task O
, O
which O
is O
the O
simplest O
NA B-TaskName
- O
task O
that O
includes O
a O
long O
- O
range O
dependency O
with O
an O
in- O
terfering O
noun O
, O
in O
both O
SP O
and O
PS O
conditions O
. O

Recall O
the O
standard O
LSTM B-TaskName
memory O
update O
and O
output O
rules O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
): O

Ct O
= O
ftCt 1+iteCt O
( O
1 O
) O
ht O
= O
ottanh O
( O
Ct O
) O
; O
( O
2 O
) O
where O
ft O
; O
it O
; O
ot2(0;1)are O
gating O
scalars O
com- O
puted O
by O
the O
network O
, O
and O
eCt2( 1;1)is O
an O
up- O
date O
candidate O
for O
cell O
value O
. O

Consider O
now O
how O
a O
number O
unit O
may O
reliably O
encode O
and O
store O
subject O
number O
across O
interfer- O
ing O
nouns O
. O

Figure O
1c O
exempliﬁes O
this O
for O
a O
singular O
5Units O
1 O
- O
650 O
belong O
to O
the O
ﬁrst O
layer O
, O
651 O
- O
1300 O
to O
the O
sec- O
ond O
. O

All O
units O
detected O
by O
our O
analyses O
come O
from O
the O
latter O
. O

15 O
−1.51.5 O
˜Ct O
01 O
it O
01 O
ft O
−1.51.5 O
Ct O
The O
boy(s O
) O
near O
the O
car(s O
) O
greet(s O
) O
the01 O
ot(a O
) O
988 O
( O
singular O
) O
−1.51.5 O
˜Ct O
01 O
it O
01 O

ft O
−1.51.5 O
Ct O
The O
boy(s O
) O
near O
the O
car(s O
) O
greet(s O
) O

the01 O
ot O
( O
b O
) O
776 O
( O
plural O
) O
−1.51.5˜Ct O
01it O
01 O
ft O
−1.51.5Ct O

The O
boy(s O
) O
near O
the O
car(s O
) O
greet(s O
) O
the01ot O
( O
c O
) O
Prediction O
( O
singular O
) O
( O
d O
) O
Efferent O
weights O
of O
the O
LR O
- O
units O
( O
776 O
and O
988 O
) O
, O
the O
syntax O
unit O
( O
1150 O
; O
section O
4.3 O
) O
and O
two O
arbitrary O
units O
( O
651 O
and O
1300 O
) O
. O

Figure O
1 O
: O
( O
a O
) O
to O
( O
c O
) O
– O
Cell O
and O
gate O
activations O
during O
processing O
of O
sentences O
with O
a O
prepositional O
phrase O
between O
subject O
and O
verb O
. O

Values O
in O
( O
a O
) O
and O
( O
b O
) O
are O
averaged O
across O
all O
condition O
sentences O
, O
with O
error O
bars O
showing O
standard O
deviations O
. O

( O
d O
) O
– O
Efferent O
weights O
of O
speciﬁc O
units O
at O
the O
output O
layer O
to O
singular O
and O
plural O
verb O
forms O
. O

unit O
, O
showing O
the O
desired O
gate O
and O
cell O
dynamics O
. O

The O
four O
conditions O
are O
represented O
with O
separated O
curves O
- O
pink O
for O
plural O
subject O
, O
light O
blue O
for O
sin- O
gular O
, O
and O
dashed O
lines O
for O
incongruent O
conditions O
. O

Gate O
and O
cell O
activity O
at O
time O
points O
unrelated O
to O
solving O
the O
NA B-TaskName
- O
task O
are O
masked O
with O
white O
, O
as O
we O
do O
not O
make O
precise O
predictions O
for O
them O
. O

The O
update O
rule O
of O
the O
LSTM B-TaskName
cell O
has O
two O
terms O
( O
Eq O
. O
1).6In O
the O
ﬁrst O
, O
ftCt 1 O
, O
the O
forget O
gate O
controls O
whether O
to O
keep O
the O
previous O
cell O
content O
( O
ft= O
1 O
: O
perfect O
remembering O
) O
or O
forget O
it O
( O
ft= O
0 O
: O
complete O
forgetting O
) O
. O

In O
the O
second O
, O
it~Ct O
, O
the O
6We O
abuse O
notation O
here O
, O
using O
the O
symbols O
denoting O
whole O
layers O
in O
equations O
( O
1 O
) O
and O
( O
2 O
) O
to O
denote O
the O
compo- O
nents O
of O
single O
cells.input O
gate O
controls O
whether O
the O
information O
cur- O
rently O
presented O
to O
the O
network O
, O
as O
encoded O
by O
~Ct O
, O
should O
be O
written O
onto O
the O
cell O
( O
it= O
1 O
: O
full O
ac- O
cess O
) O
or O
not O
( O
it= O
0 O
) O
. O

The O
singular O
unit O
can O
thus O
use O
these O
gates O
to O
reliably O
store O
number O
informa- O
tion O
across O
long O
- O
range O
dependencies O
. O

Speciﬁcally O
, O
the O
unit O
can O
( O
enumeration O
follows O
the O
same O
or- O
der O
as O
the O
panels O
in O
Figure O
1c O
): O
( O
1 O
) O
encode O
sub- O
ject O
number O
via O
~Ctsubject O
with O
different O
values O
for O
singular O
and O
plural O
; O
( O
2 O
) O
open O
the O
input O
gate O
only O
when O
a O
singular O
subject O
is O
presented O
( O
itsubject O
= O
1 O
in O
light O
- O
blue O
curves O
only O
) O
and O
protect O
it O
from O
in- O
terfering O
nouns O
( O
it= O
0 O
; O
tsubject O
< O
t O
< O
t O
verb O
) O
; O
( O
3 O
) O
at O
the O
same O
time O
, O
clear O
the O
cell O
from O
previ- O
ously O
stored O
information O
( O
ftsubject O
= O
0 O
) O
and O
then O

16store O
subject O
number O
across O
the O
entire O
dependency O
( O
ft= O
1 O
; O
tsubject O
< O
t O
< O
t O
verb O
) O
; O
( O
4 O
) O
this O
will O
result O
in O
stable O
encoding O
of O
subject O
number O
in O
the O
cell O
Ct O
throughout O
the O
dependency O
; O
( O
5 O
) O
ﬁnally O
, O
output O
sub- O
ject O
number O
at O
the O
right O
moment O
, O
when O
predicting O
the O
verb O
form O
( O
otverb 1= O
1 O
) O
( O
Eq O
. O
2 O
) O
. O

Figures O
1a O
and O
1b O
present O
the O
actual O
gate O
and O
cell O
dynamics O
of O
the O
singular O
and O
plural O
units O
. O

Both O
units O
follow O
the O
general O
solution O
for O
reliable O
number O
storage O
described O
above O
. O

Note O
that O
for O
~Ctandit O
, O
and O
as O
a O
result O
also O
for O
Ct O
, O
the O
plural O
unit O
‘ O
mirrors O
’ O
the O
singular O
unit O
with O
respect O
to O
sub- O
ject O
number O
( O
pink O
curves O
of O
PP O
and O
PS O
vs. O
Light O
blue O
curves O
of O
SS O
and O
SP O
) O
. O

This O
is O
in O
accordance O
with O
the O
results O
of O
the O
ablation O
experiments O
, O
which O
showed O
that O
ablating O
these O
units O
had O
an O
effect O
that O
depended O
on O
the O
grammatical O
number O
of O
the O
sub- O
ject O
( O
Table O
2 O
) O
. O

This O
provides O
complementary O
sup- O
port O
for O
the O
identiﬁcation O
of O
these O
units O
as O
‘ O
singu- O
lar O
’ O
and O
‘ O
plural O
’ O
. O

A O
single O
divergence O
between O
the O
solution O
de- O
picted O
in O
Figure O
1c O
and O
the O
actual O
dynamics O
of O
the O
number O
units O
is O
that O
input O
gate O
activity O
is O
smaller O
, O
but O
not O
zero O
, O
at O
the O
time O
step O
immediately O
fol- O
lowing O
the O
subject O
. O

One O
speculative O
explanation O
is O
that O
this O
might O
be O
useful O
to O
process O
compound O
nouns O
. O

In O
these O
cases O
, O
subject O
number O
information O
is O
stored O
with O
the O
second O
noun O
, O
whereas O
in O
the O
case O
of O
simple O
nouns O
there O
is O
no O
‘ O
risk O
’ O
of O
encountering O
an O
interfering O
noun O
immediately O
after O
the O
subject O
, O
making O
the O
delay O
in O
closing O
the O
gate O
safe O
. O

The O
singular O
and O
plural O
units O
had O
emerged O
at O
the O
second O
layer O
of O
the O
network O
. O

This O
seems O
appropri- O
ate O
since O
number O
information O
needs O
to O
be O
directly O
projected O
to O
the O
output O
layer O
for O
correct O
verb O
- O
form O
prediction O
. O

Moreover O
, O
number O
- O
unit O
output O
should O
be O
projected O
differently O
to O
singular O
and O
plural O
verb O
forms O
in O
the O
output O
layer O
, O
only O
increasing O
activ- O
ity O
in O
output O
units O
representing O
the O
suitable O
form O
. O

For O
example O
, O
for O
the O
singular O
unit O
, O
since O
singu- O
lar O
subjects O
are O
encoded O
with O
a O
negative O
value O
( O
Ctverb 1< 1 O
in O
ﬁgure O
1a O
) O
, O
the O
more O
negative O
its O
efferent O
weights O
to O
singular O
verb O
forms O
in O
the O
output O
layer O
, O
the O
higher O
the O
probabilities O
of O
these O
verb O
forms O
would O
be O
. O

Figure O
1d O
shows O
the O
effer- O
ent O
weights O
of O
the O
LR O
- O
number O
units O
to O
all O
verbs O
in O
our O
data O
- O
sets O
. O

We O
found O
that O
, O
indeed O
, O
the O
effer- O
ent O
weights O
to O
the O
singular O
and O
plural O
verb O
forms O
are O
segregated O
from O
each O
other O
, O
with O
weight O
signs O
that O
correspond O
to O
the O
negative O
encoding O
of O
sub- O
ject O
number O
used O
by O
both O
singular O
and O
plural O
units O
. O

Figure O
2 O
: O
Generalization O
across O
time O
of O
subject O
- O
number O
prediction O
. O

Error O
bars O
represent O
standard O
deviations O
across O
cross O
- O
validation O
splits O
. O

Two O
other O
arbitrary O
units O
, O
651 O
and O
1300 O
, O
and O
the O
syntax O
unit O
1150 O
to O
be O
described O
below O
( O
Section O
4.3 O
) O
do O
not O
have O
segregated O
efferent O
weights O
to O
verb O
forms O
, O
as O
expected O
. O

4.2 O
Short O
- O
Range O
Number O
Information O
Performance O
on O
the O
easier O
NA B-TaskName
- O
tasks O
( O
Simple O
, O
Adv O
, O
2Adv O
) O
was O
not O
impaired O
by O
single O
- O
unit O
ablations O
. O

This O
suggests O
that O
number O
may O
be O
encoded O
also O
elsewhere O
in O
the O
network O
, O
perhaps O
via O
a O
more O
dis- O
tributed O
code O
. O

To O
verify O
this O
, O
we O
tested O
whether O
subject O
number O
can O
be O
decoded O
from O
the O
whole O
pattern O
of O
activities O
in O
the O
network O
( O
excluding O
the O
two O
LR O
- O
number O
units O
) O
and O
whether O
this O
decoding O
is O
stable O
across O
time O
( O
see O
Giulianelli O
et O
al O
. O
, O
2018 O
, O
for O
similar O
observations O
and O
related O
methods O
) O
. O

We O
expected O
this O
distributed O
activity O
to O
track O
number O
in O
a O
small O
time O
window O
after O
the O
subject O
, O
but O
, O
un- O
like O
the O
LR O
- O
number O
units O
, O
to O
be O
affected O
by O
incon- O
gruent O
intervening O
nouns O
. O

We O
trained O
a O
linear O
model O
to O
predict O
the O
gram- O
matical O
number O
of O
the O
subject O
from O
network O
activ- O
ity O
in O
response O
to O
the O
presentation O
of O
the O
subject O
, O
and O
tested O
its O
prediction O
on O
test O
sets O
from O
all O
time O
points O
( O
King O
and O
Dehaene O
, O
2014 O
) O
, O
in O
incongruent O
conditions O
only O
of O
the O
nounPP B-TaskName
task O
. O

We O
used O
Area B-MetricName
under I-MetricName
of I-MetricName
Curve I-MetricName
( O
AUC B-MetricName
) O
to O
evaluate O
model O
perfor- O
mance O
. O

Figure O
2 O
shows O
decoding O
across O
time O
of O
subject O
number O
from O
cell O
activity O
of O
each O
number O
unit O
separately O
and O
from O
cell O
activity O
of O
the O
entire O
network O
without O
these O
two O
units O
( O
‘ O
Full O
model O
mi- O
nus O
LR O
- O
units O
’ O
) O
. O

Results O
show O
that O
number O
infor- O
mation O
can O
be O
efﬁciently O
decoded O
from O
other O
units O
in O
the O
network O
, O
and O
that O
this O
information O
can O
be O
carried O
for O
several O
time O
steps O
( O
relatively O
high O
AUC B-MetricName
up O
to O
the O
second O
determiner O
) O
. O

However O
, O
the O
way O
in O
which O
these O
units O
encode O
number O
is O
sensitive O
to O
the O
last O
encountered O
noun O
, O
with O
AUC B-MetricName
decreasing O

17 O
( O
a O
) O
2Adv O
  O
( O
b O
) O
nounPP B-TaskName
  O
( O
c O
) O
subject O
relative O
( O
d O
) O
Two O
embeddings O
with O
subject O
relatives O
Figure O
3 O
: O
Cell O
activity O
of O
syntax O
unit O
1150 O
while O
processing O
various O
syntactic O
structures O
. O

Values O
averaged O
across O
all O
stimuli O
in O
an O
NA B-TaskName
- O
task O
, O
with O
error O
bars O
representing O
standard O
deviations O
. O

Relative O
clause O
NA B-TaskName
- O
task O
stimuli O
were O
speciﬁcally O
generated O
for O
this O
visualization O
. O

to O
zero O
around O
the O
second O
noun O
( O
‘ O
cars O
’ O
) O
, O
whereas O
test O
performance O
of O
the O
models O
trained O
on O
cell O
ac- O
tivity O
of O
the O
LR O
- O
number O
units O
is O
consistently O
high O
. O

This O
conﬁrms O
that O
number O
prediction O
is O
supported O
both O
by O
the O
LR O
- O
number O
units O
, O
and O
by O
distributed O
activation O
patterns O
of O
other O
short O
- O
range O
( O
SR O
) O
num- O
ber O
units O
. O

The O
latter O
, O
however O
, O
are O
not O
syntax- O
sensitive O
, O
and O
simply O
encode O
the O
number O
of O
the O
last O
noun O
encountered O
. O

A O
full O
description O
of O
the O
SR O
- O
number O
units O
is O
be- O
yond O
our O
scope O
. O

However O
, O
we O
note O
that O
10 O
SR- O
number O
units O
in O
the O
second O
layer O
of O
the O
network O
were O
identiﬁed O
, O
which O
had O
efferent O
weights O
with O
a O
similar O
segregated O
structure O
as O
that O
of O
the O
LR O
units O
( O
Figure O
1d O
) O
. O

These O
units O
were O
indeed O
sen- O
sitive O
to O
the O
last O
encountered O
noun O
: O
subject O
num- O
ber O
could O
be O
decoded O
from O
single O
- O
unit O
cell O
activ- O
ity O
during O
its O
presentation O
( O
AUC B-MetricName
> O
0:9 O
) O
, O
but O
ac- O
tivity O
‘ O
swaps O
’ O
once O
an O
interfering O
noun O
appears O
( O
i.e. O
, O
AUC B-MetricName
decreases O
to O
zero O
in O
a O
generalization- O
across O
- O
time O
analysis O
) O
. O

Finally O
, O
to O
validate O
the O
role O
of O
SR O
- O
number O
units O
in O
encoding O
number O
for O
eas- O
ier O
NA B-TaskName
- O
tasks O
, O
we O
ablated O
both O
SR O
and O
LR O
number O
units O
( O
12 O
in O
total O
) O
or O
SR O
units O
only O
( O
10 O
in O
total O
) O
and O
evaluated O
network O
performance O
on O
these O
NA- O
tasks O
. O

Both O
experiments O
resulted O
in O
a O
signiﬁcant O
reduction O
in O
task O
performance O
compared O
to O
1,000 O
random O
equi O
- O
size O
ablations O
( O
p O
< O
0:01 O
in O
all O
‘ O
eas- O
ier O
’ O
tasks O
) O
. O

Intriguingly O
, O
we O
observed O
qualitatively O
that O
LR O
units O
are O
almost O
always O
making O
the O
right O
predic- O
tion O
, O
even O
when O
the O
network O
predicts O
the O
wrong O
number O
. O

The O
wrong O
outcome O
, O
in O
such O
cases O
, O
might O
be O
due O
to O
interference O
from O
the O
syntax O
- O
insensitive O
SR O
units O
. O

We O
leave O
the O
study O
of O
LR O
- O
SR O
unit O
inter- O
play O
to O
future O
work.4.3 O
Syntax O
Units O
We O
saw O
how O
the O
input O
and O
forget O
gates O
of O
the O
LR- O
number O
units O
control O
the O
ﬂow O
of O
subject O
- O
number O
information O
. O

It O
remains O
unclear O
, O
however O
, O
how O
the O
dynamics O
of O
these O
gates O
are O
controlled O
by O
the O
net- O
work O
. O

We O
hypothesized O
that O
other O
units O
in O
the O
net- O
work O
may O
encode O
information O
about O
the O
syntac- O
tic O
structure O
of O
the O
sentence O
, O
and O
thus O
about O
the O
subject O
- O
verb O
dependency O
. O

These O
units O
could O
then O
control O
and O
coordinate O
the O
opening O
and O
closing O
of O
the O
input O
and O
forget O
gates O
of O
the O
number O
units O
. O

To O
identify O
such O
’ O
syntax O
’ O
units O
, O
we O
tested O
from O
which O
units O
syntactic O
information O
can O
be O
efﬁ- O
ciently O
decoded O
. O

We O
used O
depth O
of O
the O
syntac- O
tic O
tree O
as O
a O
proxy O
for O
syntactic O
structure O
( O
Nel- O
son O
et O
al O
. O
, O
2017 O
) O
and O
trained O
an O
L2 O
- O
regularized O
regression O
model O
to O
predict O
syntactic O
tree O
- O
depth O
from O
the O
hidden O
- O
state O
activity O
of O
all O
units O
. O

In O
all O
experiments O
, O
we O
used O
the O
data O
presented O
in O
Sec- O
tion O
3 O
above O
and O
performed O
a O
nested O
5 O
- O
fold O
cross- O
validation O
procedure O
. O

Word O
frequency O
, O
which O
was O
added O
as O
a O
covariate O
to O
the O
model O
, O
had O
a O
negligi- O
ble O
effect O
on O
the O
results O
. O

Syntactic O
tree O
- O
depth O
was O
found O
to O
be O
efﬁciently O
decodable O
from O
network O
activity O
( O
R2 O
test set= O
0:850:009 O
; O
covariate- O
corrected O
) O
. O

A O
small O
subset O
of O
‘ O
syntax O
’ O
units O
had O
relatively O
high O
weights O
in O
the O
regression O
model O
( O
mean O
weight O
= O
7:610 4 O
, O
SD= O
7:8610 2 O
; O
cut- O
off O
for O
outlier O
weights O
was O
set O
to O
three O
SDs O
) O
. O

Since O
the O
interpretation O
of O
the O
regression O
weights O
may O
depend O
on O
possible O
correlations O
among O
the O
fea- O
tures O
, O
we O
also O
tested O
the O
causal O
effect O
of O
these O
units O
on O
NA B-TaskName
- O
task O
performance O
. O

Ablating O
the O
syntax O
units O
together O
resulted O
in O
signiﬁcant O
performance O
reduction O
in O
NA B-TaskName
- O
tasks O
that O
have O
an O
interfering O
noun O
: O

Linzen O
NA B-TaskName
- O
task O
: O
p= O
0:024 O
, O
nounPPAdv- O

18 O
( O
a O
) O
Input O
gate O
  O
( O
b O
) O
Forget O
gate O
Figure O
4 O
: O
Connectivity O
among O
the O
syntax O
unit O
1150 O
and O
LR O
- O
number O
units O
776 O
and O
988 O
. O

Projecting O
units O
are O
on O
the O
table O
rows O
. O

Blue O
background O
highlights O
outlier O
values O
( O
jz scorej>3 O
) O
. O

Weights O
from O
the O
syntax O
unit O
are O
marked O
with O
large O
diamond O
markers O
and O
are O
explicitly O
labeled O
in O
the O
plots O
. O

SP O
: O
p= O
0:011 O
, O
nounPPAdv B-TaskName
- O
PS O
: O
p= O
0:034 O
, O
nounPP B-TaskName
- O
SP O
: O
p O
< O
0:001and O
marginally O
signiﬁcant O
in O
nounPP B-TaskName
- O
PS O
: O
p= O
0:052(compared O
to O
1000 O
ran- O
dom O
ablations O
of O
subsets O
of O
units O
of O
the O
same O
size O
) O
. O

To O
gain O
further O
insight O
regarding O
the O
functioning O
of O
the O
syntax O
units O
, O
we O
next O
visualized O
their O
gate O
and O
cell O
dynamics O
during O
sentence O
processing O
. O

We O
found O
that O
cell O
activity O
of O
unit O
1150 O
, O
which O
also O
had O
one O
of O
the O
highest O
weights O
in O
the O
regression O
model O
, O
was O
remarkably O
structured O
. O

The O
activity O
of O
this O
unit O
increases O
across O
the O
entire O
subject- O
verb O
dependency O
and O
drops O
abruptly O
right O
after O
. O

Figures O
3a O
and O
3b O
show O
cell O
activity O
of O
this O
unit O
during O
the O
processing O
of O
stimuli O
from O
the O
2Adv O
and O
nounPP B-TaskName
tasks O
. O

We O
found O
the O
same O
dynamics O
in O
cases O
where O
another O
verb O
occurs O
between O
sub- O
ject O
and O
main O
verb O
, O
as O
in O
subject O
relatives O
( O
Figure O
3c O
) O
, O
and O
in O
exceptionally O
long O
- O
distance O
dependen- O
cies O
with O
two O
interfering O
nouns O
and O
verbs O
( O
Figure O
3d O
) O
. O

Taken O
together O
, O
these O
results O
suggest O
that O
unit O
1150 O
consistently O
encodes O
subject O
- O
verb O
dependen- O
cies O
in O
a O
syntax O
- O
sensitive O
manner O
. O

Other O
syntax O
units O
did O
not O
show O
an O
easily O
interpretable O
dynam- O
ics O
and O
had O
no O
clear O
interactions O
with O
the O
number O
units O
in O
the O
analysis O
discussed O
next O
. O

This O
suggests O
that O
they O
perform O
different O
syntactic O
, O
or O
possibly O
other O
, O
functions O
. O

4.4 O
Syntax O
- O
Number O
Units O
Connections O
We O
ﬁnally O
look O
at O
the O
connections O
that O
were O
learned O
by O
the O
LSTM B-TaskName
between O
syntax O
unit O
1150 O
, O
which O
appears O
to O
be O
more O
closely O
involved O
in O
tracking O
subject O
- O
verb O
agreement O
, O
and O
the O
LR O
num- O
ber O
units O
, O
as O
well O
as O
at O
the O
connections O
between O
the O
LR O
- O
number O
units O
themselves O
. O

For O
each O
unit O
pair O
, O
there O
are O
4 O
connection O
types O
, O
one O
for O
each O
com- O
ponent O
of O
the O
target O
cell O
( O
to O
the O
3 O
gates O
and O
to O
the O
update O
candidate O
) O
. O

We O
focus O
on O
input O
and O
forget O
gates O
, O
as O
they O
control O
the O
ﬂow O
and O
storage O
of O
num O
- O
ber O
information O
. O

Figures O
4a O
and O
4b O
show O
the O
distributions O
of O
all O
afferent O
recurrent O
weights O
to O
the O
input O
and O
forget O
gates O
of O
the O
LR O
- O
number O
units O
, O
scaled O
by O
the O
maxi- O
mal O
activity O
htof O
the O
pre O
- O
synaptic O
units O
during O
the O
nounPP B-TaskName
task O
( O
this O
scaling O
evaluates O
the O
effective O
in- O
put O
to O
the O
units O
and O
did O
not O
change O
the O
conclusions O
described O
below O
) O
. O

We O
found O
that O
the O
weights O
from O
the O
syntax O
unit O
to O
the O
forget O
gate O
of O
both O
776 O
and O
988 O
are O
exceptionally O
high O
in O
the O
positive O
direc- O
tion O
compared O
to O
all O
other O
afferent O
connections O
in O
the O
network O
( O
z score O
= O
8:1;11:2 O
, O
respec- O
tively O
) O
and O
those O
to O
their O
input O
gates O
exception- O
ally O
negative O
( O
z score O
= O
 16:2; 7:2 O
) O
. O

Since O
the O
cell O
activity O
of O
syntax O
unit O
1150 O
is O
positive O
across O
the O
entire O
subject O
- O
verb O
dependency O
( O
e.g. O
, O
Figure O
3d O
) O
, O
the O
connectivity O
from O
the O
syntax O
unit O
drives O
the O
number O
unit O
forget O
gates O
towards O
one O
( O
Wf O
776;1150h1150 O

0andWf O
988;1150h1150 O

0 O
; O
tsubject O
< O
t O
< O
t O
verb O
) O
and O
their O
input O
gates O
towards O
zero O
( O
Wi O
776;1150h1150 O

0andWi O
988;1150h1150 O

  O
0 O
) O
. O

Looking O
at O
the O
right O
- O
hand O
- O
side O
of O
Eq O
. O

( O
1 O
) O
, O
this O
means O
that O
the O
ﬁrst O
term O
becomes O
dominant O
and O
the O
second O
vanishes O
, O
suggesting O
that O
, O
across O
the O
entire O
dependency O
, O
the O
syntax O
unit O
conveys O
a O
‘ O
re- O
member O
ﬂag O
’ O
to O
the O
number O
units O
. O

Similarly O
, O
when O
the O
activity O
of O
the O
syntax O
unit O
becomes O
negative O
at O
the O
end O
of O
the O
dependency O
, O
it O
conveys O
an O
‘ O
update O
ﬂag O
’ O
. O

Last O
, O
we O
note O
that O
the O
reciprocal O
connectivity O
between O
the O
two O
LR O
- O
number O
units O
is O
always O
pos- O
itive O
, O
to O
both O
input O
and O
forget O
gates O
( O
with O
jz  O
scorej>3for O
the O
776 O
-to-988 O
direction O
) O
. O

Since O
their O
activity O
is O
negative O
throughout O
the O
subject- O
verb O
dependency O
( O
Figures O
1a O
and O
1b O
) O
, O
this O
means O
that O
they O
are O
mutually O
inhibiting O
, O
thus O
steering O
to- O
wards O
an O
unequivocal O
signal O
about O
the O
grammati- O
cal O
number O
of O
the O
subject O
to O
the O
output O
layer O
. O

195 O
Summary O
and O
Discussion O
We O
provided O
the O
ﬁrst O
detailed O
description O
of O
the O
underlying O
mechanism O
by O
which O
an O
LSTM B-TaskName
language O
- O
model O
performs O
long O
- O
distance O
number O
agreement O
. O

Strikingly O
, O
simply O
training O
an O
LSTM B-TaskName
on O
a O
language O
- O
model O
objective O
on O
raw O
corpus B-DatasetName
data I-DatasetName
brought O
about O
single O
units O
carrying O
exceptionally O
speciﬁc O
linguistic O
information O
. O

Three O
of O
these O
units O
were O
found O
to O
form O
a O
highly O
interactive O
lo- O
cal O
network O
, O
which O
makes O
up O
the O
central O
part O
of O
a O
‘ O
neural O
’ O
circuit O
performing O
long O
- O
distance O
number O
agreement O
. O

One O
of O
these O
units O
encodes O
and O
stores O
gram- O
matical O
number O
information O
when O
the O
main O
sub- O
ject O
of O
a O
sentence O
is O
singular O
, O
and O
it O
successfully O
carries O
this O
information O
across O
long O
- O
range O
depen- O
dencies O
. O

Another O
unit O
similarly O
encodes O
plurality O
. O

These O
number O
units O
show O
that O
a O
highly O
local O
en- O
coding O
of O
linguistic O
features O
can O
emerge O
in O
LSTMs B-TaskName
during O
language O
- O
model O
training O
, O
as O
was O
previously O
suggested O
by O
theoretical O
studies O
of O
artiﬁcial O
neural O
networks O
( O
e.g. O
, O
Bowers O
, O
2009 O
) O
and O
in O
neuroscience O
( O
e.g. O
, O
Kutter O
et O
al O
. O
, O
2018 O
) O
. O

Our O
analysis O
also O
identiﬁed O
units O
whose O
activity O
correlates O
with O
syntactic O
complexity O
. O

These O
units O
, O
as O
a O
whole O
, O
affect O
performance O
on O
the O
agreement O
tasks O
. O

We O
further O
found O
that O
one O
of O
them O
encodes O
the O
main O
subject O
- O
verb O
dependency O
across O
various O
syntactic O
constructions O
. O

Moreover O
, O
the O
highest O
af- O
ferent O
weights O
to O
the O
forget O
and O
input O
gates O
of O
both O
LR O
- O
number O
units O
were O
from O
this O
unit O
. O

A O
natural O
interpretation O
is O
that O
this O
unit O
propagates O
syntax O
- O
based O
remember O
and O
update O
ﬂags O
that O
con- O
trol O
when O
the O
number O
units O
store O
and O
release O
infor- O
mation O
. O

Finally O
, O
number O
is O
also O
redundantly O
encoded O
in O
a O
more O
distributed O
way O
, O
but O
the O
latter O
mechanism O
is O
unable O
to O
carry O
information O
across O
embedded O
syntactic O
structures O
. O

The O
computational O
burden O
of O
tracking O
number O
information O
thus O
gave O
rise O
to O
two O
types O
of O
units O
in O
the O
network O
, O
encoding O
similar O
in- O
formation O
with O
distinct O
properties O
and O
dynamics O
. O

The O
relationship O
we O
uncovered O
and O
character- O
ized O
between O
syntax O
and O
number O
units O
suggests O
that O
agreement O
in O
an O
LSTM B-TaskName
language O
- O
model O
can- O
not O
be O
entirely O
explained O
away O
by O
superﬁcial O
heuristics O
, O
and O
the O
networks O
have O
, O
to O
some O
extent O
, O
learned O
to O
build O
and O
exploit O
structure O
- O
based O
syn- O
tactic O
representations O
, O
akin O
to O
those O
conjectured O
to O
support O
human O
- O
sentence O
processing O
. O

In O
future O
work O
, O
we O
intend O
to O
explore O
how O
the O
en O
- O
coding O
pattern O
we O
found O
varies O
across O
network O
ar- O
chitectures O
and O
hyperparameters O
, O
as O
well O
as O
across O
languages O
and O
domains O
. O

We O
also O
would O
like O
to O
investigate O
the O
timecourse O
of O
emergence O
of O
the O
found O
behaviour O
over O
training O
time O
. O

More O
generally O
, O
we O
hope O
that O
our O
study O
will O
inspire O
more O
analyses O
of O
the O
inner O
dynamics O
of O
LSTMs B-TaskName
and O
other O
sequence O
- O
processing O
networks O
, O
complementing O
the O
currently O
popular O
“ O
black O
- O
box O
probing O
” O
approach O
. O

Besides O
bringing O
about O
a O
mechanistic O
understanding O
of O
language O
process- O
ing O
in O
artiﬁcial O
models O
, O
this O
could O
inform O
work O
on O
human O
- O
sentence O
processing O
. O

Indeed O
, O
our O
study O
yields O
particular O
testable O
predictions O
on O
brain O
dy- O
namics O
, O
given O
that O
the O
computational O
burden O
of O
long O
- O
distance O
agreement O
remains O
the O
same O
for O
ar- O
tiﬁcial O
and O
biological O
neural O
network O
, O
despite O
im- O
plementation O
differences O
and O
different O
data O
sizes O
required O
for O
language O
acquisition O
. O

We O
conjecture O
a O
similar O
distinction O
between O
SR O
and O
LR O
units O
to O
be O
found O
in O
the O
human O
brain O
, O
as O
well O
as O
an O
in- O
teraction O
between O
syntax O
- O
processing O
and O
feature- O
carrying O
units O
such O
as O
the O
LR O
units O
, O
and O
plan O
to O
test O
these O
in O
future O
work O
. O

Acknowledgments O
We O
would O
like O
to O
thank O
Kristina O
Gulordava O
, O
Jean- O
Remi O
King O
, O
Tal O
Linzen O
, O
Gabriella O
Vigliocco O
and O
Christophe O
Pallier O
for O
helpful O
feedback O
and O
com- O
ments O
on O
the O
work O
. O

References O
Yossi O
Adi O
, O
Einat O
Kermany O
, O
Yonatan O
Belinkov O
, O
Ofer O
Lavi O
, O
and O
Yoav O
Goldberg O
. O
2017 O
. O

Fine O
- O
grained O
analysis O
of O
sentence O
embeddings O
using O
auxil- O
iary O
prediction O
tasks O
. O

In O
Proceedings O
of O
ICLR O
Conference O
Track O
, O
Toulon O
, O
France O
. O

Published O
online O
: O
https://openreview.net/group O
? O

id O
= O
ICLR.cc/2017 O
/ O
conference O
. O

Guillaume O
Alain O
and O
Yoshua O
Bengio O
. O

2017 O
. O

Under- O
standing O
intermediate O
layers O
using O
linear O
classiﬁer O
probes O
. O

In O
Proceedings O
of O
ICLR O
Conference O
Track O
, O
Toulon O
, O
France O
. O

Jean O
- O
Philippe O
Bernardy O
and O
Shalom O
Lappin O
. O
2017 O
. O

Us- O
ing O
deep O
neural O
networks O
to O
learn O
syntactic O
agree- O
ment O
. O

Linguistic O
Issues O
in O
Language O
Technology O
, O
15(2):1–15 O
. O

Jeffrey O
Bowers O
. O
2009 O
. O

On O
the O
biological O
plausibility O
of O
grandmother O
cells O
: O
Implications O
for O
neural O
network O
theories O
in O
psychology O
and O
neuroscience O
. O

Psycho- O

logical O
Review O
, O
116(1):220–251 O
. O

20Shammur O
Chowdhury O
and O
Roberto O
Zamparelli O
. O

2018 O
. O

RNN B-TaskName
simulations O
of O
grammaticality O
judgments O
on O
long O
- O
distance O
dependencies O
. O

In O
Proceedings O
of O
COLING O
, O
pages O
133–144 O
, O
Santa O
Fe O
, O
NM O
. O

Lieke O
Gelderloos O
and O
Grzegorz O
Chrupała O
. O

2016 O
. O

From O
phonemes O
to O
images O
: O
levels O
of O
representation O
in O
a O
recurrent O
neural O
model O
of O
visually O
- O
grounded O
lan- O
guage O
learning O
. O

In O
Proceedings O
of O
COLING O
2016 O
, O
the O
26th O
International O
Conference O
on O
Computational O
Linguistics O
: O
Technical O
Papers O
, O
pages O
1309–1319 O
. O

Mario O
Giulianelli O
, O
Jack O
Harding O
, O
Florian O
Mohnert O
, O
Dieuwke O
Hupkes O
, O
and O
Willem O
Zuidema O
. O

2018 O
. O

Un- O
der O
the O
hood O
: O
Using O
diagnostic O
classiﬁers O
to O
investi- O
gate O
and O
improve O
how O
language O
models O
track O
agree- O
ment O
information O
. O

In O
Proceedings O
of O
the O
EMNLP B-TaskName
BlackboxNLP B-TaskName
Workshop O
, O
pages O
240–248 O
, O
Brussels O
, O
Belgium O
. O

Kristina O
Gulordava O
, O
Piotr O
Bojanowski O
, O
Edouard O
Grave O
, O
Tal O
Linzen O
, O
and O
Marco O
Baroni O
. O

2018 O
. O

Colorless O
green O
recurrent O
networks O
dream O
hierarchically O
. O

In O
Proceedings O
of O
NAACL O
, O
pages O
1195–1205 O
, O
New O
Or- O
leans O
, O
LA O
. O

Sepp O
Hochreiter O
and O
J O
¨urgen O
Schmidhuber O
. O

1997 O
. O

Long O
short O
- O
term O
memory O
. O

Neural O
Computation O
, O
9(8):1735–1780 O
. O

Dieuwke O
Hupkes O
, O
Sara O
Veldhoen O
, O
and O
Willem O
Zuidema O
. O

2018 O
. O

Visualisation O
and O
’ O
diagnostic O
classi- O
ﬁers O
’ O
reveal O
how O
recurrent O
and O
recursive O
neural O
net- O
works O
process O
hierarchical O
structure O
. O

Journal O
of O
Ar- O
tiﬁcial O
Intelligence O
Research O
, O
61:907–926 O
. O

Jaap O
Jumelet O
and O
Dieuwke O
Hupkes O
. O

2018 O
. O

Do O
lan- O
guage O
models O
understand O
anything O
? O

on O
the O
ability O
of O
lstms O
to O
understand O
negative O
polarity O
items O
. O

In O
Proceedings O
of O
the O
2018 O
EMNLP B-TaskName
Workshop O
Black- O

boxNLP B-TaskName
: O

Analyzing O
and O
Interpreting O
Neural O
Net- O
works O
for O
NLP B-TaskName
, O
pages O
222–231 O
. O

Andrej O
Karpathy O
, O
Justin O
Johnson O
, O
and O
Fei O
- O
Fei O
Li O
. O
2016 O
. O

Visualizing O
and O
understanding O
recur- O
rent O
networks O
. O

In O
Proceedings O
of O
ICLR O
Work- O
shop O
Track O
, O
San O
Juan O
, O
Puerto O
Rico O
. O

Published O
online O
: O
https://openreview.net/group O
? O

id O
= O
ICLR.cc/2016 O
/ O
workshop O
. O

Yova O
Kementchedjhieva O
and O
Adam O
Lopez O
. O
2018 O
. O

‘ O
In- O
dicatements O
’ O
that O
character O
language O
models O
learn O
English O
morpho O
- O
syntactic O
units O
and O
regularities O
. O

In O
Proceedings O
of O
the O
EMNLP B-TaskName
Workshop O
on O
analyzing O
and O
interpreting O
neural O
networks O
for O
NLP B-TaskName
, O
Brussels O
, O
Belgium O
. O

In O
press O
. O

Jean O
- O
R O
´ O
emi O
King O
and O
Stanislas O
Dehaene O
. O

2014 O
. O

Charac- O
terizing O
the O
dynamics O
of O
mental O
representations O
: O
The O
temporal O
generalization O
method O
. O

Trends O
in O
Cogni- O
tive O
Sciences O
, O
18(4):203–210 O
. O

Adhiguna O
Kuncoro O
, O
Chris O
Dyer O
, O
John O
Hale O
, O
and O
Phil O
Blunsom O
. O

2018a O
. O

The O
perils O
of O
natural O
behavioral O
tests O
for O
unnatural O
models O
: O
The O
case O
of O
numberagreement O
. O

Poster O
presented O
at O
the O
Learning O
Lan- O
guage O
in O
Humans O
and O
in O
Machines O
conference O
, O
on- O
line O
at O
: O
https://osf.io/view/L2HM/ O
. O

Adhiguna O
Kuncoro O
, O
Chris O
Dyer O
, O
John O
Hale O
, O
Dani O
Yo- O
gatama O
, O
Stephen O
Clark O
, O
and O
Phil O
Blunsom O
. O

2018b O
. O

LSTMs B-TaskName
can O
learn O
syntax O
- O
sensitive O
dependencies O
well O
, O
but O
modeling O
structure O
makes O
them O
better O
. O

In O
Proceedings O
of O
ACL O
, O
pages O
1426–1436 O
, O
Melbourne O
, O
Australia O
. O

Esther O
Kutter O
, O
Jan O
Bostroem O
, O
Christian O
Elger O
, O
Florian O
Mormann O
, O
and O
Andreas O
Nieder O
. O

2018 O
. O

Single O
neu- O
rons O
in O
the O
human O
brain O
encode O
numbers O
. O

Neuron O
, O
100(3):753–761 O
. O

Jiwei O
Li O
, O
Xinlei O
Chen O
, O
Eduard O
Hovy O
, O
and O
Dan O
Jurafsky O
. O
2016 O
. O

Visualizing O
and O
understanding O
neural O
mod- O
els O
in O
NLP B-TaskName
. O

In O
Proceedings O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
: O
Human O
Language O
Technologies O
( O
NAACL- O
HLT O
) O
, O
pages O
681–691 O
. O

Tal O
Linzen O
, O
Emmanuel O
Dupoux O
, O
and O
Yoav O
Goldberg O
. O
2016 O
. O

Assessing O
the O
ability O
of O
LSTMs B-TaskName
to O
learn O
syntax O
- O
sensitive O
dependencies O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
4:521 O
– O
535 O
. O

Tal O
Linzen O
and O
Brian O
Leonard O
. O

2018 O
. O

Distinct O
patterns O
of O
syntactic O
agreement O
errors O
in O
recurrent O
networks O
and O
humans O
. O

In O
Proceedings O
of O
CogSci O
, O
pages O
692 O
– O
697 O
, O
Austin O
, O
TX O
. O

Rebecca O
Marvin O
and O
Tal O
Linzen O
. O

2018 O
. O

Targeted O
syn- O
tactic O
evaluation O
of O
language O
models O
. O

In O
Proceed- O
ings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1192–1202 O
. O

Matthew O
Nelson O
, O
Imen O
El O
Karoui O
, O
Kristof O
Giber O
, O
Xi- O
aofang O
Yang O
, O
Laurent O
Cohen O
, O
Hilda O
Koopman O
, O
Syd- O
ney O
Cash O
, O
Lionel O
Naccache O
, O
John O
Hale O
, O
Christophe O
Pallier O
, O
and O
Stanislas O
Dehaene O
. O

2017 O
. O

Neurophysio- O
logical O
dynamics O
of O
phrase O
- O
structure O
building O
during O
sentence O
processing O
. O

Proceedings O
of O
the O
National O
Academy O
of O
Sciences O
, O
114(18):E3669 O
– O
E3678 O
. O

Alec O
Radford O
, O
Rafal O
Jozefowicz O
, O
and O
Ilya O
Sutskever O
. O
2017 O
. O

Learning O
to O
generate O
reviews O
and O
discovering O
sentiment O
. O

https://arxiv.org/abs/1704 O
. O
01444 O
. O

Zhiyuan O
Tang O
, O
Ying O
Shi O
, O
Dong O
Wang O
, O
Yang O
Feng O
, O
and O
Shiyue O
Zhang O
. O

2017 O
. O

Memory O
visualization O
for O
gated O
recurrent O
neural O
networks O
in O
speech O
recogni- O
tion O
. O

In O
Acoustics O
, O
Speech O
and O
Signal O
Processing O
( O
ICASSP O
) O
, O
2017 O
IEEE O
International O
Conference O
on O
, O
pages O
2736–2740 O
. O

IEEE O
. O

Ethan O
Wilcox O
, O
Roger O
Levy O
, O
Takashi O
Morita O
, O
and O
Richard O
Futrell O
. O

2018 O
. O

What O
do O
RNN B-TaskName
language O
models O
learn O
about O
ﬁller O
– O
gap O
dependencies O
? O

In O
Proceedings O
of O
the O
2018 O
EMNLP B-TaskName
Workshop O
Black- O

boxNLP B-TaskName
: O

Analyzing O
and O
Interpreting O
Neural O
Net- O
works O
for O
NLP B-TaskName
, O
pages O
211–221 O
. O

Unsupervised O
Stance B-TaskName
Detection I-TaskName
for O
Arguments O
from O
Consequences O

Jonathan O
Kobbe O
, O
Ioana O
Hulpus O
¸ O
, O
Heiner O
Stuckenschmidt O
University O
of O
Mannheim O
fjonathan O
, O
ioana O
, O
heiner O
g@informatik.uni-mannheim.de O
Abstract O
Social O
media O
platforms O
have O
become O
an O
essen- O
tial O
venue O
for O
online O
deliberation O
where O
users O
discuss O
arguments O
, O
debate O
, O
and O
form O
opin- O
ions O
. O

In O
this O
paper O
, O
we O
propose O
an O
unsuper- O
vised O
method O
to O
detect O
the O
stance O
of O
argumen- O
tative O
claims O
with O
respect O
to O
a O
topic O
. O

Most O
related O
work O
focuses O
on O
topic O
- O
speciﬁc O
super- O
vised O
models O
that O
need O
to O
be O
trained O
for O
every O
emergent O
debate O
topic O
. O

To O
address O
this O
limita- O
tion O
, O
we O
propose O
a O
topic O
independent O
approach O
that O
focuses O
on O
a O
frequently O
encountered O
class O
of O
arguments O
, O
speciﬁcally O
, O
on O
arguments O
from O
consequences O
. O

We O
do O
this O
by O
extracting O
the O
effects O
that O
claims O
refer O
to O
, O
and O
proposing O
a O
means O
for O
inferring O
if O
the O
effect O
is O
a O
good O
or O
bad O
consequence O
. O

Our O
experiments O
provide O
promising O
results O
that O
are O
comparable O
to O
, O
and O
in O
particular O
regards O
even O
outperform O
BERT B-MethodName
. O

Furthermore O
, O
we O
publish O
a O
novel O
dataset O
of O
ar- O
guments O
relating O
to O
consequences O
, O
annotated O
with O
Amazon O
Mechanical O
Turk O
. O

1 O
Introduction O
In O
the O
context O
of O
decision O
making O
it O
is O
crucial O
to O
compare O
positive O
and O
negative O
effects O
that O
result O
from O
a O
potential O
decision O
. O

Indeed O
, O
arguing O
for O
or O
against O
something O
because O
of O
its O
possible O
conse- O
quences O
is O
a O
frequent O
form O
of O
argumentation O
( O
Reis- O
ert O
et O
al O
. O
, O
2018 O
; O
Al O
- O
Khatib O
et O
al O
. O
, O
2020 O
) O
. O

In O
this O
pa- O
per O
, O
we O
address O
the O
classical O
stance B-TaskName
detection I-TaskName
prob- I-TaskName
lem I-TaskName
paying O
special O
attention O
to O
such O
arguments O
. O

Stance B-TaskName
detection I-TaskName
, O
also O
called O
stance B-TaskName
classiﬁca- I-TaskName
tion I-TaskName
, O
is O
the O
task O
to O
decide O
whether O
a O
text O
is O
in O
favor O
of O
, O
against O
, O
or O
unrelated O
to O
a O
given O
topic O
. O

This O
prob- O
lem O
is O
related O
to O
opinion B-TaskName
mining I-TaskName
, O
but O
while O
opinion B-TaskName
mining I-TaskName
focuses O
on O
the O
sentiment O
polarity O
explicitly O
expressed O
by O
a O
text O
, O
stance B-TaskName
detection I-TaskName
aims O
to O
deter- O
mine O
the O
position O
that O
the O
text O
holds O
with O
respect O
to O
a O
topic O
that O
is O
generally O
more O
abstract O
and O
might O
not O
be O
mentioned O
in O
the O
text O
. O

As O
such O
, O
in O
stancedetection B-TaskName
, O
texts O
can O
transmit O
a O
negative O
sentiment O
or O
opinion O
, O
but O
be O
in O
favor O
of O
the O
targeted O
topic O
. O

For O
example O
, O
the O
text O
Holocaust O
denial O
psychologically O
harms O
Holocaust O
survivors O
expresses O
a O
negative O
opinion O
, O
but O
its O
stance O
towards O
Criminalization O
of O
Holocaust O
denial O
is O
positive.1 O

Recently O
, O
the O
problem O
of O
stance B-TaskName
detection I-TaskName
has O
received O
growing O
attention O
from O
the O
scientiﬁc O
com- O
munity O
, O
as O
shown O
by O
the O
recent O
survey O
of O
K O
¨uc O
¸¨uk O
and O
Can O
( O
2020 O
) O
. O

Most O
approaches O
tackle O
this O
prob- O
lem O
by O
learning O
stance B-TaskName
classiﬁcation I-TaskName
models O
for O
each O
topic O
. O

While O
this O
can O
achieve O
good O
results O
, O
new O
models O
need O
to O
be O
trained O
for O
each O
new O
topic O
of O
interest O
, O
generally O
entailing O
large O
annotation O
stud- O
ies O
. O

While O
we O
admit O
that O
a O
one O
- O
size-ﬁts O
- O
all O
approach O
to O
stance B-TaskName
detection I-TaskName
is O
currently O
unfeasible O
, O
we O
take O
a O
different O
perspective O
. O

Rather O
than O
targeting O
topic- O
dependent O
models O
, O
we O
target O
a O
subclass O
of O
argu- O
ments O
. O

Speciﬁcally O
, O
we O
focus O
on O
arguments O
that O
have O
been O
classiﬁed O
by O
Walton O
et O
al O
. O

( O
2008 O
) O
under O
theargument O
from O
consequences O
scheme O
. O

They O
contain O
a O
premise O
of O
the O
form O
If O
A O
is O
brought O
about O
, O
then O
good O
( O
bad O
) O
consequences O
will O
( O
may O
plausibly O
) O
occur O
, O
and O
a O
conclusion O
A O
should O
( O
not O
) O
be O
brought O
about O
. O

In O
most O
real O
- O
life O
arguments O
of O
this O
type O
, O
the O
consequences O
are O
expressed O
, O
but O
the O
interpretation O
that O
they O
are O
good O
orbad O
, O
as O
well O
as O
the O
conclusion O
, O
are O
most O
often O
implicit O
. O

The O
task O
of O
stance B-TaskName
detec- I-TaskName
tion I-TaskName
is O
then O
to O
determine O
if O
the O
argument O
is O
against O
or O
in O
favor O
of O
A. O
Our O
solution O
to O
ﬁnd O
the O
stance O
of O
such O
arguments O
revolves O
around O
extracting O
and O
analyzing O
cause O
- O
effect O
relations O
in O
order O
to O
infer O
if O
the O
consequences O
are O
good O
orbad O
. O

We O
conducted O
an O
Amazon O
Mechanical O
Turk O
( O
AMT O
) O
study O
, O
in O
which O
we O
crowdsourced O
anno- O
tations O
for O
1894 O
arguments O
extracted O
from O
Debate- O
pedia O
. O

We O
compared O
our O
system O
’s O
performance O
to O
a O
sentiment B-TaskName
analysis I-TaskName
baseline O
and O
a O
ﬁne O
- O
tuned O
BERT B-MethodName
model O
. O

The O
results O
show O
that O
our O
results O
are O
comparable O
and O
, O
in O
some O
settings O
, O
even O
bet- O
ter O
than O
BERT’s B-MethodName
Aside O
from O
not O
needing O
anno- O
tated O
training O
data O
, O
we O
stress O
the O
advantage O
of O
our O
approach O
for O
providing O
human O
- O
understandable O
ex- O
planations O
to O
the O
results O
, O
and O
to O
provide O
, O
as O
a O
by- O
product O
, O
cause O
- O
effect O
relations O
between O
concepts O
brought O
up O
in O
arguments O
. O

The O
paper O
is O
structured O
as O
follows O
. O

Section O
2 O
po- O
sitions O
our O
contributions O
with O
respect O
to O
related O
literature O
. O

Section O
3 O
presents O
our O
proposed O
ap- O
proach O
. O

Section O
4 O
describes O
our O
crowdsourced B-DatasetName
dataset O
, O
which O
we O
use O
in O
Section O
5 O
to O
evaluate O
our O
approach O
. O

Lastly O
, O
Section O
6 O
concludes O
the O
paper O
. O

2 O
Related O
Work O
Stance B-TaskName
detection I-TaskName
has O
been O
studied O
on O
various O
types O
of O
formal O
texts O
such O
as O
congressional O
debates O
( O
Thomas O
et O
al O
. O
, O
2006 O
) O
and O
company O
- O
internal O
discus- O
sions O
( O
Murakami O
and O
Raymond O
, O
2010 O
) O
. O

However O
, O
like O
most O
recent O
related O
work O
on O
the O
topic O
, O
we O
are O
particularly O
interested O
in O
informal O
texts O
from O
online O
social O
media O
. O

The O
vast O
majority O
of O
previous O
approaches O
pro- O
poses O
supervised O
methods O
, O
using O
traditional O
ma- O
chine O
learning O
algorithms O
( O
Somasundaran O
and O
Wiebe O
, O
2010 O
; O

Anand O
et O
al O
. O
, O
2011 O
; O
Hasan O
and O
Ng O
, O
2013 O
; O
Faulkner O
, O
2014 O
; O
Sobhani O
et O
al O
. O
, O
2016 O
; O
Adda- O
wood O
et O
al O
. O
, O
2017 O
) O
and O
more O
recently O
, O
various O
deep O
neural O
networks O
architectures O
( O
Sun O
et O
al O
. O
, O
2018 O
; O
Du O
et O
al O
. O
, O
2017 O
; O
Dey O
et O
al O
. O
, O
2018 O
; O
Ghosh O
et O
al O
. O
, O
2019 O
) O
. O

These O
approaches O
, O
most O
of O
which O
have O
been O
trig- O
gered O
by O
a O
recent O
SemEval B-TaskName
shared I-TaskName
task3(Moham- O
mad O
et O
al O
. O
, O
2016 O
) O
, O
learn O
topic O
- O
speciﬁc O
models O
. O

Thus O
, O
new O
topics O
require O
new O
models O
whose O
training O
en- O
tails O
large O
user O
annotation O
studies O
. O

In O
contrast O
, O
we O
propose O
a O
fully O
unsupervised O
, O
topic O
- O
independent O
method O
, O
and O
rather O
target O
a O
particular O
but O
frequent O
class O
of O
claims O
, O
those O
that O
refer O
to O
consequences O
. O

Among O
the O
unsupervised O
approaches O
, O
the O
most O
prominent O
one O
is O
this O
of O
Somasundaran O
and O
Wiebe O
( O
2009 O
) O
, O
which O
got O
extended O
by O
Konjengbam O
et O
al O
. O

( O
2018 O
) O
and O
Ghosh O
et O
al O
. O

( O
2018 O
) O
. O

However O
, O
they O
focus O
on O
non O
- O
ideological O
topics O
( O
usually O
products O
, O
e.g. O
, O
iPhone O
vs. O
Galaxy O
) O
. O

In O
contrast O
, O
we O
target O
ideological O
topics O
( O
e.g. O
, O
Gay O
Marriage O
, O
Abortion O
) O
whose O
stance O
is O
harder O
to O
detect O
due O
to O
less O
fre- O
quent O
use O
of O
sentiment O
words O
and O
a O
wider O
variety O
of O
brought O
up O
issues O
and O
arguments O
( O
Rajendran O
et O
al O
. O
, O
2016 O
; O
Wang O
et O
al O
. O
, O
2019 O
) O
. O

On O
the O
one O
hand O
, O
these O
works O
extract O
topic O
aspects O
( O
e.g. O
, O
screen O
resolution O
, O
battery O
) O
and O
polarities O
towards O
these O
aspects O
, O
a O
step O
that O
is O
unfeasible O
for O
ideological O
topics O
. O

On O
the O
other O
hand O
, O
like O
these O
works O
, O
we O
also O
use O
syntactic O
rules O
, O
but O
not O
for O
pairing O
aspects O
to O
opinions O
, O
but O
for O
extracting O
triples O
that O
correspond O
to O
statements O
about O
effects O
over O
opinion O
words O
. O

Another O
class O
of O
stance B-TaskName
detection I-TaskName
approaches O
uses O
the O
context O
of O
the O
post O
, O
such O
as O
its O
relations O
to O
other O
posts O
in O
the O
debate O
, O
the O
network O
of O
authors O
, O
or O
the O
author O
’s O
identity O
( O
Hasan O
and O
Ng O
, O
2013 O
; O
Sridhar O
et O
al O
. O
, O
2014 O
; O
Addawood O
et O

al O
. O
, O
2017 O
; O
Bar O
- O
Haim O
et O
al O
. O
, O
2017b O
) O
. O

By O
contrast O
, O
we O
target O
claim O
- O
topic O
pairs O
in O
isolation O
. O

Another O
aspect O
that O
sets O
our O
work O
apart O
from O
most O
related O
work O
is O
that O
, O
except O
for O
the O
approaches O
that O
target O
tweets O
, O
most O
focus O
on O
longer O
texts O
while O
we O
consider O
short O
, O
one O
- O
sentence O
claims O
. O

In O
this O
re- O
gard O
, O
but O
not O
only O
, O
the O
stance O
detection O
work O
that O
is O
closest O
to O
ours O
is O
the O
partly O
supervised O
system O
of O
Bar O
- O
Haim O
et O
al O
. O
( O
2017a O
) O
. O

They O
also O
propose O
a O
topic- O
independent O
solution O
to O
stance B-TaskName
detection I-TaskName
for O
short O
claims O
without O
considering O
context O
, O
but O
they O
do O
not O
speciﬁcally O
address O
arguments O
from O
consequences O
. O

While O
they O
follow O
a O
similar O
sequence O
of O
steps O
as O
we O
do O
, O
they O
propose O
different O
approaches O
for O
each O
step O
. O

For O
instance O
, O
they O
propose O
a O
supervised O
approach O
to O
detect O
the O
target O
of O
a O
claim O
’s O
opinion O
, O
while O
we O
do O
it O
in O
an O
unsupervised O
manner O
. O

They O
focus O
pri- O
marily O
on O
detecting O
contrastive O
relations O
between O
phrases O
, O
while O
our O
focus O
is O
on O
detecting O
effects O
. O

In O
this O
last O
regard O
, O
the O
works O
can O
be O
considered O
complementary O
. O

Regarding O
the O
analysis O
of O
arguments O
from O
con- O
sequences O
, O
Reisert O
et O
al O
. O

( O
2018 O
) O
provide O
and O
use O
scheme O
dependent O
templates O
to O
analyze O
the O
struc- O
ture O
of O
arguments O
. O

Their O
work O
is O
rather O
concep- O
tual O
and O
focuses O
on O
annotations O
. O

Very O
recently O

, O
Al O
- O
Khatib O
et O
al O
. O

( O
2020 O
) O
built O
, O
on O
similar O
intuitions O
as O
ours O
, O
an O
approach O
for O
creating O
argumentation O
knowledge O
graphs O
based O
on O
cause O
- O
effect O
relations O
. O

Their O
work O
comes O
to O
reinforce O
the O
usefulness O
of O
addressing O
arguments O
from O
consequences O
. O

To O
sum O
up O
, O
our O
contribution O
is O
three O
- O
fold O
: O
( O
i O
) O
we O
propose O
a O
fully O
unsupervised O
approach O
for O
stance B-TaskName
detection I-TaskName
, O
focusing O
on O
arguments O
that O
refer O
to O
con- O
sequences O
; O
( O
ii O
) O
we O
deﬁne O
rules O
over O
grammatical O
dependencies O
that O
exploit O
sentiment O
as O
well O
as O
ef- O
fect O
words O
in O
order O
to O
determine O
good B-MetricValue
and O
bad B-MetricValue
con- O
sequences O
; O
( O
iii O
) O
we O
publish O
a O
new O
stance B-TaskName
detection I-TaskName
dataset O
that O
labels O
claims O
that O
refer O
to O
consequences O
, O
and O
which O
was O
crowdsourced B-DatasetName
on O
AMT O
. O

3 O

Our O
Approach O
Given O
an O
argumentative O
claim O
and O
a O
topic O
, O
our O
task O
is O
to O
detect O
the O
stance O
that O
the O
claim O
has O
with O
respect O
to O
the O
topic O
. O

Statements O
such O
as O
the O
claim O
or O
topic O
usually O
express O
a O
positive B-MetricValue
( O
favorable B-MetricValue
) O
or O
negative B-MetricValue
( O
unfavorable B-MetricValue
) O
position O
to O
a O
concept O
that O
we O
call O
thetarget O
. O

As O
such O
, O
the O
target O
is O
a O
phrase O
that O
belongs O
to O
the O
statement O
. O

In O
the O
example O
shown O
in O
Table O
1 O
, O
the O
target O
of O
both O
topic O
and O
claim O
is O
medical O
marijuana O
. O

Our O
solution O
starts O
by O
ﬁrst O
determining O
the O
stance O
of O
the O
claim O
and O
of O
the O
topic O
towards O
their O
respective O
targets O
TcandTt O
. O

We O
then O
use O
these O
stances O
and O
the O
semantic O
relation O
between O
the O
targets O
to O
determine O
the O
claim O
’s O
stance O
towards O
the O
topic O
. O

The O
overarching O
intuition O
behind O
our O
approach O
is O
that O
when O
the O
stance O
of O
a O
statement O
towards O
its O
target O
is O
favorable B-MetricValue
, O
the O
text O
either O
highlights O
the O
desirable O
consequences O
of O
the O
target O
being O
brought O
about O
( O
e.g. O
, O
Electing O
an O
EU O
president O
directly O
will O
increase O
accountability O
) O
, O
or O
it O
highlights O
the O
nega- O
tive O
consequences O
if O
the O
target O
is O
not O
brought O
about O
( O
e.g. O
, O
Sinking O
organic O
blooms O
can O
render O
the O
deep O
sea O
anoxic O
) O
. O

At O
the O
core O
of O
our O
approach O
resides O
what O
we O
call O
the O
effect B-HyperparameterName
triple I-HyperparameterName
. O

The O
effect B-HyperparameterName
triple I-HyperparameterName
is O
a O
triple O
of O
the O
form<(T;dir);(P;e O

) O
; O
( O
O;sent O
) O
> O
. O

The O
( O
T;dir)pair O
represents O
the O
target O
Tof O
the O
state- O
ment O
and O
if O
the O
statement O
refers O
to O
a O
magniﬁcation O
( O
dir= O
1 O
) O
( O
e.g. O
legalizing O
medical O
marijuana O
) O
, O
or O
a O
reduction O
( O
dir= 1 O
) O
of O
the O
target O
( O
e.g. O
banning O
medical O
marijuana O
) O
. O

The O
( O
P;e O

) O
pair O
represents O
the O
predicate O
Pthat O
hasTas O
the O
subject O
, O
together O
with O
the O
effect O
e O

that O
it O
has O
over O
the O
object O

O. O

The O
effect O
can O
be O
positive O
( O
e O

= O
+1 O
) O
or O
negative O
( O
e O

= O
 1 O
) O
. O

Lastly O
, O
the O
( O
O;sent O
) O
pair O
represents O
the O
object O
over O
which O
Thas O
the O
effect O
P. O

We O
ex- O
pect O
the O
sentiment B-MetricName
of O
an O
object O
to O
reﬂect O
whether O
it O
is O
generally O
regarded O
as O
a O
good B-MetricValue
thing O
( O
sent O
= O
+1 O
) O
ora O
bad B-MetricValue
thing O
( O
sent O
= O
 1).Our O
approach O
’s O
core O
idea O
is O
to O
distill O
such O
an O
effect O
triple O
from O
the O
claim O
and O
use O
it O
to O
infer O
the O
claim O
’s O
stance O
towards O
Tc O
. O

We O
further O
determine O
( O
Tt;dir)to O
infer O
the O
topic O
’s O
stance O
towards O
Tt O
. O

Us- O
ing O
these O
stances O
, O
together O
with O
the O
relation O
be- O
tween O
the O
claim O
’s O
and O
the O
topic O
’s O
target O
, O
we O
ﬁnally O
decide O
the O
claim O
’s O
stance O
with O
respect O
to O
the O
topic O
. O

We O
now O
describe O
the O
lexicons O
we O
use O
as O
well O
as O
each O
of O
these O
steps O
in O
more O
detail O
. O

3.1 O
Lexicons O
For O
determining O
dir O
, O
e O

, O
and O
sent O
, O
we O
use O
an O
ef- O
fect O
verb O
lexicon O
and O
a O
sentiment O
lexicon O
that O
we O
describe O
in O
the O
following O
. O

The O
ECF O
Effect O
Lexicon O
To O
identify O
verbs O
and O
nominalized O
verbs O
that O
indicate O
effects O
on O
their O
direct O
objects O
, O
we O
extend O
the O
connotation O
frames O
( O
Rashkin O
et O
al O
. O
, O
2016 O
) O
. O

The O
connotation O
frames O
lexicon O
consists O
of O
a O
list O
of O
947 O
verbs O
, O
manually O
an- O
notated O
with O
values O
in O
the O
[ O
 1;1]range O
, O
indicating O
if O
the O
verb O
implies O
a O
positive O
or O
negative O
effect O
over O
its O
object O
. O

We O
consider O
the O
entries O
with O
scores O
in O
the O
range O
[ O
 0:1;0:1]as O
a O
neutral O
effect O
( O
e.g. O
, O
use O
, O
say O
, O
seem O
) O
, O
and O
we O
ﬁlter O
them O
out O
. O

We O
call O
the O
845 O
remaining O
words O
in O
the O
lexicon O
effect O
words O
. O

We O
extend O
the O
list O
of O
effect O
words O
by O
adding O
all O
words O
in O
the O
same O
WordNet O
( O
Fellbaum O
, O
2010 O
) O
synset O
as O
the O
effect O
words O
, O
as O
long O
as O
there O
is O
no O
contradic- O
tion O
. O

A O
contradiction O
occurs O
when O
a O
new O
candidate O
effect O
word O
shares O
a O
synset O
with O
both O
a O
negative O
and O
a O
positive O
effect O
word O
. O

This O
way O
, O
we O
obtain O
2508 O
effect O
words O
. O

We O
call O
this O
lexicon O
the O
extended O
connotation O
frames O
lexicon O
( O
ECF O
) O
. O

As O
ECF O
only O
contains O
verbs O
, O
we O
use O
it O
via O
the O
stems O
of O
the O
words O
, O
mainly O
to O
also O
get O
the O
effects O
of O
nominalized O
verbs O
. O

In O
our O
experiments O
, O
we O
compare O
the O
performance O
of O
this O
lexicon O
with O
+ O
/-EffectWordNet O
( O
Choi O
and O
Wiebe O
, O
2014)(EWN O
) O
. O

The O
Sentiment O
Lexicon O

In O
order O
to O
determine O
if O
the O
object O
of O
the O
effect O
is O
something O
good B-MetricValue
or O
bad B-MetricValue
, O
we O
combine O
several O
commonly O
used O
senti- O
ment O
lexicons O
: O
( O
i O
) O
the O
MPQA O
lexicon4(Wilson O
et O
al O
. O
, O
2005 O
) O
, O
( O
ii O
) O
the O
opinion O
lexicon O
of O
Hu O
and O
Liu O
( O
2004 O
) O
, O
and O
( O
iii O
) O
the O
sentiment O
lexicon O
of O
Toledo- O
Ronen O
et O
al O
. O

( O
2018 O
) O

( O
uni- O
and O
bigrams O
, O
using O
a O
threshold O
of0:2 O
) O
. O

The O
composed O
lexicon O
con- O
tains O
sentiment O
values O
in O
the O
range O

[ O
 1;1 O
] O
. O

For O
many O
words O
, O
the O
polarities O
of O
their O
sentiment O
and O
of O
their O
effect O
are O
the O
same O
( O
e.g. O
, O
kill O
, O
love O
) O
. O

Still O
, O
there O
are O
important O
exceptions O
, O
such O
as O
reduce O
, O
which O
has O
neutral O
sentiment O
but O
indicates O
a O
negative O
effect O
, O
or O
conquer O
, O
which O
has O
a O
slightly O
positive O
sentiment O
but O
indicates O
a O
negative O
effect O
. O

3.2 O
Effect O
Triple O
Extraction O
Target O
Identiﬁcation O
To O
detect O
the O
targets O
of O
the O
claim O
( O
Tc O
) O
and O
topic O
( O
Tt O
) O
, O
we O
assume O
that O
Tcis O
se- O
mantically O
related O
to O
the O
topic O
, O
or O
more O
speciﬁcally O
, O
toTt O
. O

Thus O
, O
we O
identify O
TcandTtsimultaneously O
by O
following O
three O
strategies O
. O

The O
use O
of O
the O
second O
and O
third O
strategies O
is O
conditioned O
on O
the O
previous O
strategies O
to O
have O
failed O
to O
identify O
a O
pair O
of O
targets O
. O

First O
, O
we O
look O
for O
a O
pair O
of O
nouns O
that O
are O
identical O
or O
have O
the O
same O
lemma O
. O

We O
use O
Stanford O
Core O
NLP O
( O
Manning O
et O
al O
. O
, O
2014 O
) O
for O
POS B-TaskName
tagging I-TaskName
and O
lemmatizing B-TaskName
. O

Second O
, O
we O
look O
for O
a O
pair O
consisting O
of O
an O
acronym O
( O
e.g. O
, O
ICC O
) O
and O
a O
word O
sequence O
whose O
ﬁrst O
letters O
form O
the O
acronym O
( O
e.g. O
, O
Interna- O
tional O
Criminal O
Court O
) O
. O

Third O
, O
we O
look O
for O
pairs O
of O
nouns O
that O
are O
synonyms O
or O
antonyms O
according O
to O
Thesaurus.plus5 O
. O

Besides O
returning O
TcandTt O
, O
we O
also O
return O
a O
valuer= O
+1 O
if O
the O
two O
targets O
have O
been O
found O
to O
be O
synonyms O
and O
r= 1if O
they O
are O
antonyms O
. O

Thus O
, O
ﬁrst O
and O
second O
strategies O
only O
return O
r= O
1 O
while O
the O
third O
strategy O
returns O
1or 1 O
. O

Target O
Direction O
Determination O
As O
described O
earlier O
, O
each O
target O
is O
accompanied O
by O
a O
dirvalue O
which O
indicates O
if O
the O
statement O
refers O
to O
a O
phe- O
nomenon O
of O
ampliﬁcation O
or O
reduction O
of O
the O
target O
. O

We O
detect O
this O
by O
searching O
for O
a O
word O
whose O
ob- O
ject O
is O
the O
target O
by O
using O
Patterns O
1 O
and O
2 O
shown O
in O
Table O
2 O
. O

The O
word O
is O
then O
looked O
- O
up O
in O
the O
ef- O
fect O
lexicon O
. O

If O
a O
negative O
effect O
is O
found O
, O
then O
dir= 1 O
, O
otherwise O
dir= O
1 O
. O

We O
call O
the O
word O
thetarget O
effector O
, O
or O
just O
effector O
. O

In O
the O
claim O
in O
Table O
1 O
, O
the O
effector O
is O
legalizing O
and O
expresses O
an O
ampliﬁcation O
of O
the O
target O
( O
dir= O
1 O
) O
. O

Detecting O
Predicates O
and O
Their O
Effects O
Effect O
words O
are O
commonly O
used O
in O
arguments O
from O
con- O
sequences O
to O
express O
a O
( O
potential O
) O
effect O
that O
the O
target O
has O
or O
might O
have O
over O
another O
object O
. O

For O
example O
, O
in O
the O
claim O
in O
Table O
1 O
, O
the O
effect O
word O
increase O
expresses O
a O
positive O
effect O
that O
the O
( O
ampli- O
ﬁed O
) O
target O
has O
over O
the O
objects O
use O
, O
abuse O
. O

We O
detect O
this O
effect O
of O
the O
target O
by O
using O
Pat- O
tern O
3 O
to O
ﬁnd O
a O
predicate O
whose O
subject O
is O
either O
the O
target O
or O
its O
effector O
, O
and O
by O
looking O
up O
this O
predicate O
in O
the O
effect O
lexicon O
. O

We O
thereby O
set O
e O

  O
to1or 1 O
, O
depending O
on O
if O
the O
effect O
is O
positive O
or O
negative O
. O

In O
our O
running O
example O
, O
the O
( O
P;e O

) O
pair O
becomes O
( O
increase; 1)because O
of O
the O
negation O
, O
as O
we O
explain O
below O
. O

Telling O
good O
from O
bad O
The O
last O
effect O
triple O
com- O
ponent O
we O
detect O
is O
( O
O;sent O
) O
. O

To O
this O
end O
, O
we O
search O
the O
dependency O
graph O
for O
instantiations O
of O
Patterns O
1 O
or O
2 O
, O
where O
Pis O
the O
predicate O
that O
has O
been O
detected O
to O
express O
the O
target O
’s O
effect O
. O

If O
such O
an O
object O
is O
found O
, O
we O
use O
the O
sentiment O
lexicon O
by O
ﬁrst O
searching O
for O
the O
exact O
word O
and O
, O
if O
not O
available O
, O
for O
the O
word O
’s O
lemma O
. O

We O
set O
sent O
to 1 O
if O
the O
word O
bears O
a O
negative O
sentiment O
or O
to O
1other- O
wise O
. O

In O
our O
example O
, O
the O
( O
O;sent O
) O
pair O
becomes O
( O
abuse; 1)because O
the O
word O
useis O
neutral O
per O
se O
. O

The O
sentiment O
of O
a O
word O
is O
overwritten O
by O
the O
sentiment O
of O
its O
modiﬁers O
, O
as O
shown O
in O
Pattern O
4 O
in O
Table O
2 O
. O

In O
the O
provided O
example O
in O
the O
table O
, O
one O
can O
see O
that O
the O
modiﬁer O
terrorist O
dominates O
the O
sentiment O
of O
the O
positive O
word O
haven O
. O

Conse- O

quently O
, O
both O
terrorist O
haven O
andterrorist O
attack O
are O
considered O
generally O
bad O
. O

Negation O
We O
deal O
with O
negations O
for O
each O
effect O
triple O
component O
. O

We O
identify O
negations O
by O
look- O
ing O
for O
Patterns O
5 O
, O
6 O
, O
and O
7 O
, O
as O
shown O
in O
Table O
2 O
. O

Patterns O
5 O
and O
6 O
make O
use O
of O
a O
manually O
created O
list O
of O
all O
negative O
English O
prepositions6 O
. O

The O
existence O
of O
a O
negation O
affecting O
the O
target O
, O
predicate O
, O
or O
ob- O
ject O
toggles O
the O
sign O
of O
the O
corresponding O
value O
- O
dir O
, O
e O

orsent O
, O
respectively O
. O

3.3 O
Inferring O
the O
Stance O
Towards O
the O
Target O
To O
infer O
the O
stance O
that O
a O
statement O
expresses O
towards O
its O
target O
, O
we O
use O
the O
intuition O
that O
the O
stance O
is O
unfavorable O
when O
the O
text O
expresses O
negative O
consequences O
of O
the O
target O
, O
and O
posi- O
tive O
otherwise O
. O

Thus O
, O
we O
deﬁne O
that O
the O
stance O
towards O
the O
target O
is O
positive O
in O
exactly O
the O
fol- O
lowing O
four O
cases O
: O
( O
i O
) O
the O
target O
’s O
ampliﬁcation O
implies O
a O
positive O
effect O
over O
something O
good O
( O
dir O
= O
e O

= O
sent O
= O
+1 O
) O
; O
( O
ii O
) O
the O
target O
’s O
ampli- O
ﬁcation O
implies O
a O
negative O
effect O
over O
something O
bad O
( O
dir=+1;e O

= O
sent O
= O
 1 O
) O
; O
( O
iii O
) O
the O
target O
’s O
reduction O
implies O
a O
negative O
effect O
over O
something O
good O
( O
dir O
= O
e O

= O
 1;sent O
= O
+1 O
) O
; O
( O
iv O
) O
the O
target O
’s O
reduction O
implies O
a O
positive O
effect O
over O
something O
bad O
( O
dir=+1;e O

= O
 1;sent O
= O
+1 O
) O
. O

Hence O
, O
the O
stance O
is O
favorable O
towards O
the O
target O
if O
the O
mul- O
tiplication O
of O
the O
three O
components O
’ O
values O
is O
+1 O
. O

Consequently O
, O
we O
deﬁne O
the O
stance O
of O
a O
statement O
towards O
the O
target O
as O
s O
= O
dire O

valand O
interpret O
s= O
1asIn O
favor O
ands= 1asAgainst O
. O

3.4 O
Inferring O
the O
Stance O
of O
the O
Claim O
Towards O
the O
Topic O
The O
steps O
above O
can O
be O
executed O
analogously O
for O
the O
claim O
and O
the O
topic O
. O

However O
, O
due O
to O
the O
na- O
ture O
of O
the O
text O
expressing O
the O
topic O
, O
we O
only O
aim O
to O
extract O
an O
effect B-HyperparameterName
triple I-HyperparameterName
from O
the O
claim O
. O

For O
the O
topic O
, O
we O
detect O
its O
target O
and O
set O
the O
stance O
to O
its O
corresponding O
dirvalue O
. O

We O
denote O
the O
stances O
of O
the O
claim O
and O
topic O
towards O
their O
respective O
targets O
asscandst O
. O

To O
infer O
the O
claim O
’s O
stance O
towards O
the O
topic O
, O
we O
need O
to O
consider O
the O
relation O
between O
TcandTt O
, O
i.e. O
, O
the O
value O
of O
ras O
described O
in O
Sec- O
tion O
3.2 O
. O

We O
then O
deﬁne O
the O
ﬁnal O
result O
of O
the O
analysis O
as O
 O
= O
scstr O
. O

Table O
3 O
presents O
further O
examples O
of O
how O
our O
ap- O
proach O
detects O
the O
stance O
of O
the O
claim O
towards O
the O
topic O
. O

As O
illustrated O
in O
the O
examples O
, O
the O
straightfor- O
ward O
interpretability O
of O
the O
stance B-TaskName
detection I-TaskName
process O
can O
be O
easily O
used O
for O
producing O
human O
- O
readable O
explanations O
for O
the O
returned O
results O
. O

This O
is O
partic- O
ularly O
relevant O
for O
helping O
users O
get O
more O
control O
over O
the O
process O
, O
particularly O
in O
light O
of O
subsequent O
applications O
on O
top O
of O
stance B-TaskName
detection I-TaskName
. O

3.5 O
Alternative O
Strategies O
We O
denote O
the O
process O
in O
which O
all O
the O
previous O
steps O
are O
fulﬁlled O
and O
an O
effect B-HyperparameterName
triple I-HyperparameterName
is O
extracted O
asTPO O
. O

However O
, O
due O
to O
a O
variety O
of O
reasons O
that O
we O
analyze O
in O
Section O
5.4 O
, O
we O
might O
fail O
to O
extract O
a O
complete O
effect B-HyperparameterName
triple I-HyperparameterName
. O

One O
such O
case O
is O
when O
an O
adjective O
expresses O
an O
effect O
, O
for O
instance O
, O
Holo- O
caust O
denial O
is O
discriminatory O
. O

For O
that O
reason O
, O
if O
we O
identify O
TandP O
, O
but O
notO O
, O
we O
set O
e O

to O
the O
sentiment O
polarity O
of O
P O
, O
and O
sent O
to+1by O
default O
. O

We O
refer O
to O
this O
strategy O
as O
TP O
. O

Another O
potential O
situation O
is O
that O
the O
system O
detects O
( O
P;e O

) O
and(O;sent O
) O
, O
but O
it O
can O
not O
relate O
them O
toT. O

One O
cause O
can O
be O
that O
we O
fail O
to O
identify O
T. O
If O
so O
, O
dir= O
+1 O
by O
default O
. O

Another O
cause O
can O
be O
that O
Tis O
found O
, O
but O
we O
can O
not O
infer O
its O
relation O
toP. O
In O
this O
case O
, O
we O
consider O

that O
the O

identiﬁed O
target O
is O
the O
subject O
of O
Pand O
set O
( O
T;dir O
) O
accordingly O
. O

We O
refer O
to O
this O
strategy O
as O
PO O
. O

Lastly O
, O
if O
all O
above O
strategies O
fail O
to O
create O
an O
effect B-HyperparameterName
triple I-HyperparameterName
, O
we O
use O
a O
heuristic O
: O
if O
Twas O
found O
, O
dir O
is O
set O
accordingly O
. O

Otherwise O
dir= O
1by O
default O
. O

For O
the O
remaining O
words O
in O
the O
statement O
, O
we O
check O
their O
sentiment O
score O
, O
still O
using O
Pattern O
4 O
, O
toggling O
the O
sign O
if O
it O
is O
negated O
. O

The O
sum O
of O
the O
sentiment O
scores O
is O
then O
multiplied O
with O
dir O
. O

The O
stance O
is O
considered O
favorable O
or O
not O
depending O
on O
the O
sign O
of O
the O
result O
. O

We O
refer O
to O
this O
strategy O
as O
Heuristic O
. O

4 O
Dataset O
Generation O
To O
evaluate O
our O
approach O
, O
we O
need O
stance O
annotated O
topic O
- O
claim O
pairs O
, O
as O
well O
as O
annotations O
if O
the O
topic- O
claim O
pair O
refers O
to O
a O
consequence O
or O
not O
. O

4.1 O
Data O
Collection O
To O
create O
such O
a O
corpus O
, O
we O
run O
an O
AMT O
crowd- O
sourcing O
study O
, O
where O
we O
annotate O
claims O
and O
top- O
ics O
extracted O
from O
Debatepedia B-DatasetName
. O

We O
only O
use O
the O
236Featured O
Debate O
Digest O
articles O
as O
they O
are O
of O
higher O
quality O
. O

They O
contain O
more O
than O
10,000 O
arguments O
labeled O
by O
their O
author O
as O
either O
pro O
or O
con O
the O
debate O
’s O
topic O
. O

Usually O
, O
the O
arguments O
start O
with O
a O
bolded O
, O
one O
- O
sentence O
summary O
, O
which O
serves O
as O
the O
argument O
’s O
claim O
. O

We O
exclusively O
use O
these O
claims O
and O
pair O
them O
to O
the O
debate O
’s O
topic O
. O

We O
ex- O
clude O
16 O
debates O
whose O
topics O
contain O
vsoror(e.g O
. O

Democrats O
vs. O
Republicans O
) O
, O
and O
30 O
debates O
with- O
out O
a O
title O
question O
. O

To O
create O
a O
balanced O
dataset O
that O
covers O
a O
large O
variety O
of O
topics O
, O
we O
randomly O
selected O
5 O
pro O
and O
5 O
con O
arguments O
of O
each O
debate O
. O

If O
a O
debate O
contains O
less O
than O
5 O
pro O
and O
5 O
con O
ar- O
guments O
, O
we O
select O
the O
maximum O
equal O
number O
of O
pro O
and O
con O
arguments O
. O

We O
obtain O
190 O
different O
topics O
and O
1894 O
arguments O
. O

4.2 O
Crowdsourcing O
Study O
The O
annotation O
task O
consisted O
of O
the O
debate O
’s O
topic O
, O
one O
of O
its O
claims O
, O
and O
two O
questions O
. O

The O
ﬁrst O
ques- O
tion O
was O
to O
select O
the O
stance O
of O
the O
claim O
towards O
the O
topic O
, O
out O
of O
the O
following O
choices O
: O
in O
favor O
, O
against O
, O
neither O
andI O
do O
n’t O
know O
. O

Although O
we O
have O
the O
original O
arguments O
’ O
stances O
, O
this O
question O
helps O
us O
check O
how O
clear O
the O
claim O
is O
when O
taken O
out O
of O
the O
debate O
’s O
context O
. O

The O
second O
question O
was O
whether O
the O
claim O
refers O
to O
a O
consequence O
re- O
lated O
to O
the O
topic O
, O
with O
possible O
answers O
yes O
, O

no O

and O
I O
do O
n’t O
know O
. O

Each O
topic O
- O
claim O
pair O
was O
annotated O
by O
10 O
annotators O
living O
in O
the O
US O
with O
a O
HIT O
ap- O
proval O
rate O
greater O
than O
98 O
% O
and O
more O
than O
10,000 O
approved O
HITs O
in O
total O
. O

Overall O
, O
277 O
annotators O
worked O
on O
the O
task O
. O

4.3 O
Agreement O
and O
Reliability O
Table O
4 O
shows O
the O
inter O
- O
annotator O
agreement O
per O
number O
of O
valid O
annotations O
, O
i.e. O
, O
annotations O
that O
are O
not O
I O
do O
n’t O
know O
. O

Since O
we O
have O
many O
anno- O
tators O
, O
Fleiss O
is O
particularly O
low O
on O
consequence O
annotation O
, O
but O
still O
indicates O
higher O
agreement O
than O
random O
. O

To O
give O
an O
agreement O
estimate O
less O
sen- O
sitive O
to O
individual O
outliers O
, O
we O
also O
compute O
0 O
as O
the O
Fleiss O
kappa O
between O
two O
“ O
experts O
” O
, O
where O
each O
expert O
brings O
together O
half O
of O
the O
number O
of O
annotators O
and O
its O
annotation O
is O
decided O
with O
MACE O
( O
Hovy O
et O
al O
. O
, O
2013 O
) O
. O

Figure O
1 O
shows O
the O
reliability O
of O
individual O
anno- O
tators O
. O

Although O
there O
is O
a O
weak O
correlation O
among O
the O
reliability O
of O
the O
two O
tasks O
( O
Pearson O
.41 O
) O
, O
some O
annotators O
are O
quite O
reliable O
in O
annotating O
stances O
, O
but O
highly O
unreliable O
in O
annotating O
consequences O
. O

This O
indicates O
that O
the O
latter O
task O
was O
unclear O
to O
some O
of O
the O
annotators O
. O

To O
understand O
why O
the O
annotators O
usually O
disagree O
, O
we O
investigated O
such O

56instances O
and O
identiﬁed O
several O
possible O
reasons O
: O
Complexity O
In O
the O
topic O
- O
claim O
pair O
Criminal- O
ization O
of O
Holocaust O
denial O
– O
Danger O
of O
public O
accepting O
holocaust O
denial O
should O
be O
fought O
by O
logic O
, O
both O
topic O
and O
claim O
have O
a O
negative O
stance O
towards O
holocaust O
denial O
, O
which O
suggests O
the O
label O
in O
favor O
. O

Still O
, O
by O
proposing O
a O
different O
solution O
than O
criminalization O
, O
the O
claim O
is O
against O
the O
topic O
. O

Missing O
Background O
Knowledge O
Many O
argu- O
ments O
involve O
non O
- O
trivial O
background O
knowledge O
: O
Israeli O
military O
assault O
in O
Gaza O
– O
Hamas O
was O
ﬁrst O
to O
escalate O
conﬂict O
following O
end O
of O
ceaseﬁre O
. O

Ambiguity O
According O
to O
the O
pair O
2009 O
US O
eco- O
nomic O
stimulus O
– O
Stimulus O
risks O
being O
too O
small O
not O
too O
large O
, O
a O
small O
stimulus O
is O
bad O
while O
an O
appro- O
priate O
stimulus O
is O
good O
. O

Ethical O
Judgement O
Different O
judgments O
on O
what O
is O
good O
and O
bad O
can O
lead O
to O
different O
stance O
labels O
: O

Ban O
on O
human O
reproductive O
cloning O
– O
Cloning O
will O
involve O
the O
creation O
of O
children O
for O
predetermined O
roles O
. O

Lack O
of O
Conceptual O
Clarity O
Especially O
decid- O
ing O
whether O
the O
claim O
refers O
to O
a O
consequence O
re- O

lated O
to O
the O
topic O
can O
be O
a O
matter O
of O
judgment O
. O

For O
example O
, O
in O
Health O
insurance O
mandates O
– O
Insur- O
ance O
mandates O
violate O
the O
rights O
of O
employers O
, O
the O
violation O
of O
rights O
can O
be O
seen O
as O
a O
consequence O
or O
as O
a O
property O
of O
insurance O
mandates O
. O

4.4 O
Final O
Dataset O
To O
account O
for O
unreliable O
annotators O
, O
we O
compute O
the O
annotation O
result O
with O
MACE O
. O

As O
such O
, O
we O
ﬁnd O
that O
for O
81:36 O
% O
of O
the O
annotated O
arguments O
, O
the O
stance O
label O
obtained O
via O
MACE O
is O
the O
same O
as O
the O
original O
stance O
label O
. O

By O
comparison O
, O
the O
majority O
vote O
matches O
79:30 O
% O
of O
the O
original O
stance O
labels O
. O

Since O
disagreements O
between O
the O
MACE O
annota- O
tion O
and O
the O
original O
stance O
might O
indicate O
that O
the O
claim O
’s O
stance O
is O
unclear O
outside O
the O
debate O
’s O
con- O
text O
, O
we O
exclude O
from O
the O
dataset O
all O
such O
pairs O
. O

For O
example O
, O
the O
original O
label O
of O
the O
pair O
Is O
Wikipedia O
valuable O
? O

– O
Wikipedia O
is O
online O
and O
interactive O
, O
unlike O
other O
encyclopedias O
iscon O
, O
because O
, O
in O
its O
context O
, O
it O
was O
discussed O
whether O
Wikipedia O
is O
an O
encyclopedia O
or O
not O
. O

In O
contrast O
, O
the O
result O
of O
our O
annotation O
is O
pro O
. O

Since O
the O
original O
labels O
are O
only O
proorcon O
, O
all O
pairs O
that O
our O
study O
determined O
as O
neither O
are O
removed O
. O

This O
ﬁlter O
resulted O
in O
a O
total O
of O
1502 O
pairs O
, O
out O
of O
which O
822have O
been O
annotated O
to O
relate O
to O
consequences O
. O

: O
Class O
distributions O
5 O
Evaluation O
5.1 O
Data O
We O
report O
results O
both O
on O
the O
822 O
pairs O
that O
relate O
to O
consequences O
, O
denoted O
by O
conseq O
, O
and O
on O
the O
rest O
of O
the O
pairs O
, O
denoted O
by O
other O
, O
as O
well O
as O
on O
their O
union O
, O
denoted O
by O
debate O
. O

For O
checking O
the O
performance O
of O
the O
systems O
on O
an O
independent O
dataset O
, O
we O
also O
use O
the O
claim B-DatasetName
stance I-DatasetName
dataset O
published O
by O
Bar O
- O
Haim O
et O
al O
. O
( O
2017a O
) O
. O

This O
dataset O
contains O
55 O
topics O
of O
ide- O
bate O
and O
2394 O
manually O
collected O
claims O
from O
Wikipedia O
. O

We O
denote O
this O
dataset O
by O
wiki O
. O

As O
Bar- O
Haim O
et O
al O
. O

( O
2017a O
, O
b O
) O
do O
, O
when O
working O
with O
this O
dataset O
, O
we O
use O
only O
the O
topic O
’s O
target O
and O
not O
the O
entire O
topic O
to O
ensure O
comparability O
. O

Table O
5 O
shows O
the O
class O
distribution O
of O
the O
datasets O
. O

5.2 O
Compared O
systems O
We O
evaluate O
our O
system O
with O
the O
effect B-MethodName
lexicon I-MethodName
lexi- I-MethodName
con I-MethodName
that O
we O
describe O
in O
Section O
3.1 O
( O
ECF B-MethodName
) O
, O
as O
well O
as O
with O
the O
+ B-MethodName
/ I-MethodName
- I-MethodName
EffectWordNet I-MethodName
( O
EWN B-MethodName
) O
. O

For O
compar- O
ison O
, O
we O
implement O
two O
other O
approaches O
: O
sent B-MethodName
As O
a O
baseline O
, O
we O
use O
a O
system O
that O
simply O
sums O
up O
all O
the O
sentiment O
scores O
in O
the O
claim O
. O

For O
the O
wiki O
dataset O
, O
the O
sign O
is O
switched O
if O
the O
topic O
sentiment O
is O
negative O
. O

BERT B-MethodName
As O
state O
of O
the O
art O
, O
we O
use O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
which O
was O
recently O
shown O
to O
outper- O
form O
a O
series O
of O
alternative O
stance O
detection O
sys- O
tems O
( O
Ghosh O
et O
al O
. O
, O
2019 O
) O
. O

We O
ﬁne O
- O
tune O
BERT B-MethodName
us- O
ing O
the O
large O
, O
uncased O
pre O
- O
trained O
weights.10Just O
as O
Schiller O
et O
al O
. O

( O
2020 O
) O
, O
we O
set O
the O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
to O
5 B-HyperparameterValue
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
16 B-HyperparameterValue
. O

The O
input O
are O
topic- O
claim O
pairs O
. O

We O
perform O
10 O
- O
fold O
cross O
- O
validation O
with O
a O
train B-HyperparameterName
- I-HyperparameterName
dev I-HyperparameterName
- I-HyperparameterName
test I-HyperparameterName
ratio I-HyperparameterName
of O
( O
70/20/10 B-MetricValue
) O
, O
ensuring O
that O
each O
topic O
exclusively O
occurs O
in O
one O
set O
. O

5.3 O
Results O
and O
Discussion O
The O
results O
that O
compare O
our O
system O
to O
BERT O
and O
the O
sentiment O
detection O
baseline O
are O
presented O
in O
Table O
6 O
. O

First O
, O
as O
expected O
, O
our O
system O
performs O
better O
on O
arguments O
related O
to O
consequences O
than O
on O
other O
arguments O
, O
with O
a O
macro B-MetricName
- I-MetricName
F1 I-MetricName
difference O
of O
10pp B-MetricValue
between O
conseq O
andother O
. O

Further O
, O
our O
sys- O
tem O
with O
both O
lexicon O
settings O
consistently O
outper- O
forms O
the O
sent O
baseline O
, O
but O
its O
macro B-MetricName
- I-MetricName
F1 I-MetricName
score O
is O
outperformed O
by O
BERT B-MethodName
on O
conseq O
and O
wiki O
, O
and O
its O
accuracy B-MetricName
is O
outperformed O
by O
BERT B-MethodName
on O
all O
datasets O
. O

This O
is O
not O
surprising O
, O
given O
that O
we O
use O
BERT B-MethodName
pre O
- O
trained O
and O
then O
ﬁne O
- O
tuned O
to O
our O
data O
. O

In- O
terestingly O
, O
our O
system O
with O
ECF B-MethodName
achieves O
better O
results O
than O
BERT B-MethodName
in O
terms O
of O
macro B-MetricName
F1 I-MetricName
score O
on O
the O
arguments O
that O
are O
not O
related O
to O
consequences O
( O
other O
) O
, O
and O
on O
the O
complete O
debate O
dataset O
. O

This O
indicates O
that O
our O
method O
can O
deal O
reasonably O
well O
with O
arguments O
that O
are O
not O
from O
consequences O
. O

Concerning O
the O
two O
stance O
classes O
, O
with O
both O
lexicon O
settings O
, O
our O
system O
is O
better O
than O
BERT B-MethodName
at O
predicting O
the O
proclass O
in O
arguments O
from O
con- O
sequences O
, O
but O
is O
outperformed O
on O
the O
conclass O
. O

Another O
interesting O
result O
is O
that O
on O
conseq O
, O
our O
system O
has O
a O
quite O
similar O
performance O
on O
the O
pro O
andconclasses O
with O
both O
lexicon O
settings O
. O

In O
con- O
trast O
, O
BERT B-MethodName
’s O
performance O
varies O
drastically O
, O
with O
a O
difference O
of O
approximately O
17pp B-MetricName
in O
favor O
of O
the O
conclass O
. O

BERT B-MethodName
’s O
high O
variability O
is O
also O
indicated O
by O
the O
high O
standard O
deviation O
on O
the O
10 O
folds O
. O

For O
comparison O
, O
we O
also O
computed O
the O
F1 B-MethodName
macro B-MetricName 
stan- O
dard O
deviation O
of O
our O
system O
with O
ECF B-MethodName
when O
run O
on O
the O
same O
10 O
folds O
, O
and O
the O
values O
lie O
between O
: O
.03 B-MetricValue
on O
debate O
and O
.07 B-MetricValue
on O
conseq O
. O

This O
indicates O
that O
our O
unsupervised O
approach O
is O
more O
robust O
with O
more O
predictable O
performance O
. O

Concerning O
the O
two O
effect O
lexicons O
, O
our O
system O
performs O
consistently O
better O
when O
using O
ECF B-MethodName
than O
when O
using O
EWN B-MethodName
. O

Our O
analysis O
indicates O
that O
the O
high O
coverage O
of O
the O
EWN B-MethodName
lexicon O
comes O
at O
the O
expense O
of O
accuracy B-MetricName
. O

Therefore O
, O
in O
the O
following O
, O
we O
will O
only O
refer O
to O
our O
system O
using O
ECF B-MethodName
. O

Regarding O
the O
two O
datasets O
debate O
and O
wiki O
, O
BERT B-MethodName
outperforms O
our O
system O
, O
with O
quite O
a O
high O
margin O
particularly O
on O
the O
wiki O
data O
. O

The O
accu- B-MetricName
racy I-MetricName
that O
Bar O
- O
Haim O
et O
al O
. O
( O
2017a O
, O
b O
) O
report O
on O
the O
wiki O
data O
, O
when O
no O
context O
features O
are O
used O
, O
is O
. B-MetricValue
68 I-MetricValue
which O
is O
lower O
than O
BERT B-MethodName
’s O
( O
. B-MetricValue
70 I-MetricValue
) O
but O
higher O
than O
ours O
(. B-MetricName
65 I-MetricName
for O
evaluating O
on O
the O
dedicated O
test O
set O
) O
. O

This O
is O
not O
surprising O
given O
that O
the O
data O
con- O
tains O
general O
arguments O
. O

Nevertheless O
, O
as O
our O
ap- O
proach O
only O
targets O
a O
subclass O
of O
these O
arguments O
, O
the O
results O
are O
quite O
promising O
. O

Unfortunately O
, O
Bar- O
Haim O
et O
al O
. O
( O
2017a O
, O
b O
) O
’s O
system O
is O
proprietary O
and O
we O
could O
not O
evaluate O
it O
on O
our O
conseq O
data O
. O

Table O
7 O
provides O
further O
insights O
into O
our O
solu- O
tion O
. O

First O
, O
on O
all O
Debatepedia O
based O
datasets O
, O
we O
ﬁnd O
a O
target O
in O
more O
than O
. B-MetricValue
75 I-MetricValue
of O
the O
data O
instances O
, O
and O
overall O
, O
the O
results O
are O
slightly O
better O
when O
a O
target O
is O
found O
. O

Most O
of O
the O
targets O
are O
found O
by O
word O
similarity O
and O
the O
fewest O
by O
the O
acronym O
. O

The O
results O
obtained O
on O
the O
instances O
where O
the O
target O
was O
found O
by O
synonym O
/ O
antonym O
relations O
are O
sig- O
niﬁcantly O
lower O
than O
those O
obtained O
when O
the O
target O
was O
found O
with O
the O
other O
two O
strategies O
. O

This O
in- O
dicates O
that O
the O
approach O
is O
sensitive O
to O
semantic O
drift O
in O
target O
identiﬁcation O
. O

Overall O
, O
we O
identify O
a O
potential O
consequence O
( O
TPO O
/ O
TP O
/ O
PO O
) O
for O
. B-MetricValue
6 I-MetricValue
of O
the O
arguments O
in O
conseq O
. O

While O
the O
results O
are O
quite O
good O
on O
all O
datasets O
when O
we O
detect O
a O
complete O
effect O
triple O
( O
TPO O
) O
, O
they O
are O
overtaken O
by O
results O
of O
the O
TPcases O
. O

Together O
, O
the O
instances O
solved O
with O
TPO O
andTPstrategies O
amount O
to O
. B-MetricValue
44 I-MetricValue
of O
the O
conseq O
dataset O
but O
to O
much O
lower O
on O
the O
other O
datasets O
( O
e.g. O
, O
only O
. B-MetricValue
17 I-MetricValue
on O
the O
wiki O
) O
. O

The O
performance O
on O
the O
POcases O
is O
com- O
parable O
to O
the O
performance O
on O
the O
Heuristic O
cases O
, O
and O
signiﬁcantly O
lower O
than O
when O
TPO O
orTPcould O
be O
applied O
. O

Depending O
on O
the O
dataset O
, O
the O
system O
needed O
to O
apply O
the O
Heuristic O
strategy O
on O
. B-MetricValue
4 I-MetricValue
to O
.61 B-MetricValue
of O
the O
instances O
. O

Our O
efforts O
for O
future O
work O
are O
directed O
towards O
helping O
the O
system O
make O
sense O
of O
more O
of O
the O
claims O
so O
that O
the O
number O
of O
times O
it O
needs O
to O
fallback O
to O
POandHeuristic O
are O
reduced O
. O

5.4 O
Error O
Analysis O
To O
better O
understand O
the O
limitations O
of O
our O
ap- O
proach O
, O
we O
analyzed O
the O
errors O
on O
the O
conseq O
data O
and O
found O
several O
reasons O
for O
wrong O
predictions O
: O
Incomplete O
list O
of O
patterns O
Some O
arguments O
can O
not O
be O
meaningfully O
analyzed O
with O
our O
current O
list O
of O
patterns O
. O

We O
plan O
to O
extend O
this O
list O
with O
more O
complex O
patterns O
, O
while O
we O
are O
also O
working O
on O
automatically O
learning O
such O
patterns O
from O
data O
. O

Conceptual O
errors O
We O
assume O
that O
positive O
ef- O
fects O
on O
something O
negative O
result O
in O
something O
negative O
( O
e.g. O
, O
War O
in O
Iraq O
has O
helped O
terrorist O
re- O
cruitment O
. O
) O
. O

However O
, O
this O
is O
not O
always O
the O
case O
( O
e.g. O
, O
Privatizing O
social O
security O
helps O
the O
poor O
. O
) O
. O

Finding O
the O
targets O
As O
shown O
in O
Table O
7 O
, O
we O
often O
fail O
to O
detect O
targets O
. O

For O
example O
, O
our O
tar- O
get O
detection O
strategies O
fail O
on O
the O
claim O
- O
topic O
pair O
Standardized O
tests O
ensure O
students O
learn O
essential O
information O
. O

– O
No O
Child O
Left O
Behind O
Act O
. O

In O
this O
speciﬁc O
case O
, O
there O
is O
a O
hypernym O
relation O
between O
the O
topic O
and O
Standardized O
tests O
. O

Further O
, O
we O
found O
that O
our O
straightforward O
approach O
to O
identifying O
targets O
and O
the O
relations O
between O
them O
is O
one O
of O
the O
core O
reasons O
for O
our O
approach O
’s O
poorer O
perfor- O
mance O
on O
the O
wikidata O
compared O
to O
the O
debate O
data O
. O

Improving O
the O
target O
ﬁnding O
strategy O
by O
leveraging O
additional O
semantic O
knowledge O
is O
one O
of O
the O
core O
directions O
for O
our O
future O
work O
. O

Missing O
/ O
wrong O
lexicon O
entries O
For O
many O
words O
, O
we O
are O
missing O
an O
entry O
in O
our O
lexicons O
, O
or O
the O
entry O
exists O
but O
is O
questionable O
. O

For O
instance O
, O
in O
the O
sentiment O
lexicon O
, O
Palestinian O
is O
annotated O
with O
a O
negative O
sentiment O
. O

Also O
, O
sometimes O
the O
effect O
on O
the O
object O
seems O
to O
be O
mixed O
up O
with O
the O
word O
’s O
overall O
effect O
. O

For O
example O
, O
solve O
has O
a O
pos O
- O
itive O
effect O
on O
the O
object O
in O
both O
ECF O
and O
EWN O
lexicons O
, O
but O
arguably O
when O
a O
problem O
is O
solved O
, O
it O
undergoes O
a O
reduction O
( O
e.g. O
Reforestation O
, O
[ O
... O
] O
can O
help O
solve O
global O
warming O
) O
. O

Ambiguity O
Some O
words O
have O
a O
positive O
or O
nega- O
tive O
effect O
depending O
on O
the O
sense O
with O
which O
they O
are O
used O
( O
e.g. O
, O
push O
vs.push O
for O
) O
. O

In O
the O
effect O
lexi- O
con O
, O
we O
have O
only O
one O
entry O
per O
word O
. O

In O
the O
EWN B-MethodName
, O
there O
are O
multiple O
senses O
, O
but O
we O
always O
use O
the O
most O
probable O
effect O
. O

Word O
sense O
disambiguation O
is O
required O
for O
these O
cases O
, O
which O
is O
known O
to O
be O
very O
challenging O
for O
verbs O
. O

However O
, O
a O
potential O
solution O
could O
be O
to O
annotate O
VerbNet O
frames O
with O
effects O
, O
but O
this O
is O
outside O
the O
scope O
of O
this O
work O
. O

Text O
parsing O
errors O
As O
our O
method O
relies O
on O
the O
output O
of O
the O
dependency O
parser O
, O
the O
Lemma- O
tizer O
, O
the O
POS O
tagger O
, O
and O
the O
Stemmer O
, O
their O
errors O
naturally O
propagate O
. O

6 O
Conclusion O
and O
Future O
Work O
We O
propose O
a O
fully O
unsupervised O
method O
to O
detect O
the O
stance O
of O
arguments O
from O
consequences O
in O
on- O
line O
debates O
. O

The O
method O
exploits O
grammatical O
dependencies O
and O
lexicons O
to O
identify O
effect O
words O
and O
their O
impact O
. O

For O
our O
evaluation O
, O
we O
annotated O
arguments O
from O
Debatepedia O
regarding O
their O
stance O
and O
whether O
they O
involve O
consequences O
or O
not O
. O

The O
results O
we O
obtained O
are O
motivating O
. O

Our O
method O
is O
comparable O
to O
BERT O
while O
being O
more O
robust O
. O

Besides O
the O
future O
extensions O
of O
this O
approach O
that O
we O
mentioned O
in O
our O
results O
discussion O
and O
error O
analysis O
, O
this O
work O
opens O
several O
interesting O
research O
paths O
. O

Mainly O
, O
its O
good O
performance O
on O
the O
claims O
that O
refer O
to O
consequences O
reinforces O
our O
intuition O
that O
designing O
systems O
tailored O
for O
partic- O
ular O
argumentation O
schemes O
might O
be O
a O
good O
alter- O
native O
to O
topic O
- O
speciﬁc O
models O
. O

Therefore O
, O
we O
plan O
to O
complement O
this O
work O
with O
approaches O
for O
other O
frequently O
applied O
schemes O
such O
as O
arguments O
by O
expert O
opinion O
andarguments O
by O
example O
. O

Acknowledgments O
This O
work O
has O
been O
funded O
by O
the O
Deutsche O
Forschungsgemeinschaft O
( O
DFG O
) O
within O
the O
project O
ExpLAIN O
, O
Grant O
Number O
STU O
266/14 O
- O
1 O
, O
as O
part O
of O
the O
Priority O
Program O
” O
Robust O
Argumentation O
Machines O
( O
RATIO O
) O
” O
( O
SPP-1999 O
) O
. O

59References O
Aseel O
Addawood O
, O
Jodi O
Schneider O
, O
and O
Masooda O
Bashir O
. O
2017 O
. O
Stance O
classiﬁcation O
of O
twitter O
debates O
: O
The O
encryption O
debate O
as O
a O
use O
case O
. O

In O
Proceedings O
of O
the O
8th O
International O
Conference O
on O
Social O
Media O
& O
Society O
, O
# O
SMSociety17 O
, O
New O
York O
, O
NY O
, O
USA O
. O
Asso- O
ciation O
for O
Computing O
Machinery O
. O

Khalid O
Al O
- O
Khatib O
, O
Yufang O
Hou O
, O
Henning O
Wachsmuth O
, O
Charles O
Jochim O
, O
Francesca O
Bonin O
, O
and O
Benno O
Stein O
. O
2020 O
. O
End O
- O
to O
- O
end O
argumentation O
knowledge O
graph O
construction O
. O
In O
Proceedings O
of O
the O
Thirty O
- O
Fourth O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
( O
AAAI O
2020 O
) O
. O

Pranav O
Anand O
, O
Marilyn O
Walker O
, O
Rob O
Abbott O
, O
Jean O
E. O
Fox O
Tree O
, O
Robeson O
Bowmani O
, O
and O
Michael O
Minor O
. O
2011 O
. O

Cats O
rule O
and O
dogs O
drool O
! O
: O
Classifying O
stance O
in O
online O
debate O
. O
In O
Proceedings O
of O
the O
2nd O
Work- O
shop O
on O
Computational O
Approaches O
to O
Subjectivity O
and O
Sentiment O
Analysis O
( O
WASSA O
2.011 O
) O
, O
pages O
1 O
– O
9 O
, O
Portland O
, O
Oregon O
. O
Association O
for O
Computational O
Linguistics O
. O

Roy O
Bar O
- O
Haim O
, O
Indrajit O
Bhattacharya O
, O
Francesco O
Din- O
uzzo O
, O
Amrita O
Saha O
, O
and O
Noam O
Slonim O
. O
2017a O
. O
Stance O
classiﬁcation O
of O
context O
- O
dependent O
claims O
. O
InProceedings O
of O
the O
15th O
Conference O
of O
the O
Euro- O
pean O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Volume O
1 O
, O
Long O
Papers O
, O
pages O
251–261 O
, O
Valencia O
, O
Spain O
. O

Association O
for O
Computational O
Lin- O
guistics O
. O

Roy O
Bar O
- O
Haim O
, O
Lilach O
Edelstein O
, O
Charles O
Jochim O
, O
and O
Noam O
Slonim O
. O
2017b O
. O
Improving O
claim O
stance O
clas- O
siﬁcation O
with O
lexical O
knowledge O
expansion O
and O
con- O
text O
utilization O
. O
In O
Proceedings O
of O
the O
4th O
Work- O
shop O
on O
Argument O
Mining O
. O
Association O
for O
Compu- O
tational O
Linguistics O
. O

Yoonjung O
Choi O
and O
Janyce O
Wiebe O
. O
2014 O
. O
+ O
/- O
EffectWordNet O
: O
Sense O
- O
level O
lexicon O
acquisition O
for O
opinion O
inference O
. O
In O
Proceedings O
of O
the O
2014 O
Con- O
ference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
1181–1191 O
, O
Doha O
, O
Qatar O
. O
Association O
for O
Computational O
Linguistics O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O
Bert O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understand- O
ing O
. O

Kuntal O
Dey O
, O
Ritvik O
Shrivastava O
, O
and O
Saroj O
Kaushik O
. O
2018 O
. O
Topical O
stance O
detection O
for O
twitter O
: O
A O
two- O
phase O
lstm O
model O
using O
attention O
. O
In O
Advances O
in O
Information O
Retrieval O
, O
pages O
529–536 O
, O
Cham O
. O
Springer O
International O
Publishing O
. O

Jiachen O
Du O
, O
Ruifeng O
Xu O
, O
Yulan O
He O
, O
and O
Lin O
Gui O
. O
2017 O
. O
Stance O
classiﬁcation O
with O
target O
- O
speciﬁc O
neural O
at- O
tention O
. O
In O
Proceedings O
of O
the O
Twenty O
- O
Sixth O
Inter- O
national O
Joint O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
IJCAI-17 O
, O
pages O
3988–3994. O

Adam O
Faulkner O
. O
2014 O
. O
Automated O
classiﬁcation O
of B-TaskName 
stance O
in O
student O
essays O
: O
An O
approach O
using O
stance O
target O
information O
and O
the O
wikipedia O
link O
- O
based O
mea- O
sure O
. O
Proceedings O
of O
the O
27th O
International O
Florida O
Artiﬁcial O
Intelligence O
Research O
Society O
Conference O
, O
FLAIRS O
2014 O
, O
pages O
174–179 O
. O

Christiane O
Fellbaum O
. O
2010 O
. O
Princeton O
university O
: O
About O
wordnet O
. O

Shalmoli O
Ghosh O
, O
Prajwal O
Singhania O
, O
Siddharth O
Singh O
, O
Koustav O
Rudra O
, O
and O
Saptarshi O
Ghosh O
. O
2019 O
. O
Stance O
detection O
in O
web O
and O
social O
media O
: O
A O
comparative O
study O
. O
In O
Experimental O
IR O
Meets O
Multilinguality O
, O
Multimodality O
, O
and O
Interaction O
, O
pages O
75–87 O
, O
Cham O
. O
Springer O
International O
Publishing O
. O

Subrata O
Ghosh O
, O
Konjengbam O
Anand O
, O
Sailaja O
Rajanala O
, O
A O
Bharath O
Reddy O
, O
and O
Manish O
Singh O
. O
2018 O
. O
Unsu- O
pervised O
Stance O
Classiﬁcation O
in O
Online O
Debates O
. O
In O
Proceedings O
of O
the O
ACM O
India O
Joint O
International O
Conference O
on O
Data O
Science O
and O
Management O
of O
Data O
, O
CoDS O
- O
COMAD O
’ O
18 O
, O
pages O
30–36 O
, O
New O
York O
, O
NY O
, O
USA O
. O
ACM O
. O
Event O
- O
place O
: O
Goa O
, O
India O
. O

Kazi O
Saidul O
Hasan O
and O
Vincent O
Ng O
. O
2013 O
. O
Extra- O
linguistic O
constraints O
on O
stance O
recognition O
in O
ideo- O
logical O
debates O
. O
In O
Proceedings O
of O
the O
51st O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
816–821 O
, O
Soﬁa O
, O
Bulgaria O
. O
Association O
for O
Computational O
Lin- O
guistics O
. O

Dirk O
Hovy O
, O
Taylor O
Berg O
- O
Kirkpatrick O
, O
Ashish O
Vaswani O
, O
and O
Eduard O
Hovy O
. O
2013 O
. O
Learning O
whom O
to O
trust O
with O
MACE O
. O
In O
Proceedings O
of O
the O
2013 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
1120–1130 O
, O
Atlanta O
, O
Georgia O
. O
Association O
for O
Computational O
Linguistics O
. O

Minqing O
Hu O
and O
Bing O
Liu O
. O
2004 O
. O
Mining O
and O
sum- O
marizing O
customer O
reviews O
. O
In O
Proceedings O
of O
the O
Tenth O
ACM O
SIGKDD O
International O
Conference O
on O
Knowledge O
Discovery O
and O
Data O
Mining O
, O
KDD O
’ O
04 O
, O
page O
168–177 O
, O
New O
York O
, O
NY O
, O
USA O
. O
Association O
for O
Computing O
Machinery O
. O

Anand O
Konjengbam O
, O
Subrata O
Ghosh O
, O
Nagendra O
Kumar O
, O
and O
Manish O
Singh O
. O
2018 O
. O
Debate O
stance O
classiﬁca- O
tion O
using O
word O
embeddings O
. O
In O
Big O
Data O
Analytics O
and O
Knowledge O
Discovery O
, O
pages O
382–395 O
, O
Cham O
. O
Springer O
International O
Publishing O
. O

Dilek O
K O
¨uc O
¸¨uk O
and O
Fazli O
Can O
. O
2020 O
. O
Stance O
detection O
: O
A O
survey O
. O
ACM O
Comput O
. O
Surv O
. O
, O
53(1 O
) O
. O

Christopher O
Manning O
, O
Mihai O
Surdeanu O
, O
John O
Bauer O
, O
Jenny O
Finkel O
, O
Steven O
Bethard O
, O
and O
David O
McClosky O
. O
2014 O
. O
The O
Stanford O
CoreNLP O
Natural O
Language O
Processing O
Toolkit O
. O

In O
Proceedings O
of O
52nd O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
: O
System O
Demonstrations O
, O
pages O
55–60 O
, O
Bal- O
timore O
, O
Maryland O
. O
Association O
for O
Computational O
Linguistics O
. O

Saif O
Mohammad O
, O
Svetlana O
Kiritchenko O
, O
Parinaz O
Sob- O
hani O
, O
Xiaodan O
Zhu O
, O
and O
Colin O
Cherry O
. O
2016 O
. O
SemEval-2016 O
task O
6 O
: O
Detecting O
stance O
in O
tweets O
. O

InProceedings O
of O
the O
10th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval-2016 O
) O
, O
pages O
31 O
– O
41 O
, O
San O
Diego O
, O
California O
. O
Association O
for O
Computa- O
tional O
Linguistics O
. O

Akiko O
Murakami O
and O
Rudy O
Raymond O
. O
2010 O
. O
Sup- O
port O
or O
oppose O
? O
classifying O
positions O
in O
online O
de- O
bates O
from O
reply O
activities O
and O
opinion O
expressions O
. O
InColing O
2010 O
: O
Posters O
, O
pages O
869–875 O
, O
Beijing O
, O
China O
. O
Coling O
2010 O
Organizing O
Committee O
. O

Pavithra O
Rajendran O
, O
Danushka O
Bollegala O
, O
and O
Simon O
Parsons O
. O
2016 O
. O
Contextual O
stance O
classiﬁcation O
of B-TaskName 
opinions O
: O
A O
step O
towards O
enthymeme O
reconstruction O
in O
online O
reviews O
. O
In O
Proceedings O
of O
the O
Third O
Work- O
shop O
on O
Argument O
Mining O
( O
ArgMining2016 O
) O
, O
pages O
31–39 O
, O
Berlin O
, O
Germany O
. O
Association O
for O
Computa- O
tional O
Linguistics O
. O

Hannah O
Rashkin O
, O
Sameer O
Singh O
, O
and O
Yejin O
Choi O
. O
2016 O
. O
Connotation O
Frames O
: O
A O
Data O
- O
Driven O
Investigation O
. O
InProceedings O
of O
the O
54th O
Annual O
Meeting O
of O
the O
As- O
sociation O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
311–321 O
. O
Association O
for O
Com- O
putational O
Linguistics O
. O
Event O
- O
place O
: O
Berlin O
, O
Ger- O
many O
. O

Paul O
Reisert O
, O
Naoya O
Inoue O
, O
Tatsuki O
Kuribayashi O
, O
and O
Kentaro O
Inui O
. O
2018 O
. O
Feasible O
Annotation O
Scheme O
for O
Capturing O
Policy O
Argument O
Reasoning O
using O
Ar- O
gument O
Templates O
. O
In O
Proceedings O
of O
the O
5th O
Work- O
shop O
on O
Argument O
Mining O
, O
pages O
79–89 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguis- O
tics O
. O

Benjamin O
Schiller O
, O
Johannes O
Daxenberger O
, O
and O
Iryna O
Gurevych O
. O
2020 O
. O
Stance O
detection O
benchmark O
: O
How O
robust O
is O
your O
stance O
detection O
? O

Parinaz O
Sobhani O
, O
Saif O
Mohammad O
, O
and O
Svetlana O
Kir- O
itchenko O
. O
2016 O
. O
Detecting O
stance O
in O
tweets O
and O
ana- O
lyzing O
its O
interaction O
with O
sentiment O
. O
In O
Proceedings O
of O
the O
Fifth O
Joint O
Conference O
on O
Lexical O
and O
Com- O
putational O
Semantics O
, O
pages O
159–169 O
, O
Berlin O
, O
Ger- O
many O
. O
Association O
for O
Computational O
Linguistics O
. O

Swapna O
Somasundaran O
and O
Janyce O
Wiebe O
. O
2009 O
. O
Rec- O
ognizing O
Stances O
in O
Online O
Debates O
. O
In O
Proceed- O
ings O
of O
the O
Joint O
Conference O
of O
the O
47th O
Annual O
Meeting O
of O
the O
ACL O
and O
the O
4th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
of O
the O
AFNLP O
: O
Volume O
1 O
- O
Volume O
1 O
, O
ACL O
’ O
09 O
, O
pages O
226 O
– O
234 O
, O
Stroudsburg O
, O
PA O
, O
USA O
. O

Association O
for O
Com- O
putational O
Linguistics O
. O

Event O
- O
place O
: O
Suntec O
, O
Singa- O
pore O
. O

Swapna O
Somasundaran O
and O
Janyce O
Wiebe O
. O
2010 O
. O
Rec- O
ognizing O
stances O
in O
ideological O
on O
- O
line O
debates O
. O
In O
Proceedings O
of O
the O
NAACL O
HLT O
2010 O

Workshop O
on O
Computational O
Approaches O
to O
Analysis O
and O
Genera- O
tion O
of O
Emotion O
in O
Text O
, O
pages O
116–124 O
, O
Los O
Ange- O
les O
, O
CA O
. O
Association O
for O
Computational O
Linguistics O
. O

Dhanya O
Sridhar O
, O
Lise O
Getoor O
, O
and O
Marilyn O
Walker O
. O
2014 O
. O
Collective O
stance O
classiﬁcation O
of O
posts O
in O
online O
debate O
forums O
. O
In O
Proceedings O
of O
the O
Joint O
Workshop O
on O
Social O
Dynamics O
and O
Personal O
At- O
tributes O
in O
Social O
Media O
, O
pages O
109–117 O
, O
Baltimore O
, O
Maryland O
. O
Association O
for O
Computational O
Linguis- O
tics O
. O

Qingying O
Sun O
, O
Zhongqing O
Wang O
, O
Qiaoming O
Zhu O
, O
and O
Guodong O
Zhou O
. O
2018 O
. O
Stance O
detection O
with O
hierar- O
chical O
attention O
network O
. O
In O
Proceedings O
of O
the O
27th O
International O
Conference O
on O
Computational O
Linguis- O
tics O
, O
pages O
2399–2409 O
, O
Santa O
Fe O
, O
New O
Mexico O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O

Matt O
Thomas O
, O
Bo O
Pang O
, O
and O
Lillian O
Lee O
. O
2006 O
. O
Get O
out O
the O
vote O
: O
Determining O
support O
or O
opposition O
from O
congressional O
ﬂoor O
- O
debate O
transcripts O
. O
In O
Proceed- O
ings O
of O
the O
2006 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
327–335 O
, O
Sydney O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O

Orith O
Toledo O
- O
Ronen O
, O
Roy O
Bar O
- O
Haim O
, O
Alon O
Halfon O
, O
Charles O
Jochim O
, O
Amir O
Menczel O
, O
Ranit O
Aharonov O
, O
and O
Noam O
Slonim O
. O
2018 O
. O
Learning O
sentiment O
com- O
position O
from O
sentiment O
lexicons O
. O
In O
Proceedings O
of O
the O
27th O
International O
Conference O
on O
Computa- O
tional O
Linguistics O
, O
pages O
2230–2241 O
, O
Santa O
Fe O
, O
New O
Mexico O
, O
USA O
. O
Association O
for O
Computational O
Lin- O
guistics O
. O

Douglas O
Walton O
, O
Christopher O
Reed O
, O
and O
Fabrizio O
Macagno O
. O
2008 O
. O
Argumentation O
Schemes O
. O
Cam- O
bridge O
University O
Press O
. O

Rui O
Wang O
, O
Deyu O
Zhou O
, O
Mingmin O
Jiang O
, O
Si O
Jiasheng O
, O
and O
Yang O
Yang O
. O
2019 O
. O
A O
survey O
on O
opinion O
mining O
: O
from O
stance O
to O
product O
aspect O
. O
IEEE O
Access O
, O
PP:1 O
– O
1 O
. O

Theresa O
Wilson O
, O
Janyce O
Wiebe O
, O
and O
Paul O
Hoffmann O
. O
2005 O
. O
Recognizing O
Contextual O
Polarity O
in O
Phrase- O
level O
Sentiment O
Analysis O
. O
In O
Proceedings O
of O
the O
Conference O
on O
Human O
Language O
Technology O
and O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
HLT O
’ O
05 O
, O
pages O
347–354 O
, O
Stroudsburg O
, O
PA O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Event- O
place O
: O
Vancouver O
, O
British O
Columbia O
, O
Canada O
. O

DeepRapper B-MethodName
: O
Neural O
Rap B-TaskName
Generation I-TaskName
with O
Rhyme O
and O
Rhythm O
Modeling O
Lanqing O
Xue1 O
, O
Kaitao O
Song2 O
, O
Duocai O
Wu3 O
, O
Xu O
Tan4 O
, O
Nevin O
L. O
Zhang1 O
, O
Tao O
Qin4 O
, O
Wei O
- O
Qiang O
Zhang5 O
, O
Tie O
- O
Yan O
Liu4 O
1The O
Hong O
Kong O
University O
of O
Science O
and O
Technology O
2Nanjing O
University O
of O
Science O
and O
Technology O
3Fudan O
University4Microsoft O

Research O
Asia5Tsinghua O
University O
flxueaa O
, O
lzhang O
g@cse.ust.hk O
kt.song@njust.edu.cn O
dcwu18@fudan.edu.cn O
fxuta O
, O
taoqin O
, O
tyliu O
g@microsoft.com O
wqzhang@tsinghua.edu.cn O
Abstract O
Rap B-TaskName
generation I-TaskName
, O
which O
aims O
to O
produce O
lyrics O
and O
corresponding O
singing O
beats O
, O
needs O
to O
model O
both O
rhymes O
and O
rhythms O
. O

Previous O
works O
for O
rap B-TaskName
generation I-TaskName
focused O
on O
rhyming O
lyrics O
but O
ignored O
rhythmic O
beats O
, O
which O
are O
important O
for O
rap O
performance O
. O

In O
this O
paper O
, O
we O
develop O
DeepRapper B-MethodName
, O
a O
Transformer O
- O
based O
rap O
generation O
system O
that O
can O
model O
both O
rhymes O
and O
rhythms O
. O

Since O
there O
is O
no O
avail- O
able O
rap O
dataset O
with O
rhythmic O
beats O
, O
we O
de- O
velop O

a O
data O
mining O
pipeline O
to O
collect O
a O
large- O
scale O
rap O
dataset O
, O
which O
includes O
a O
large O
num- O
ber O
of O
rap O
songs O
with O
aligned O
lyrics O
and O
rhyth- O
mic O
beats O
. O

Second O
, O
we O
design O
a O
Transformer- O
based O
autoregressive O
language O
model O
which O
carefully O
models O
rhymes O
and O
rhythms O
. O

Specif- O

ically O
, O
we O
generate O
lyrics O
in O
the O
reverse O
or- O
der O
with O
rhyme O
representation O
and O
constraint O
for O
rhyme O
enhancement O
and O
insert O
a O
beat O
sym- O
bol O
into O
lyrics O
for O
rhythm O
/ O
beat O
modeling O
. O

To O
our O
knowledge O
, O
DeepRapper B-MethodName
is O
the O
ﬁrst O
sys- O
tem O
to O
generate O
rap O
with O
both O
rhymes O
and O
rhythms O
. O

Both O
objective O
and O
subjective O
evalu- O
ations O
demonstrate O
that O
DeepRapper B-MethodName
generates O
creative O
and O
high O
- O
quality O
raps O
with O
rhymes O
and O
rhythms O
. O

1 O
Introduction O
Rap O
is O
a O
musical O
form O
originating O
from O
America O
in O
1970s O
, O
and O
has O
quickly O
developed O
as O
one O
of O
the O
mainstream O
music O
genres O
in O
the O
world O
( O
Keyes O
, O
2004 O
) O
. O

With O
the O
rapid O
development O
of O
artiﬁcial O
intelligence O
, O
automatic O 
rap B-TaskName
lyrics I-TaskName
generation I-TaskName
has O
drawn O
attention O
from O
academia O
( O
Potash O
et O

al O
. O
, O
2015 O
; O
Malmi O
et O
al O
. O
, O
2016 O
; O
Liang O
et O
al O
. O
, O
2018 O
; O
Nikolov O
et O
al O
. O
, O
2020 O
) O

. O

Generally O
speaking O
, O
rap O
lyrics O
need O
to O
be O
semantically O
meaningful O
and O
fashionable O
to O
con- O
vey O
interesting O
stories O
or O
express O
feelings O
. O

Different O
from O
natural O
language O
or O
other O
artistic O
genres O
( O
e.g. O
, O
 O
Corresponding O
author O
: O
Xu O
Tan O
, O
xuta@microsoft.comlyrics O
or O
poetry O
) O
, O
rap O
has O
distinctive O
characteris- O
tics O
: O
1 O
) O
it O
usually O
contains O
complex O
rhyme O
patterns O
among O
several O
consecutive O
sentences O
, O
which O
are O
the O
key O
to O
form O
a O
good O
ﬂow O
; O
2 O
) O
it O
needs O
to O
align O
with O
the O
singing O
beat O
since O
rap O
lyrics O
are O
usually O
rapped O
according O
to O
some O
rhythmic O
accompani- O
ments O
. O

Therefore O
, O
how O
to O
generate O
rap O
lyrics O
with O
good O
rhymes O
and O
rhythms O
is O
a O
troublesome O
prob- O
lem O
. O

Previous O
works O
( O
Potash O
et O
al O
. O
, O
2015 O
; O
Malmi O
et O
al O
. O
, O
2016 O
; O
Liang O
et O
al O
. O
, O
2018 O
; O
Nikolov O
et O
al O
. O
, O
2020 O
) O
for O
rap B-TaskName
generation I-TaskName
mainly O
focused O
on O
lyric O
generation O
and O
some O
of O
them O
developed O
strategies O
for O
rhyme O
modeling O
. O

Potash O
et O

al O
. O

( O
2015 O
) O
directly O
added O
a O
“ O
< O
endLine O
> O
” O
token O
at O
the O
end O
of O
verse O
lines O
and O
ex- O
pected O
to O
learn O
rhyme O
patterns O
implicitly O
. O

Nikolov O
et O
al O
. O

( O
2020 O
) O
applied O
a O
two O
- O
step O
strategy O
, O
which O
ﬁrst O
generates O
rap O
lyrics O
and O
then O
adds O
rhyme O
tokens O
to O
the O
end O
of O
generated O
lyrics O
. O

However O
, O
these O
meth- O
ods O
can O
not O
guarantee O
the O
rhyme O
patterns O
for O
every O
lyric O
line O
and O
only O
care O
the O
rhyme O
on O
the O
last O
token O
. O

Although O
many O
works O
have O
studied O
rhyming O
mod- O
eling O
in O
other O
artistic O
genres O
( O
e.g. O
, O
poetry O
) O
( O
Li O
et O
al O
. O
, O
2020 O
; O
Van O
de O
Cruys O
, O
2020 O
; O
Liu O
et O
al O
. O
, O
2020 O
) O
, O
they O
are O
not O
suitable O
for O
rap B-TaskName
generation I-TaskName
due O
to O
the O
com- O
plex O
rhyme O
structure O
in O
rap O
. O

For O
example O
, O
poetry O
needs O
to O
rhyme O
with O
only O
the O
last O
word O
in O
each O
sen- O
tence O
, O
while O
rap O
rhymes O
with O
multiple O
consecutive O
tokens O
at O
the O
end O
of O
each O
sentence O
. O

No O
previous O
works O
have O
studied O
rhythm O
model- O

ing O
( O
i.e. O
, O
beats O
in O
rap O
) O
, O
to O
our O
knowledge O
. O

One O
of O
the O
main O
reasons O
is O
the O
lack O
of O
rap O
datasets O
with O
beat O
- O
lyric O
alignment O
. O

Consequently O
, O
the O
generation O
of O
lyrics O
without O
rhythmic O
beats O
can O
not O
be O
regarded O
as O
a O
full O
rap B-TaskName
generation I-TaskName
. O

In O
this O
paper O
, O
we O
develop O
DeepRapper B-MethodName
, O
a O
Trans- O
former O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
based O
rap O
generation O
system O
which O
can O
model O
both O
rhymes O
and O
rhythms O
. O

To O
build O
the O
system O
, O
since O
there O
is O
no O
available O
rap O
datasets O
with O
aligned O
rhythmic O
beats O
, O
we O
design O
a O

70data O
mining O
pipeline O
and O
collect O
a O
large O
- O
scale O
rap O
dataset O
for O
rhythm O
modeling O
. O

Speciﬁcally O
, O
we O
ﬁrst O
crawl O
many O
rap O
songs O
, O
each O
song O
with O
both O
rap O
lyrics O
and O
audios O
, O
from O
the O
Web O
. O

For O
each O
crawled O
rap O
song O
, O
we O
perform O
a O
series O
of O
data O
preprocessing O
steps O
to O
extract O
rhythmic O
beats O
as O
well O
as O
beat O
- O
lyric O
alignment O
. O

To O
better O
model O
rhyme O
, O
we O
generate O
the O
words O
in O
a O
rap O
sentence O
from O
right O
to O
left O
in O
an O
au- O
toregressive O
manner O
. O

Doing O
so O
we O
can O
easily O
iden- O
tify O
the O
last O
few O
words O
of O
a O
sentence O
( O
now O
become O
the O
ﬁrst O
words O
of O
the O
reverse O
sentence O
) O
to O
rhyme O
with O
. O

Additionally O
, O
we O
incorporate O
several O
rhyme- O
related O
representations O
into O
our O
language O
model O
to O
further O
improve O
the O
rhyming O
quality O
, O
and O
encourage O
theN O
- O
gram O
rhyme O
in O
generated O
rap O
lyrics O
through O
rhyme O
constraint O
during O
inference O
. O

We O
use O
a O
spe- O
cial O
token O

[ O
BEAT O
] O
to O
represent O
the O
rhythmic O
beat O
and O
insert O
it O
into O
lyrics O
right O
before O
the O
correspond- O
ing O
word O
. O

In O
this O
way O
, O
we O
can O
model O
the O
beat O
in O
the O
lyric O
sequence O
both O
in O
training O
and O
generation O
. O

Inspired O
by O
the O
success O
of O
pre O
- O
trained O
language O
models O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Radford O
et O
al O
. O
, O
2018 O
; O
Yang O
et O
al O
. O
, O
2019 O
; O
Song O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
we O
incorporate O
pre O
- O
training O
into O
our O
sys- O
tem O
. O

To O
obtain O
large O
- O
scale O
data O
for O
pre O
- O
training O
, O
we O
also O
use O
our O
data O
mining O
pipeline O
to O
collect O
another O
two O
datasets O
: O
1 O
) O
non O
- O
rap O
songs O
with O
aligned O
beats O
, O
which O
can O
be O
larger O
than O
rap O
dataset O
since O
non O
- O
rap O
songs O
are O
more O
general O
than O
rap O
songs O
; O
2 O
) O
pure O
lyrics O
, O
which O
can O
be O
even O
larger O
than O
non O
- O
rap O
songs O
. O

In O
the O
pre O
- O
training O
stage O
, O
we O
pre O
- O
train O
our O
Deep- B-MethodName
Rapper I-MethodName
model O
based O
on O
the O
above O
two O
datasets O
. O

Then O
we O
ﬁne O
- O
tune O
our O
pre O
- O
trained O
model O
on O
the O
rap O
songs O
with O
aligned O
beats O
. O

The O
ﬁne O
- O
tuned O
model O
is O
used O
for O
ﬁnal O
rap B-TaskName
generation I-TaskName
. O

Both O
objective O
and O
subjective O
evaluations O
verify O
the O
advantages O
of O
DeepRapper B-MethodName
in O
generating O
rap O
lyrics O
with O
rhymes O
and O
rhythms O
. O

Our O
main O
contributions O
can O
be O
summarized O
as O
follows O
: O
•To O
model O
rhythms O
in O
rap B-TaskName
generation I-TaskName
, O
we O
de- O
velop O

a O
data O
mining O
pipeline O
to O
create O
rap O
datasets O
with O
aligned O
rhythmic O
beats O
. O

•To O
better O
model O
rhymes O
, O
we O
design O
an O
autore- O
gressive O
language O
model O
to O
generate O
rap O
lyrics O
from O
right O
to O
left O
with O
rhyme O
constraint O
. O

As O
far O
as O
we O
know O
, O
DeepRapper B-MethodName
is O
the O
ﬁrst O
to O
explicitly O
model O
N O
- O
gram O
rhymes O
. O

•We O
elaborately O
insert O
the O
beat O
token O
inside O
lyrics O
to O
model O
the O
rhythmic O
beats O
. O

To O
ourknowledge O
, O
DeepRapper B-MethodName
is O
the O
ﬁrst O
system O
that O
models O
rhythms O
for O
rap B-TaskName
generation I-TaskName
. O

2 O
Background O
Since O
DeepRapper B-MethodName
generates O
rap O
lyrics O
with O
both O
rhyme O
and O
rhythm O
modeling O
, O
in O
this O
section O
, O
we O
brieﬂy O
introduce O
the O
related O
background O
: O
lyric O
gen- O
eration O
, O
rhyme O
modeling O
and O
rhythm O
modeling O
. O

Lyric O
Generation O
Broadly O
speaking O
, O
lyric O
gen- O
eration O
can O
cover O
rap B-TaskName
lyric I-TaskName
generation I-TaskName
( O
Potash O
et O
al O
. O
, O
2015 O
; O
Nikolov O
et O
al O
. O
, O
2020 O
; O

Liang O
et O
al O
. O
, O
2018 O
) O
, O
song O
lyric O
generation O
( O
Watanabe O
et O
al O
. O
, O
2018 O
; O
Lu O
et O
al O
. O
, O
2019 O
; O
Chen O
and O
Lerch O
, O
2020 O
; O
Sheng O
et O
al O
. O
, O
2020 O
) O
, O
general O
poetry O
generation O
( O
Zhang O
and O
Lapata O
, O
2014 O
; O
Lau O
et O
al O
. O
, O
2018 O
; O
Li O
et O
al O
. O
, O
2020 O
; O
Liu O
et O
al O
. O
, O
2020 O
) O
and O
etc O

. O

Different O
from O
previous O
works O
that O
leverage O
language O
model O
to O
generate O
lyrics O
similar O
to O
natural O
language O
, O
in O
this O
paper O
, O
we O
introduce O
a O
novel O
lan- O
guage O
model O
for O
rap B-TaskName
generation I-TaskName
, O
with O
well O
- O
designed O
rhyme O
and O
rhythm O
modeling O
to O
ﬁt O
the O
characteris- O
tics O
of O
rap O
lyrics O
. O

Additionally O
, O
inspired O
by O
the O
successes O
of O
pre O
- O
trained O
language O
models O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
; O
Radford O
et O
al O
. O
, O
2019 O
; O
Song O
et O
al O
. O
, O
2019 O
) O
in O
NLP O
applications O
, O
we O
also O
incorporate O
pre O
- O
training O
into O
our O
model O
to O
further O
improve O
the O
quality O
of O
rap B-TaskName
generation I-TaskName
. O

Rhyme O
Modeling O
Rhyme O
modeling O
plays O
an O
im- O
portant O
role O
in O
rap B-TaskName
generation I-TaskName
, O
which O
requires O
the O
last O
few O
tokens O
in O
consecutive O
sentences O
to O
have O
the O
same O
rhyme O
pattern O
. O

Existing O
rap O
genera- O
tion O
systems O
either O
directly O
add O
a O
special O
token O
“ O
< O
endLine O
> O
” O
at O
the O
end O
of O
rap O
lyric O
to O
encour- O
age O
the O
model O
to O
learn O
rhyme O
structure O
( O
Potash O
et O

al O
. O
, O
2015 O
) O
, O
or O
introduce O
a O
two O
- O
step O
strategy O
for O
rhyme O
modeling O
that O
ﬁrst O
generates O
rap O
lyrics O
and O
then O
adds O
rhyme O
tokens O
after O
the O
generated O
lyrics O
( O
Nikolov O
et O
al O
. O
, O
2020 O
) O
. O

However O
, O
these O
works O
only O
focused O
on O
unigram O
rhyme O
while O
rap O
appre- O
ciates O
more O
for O
n O
- O
gram O
rhyme O
. O

Although O
a O
lot O
of O
works O
have O
explored O
rhyme O
modeling O
in O
other O
gen- O
res O
, O
most O
of O
them O
can O
not O
be O
directly O
used O
for O
rap B-TaskName
generation I-TaskName
. O

For O
example O
, O
poetry O
generation O
( O
Lau O
et O
al O
. O
, O
2018 O
; O
Zhipeng O
et O

al O
. O
, O
2019 O
; O
Liao O
et O
al O
. O
, O
2019 O
; O
Li O
et O
al O
. O
, O
2020 O
) O
usually O
used O
pre O
- O
deﬁned O
format O
to O
control O
the O
rhyme O
pattern O
since O
poetry O
usually O
has O
ﬁxed O
number O
of O
words O
and O
only O
cares O
the O
rhyme O
pattern O
for O
the O
last O
word O
. O

However O
, O
rap O
lyrics O
have O
diverse O
rhyme O
structures O
across O
multiple O
consecu- O
tive O
sentences O
and O
most O
importantly O
multiple O
con- O
secutive O
words O
. O

Therefore O
, O
we O
introduce O
N O
- O
gram O
rhyme O
modeling O
in O
DeepRapper B-MethodName
to O
handle O
the O
dis- O
tinctive O
rhyme O
patterns O
in O
rap O
. O

Besides O
, O
we O
also O
train O
our O
language O
model O
in O
a O
reverse O
order O
( O
i.e. O
, O
right O
to O
left O
) O
, O
similar O
to O
previous O
works O
( O
Van O
de O
Cruys O
, O
2020 O
) O
, O
to O
better O
model O
rhymes O
since O
they O
always O
occur O
at O
the O
end O
of O
sentence O
. O

Rhythm O
Modeling O
Rhythm O
modeling O
is O
usually O
used O
in O
music O
generation O
( O
Zhu O
et O
al O
. O
, O
2018 O
; O
Huang O
and O
Yang O
, O
2020 O
; O

Ren O
et O
al O
. O
, O
2020 O
) O
which O
generates O
the O
duration O
of O
notes O
along O
with O
the O
note O
pitch O
to O
form O
rhythmic O
beats O
in O
melody O
and O
accompaniment O
generation O
. O

Different O
from O
music O
generation O
, O
rap O
cares O
more O
about O
rhythmic O
beats O
instead O
of O
note O
pitches O
( O
i.e. O
melody O
) O
. O

In O
this O
way O
, O
the O
generated O
rap O
lyrics O
need O
to O
align O
with O
the O
corresponding O
rhythmic O
beats O
in O
order O
to O
be O
rapped O
, O
otherwise O
it O
can O
not O
be O
regarded O
as O
a O
complete O
rap O
. O

However O
, O
to O
the O
best O
of O
our O
knowledge O
, O
none O
of O
previous O
works O
have O
studied O
the O
rhythm O
modeling O
in O
rap B-TaskName
generation I-TaskName
. O

In O
this O
paper O
, O
we O
introduce O
a O
novel O
beat O
modeling O
strategy O
in O
DeepRapper B-MethodName
for O
rhythm B-TaskName
generation I-TaskName
. O

3 O
Rap O
Dataset O
Mining O
Previous O
works O
( O
Potash O
et O
al O
. O
, O
2015 O
; O
Liang O
et O
al O
. O
, O
2018 O
; O
Nikolov O
et O
al O
. O
, O
2020 O
) O
for O
rap B-TaskName
generation I-TaskName
usu- O
ally O
used O
rap O
datasets O
with O
only O
lyrics O
, O
without O
con- O
sidering O
the O
rhythmic O
beat O
information O
. O

To O
model O
rhythm O
in O
rap B-TaskName
generation I-TaskName
, O
the O
rap O
dataset O
should O
contain O
lyrics O
with O
aligned O
rhythmic O
beats O
. O

How- O
ever O
, O
beat O
alignments O
are O
quite O
difﬁcult O
to O
obtain O
, O
since O
their O
annotations O
require O
musicians O
with O
pro- O
fessional O
knowledge O
to O
identify O
stressing O
syllable O
in O
rap O
songs O
. O

To O
handle O
this O
problem O
, O
we O
design O
a O
data O
mining O
pipeline O
to O
automatically O
extract O
beat- O
lyric O
alignments O
. O

In O
this O
section O
, O
we O
introduce O
the O
details O
of O
the O
data O
mining O
pipeline O
and O
our O
mined O
dataset O
based O
on O
this O
pipeline O
. O

3.1 O
Data O
Mining O
Pipeline O
Figure O
1 O
overviews O
our O
data O
mining O
pipeline O
, O
which O
consists O
of O
5 O
steps O
: O
data O
crawling O
, O
vocal O
and O
accom- O
paniment O
separation O
, O
vocal O
and O
lyric O
alignment O
, O
beat O
detection O
, O
and O
lyric O
and O
beat O
alignment O
. O

Data O
Crawling O
To O
mine O
a O
large O
- O
scale O
rap O
dataset O
, O
we O
ﬁrst O
crawl O
a O
large O
amount O
of O
rap O
songs O
with O
both O
lyrics O
and O
singing O
audios O
from O
the O
Web O
. O

To O
ensure O
the O
lyric O
and O
audio O
can O
be O
aligned O
in O
the O
sentence O
level O
which O
is O
beneﬁcial O
for O
our O
later O
word O
- O
level O
beat O
alignment O
, O
we O
also O
crawl O
the O
start O
and O
end O
time O
of O
each O
lyric O
sentence O
corresponding O
to O
the O
audio O
. O

Vocal O
and O
Accompaniment O
Separation O
For O
each O
rap O
song O
, O
we O
utilize O
Spleeter O
( O
Hennequin O
et O
al O
. O
, O
2020)1 O
, O
a O
public O
music O
separation O
tool O
, O
to O
separate O
the O
vocal O
( O
containing O
rap O
singing O
) O
and O
accompani- O
ment O
( O
containing O
rhythmic O
beats O
) O
from O
the O
crawled O
rap O
audio O
. O

Vocal O
and O
Lyric O
Alignment O

We O
split O
the O
sepa- O
rated O
vocals O
into O
the O
sentence O
level O
according O
to O
the O
crawled O
start O
and O
end O
time O
of O
each O
lyric O
sen- O
tence O
, O
and O
thus O
we O
can O
get O
the O
vocal O
- O
lyric O
align- O
ments O
in O
the O
sentence O
level O
. O

We O
convert O
lyrics O
into O
phonemes O
via O
Phonemizer2and O
utilize O
Montreal O
Forced O
Aligner3to O
obtain O
vocal O
- O
lyric O
alignments O
in O
the O
phoneme O
level O
. O

Based O
on O
these O
phoneme O
- O
level O
vocal O
- O
lyric O
alignments O
, O
we O
obtain O
the O
correspond- O
ing O
timestamp O
of O
each O
word O
in O
the O
singing O
audio O
. O

Beat O
Detection O

To O
obtain O
the O
alignments O
be- O
tween O
lyrics O
and O
beats O
, O
we O
need O
to O
know O
the O
times- O
tamp O
of O
each O
beat O
. O

Therefore O
, O
we O
use O
a O
beat O
track O
detection O
tool O
, O
Librosa O
( O
McFee O
et O
al O
. O
, O
2020)4 O
, O
to O
track O
the O
timestamp O
of O
each O
beat O
from O
the O
separated O
accompaniment O
that O
obtained O
from O
the O
second O
step O
. O

Lyric O
and O
Beat O
Alignment O
After O
we O
obtain O
the O
timestamp O
of O
each O
word O
and O
each O
beat O
, O
we O
can O
align O
them O
together O
according O
to O
their O
timestamps O
. O

How- O
ever O
, O
since O
a O
rapper O
may O
not O
sing O
a O
word O
exactly O
following O
the O
beat O
, O
directly O
using O
the O
timestamp O
to O
exactly O
match O
the O
word O
and O
beat O
is O
inappropriate O
. O

Therefore O
, O
we O
propose O
an O
approximate O
method O
to O
align O
them O
. O

Denote O
the O
word O
sequence O
of O
a O
lyric O
1https://github.com/deezer/spleeter O
2https://github.com/bootphon/phonemizer O
3https://github.com/MontrealCorpusTools/Montreal- O
Forced O
- O
Aligner O
4https://github.com/librosa/librosa O

72sentence O
as O
W O
= O
fw1;w2;;wjWjg O
, O
and O
its O
beat O
sequence O
as O
B O
= O
fb1;b2;;bjBjg O
, O
wherewiand O
bjrepresenti O
- O
th O
word O
and O
j O
- O
th O
beat O
. O

We O
use O
Twi O
andTbjto O
represent O
the O
timestamps O
of O
wiandbj O
respectively O
. O

For O
each O
beat O
bj O
, O
we O
ﬁrst O
ﬁlter O
out O
a O
word O
set O
~W O

= O
fw O
: O


Tbj Tw O


r=2;w2Wg O
, O
whererrepresents O
the O
average O
duration O
of O
each O
word O
in O
the O
song O
( O
i.e. O
, O
the O
total O
duration O
divides O
the O
number O
of O
words O
) O
. O

Next O
, O
word O
wiis O
aligned O
with O
beatbjif O
it O
satisﬁes O
the O
following O
condition O
: O
wi= O
min O
wjTbj Twj;w2 O
~ O
W O
: O
( O
1 O
) O

3.2 O
Mined O
Datasets O
Using O
the O
above O
data O
mining O
pipeline O

, O
we O
obtain O
a O
rap O
lyric O
dataset O
with O
aligned O
beats O
( O
named O
as O
D B-DatasetName
- I-DatasetName
RAP I-DatasetName
, O
where O
D O
represents O
“ O
dataset O
” O
) O
, O
which O
sat- O
isﬁes O
the O
requirements O
of O
building O
a O
rap O
genera- O
tion O
system O
with O
both O
rhyme O
and O
rhythm O
model- O
ing O
. O

We O
split O
the O
D B-DatasetName
- I-DatasetName
RAP B-DatasetName
dataset O
into O
the O
training B-HyperparameterName
and I-HyperparameterName
validation I-HyperparameterName
set I-HyperparameterName
with I-HyperparameterName
a I-HyperparameterName
ratio I-HyperparameterName
of O
4:1 B-HyperparameterValue
. O

Since O
rap O
is O
only O
one O
of O
music O
genres O
and O
the O
number O
of O
rap O
songs O
is O
usually O
smaller O
compared O
with O
more O
general O
songs O
, O
we O
also O
mine O
another O
two O
datasets O
to O
pre O
- O
train O
our O
DeepRapper B-MethodName
model O
with O
the O
same O
mining O
pipeline O
: O
1 O
) O
non O
- O
rap O
songs O
with O
aligned O
beats O
( O
named O
as O
D B-DatasetName
- I-DatasetName
SONG I-DatasetName
) O
; O
2 O
) O
pure O
lyrics O
without O
aligned O
beats O
( O
named O
as O
D B-DatasetName
- I-DatasetName
LYRIC I-DatasetName
) O
. O

We O
summarize O
the O
statistics O
of O
the O
three O
datasets O
in O
Table O
1 O
and O
show O
a O
rap O
song O
with O
aligned O
beats O
from O
D O
- O
Rap O
in O
Figure O
2 O
. O
4 O
Rap O
Generation O
Model O

In O
this O
section O
, O
we O
introduce O
the O
architecture O
of O
our O
rap O
generation O
model O
, O
and O
the O
details O
of O
its O
rhyme O
modeling O
and O
rhythm O
modeling O
. O

4.1 O
Model O
Overview O
Figure O
3 O
illustrates O
the O
detailed O
architecture O
of O
our O
rap O
generation O
model O
. O

We O
use O
Trans- O
former O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
to O
build O
an O
autoregres- O
sive O
language O
model O
( O
Radford O
et O
al O
. O
, O
2018 O
, O
2019 O
) O
for O
rap B-TaskName
generation I-TaskName
, O
and O
introduce O
several O
new O
de- O
signs O
: O
1 O
) O
To O
better O
model O
rhymes O
, O
our O
model O
gen- O
erates O
a O
sentence O
from O
right O
to O
left O
, O
since O
rhyming O
words O
are O
always O
at O
the O
end O
of O
the O
sentence O
; O
2 O
) O
As O
aforementioned O
, O
rhythms O
are O
critical O
for O
rap O
per- O
formance O
, O
so O
we O
insert O
a O
special O
token O
[ O
BEAT O
] O
for O
explicit O
beat O
modeling O
; O
3 O
) O
Unlike O
original O
Trans- O
former O
with O
only O
word O
embedding O
and O
positional O
embedding O
, O
we O
add O
multiple O
additional O
embed- O
dings O
to O
better O
model O
rhymes O
and O
rhythms O
. O

Next O
, O
we O
introduce O
our O
rhyme O
modeling O
in O
subsection O
4.2 O
and O
rhythm O
modeling O
in O
subsection O
4.3 O
. O

4.2 O
Rhyme O
Modeling O
Rhymes O
are O
the O
key O
to O
form O
a O
good O
rap O
ﬂow O
. O

In O
DeepRapper B-MethodName
, O
we O
model O
rhymes O
with O
three O
compo- O
nents O
: O
1 O
) O
reverse O
- O
order O
language O
model O
; O
2 O
) O
rhyme O
representation O
; O
and O
3 O
) O
rhyme O
constraint O
. O

4.2.1 O
Reverse O
- O
Order O
Language O
Model O

Rhyming O
words O
usually O
occur O
at O
the O
end O
of O
each O
lyric O
sentence O
. O

If O
using O
a O
standard O
autoregressive O
language O
model O
and O
generating O
tokens O
from O
left O
to O
right O
, O
we O
need O
to O
identify O
whether O
the O
current O
gen- O
eration O
step O
is O
the O
end O
of O
a O
sentence O
, O
which O
decides O
whether O
to O
generate O
rhyming O
words O
to O
be O
consis- O
tent O
with O
that O
in O
previous O
sentences O
. O

Therefore O
, O
to O
better O
model O
rhymes O
, O
we O
use O
a O
reverse O
- O
order O
lan- O
guage O
model O
to O
generate O
sentences O
from O
right O
to O
left O
, O
as O
shown O
in O
Figure O
3 O
. O

Doing O
so O
we O
can O
easily O
identify O
the O
last O
few O
words O
of O
a O
sentence O
( O
now O
be- O
come O
the O
ﬁrst O
few O
words O
of O
the O
reverse O
sentence O
) O
to O
control O
their O
rhymes O
. O

Note O
that O
we O
only O
reverse O
words O
inside O
a O
sentence O
, O
and O
still O
generate O
different O
sentences O
in O
the O
original O
order O
. O

Figure O
4 O
compares O
the O
sentences O
in O
left O
- O
to O
- O
right O
order O
and O
right O
- O
to O
- O
left O
order O
, O
from O
which O
we O
can O
see O
that O
rhyming O
words O
of O
each O
sentence O
share O
the O
same O
relative O
positions O
( O
offset O
to O
the O
ﬁrst O
token O
) O
in O
the O
reverse O
order O
, O
and O
are O
easy O
to O
model O
and O
control O
. O

4.2.2 O
Rhyme O
Representation O
Rhyming O
words O
have O
two O
important O
features O
: O
1 O
) O
its O
vowel O
that O
used O
for O
rhyming O
and O
2 O
) O
its O
relative O
posi- O
tion O
in O
a O
sentence O
to O
decide O
the O
correspondence O
be- O

tween O

the O
rhyming O
words O
in O
consecutive O
sentences O
( O
e.g. O
, O
in O
the O
reverse O
order O
setting O
, O
the O
ﬁrst O
/ O
second O
word O
of O
the O
current O
sentence O
should O
be O
rhymed O
with O
the O
ﬁrst O
/ O
second O
word O
in O
the O
previous O
sentence O
) O
. O

We O
use O
the O
vowel O
in O
the O
Pinyin5of O
Chinese O
char- O
acters O
to O
represent O
their O
rhymes O
. O

To O
this O
end O
, O
we O
build O
a O
vowel O
dictionary O
F()to O
identify O
the O
vowel O
of O
each O
word O
. O

As O
shown O
in O
Figure O
3 O
, O
we O
add O
an O
ad- O
ditional O
vowel O
embedding O
Fand O
an O
intra O
- O
sentence O
relative O
positional O
embedding O
Rto O
enhance O
rhyme O
representation O
for O
each O
token O
. O

Besides O
, O
to O
better O
identify O
different O
sentences O
, O
we O
introduce O
a O
sen- O
tence O
embedding O
Sto O
differentiate O
different O
sen- O
tences O
. O

4.2.3 O
Rhyme O
Constraint O

In O
addition O
to O
reverse O
- O
order O
language O
model O
and O
rhyme O
representation O
, O
we O
also O
introduce O
rhyme O
con- O
straint O
to O
improve O
the O
quality O
of O
rhyme O
generation O
in O
inference O
. O

As O
shown O
in O
Figure O
4 O
, O
sentences O
in O
rap O
lyrics O
not O
only O
rhyme O
with O
the O
last O
token O
, O
but O
also O
with O
multiple O
consecutive O
tokens O
at O
the O
end O
. O

We O
call O
this O
phenomenon O
as O
N O
- O
gram O
rhymes O
, O
which O
mean O
the O
current O
sentence O
and O
the O
previous O
sentence O
keep O
the O
same O
rhyme O
for O
the O
last O
Nconsecutive O
tokens O
. O

To O
our O
knowledge O
, O
no O
previous O
work O
has O
inves- O
tigatedN O
- O
gram O
rhymes O
( O
N O
> O
1 O
) O
, O
although O
it O
is O
important O
to O
improve O
rap O
quality O
. O

Our O
proposed O
rhyme O
constraint O
enables O
our O
model O
to O
adjust O
the O
probability O
of O
next O
predicted O
token O
to O
further O
en- O

courageN O
- O
gram O
rhyme O
generation O
. O

The O
constraint O
is O
introduced O
as O
follows O
. O

To O
generate O
the O
i O
- O
th O
wordwiin O
the O
standard O
inference O
procedure O
, O
we O
usually O
choose O
the O
pre- O
dicted O
token O
with O
the O
maximum O
probability O
, O
i.e. O
, O
wi= O
arg O
max O
p(wjw O
< O
i; O
) O
, O
wherew O
< O
idenotes O
the O
words O
before O
position O
iin O
the O
reverse O
sentence O
andis O
the O
model O
. O

When O
the O
words O
before O
posi- O

74tioniof O
the O
current O
and O
previous O
sentence O
have O
the O
same O
rhyme O
pattern O
, O
we O
will O
use O
an O
adjusted O
probability O
distribution O
~p(wjw O
< O
i;)to O
encourage O
thei O
- O
th O
generated O
word O
to O
be O
rhymed O
according O
to O
thei O
- O
th O
word O
in O
the O
previous O
sentence O
, O
so O
as O
to O
formN O
- O
gram O
rhymes O
. O

The O
adjusted O
probability O
distribution O
~p(wjw O
< O
i;)is O
: O
~p(wjw O
< O
i; O
) O

= O

p(wjw O
< O
i; O
) O
+ O
( O
1  O

) O
(w O
) O
( O
2 O
) O
where(w)is O
a O
vowel O
check O
function O
and O

is O
a O
hyper O 
- O
parameter O
to O
balance B-HyperparameterName
the I-HyperparameterName
two I-HyperparameterName
terms I-HyperparameterName
. O

Here O
, O
(w)is O
1 O
if O
the O
predicted O
whas O
the O
same O
vowel O
with O
thei O
- O
th O
token O
in O
the O
previous O
sentence O
, O
other- O
wise O
0 O
. O

In O
other O
words O
, O
when O
predicting O
i O
- O
th O
token O
( O
iN O
) O
, O
we O
encourage O
our O
model O
to O
pay O
more O
attention O
for O
these O
words O
with O
same O
vowel O
with O
the O
i O
- O
th O
token O
in O
the O
previous O
sentence O
. O

In O
this O
way O
, O
the O
model O
tends O
to O
generate O
N O
- O
gram O
rhymes O
with O
large O
N. O
4.3 O
Rhythm O
Modeling O
Generating O
lyrics O
with O
aligned O
beats O
is O
necessary O
since O
rap O
lyrics O
need O
to O
be O
rapped O
with O
rhythmic O
beats O
. O

Therefore O
, O
we O
model O
and O
generate O
rhythmic O
beats O
along O
with O
the O
lyrics O
with O
a O
speciﬁc O
symbol O
: O
we O
regard O
beat O
as O
a O
special O
token O

[ O
BEAT O
] O
and O
insert O
it O
into O
lyric O
sequences O
for O
model O
training O
. O

As O
shown O
in O
Figure O
3 O
, O
we O
insert O

[ O
BEAT O
] O
before O
its O
aligned O
words O
like O
the O
following O
examples O
: O
“ O
我[BEAT O
] O
抬 O
头[BEAT O
] O
仰望。天空[BEAT O
] O
的苍[BEAT O
] O
茫 O
。 O
” O
. O

Rap O
usually O
contains O
different O
beat O
frequencies O
, O
i.e. O
, O
the O
ratios O
between O
the O
total O
number O
of O
words O
and O
the O
total O
number O
of O
beats O
in O
a O
rap O
song O
. O

To O
explicitly O
model O
and O
generate O
rap O
with O
different O
beat O
frequencies O
, O
we O
use O
three O
tokens O
[ O
S],[M O
] O
, O
and O
[ O
F]to O
represent O
the O
slow O
, O
medium O
and O
fast O
beat O
frequencies O
and O
add O
the O
corresponding O
tokens O
at O
the O
start O
of O
a O
rap O
song O
for O
training O
and O
inference O
. O

In O
our O
D B-DatasetName
- I-DatasetName
RAP I-DatasetName
dataset O
, O
the O
distribution O
of O
beat O
frequency O
is O
displayed O
in O
Figure O
5 O
. O

According O
to O
the O
distribution O
, O
we O
assign O
[ O
S],[M O
] O
, O
and O
[ O
F]to O
songs O
with O
beat O
frequency O
less O
than O
3 O
, O
equal O
to O
3 O
, O
and O
greater O
than O
3 O
respectively O
. O

5 O
Experimental O
Setup O
5.1 O
Model O
, O
Data O
, O
and O
Training O
Conﬁguration O
Our O
DeepRapper B-MethodName
model O
is O
built O
on O
the O
autoregres- O
sive O
Transformer O
decoder O
( O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Radford O
et O

al O
. O
, O
2018 O
, O
2019 O
) O
, O
where O
the O
hidden B-HyperparameterName
size I-HyperparameterName
, O
the O
number B-HyperparameterName
of I-HyperparameterName
attention I-HyperparameterName
heads I-HyperparameterName
and O
the O
number B-HyperparameterName
of I-HyperparameterName
Transformer I-HyperparameterName
layers I-HyperparameterName
are O
set O
as O
768 B-HyperparameterValue
, O
12 B-HyperparameterValue
, O
12 B-HyperparameterValue
. O

The O
dimension B-HyperparameterName
of I-HyperparameterName
all I-HyperparameterName
different I-HyperparameterName
kinds I-HyperparameterName
of I-HyperparameterName
embedding I-HyperparameterName
in O
DeepRapper B-MethodName
is O
set O
as O
768 B-HyperparameterValue
. O

Considering O
there O
is O
no O
existing O
pre O
- O
trained O
language O
model O
in O
reverse O
order O
, O
we O
do O
not O
utilize O
any O
pre O
- O
trained O
language O
models O
for O
initialization O
. O

Instead O
, O
we O
ﬁrst O
pre O
- O
train O
our O
model O
on O
D B-DatasetName
- I-DatasetName
LYRIC I-DatasetName
and O
D B-DatasetName
- I-DatasetName
SONG I-DatasetName
for O
2 B-HyperparameterValue
mil- I-HyperparameterValue
lions I-HyperparameterValue
steps B-HyperparameterName
, O
and O
then O
ﬁne O
- O
tune O
our O
model O
on O
D B-DatasetName
- I-DatasetName
RAP I-DatasetName
with O
3 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
as O
the O
size O
of O
D O
- O
RAP O
is O
smaller O
than O
our O
pre O
- O
training O
corpus O
. O

We O
convert O
each O
song O
to O
a O
sequence O
with O
a O
length O
of O
1024 B-HyperparameterValue
tokens O
by O
cutting O
longer O
sequence O
or O
padding O
shorter O
sequence O
. O

Our O
model O
is O
trained O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
8 B-HyperparameterValue
songs O
on O
4 O
NVIDIA O
TITAN O
V O
GPUs O
. O

We O
use O
Adam O
opti- O
mizer O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0:00015 B-HyperparameterValue
, O

1= O
0.9 B-HyperparameterValue
, O

2= O
0.999 B-HyperparameterValue
, O
and= O

10 6 B-HyperparameterValue
. O

We O
set O
the O
maxi- B-HyperparameterName
mum I-HyperparameterName
value I-HyperparameterName
of I-HyperparameterName
N I-HyperparameterName
- I-HyperparameterName
gram I-HyperparameterName
rhyme I-HyperparameterName
as O
3 B-HyperparameterValue
and O
the O
hyper- O
parameter O

in O
Equation O
2 O
as O
0.95 B-HyperparameterValue
. O

Samples O
are O
generated O
conditioned O
on O
a O
given O
sentence O
in O
refer- O
ence O
. O

5.2 O
Evaluation O
Metrics O
In O
this O
subsection O
, O
we O
introduce O
the O
objective O
and O
subjective O
metrics O
to O
evaluate O
the O
quality O
of O
the O
generated O
raps O
. O

Objective O
Evaluation O
We O
evaluate O
the O
gener- O
ated O
raps O
in O
terms O
of O
the O
quality O
of O
language O
, O
rhyme O
and O
rhythm O
. O

We O
choose O
ﬁve O
metrics O
to O
evaluate O
our O
model O
: O
1 O
) O
Perplexity B-MetricName
( O
PPL B-MetricName
) O
, O
a O
standard O
met- O
ric O
to O
evaluate O
the O
quality O
of O
a O
language O
model O
; O
2 O
) O
Rhyme B-MetricName
Accuracy I-MetricName
( O
RA B-MetricName
) O
, O
the O
ratio O
of O
sentences O
that O
have O
correctly O
predicted O
rhymes O
; O
3 O
) O
Rhyme B-MetricName
Den- I-MetricName
sity I-MetricName
( O 
RD B-MetricName 
) O
, O
the O
longest O
rhyme O
of O
a O
song O
, O
averaged O
over O
all O
songs O
, O
which O
is O
introduced O
by O
Malmi O
et O
al O
. O

( O
2016 O
) O
to O
measure O
the O
quality O
of O
rhyming O
ﬂuency O
; O
4)Combo- B-MetricName
N I-MetricName
, O
the O
maximum O
number O
of O
consecu- O
tive O
sentences O
with O
the O
same O
N O
- O
gram O
rhyme O
in O
a O
rap O
song O
, O
averaged O
over O
all O
songs O
, O
where O
we O
study O
N B-HyperparameterName
= O 
1 B-HyperparameterValue
, O 
2 B-HyperparameterValue
, O 
3 B-HyperparameterValue
; O
5)Beat B-MetricName
Accuracy I-MetricName
( O
BA B-MetricName
) O
, O
the O
accuracy O
of O
our O
model O
in O
beat O
prediction O
, O
under O
the O
teacher- O
forcing O
mode O
. O

Subjective O
Evaluation O
Similar O
to O
previous O
works O
( O
Zhang O
and O
Lapata O
, O
2014 O
; O
Nikolov O
et O
al O
. O
, O
2020 O
) O
in O
artistic O
creation O

, O
we O
also O
use O
human O
evalu- O
ation O
to O
accurately O
evaluate O
the O
quality O
of O
the O
gen- O
erated O
raps O
. O

We O
invite O
10 O
participants O
with O
profes- O
sional O
knowledge O
in O
music O
as O
human O
annotators O
to O
evaluate O
100 O
sampled O
raps O
. O

Each O
annotator O
is O
required O
to O
score O
from O
1 O
( O
Poor O
) O
to O
5 O
( O
Perfect O
) O
on O
the O
following O
perspectives O
: O
1 O
) O
the O
clearness O
of O
the O
theme O
of O
the O
rap O
lyrics O
; O
2 O
) O
the O
ﬂuency O
of O
the O
rap O
lyrics O
; O
3 O
) O
the O
quality O
of O
the O
rhyme O
; O
4 O
) O
the O
diversity O
of O
the O
rhyme O
. O

The O
averaged O
score O
of O
all O
annotators O
on O
all O
sampled O
raps O
is O
used O
as O
the O
evaluation O
score O
for O
each O
perspective O
. O

6 O
Experimental O
Results O
Results O
Table O
2 O
shows O
the O
objective O
and O
subjec- O
tive O
results O
of O
DeepRapper B-MethodName
compared O
with O
two O
base- O
lines O
: O
1 O
) O
Baseline O
: O
a O
standard O
autoregressive O
lan- O
guage O
model O
with O
the O
same O
model O
conﬁguration O
with O
DeepRapper B-MethodName
but O
without O
our O
proposed O
rhyme O
and O
rhythm O
modeling O
; O
2 O
) O
Baseline O
+ O
PT O
, O
using O
pre- O
training O
on O
Baseline O
. O

We O
have O
several O
observations O
from O
Table O
2 O
: O
1 O
) O
DeepRapper O
achieves O
better O
per- O
plexity O
, O
rhyme O
accuracy O
and O
rhyme O
density O
than O
the O
two O
baselines O
, O
which O
demonstrates O
the O
advantages O
of O
our O
method O
in O
generating O
high O
- O
quality O
rap O
lyrics O
with O
accurate O
and O
diverse O
rhymes O
. O

2 O
) O
DeepRap- B-MethodName
per I-MethodName
achieves O
better O
scores O
in O
all O
subjective O
metrics O
, O
demonstrating O
that O
DeepRapper B-MethodName
can O
generate O
high- O
quality O
and O
rhyming O
raps O
that O
accord O
with O
human O
taste O
. O

3 O
) O
Pre O
- O
training O
improves O
the O
performance O
of O
baseline O
in O
both O
objective O
and O
subjective O
met- O
rics O
, O
which O
indicates O
the O
importance O
of O
pre O
- O
training O
. O

However O
, O
its O
performance O
is O
still O
worse O
than O
Deep- B-MethodName
Rapper I-MethodName
. O

Ablation O
Studies O
To O
further O
validate O
the O
neces- O
sity O
of O
each O
component O
in O
DeepRapper B-MethodName
, O
we O
con- O
duct O
a O
series O
of O
ablation O
studies O
, O
including O
remov O
- O
ing O
rhyme O
modeling O
, O
rhythm O
modeling O
and O
pre- O
training O
, O
respectively O
. O

The O
results O
are O
reported O
in O
Table O
3 O
. O

We O
have O
several O
observations O
: O
1 O
) O
Remov- O
ing O
rhyme O
modeling O
affects O
rhyme O
quality O
a O
lot O
as O
it O
results O
in O
a O
dramatic O
drop O
in O
rhyme O
accuracy O
and O
rhyme O
density O
; O
2 O
) O
Removing O
each O
speciﬁc O
de- O
sign O
in O
rhyme O
modeling O
( O
i.e. O
, O
RO O
: O
reverse O
order O
language O
model O
, O
VE O
: O
vowel O
embedding O
, O
IPE O
: O
intra- O
sentence O
position O
embedding O
, O
SE O
: O
sentence O
em- O
bedding O
) O
causes O
worse O
rhyme B-MetricName
accuracy I-MetricName
and O
rhyme B-MetricName
density I-MetricName
. O

Speciﬁcally O
, O
while O
removing O
RO O
leads O
to O
a O
better O
PPL B-MetricName
since O
left O
- O
to O
- O
right O
order O
can O
be O
more O
easily O
modeled O
than O
right O
- O
to O
- O
left O
order O
according O
to O
the O
analysis O
in O
Wu O
et O
al O
. O

( O
2018 O
) O
, O
it O
causes O
large O
accuracy O
drop O
in O
rhyme O
quality O
. O

3 O
) O
Apparently O
, O
DeepRapper B-MethodName
without O
rhythm O
modeling O
can O
not O
pro- O
duce O
any O
beat O
information O
; O
4 O
) O
DeepRapper B-MethodName
without O
pre O
- O
training O
affects O
the O
perplexity B-MetricName
and O
rhyme B-MetricName
accu- I-MetricName
racy I-MetricName
a O
lot O
, O
however O
, O
obtains O
a O
higher O
rhyme B-MetricName
density I-MetricName
. O

The O
reason O
is O
that O
without O
pre O
- O
training O
, O
DeepRap- O
per O
tends O
to O
copy O
previous O
rhyme O
tokens O
due O
to O
the O

76lack O
of O
generalization O
( O
larger O
PPL B-MetricName
) O
. O

To O
verify O
this O
, O
we O
count O
the O
repetitive O
rate O
of O
rhyming O
words O
and O
found O
that O
the O
rate O
of O
DeepRapper B-MethodName
is O
23:8%while O
without O
pre O
- O
training O
is O
42:5 O
% O
, O
which O
is O
higher O
than O
using O
pre O
- O
training O
. O

The O
above O
results O
verify O
the O
effectiveness O
of O
each O
component O
in O
DeepRapper B-MethodName
. O

N O
- O
gram O
Rhyme O
To O
highlight O
the O
advantage O
of O
DeepRapper B-MethodName
in O
modeling O
N O
- O
gram O
rhyme O
, O
we O
use O
Combo- B-MetricName
N I-MetricName
to O
measure O
the O
ability O
of O
each O
design O
in O
DeepRapper B-MethodName
to O
model O
N O
- O
gram O
rhyme O
. O

The O
re- O
sults O
are O
reported O
in O
Table O
4 O
. O

We O
can O
ﬁnd O
that O
1 O
) O
The O
model O
without O
rhyme O
modeling O
can O
hardly O
generate O
good O
rhyme O
, O
regardless O
of O
the O
value O
of O
NinN O
- O
gram O
; O
2 O
) O
Removing O
rhyme O
constraint O
also O
weakens O
the O
capacity O
of O
generating O
N O
- O
gram O
rhyme O
. O

These O
results O
further O
demonstrate O
the O
importance O
of O
our O
rhyme O
modeling O
and O
rhyme O
constraint O
in O
generating O
multiple O
consecutive O
rhymes O
. O

Beat O
Frequency O

To O
better O
measure O
the O
beat O
qual- O
ity O
, O
we O
randomly O
generate O
about O
5,000 O
samples O
by O
DeepRapper B-MethodName
and O
DeepRapper B-MethodName
with O
beat O
frequency O
control O
. O

We O
propose O
the O
First B-MetricName
Order I-MetricName
Distribution I-MetricName
( O
FOD B-MetricName
) O
and O
the O
Second B-MetricName
Order I-MetricName
Distribution I-MetricName
( O
SOD B-MetricName
) O
and O
measure O
the O
distance O
( O
via O
Wasserstein O

Dis- O
tance O
( O
Vallender O
, O
1974 O
) O
) O
of O
these O
distributions O
be- O

tween O
the O
generated O
samples O
and O
our O
DRAP B-DatasetName
dataset O
. O

We O
deﬁne O
the O
interval O
of O
the O
current O
[ O
BEAT O
] O
as O
the O
number O
of O
words O
between O
the O
current O
[ O
BEAT O
] O
and O
the O
next O
[ O
BEAT O
] O
. O

Therefore O
, O
the O
FOD B-MetricName
is O
deﬁned O
as O
the O
distribution O
of O
the O
interval O
of O
the O
current O
[ O
BEAT O
] O
. O

Similarly O
, O
the O
SOD B-MetricName
is O
deﬁned O
the O
dis- O
tribution O
of O
the O
difference O
between O
the O
interval O
of O
the O
current O
[ O
BEAT O
] O
and O
the O
next O
[ O
BEAT O
] O
. O

The O
results O
of O
the O
distance O
are O
normalized O
into O
[ O
0;1 O
] O
and O
are O
reported O
in O
Table O
5 O
. O

It O
can O
be O
seen O
that O
DeepRapper B-MethodName
with O
beat O
frequency O
control O
achieves O
better O
performance O
in O
beat O
modeling O
, O
which O
indi- O
cates O
the O
importance O
of O
beat O
frequency O
control O
in O
beat O
modeling O
. O

Case O
Analyses O
on O
Generated O
Raps O
We O
list O
a O
sample O
case O
from O
our O
generated O
raps O
in O
Figure O
6 O
to O
demonstrate O
the O
good O
quality O
of O
the O
raps O
gen- O
erated O
by O
DeepRapper B-MethodName
. O

The O
sample O
is O
generated O
by O
feeding O
the O
ﬁrst O
sentence O
of O
the O
example O
in O
Fig- O
ure O
2 O
to O
DeepRapper B-MethodName
. O

As O
we O
can O
see O
, O
the O
gen- O
erated O
sample O
exhibits O
good O
theme O
, O
ﬂuency O
and O
rhyme O
. O

The O
sample O
is O
a O
rap O
with O
a O
number O
of O
1- O
gram O
, O
2 O
- O
gram O
, O
3 O
- O
gram O
, O
and O
even O
4 O
- O
gram O
rhyme O
. O

The O
generated O
lyrics O
depicts O
the O
fond O
memories O
of O
childhood O
and O
the O
beautiful O
visions O
for O
the O
fu- O
tures O
. O

We O
also O
provide O
a O
group O
of O
samples O
gener- O
ated O
with O
beat O
frequency O
control O
. O

To O
save O
space O
, O
we O
put O
them O
and O
the O
translation O
of O
all O
the O
sam- O
ples O
to O
Appendix O
. O

More O
samples O
are O
provided O
in O
https://deeprapper.github.io O
. O

7 O
Conclusions O
In O
this O
paper O
, O
we O
develop O
DeepRapper B-MethodName
, O
a O
novel O
Transformer O
- O
based O
rap O 
generation O 
system O
, O
which O
leverages O
rhyme O
modeling O
, O
rhythm O
modeling O
and O
pre O
- O
training O
for O
rap B-TaskName
generation I-TaskName
. O

Considering O
there O
is O
no O
available O
rap O
dataset O
with O
aligned O
rhythmic O
beats O
for O
rhythm O
modeling O
, O
we O
propose O
a O
data O
min- O
ing O
pipeline O
to O
mine O
a O
rap O
dataset O
with O
beat O
- O
lyric O
alignments O
. O

We O
leverage O
right O
- O
to O
- O
left O
generation O
, O
rhyme O
representation O
and O
rhyme O
constraint O
to O
bet- O
ter O
model O
rhyme O
and O
encourage O
N O
- O
gram O
rhyme O
, O
and O
explicitly O
model O
beat O
information O
by O
insert O
beat O
token O
beside O
the O
corresponding O
word O
in O
the O
lyric O
sequence O
. O

To O
our O
knowledge O
, O
DeepRapper B-MethodName
is O
the O
ﬁrst O
system O
to O
generate O
rap O
with O
both O
rhymes O
and O
rhythms O
. O

Both O
objective O
and O
subjective O
eval- O
uations O
demonstrate O
that O
DeepRapper B-MethodName
generates O
high O
- O
quality O
raps O
with O
good O
rhymes O
and O
rhythms O
. O

Thanks O
to O
the O
design O
of O
DeepRapper B-MethodName
, O
we O
can O
fur- O
ther O
build O
another O
rap O
singing O
system O
to O
sing O
out O
the O
raps O
according O
to O
the O
rhymes O
and O
rhythms O
, O
which O
we O
leave O
as O
future O
work O
. O

We O
also O
leave O
Multilingual O
DeepRapper B-MethodName
as O
future O
work O
. O

Acknowledgements O
We O
would O
like O
to O
acknowledge O
the O
anonymous O
re- O
viewers O
for O
their O
insightful O
comments O
. O

Research O
on O
this O
paper O
was O
supported O
by O
Hong O
Kong O
Research O
Grants O
Council O
under O
grant O
16204920 O
. O

Ethical O
Considerations O
The O
proposed O
framework O
can O
be O
considered O
a O
novel O
language O
model O
for O
rap B-TaskName
generation I-TaskName
in O
automatic O
artistic O
creation O
. O

Speciﬁcally O
, O
the O
proposed O
frame- O
work O
has O
been O
conﬁgured O
with O
novel O
rhyme O
mod- O
eling O
as O
rhyme O
is O
quite O
important O
in O
music O
genres O
. O

Therefore O
, O
our O
proposed O
framework O
is O
also O
bene- O
ﬁcial O
for O
generating O
other O
music O
genres O
. O

On O
the O
other O
hand O
, O
although O
we O
collect O
large O
- O
scale O
lyric O
data O
for O
pre O
- O
training O
, O
it O
still O
can O
not O
fully O
utilize O
the O
potential O
of O
pre O
- O
training O
. O

In O
the O
future O
, O
we O
expect O
to O
employ O
more O
large O
- O
scale O
data O
in O
the O
open O
domain O
plus O
the O
music O
domain O
for O
pre O
- O
training O
to O
improve O
the O
capacity O
of O
the O
language O
model O
. O

In O
addition O
, O
our O
training O
datasets O
may O
have O
biases O
, O
which O
may O
bring O
some O
potential O
risks O
of O
model O
bias O
. O

Hence O
, O
we O
encourage O
future O
works O
to O
study O
how O
to O
apply O
other O
techniques O
in O
mitigating O
similar O
problems O
in O
our O
framework O
. O

References O
Yihao O
Chen O
and O
Alexander O
Lerch O
. O

2020 O
. O

Melody- O
conditioned O
lyrics O
generation O
with O
seqgans O
. O

In O
arXiv O
. O

Tim O
Van O
de O
Cruys O
. O

2020 O
. O

Automatic O
poetry O
generation O
from O
prosaic O
text O
. O

In O
Proceedings O
of O
the O
58th O
An- O
nual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
2471–2480 O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
under- O
standing O
. O

In O
NAACL O
, O
pages O
4171–4186 O
. O

Romain O
Hennequin O
, O
Anis O
Khlif O
, O
Felix O
V O
oituret O
, O
and O
Manuel O
Moussallam O
. O

2020 O
. O

Spleeter O
: O
a O
fast O
and O
efﬁcient O
music O
source O
separation O
tool O
with O
pre- O
trained O
models O
. O

Journal O
of O
Open O
Source O
Software O
, O
5(50):2154 O
. O

Deezer O
Research O
. O

Yu O
- O
Siang O
Huang O
and O
Yi O
- O
Hsuan O
Yang O
. O

2020 O
. O

Pop O
music O
transformer O
: O
Beat O
- O
based O
modeling O
and O
generation O
of O
expressive O
pop O
piano O
compositions O
. O

In O
Proceedings O
of O
the O
28th O
ACM O
International O
Conference O
on O
Multi- O
media O
, O
page O
1180–1188 O
. O

Cheryl O
Lynette O
Keyes O
. O

2004 O
. O

Rap O
music O
and O
street O
con- O
sciousness O
, O
volume O
501 O
. O

University O
of O
Illinois O
Press O
. O

Jey O
Han O
Lau O
, O
Trevor O
Cohn O
, O
Timothy O
Baldwin O
, O
Julian O
Brooke O
, O
and O
Adam O
Hammond O
. O

2018 O
. O

Deep O
- O
speare O
: O
A O
joint O
neural O
model O
of O
poetic O
language O
, O
meter O
and O
rhyme O
. O

In O
Proceedings O
of O
the O
56th O
Annual O
Meet- O
ing O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1948–1958 O
. O

Piji O
Li O
, O
Haisong O
Zhang O
, O
Xiaojiang O
Liu O
, O
and O
Shuming O
Shi O
. O
2020 O
. O

Rigid O
formats O
controlled O
text O
generation O
. O

InACL O
, O
pages O
742–751 O
. O

Hongru O
Liang O
, O
Qian O
Li O
, O
Haozheng O
Wang O
, O
Hang O
Li O
, O
Jin- O
Mao O
Wei O
, O
and O
Zhenglu O
Yang O
. O

2018 O
. O

Attae O
- O
rl2 O
: O
At- O
tention O
based O
autoencoder O
for O
rap O
lyrics O
representa- O
tion O
learning O
. O

In O
Companion O
Proceedings O
of O
the O
The O
Web O
Conference O
2018 O
, O
pages O
7–8 O
. O

Yi O
Liao O
, O
Yasheng O
Wang O
, O
Qun O
Liu O
, O
and O
Xin O
Jiang O
. O

2019 O
. O

Gpt O
- O
based O
generation O
for O
classical O
chinese O
poetry O
. O

arXiv O
preprint O
arXiv:1907.00151 O
. O

Yinhan O
Liu O
, O
Myle O
Ott O
, O
Naman O
Goyal O
, O
Jingfei O
Du O
, O
Man- O
dar O
Joshi O
, O
Danqi O
Chen O
, O
Omer O
Levy O
, O
Mike O
Lewis O
, O
Luke O
Zettlemoyer O
, O
and O
Veselin O
Stoyanov O
. O

2019 O
. O

Roberta O
: O
A O
robustly O
optimized O
BERT O
pretraining O
ap- O
proach O
. O

CoRR O
, O
abs/1907.11692 O
. O

78Yusen O
Liu O
, O
Dayiheng O
Liu O
, O
and O
Jiancheng O
Lv O
. O
2020 O
. O

Deep O
poetry O
: O
A O
chinese O
classical O
poetry O
generation O
system O
. O

In O
Proceedings O
of O
the O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
volume O
34 O
, O
pages O
13626 O
– O
13627 O
. O

Xu O
Lu O
, O
Jie O
Wang O
, O
Bojin O
Zhuang O
, O
Shaojun O
Wang O
, O
and O
Jing O
Xiao O
. O
2019 O
. O

A O
syllable O
- O
structured O
, O
contextually- O
based O
conditionally O
generation O
of O
chinese O
lyrics O
. O

In O
PRICAI O
, O
volume O
11672 O
, O
pages O
257–265 O
. O

Eric O
Malmi O
, O
Pyry O
Takala O
, O
Hannu O
Toivonen O
, O
Tapani O
Raiko O
, O
and O
Aristides O
Gionis O
. O
2016 O
. O

Dopelearning B-MethodName
: O
A O
computational O
approach O
to O
rap B-TaskName
lyrics I-TaskName
generation I-TaskName
. O

InProceedings O
of O
the O
22nd O
ACM O
SIGKDD O
Inter- O
national O
Conference O
on O
Knowledge O
Discovery O
and O
Data O
Mining O
, O
pages O
195–204 O
. O

Brian O
McFee O
, O
Vincent O
Lostanlen O
, O
Alexandros O
Met- O
sai O
, O
Matt O
McVicar O
, O
Stefan O
Balke O
, O
Carl O
Thom O
´ O
e O
, O
Colin O
Raffel O
, O
Frank O
Zalkow O
, O
Ayoub O
Malek O
, O
Dana O
, O
Kyungyun O
Lee O
, O
Oriol O
Nieto O
, O
Jack O
Mason O
, O
Dan O
El- O
lis O
, O
Eric O
Battenberg O
, O
Scott O
Seyfarth O
, O
Ryuichi O
Ya- O
mamoto O
, O
Keunwoo O
Choi O
, O
viktorandreevichmorozov O
, O
Josh O
Moore O
, O
Rachel O
Bittner O
, O
Shunsuke O
Hidaka O
, O
Ziyao O
Wei O
, O
nullmightybofo O
, O
Dar O
´ O
ıo O
Here O
˜n´u O
, O
Fabian- O
Robert O
St O
¨oter O
, O
Pius O
Friesch O
, O
Adam O
Weiss O
, O
Matt O
V O
oll- O
rath O
, O
and O
Taewoon O
Kim O
. O
2020 O
. O

librosa O
/ O
librosa O
: O
0.8.0 O
. O

Nikola O
I. O
Nikolov O
, O
Eric O
Malmi O
, O
Curtis O
Northcutt O
, O
and O
Loreto O
Parisi O
. O

2020 O
. O

Rapformer B-MethodName
: O
Conditional O
rap B-TaskName
lyrics I-TaskName
generation I-TaskName
with O
denoising O
autoencoders O
. O

In O
Proceedings O
of O
the O
13th O
International O
Conference O
on O
Natural O
Language O
Generation O
, O
pages O
360–373 O
. O

Peter O
Potash O
, O
Alexey O
Romanov O
, O
and O
Anna O
Rumshisky O
. O
2015 O
. O

Ghostwriter B-MethodName
: O
Using O
an O
lstm O
for O
automatic O
rap B-TaskName
lyric I-TaskName
generation I-TaskName
. O

In O
Proceedings O
of O
the O
2015 O
Con- O
ference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1919–1924 O
. O

Alec O
Radford O
, O
Karthik O
Narasimhan O
, O
Tim O
Salimans O
, O
and O
Ilya O
Sutskever O
. O
2018 O
. O

Improving O
language O
under- O
standing O
by O
generative O
pre O
- O
training O
. O

Alec O
Radford O
, O
Jeff O
Wu O
, O
Rewon O
Child O
, O
David O
Luan O
, O
Dario O
Amodei O
, O
and O
Ilya O
Sutskever O
. O
2019 O
. O

Language O
models O
are O
unsupervised O
multitask O
learners O
. O

Yi O
Ren O
, O
Jinzheng O
He O
, O
Xu O
Tan O
, O
Tao O
Qin O
, O
Zhou O
Zhao O
, O
and O
Tie O
- O
Yan O
Liu O
. O
2020 O
. O

Popmag O
: O

Pop O
music O
ac- O
companiment O
generation O
. O

In O
Proceedings O
of O
the O
28th O
ACM O
International O
Conference O
on O
Multimedia O
, O
pages O
1198–1206 O
. O

Zhonghao O
Sheng O
, O
Kaitao O
Song O
, O
Xu O
Tan O
, O
Yi O
Ren O
, O
Wei O
Ye O
, O
Shikun O
Zhang O
, O
and O
Tao O
Qin O
. O
2020 O
. O

Songmass O
: O
Automatic O
song O
writing O
with O
pre O
- O
training O
and O
align- O
ment O
constraint O
. O

arXiv O
preprint O
arXiv:2012.05168 O
. O

Kaitao O
Song O
, O
Xu O
Tan O
, O
Tao O
Qin O
, O
Jianfeng O
Lu O
, O
and O
Tie- O
Yan O
Liu O
. O
2019 O
. O

Mass O
: O

Masked O
sequence O
to O
se- O
quence O
pre O
- O
training O
for O
language O
generation O
. O

In O
In- O
ternational O
Conference O
on O
Machine O
Learning O
, O
pages O
5926–5936.SS O
Vallender O
. O

1974 O
. O

Calculation O
of O
the O
wasserstein O
dis- O

tance O
between O
probability O
distributions O
on O
the O
line O
. O

Theory O
of O
Probability O
& O
Its O
Applications O
, O
18(4):784 O
– O
786 O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
Ł O
ukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
NIPS O
, O
pages O
5998–6008 O
. O

Kento O
Watanabe O
, O
Yuichiroh O
Matsubayashi O
, O
Satoru O
Fukayama O
, O
Masataka O
Goto O
, O
Kentaro O
Inui O
, O
and O
To- O
moyasu O
Nakano O
. O

2018 O
. O

A O
melody O
- O
conditioned O
lyrics O
language O
model O
. O

In O
NAACL O
, O
pages O
163–172 O
. O

Lijun O
Wu O
, O
Xu O
Tan O
, O
Di O
He O
, O
Fei O
Tian O
, O
Tao O
Qin O
, O
Jianhuang O
Lai O
, O
and O
Tie O
- O
Yan O
Liu O
. O

2018 O
. O

Beyond O
error O
propaga- O
tion O
in O
neural O
machine O
translation O
: O
Characteristics O
of O
language O
also O
matter O
. O

In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Lan- O
guage O
Processing O
, O
pages O
3602–3611 O
. O

Zhilin O
Yang O
, O
Zihang O
Dai O
, O
Yiming O
Yang O
, O
Jaime O
Car- O
bonell O
, O
Russ O
R O
Salakhutdinov O
, O
and O
Quoc O
V O
Le O
. O

2019 O
. O

Xlnet O
: O

Generalized O
autoregressive O
pretraining O
for O
language O
understanding O
. O

In O
Advances O
in O
neural O
in- O
formation O
processing O
systems O
, O
pages O
5753–5763 O
. O

Xingxing O
Zhang O
and O
Mirella O
Lapata O
. O

2014 O
. O

Chinese O
poetry O
generation O
with O
recurrent O
neural O
networks O
. O

In O
Proceedings O
of O
the O
2014 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
670–680 O
. O

Guo O
Zhipeng O
, O
Xiaoyuan O
Yi O
, O
Maosong O
Sun O
, O
Wenhao O
Li O
, O
Cheng O
Yang O
, O
Jiannan O
Liang O
, O
Huimin O
Chen O
, O
Yuhui O
Zhang O
, O
and O
Ruoyu O
Li O
. O
2019 O
. O

Jiuge O
: O

A O
human- O
machine O
collaborative O
chinese O
classical O
poetry O
gen- O
eration O
system O
. O

In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Lin- O
guistics O
: O
System O
Demonstrations O
, O
pages O
25–30 O
. O

Hongyuan O
Zhu O
, O
Qi O
Liu O
, O
Nicholas O
Jing O
Yuan O
, O
Chuan O
Qin O
, O
Jiawei O
Li O
, O
Kun O
Zhang O
, O
Guang O
Zhou O
, O
Furu O
Wei O
, O
Yuanchun O
Xu O
, O
and O
Enhong O
Chen O
. O

2018 O
. O

Xiaoice O
band O
: O
A O
melody O
and O
arrangement O
generation O
frame- O
work O
for O
pop O
music O
. O

In O
Proceedings O
of O
the O
24th O
ACM O
SIGKDD O
International O
Conference O
on O
Knowl- O
edge O
Discovery O
and O
Data O
Mining O
, O
page O
2837–2846 O
. O

A O
Comparison O
with O
GhostWriter B-MethodName
We O
provide O
a O
comparison O
between O
DeepRapper B-MethodName
and O
GhosterWriter B-MethodName
( O
Potash O
et O

al O
. O
, O
2015 O
) O
in O
Table O
6 O
. O

The O
results O
show O
that O
both O
DeepRapper B-MethodName
and O
base- O
lines O
outperform O
GhosterWriter B-MethodName
in O
terms O
of O
PPL B-MetricName
, O
rhyme B-MetricName
accuracy I-MetricName
, O
and O
rhyme B-MetricName
density I-MetricName
on O
rap B-TaskName
genera- I-TaskName
tion I-TaskName
tasks O
. O

B O
Samples O
with O
Beat O
Frequency O
Control O
Fast O
Figure O
7 O
provides O
a O
rap O
generated O
by O
Deep- B-MethodName
Rapper I-MethodName
with O
fast O
beat O
frequency O
, O
which O
the O
fre- O
quency O
is O
4.3 O
. O

The O
rap O
express O
ones O
beat O
wished O
to O
his O
/ O
her O
lover O
. O

The O
following O
is O
the O
translation O
of O
texts O
in O
Figure O
7 O
. O
我长大的地方像一个简朴的寨 O
The O
place O
where O
I O
grew O
up O
is O
like O
a O
simple O
village O
遥望远方镜子里的海 O
Looking O
into O
the O
distance O
, O
the O
sea O
is O
in O
the O
mirror O
看见理想很实际的说一句拜拜 O
See O
my O
dream O
and O
say O
goodbye O
这世界在我眼里和千里之外 O

The O
world O
is O
sometimes O
in O
my O
eyes O
and O
sometimes O
thousands O
of O
miles O
away O
穿过河流沙漠和人海 O
Across O
rivers O
, O
deserts O
and O
crowds O
一山万水渡过岁月冲洗我的爱 O

A O
mountain O
and O
a O
million O
rivers O
wash O
my O
love O
through O
the O
years O
和那亲爱的你一起去踩一路的尘埃 O
Step O
on O
the O
dust O
all O
the O
way O
with O
dear O
you O
我一生中最重要的你我壹一都在 O

The O
most O
important O
you O
in O
my O
life O
, O
I O
’ll O
always O
by O
your O
side O
点点轻地落在你那冰封的大门外 O
Little O
by O
little O
, O
it O
falls O
outside O
your O
frozen O
gate O
我在默默的祝福你 O
I O
am O
blessing O
you O
silently O
我在无数个夜里默默地祝福你 O

I O
have O
secretly O
blessed O
you O
for O
countless O
nights O
远远地保护着你我的爱 O
Protecting O
you O
and O
my O
love O
from O
a O
distance O
Medium O
Figure O
8 O
provides O
a O
rap O
generated O
by O
DeepRapper B-MethodName
with O
medium O
beat O
frequency O
, O
which O
the O
frequency O
is O
2.6 O
. O

The O
rap O
praises O
the O
times O
we O
live O
in O
. O

The O
following O
is O
the O
translation O
of O
texts O
in O
Figure O
8 O
. O

我长大的地方像一个简朴的寨 O
The O
place O
where O
I O
grew O
up O
is O
like O
a O
simple O
village O
简朴的看着简朴的海 O

Simply O
looking O
at O
the O
simple O
sea O
爸爸拿着一个简朴的麦 O
Dad O
holding O
a O
simple O
wheat O
有人真实的努力就有人背负着爱 O

Someone O
takes O
effort O
, O
somebody O
is O
carrying O
love O
那生活的美好让人人们热爱 O
The O
beauty O
of O
life O
makes O
people O
love O
这世界的美好纯粹是意外 O

The O
beauty O
of O
this O
world O
is O
pure O
accident O
而我长大的地方是个简朴的寨 O
And O
the O
place O
where O
I O
grew O
up O
is O
a O
simple O
village O
让我们在这里开心的喝彩 O

Let O
’s O
cheer O
happily O
here O
伟大母亲怀抱着爱 O

Great O
mother O
embrace O
love O
看着幸福的人们敞开淳朴的怀 O
Watching O
happy O
people O
open O
their O
simple O
arms O
我们最美好的这个快乐海 O
We O
are O
in O
the O
most O
beautiful O
happy O
sea O
唱出我们的时代 O

Sing O
our O
time O

Slow O
Figure O
9 O
provides O
a O
rap O
generated O
by O
Deep- B-MethodName
Rapper I-MethodName
with O
slow O
beat O
frequency O
, O
where O
the O
fre- O
quency O
is O
2.1 O
. O

The O
rap O
express O
ones O
relief O
from O
life O
. O

The O
following O
is O
the O
translation O
of O
texts O
in O
Figure O
9 O
. O

我长大的地方像一个简朴的寨 O
The O
place O
where O
I O
grew O
up O
is O
like O
a O
simple O
village O
快到有一天看见了父母的爱 O
Almost O
one O
day O
I O
saw O
the O
love O
of O
my O
parents O
我的时间你实在不用去考虑自己多坏 O

You O
do O
n’t O
have O
to O
think O
about O
how O
bad O
you O
are O
in O
my O
time O
当我脚步在外从没过的这么可爱 O

I O
’ve O
never O
been O
so O
cute O
when O
I O
’m O
out O
我只是一次旅行 O
I O
’m O
just O
a O
trip O
to O
your O
life O
你现在的校服我也想换 O
I O
want O
to O
change O
your O
current O
school O
uniform O
我曾经追你 O
I O
used O
to O
chase O
you O
你的运气也不摔 O

Your O
luck O
wo O
n’t O
fall O
毕竟上次 O
After O
all O
last O
time你爱的姑娘你也想看 O
You O
want O
to O
see O
the O
girl O
you O
love O
她们和你一定要分离 O

They O
must O
be O
separated O
from O
you O
你就这样子一笑而去 O

You O
just O
leave O
with O
a O
smile O

C O
Translation O
of O
Chinese O
Examples O
in O
the O
Paper O
Words O
in O
red O
are O
rhymes O
. O

Translation O
of O
Chinese O
in O
Figure O
2 O
我长大的地方像一个简朴的寨 O
The O
place O
where O
I O
grew O
up O
is O
like O
a O
simple O
village O
简朴的人吃着最简朴的菜 O
Simple O
people O
eat O
the O
simplest O
dishes O
简朴的话包含着简朴的爱 O
Simple O
words O
contain O
simple O
love O
简朴的道理传给一代又一代 O
Simple O
principles O
are O
passed O
on O
from O
generation O
to O
generation O

81难以忘记的画面不需相机 O

Unforgettable O
picture O
do O
not O
need O
camera O
to O
capture O
难以再闻到的是巷子里的香气 O

What O
is O
hard O
to O
smell O
is O
the O
aroma O
in O
the O
alley O
常常想起外婆家的躺椅 O
I O
often O
think O
of O
grandma O
’s O
recliner O
最珍贵的椰奶往往藏在床底 O
The O
most O
precious O
coconut O
milk O
is O
often O
hidden O
under O
the O
bed O

先填饱肚子再想那些背不下的书 O
Fill O
your O
stomach O
ﬁrst O
, O
then O
think O
about O
the O
books O
that O
you O
ca O
n’t O
remember O
外婆做的火腿肠比外面炸的酥 O
Grandma O
’s O
ham O
sausage O
is O
crispier O
than O
fried O
outside O

油烟的香味弥漫不那么大的屋 O
The O
smell O
of O
lampblack O
pervades O
the O
not O
so O
big O
house O
外婆的故事总会让大人笑着哭 O

Grandma O
’s O
stories O
always O
make O
adults O
laugh O
and O
cry O
Translation O
of O
Chinese O
in O
Figure O
3 O
我抬头仰望。天空的苍茫 O
。 O

I O
looked O
up O
. O

The O
sky O
is O
vast O
. O

Translation O
of O
Chinese O
in O
Figure O
4 O
是这座城市的气象 O
It O
is O
the O
weather O
of O
this O
city O
让你感受生命的力量 O
makes O
you O
feel O
the O
power O
of O
living O
Translation O
of O
Chinese O
in O
Figure O
6 O
我长大的地方像一个简朴的寨 O
The O
place O
where O
I O
grew O
up O
is O
like O
a O
simple O
village O
公里也许大的远方简直是个小小的寨 O

A O
small O
far O
away O
village O
偶尔穿件毛衣那样子很可爱 O
It O
is O
cute O
to O
wear O
a O
sweater O
occasionally O
远方可单纯的姑娘还是单纯的孩 O
Is O
it O
a O
simple O
girl O
or O
a O
simple O
child O
far O
away O
是放不下的故事大声的喝彩 O
Cheers O
loudly O
for O
the O
unforgettable O
story O
像快乐的小孩莫名的敞着怀 O
Happy O
kids O
like O
happy O
kids几百公里我们相约到未来 O
Through O
hundreds O
of O
kilometers O
, O
we O
meet O
in O
the O
future O
在那无尽的沙漠和海 O
In O
the O
endless O
desert O
and O
sea O
看着温暖花开 O

Watching O
the O
warm O
ﬂowers O
bloom O
花一样的在 O

Like O
ﬂowers O
be O
there O
写动人的天籁 O
Write O
moving O
sounds O
of O
nature O
跟着自由自在 O

Feeling O
the O
freedom O
消沉在那片海 O

Sometimes O
depressed O
in O
the O
sea O
不懂儿时的他们不懂什么是爱 O
I O
do O
n’t O
understand O
their O
childish O
. O

I O
do O
n’t O
know O
what O
love O
is O
到现在你看来 O

Till O
now O
you O
see O
最真的迷彩 O
It O
is O
The O
most O
true O
fantasy O

AligNART B-MethodName
: O
Non B-MethodName
- I-MethodName
autoregressive I-MethodName
Neural I-MethodName
Machine I-MethodName
Translation I-MethodName
by I-MethodName
Jointly I-MethodName
Learning I-MethodName
to I-MethodName
Estimate I-MethodName
Alignment I-MethodName
and I-MethodName
Translate I-MethodName

Jongyoon O
Song1;2Sungwon O
Kim1 O
1Data O
Science O
and O
AI O
Laboratory O
, O
Seoul O
National O
University O
, O
South O
Korea O
2Kakao O
Enterprise O
, O
South O
Korea O
3Interdisciplinary O
Program O
in O
Artiﬁcial O
Intelligence O
, O
Seoul O
National O
University O
, O
South O
Korea O
{ O
coms1580 O
, O
ksw0306 O
, O
sryoon}@snu.ac.krSungroh O
Yoon1;3y O

Abstract O
Non B-MethodName
- I-MethodName
autoregressive I-MethodName
neural I-MethodName
machine I-MethodName
translation I-MethodName
( O
NART B-MethodName
) O
models O
suffer O
from O
the O
multi O
- O
modality O
problem O
which O
causes O
translation O
inconsis- O
tency O
such O
as O
token O
repetition O
. O

Most O
recent O
ap- O
proaches O
have O
attempted O
to O
solve O
this O
problem O
by O
implicitly O
modeling O
dependencies O
between O
outputs O
. O

In O
this O
paper O
, O
we O
introduce O
AligNART B-MethodName
, O
which O
leverages O
full O
alignment O
information O
to O
explicitly O
reduce O
the O
modality O
of O
the O
target O
distribution O
. O

AligNART B-MethodName
divides O
the O
machine B-TaskName
translation I-TaskName
task O
into O
( O
i)alignment O
estimation O
and O
(ii)translation O
with O
aligned O
decoder O
in- O
puts O
, O
guiding O
the O
decoder O
to O
focus O
on O
sim- O
pliﬁed O
one O
- O
to O
- O
one O
translation O
. O

To O
alleviate O
the O
alignment O
estimation O
problem O
, O
we O
further O
pro- O
pose O
a O
novel O
alignment O
decomposition O
method O
. O

Our O
experiments O
show O
that O
AligNART B-MethodName
out- O
performs O
previous O
non O
- O
iterative O
NART O
models O
that O
focus O
on O
explicit O
modality O
reduction O
on O
WMT14 B-DatasetName
En$De I-DatasetName
and O
WMT16 B-DatasetName
Ro I-DatasetName
! I-DatasetName
En I-DatasetName
. O

Fur- O
thermore O
, O
AligNART B-MethodName
achieves O
BLEU B-MetricName
scores O
comparable O
to O
those O
of O
the O
state O
- O
of O
- O
the O
- O
art O
con- O
nectionist O
temporal O
classiﬁcation O
based O
mod- O
els O
on O
WMT14 B-DatasetName
En$De I-DatasetName
. O

We O
also O
observe O
that O
AligNART B-MethodName
effectively O
addresses O
the O
token O
rep- O
etition O
problem O
even O
without O
sequence O
- O
level O
knowledge O
distillation O
. O

1 O
Introduction O
In O
the O
neural B-TaskName
machine I-TaskName
translation I-TaskName
( O
NMT B-TaskName
) O
domain O
, O
non O
- O
autoregressive O
NMT O
( O
NART O
) O
models O
( O
Gu O
et O
al O
. O
, O
2018 O
) O
have O
been O
proposed O
to O
alleviate O
the O
low O
translation O
speeds O
of O
autoregressive O
NMT O
( O
ART O
) O
models O
. O

However O
, O
these O
models O
suffer O
from O
degen- O
erated O
translation O
quality O
( O
Gu O
et O
al O
. O
, O
2018 O
; O
Sun O
et O
al O
. O
, O
2019 O
) O
. O

To O
improve O
the O
translation O
quality O
of O
NART O
, O
several O
studies O
on O
NART O
iteratively O
reﬁne O
decoded O
outputs O
with O
minimal O
iterations O
( O
Ghazvininejad O
et O
al O
. O
, O
2019 O
; O

Kasai O
et O
al O
. O
, O
2020a O
; O
Lee O
et O
al O
. O
, O
2020 O
; O
Guo O
et O
al O
. O
, O
2020 O
; O
Saharia O
et O
al O
. O
, O
2020 O
) O
; O
other O
recent O
works O
target O
to O
improve O
NART B-MethodName
without O
iteration O
( O
Qian O
et O
al O
. O
, O
2021 O
; O
Gu O
and O
Kong O
, O
2021 O
) O
. O

One O
of O
the O
signiﬁcant O
limitations O
of O
non O
- O
iterative O
NART O
models O
is O
the O
multi O
- O
modality O
problem O
. O

This O
problem O
originates O
from O
the O
fact O
that O
the O
models O
should O
maximize O
the O
probabilities O
of O
multiple O
tar- O
gets O
without O
considering O
conditional O
dependencies O
between O
target O
tokens O
. O

For O
example O
, O
in O
English O
- O
to- O
German O
translation O
, O
a O
source O
sentence O
" O
Thank O
you O
very O
much O
. O
" O
can O
be O
translated O
to O
" O
Danke O
schön O
. O
" O

or O
" O
Vielen O
Dank O
. O
" O
. O

Under O
the O
conditional O
indepen- O
dence O
assumption O
, O
the O
non O
- O
iterative O
NART O
models O
are O
likely O
to O
generate O
improper O
translations O
such O
as O
" O
Danke O
Dank O
. O
" O

or O
" O
Vielen O
schön O
. O
" O

( O
Gu O
et O
al O
. O
, O
2018 O
) O
. O

For O
the O
same O
reason O
, O
other O
inconsistency O
prob- O
lems O
such O
as O
token O
repetition O
or O
omission O
occur O
frequently O
in O
non B-TaskName
- I-TaskName
iterative I-TaskName
NART I-TaskName
( O
Gu O
and O
Kong O
, O
2021 O
) O
. O

There O
are O
two O
main O
methods O
for O
non B-TaskName
- I-TaskName
iterative I-TaskName
NART I-TaskName
to O
address O
the O
multi O
- O
modality O
problem O
. O

Some O
works O
focus O
on O
an O
implicit O
modeling O
of O
the O
dependencies O
between O
the O
target O
tokens O
( O
Gu O
and O
Kong O
, O
2021 O
) O
. O

For O
example O
, O
Ghazvininejad O
et O
al O
. O
( O
2020 O
) O
, O
Saharia O
et O
al O
. O
( O
2020 O
) O
, O
and O
Gu O
and O
Kong O
( O
2021 O
) O
modify O
the O
objective O
function O
based O
on O
dy- O
namic O
programming O
, O
whereas O

Qian O
et O
al O
. O
( O
2021 O
) O
provide O
target O
tokens O
to O
the O
decoder O
during O
train- O
ing O
. O

On O
the O
other O
hand O
, O
other O
works O
focus O
on O
an O
ex- O
plicit O
reduction O
of O
the O
modality O
of O
the O
target O
dis- O
tribution O
by O
utilizing O
external O
source O
or O
target O
sen- O
tence O
information O
rather O
than O
modifying O
the O
objec- O
tive O
function O
. O

For O
example O
, O
Akoury O
et O
al O
. O

( O
2019 O
) O
and O
Liu O
et O
al O
. O
( O
2021 O
) O
use O
syntactic O
or O
semantic O
in- O
formation O
; O
Gu O
et O
al O
. O
( O
2018 O
) O
, O
Zhou O
et O
al O
. O
( O
2020b O
) O
, O
and O
Ran O
et O
al O
. O
( O
2021 O
) O
use O
the O
alignment O
informa- O
tion O
between O
source O
and O
target O
tokens O
. O

However O
, O
previous O
explicit O
modality O
reduction O
methods O
show O
suboptimal O
performance O
. O

Zhou O
et O

al O
. O
( O
2020b O
) O
and O
Ran O
et O
al O
. O
( O
2021 O
) O
ex- O
tract O
fertility O
( O
Brown O
et O
al O
. O
, O
1993 O
) O
and O
ordering O
information O
in O
word O
alignments O
, O
which O
enables O
the O
modeling O
of O
several O
types O
of O
mappings O
except O
for O
many O
- O
to O
- O
one O
and O
many O
- O
to O
- O
many O
cases O
. O

We O
hypoth- O
esize O
that O
leveraging O
entire O
mappings O
signiﬁcantly O
reduces O
the O
modality O
and O
is O
the O
key O
to O
performance O
improvement O
. O

In O
this O
work O
, O
we O
propose O
AligNART B-MethodName
, O
a O
non- O
iterative O
NART O
model O
that O
mitigates O
the O
multi- O
modality O
problem O
by O
utilizing O
complete O
informa- O
tion O
in O
word O
alignments O
. O

AligNART B-MethodName
divides O
the O
ma- O
chine O
translation O
task O
into O
( O
i)alignment O
estimation O
and(ii)non O
- O
autoregressive O
translation O
under O
the O
given O
alignments O
. O

Modeling O
all O
the O
type O
of O
mapping O
guides O
( O
ii)more O
close O
to O
one O
- O
to O
- O
one O
translation O
. O

In O
AligNART B-MethodName
, O
a O
module O
called O
Aligner O
is O
simply O
aug- O
mented O
to O
NAT O
( O
Gu O
et O
al O
. O
, O
2018 O
) O
which O
estimates O
alignments O
to O
generate O
aligned O
decoder O
inputs O
. O

However O
, O
it O
is O
challenging O
to O
estimate O
the O
com- O
plex O
alignment O
information O
using O
only O
source O
sentence O
during O
inference O
. O

Speciﬁcally O
, O
Aligner O
should O
simultaneously O
predict O
the O
number O
of O
tar- O
get O
tokens O
corresponding O
to O
each O
source O
token O
and O
their O
mapping O
. O

To O
overcome O
this O
problem O
, O
we O
further O
propose O
alignment O
decomposition O
which O
factorizes O
the O
alignment O
process O
into O
three O
sub- O
processes O
: O
duplication O
, O
permutation O
, O
and O
group- O
ing O
. O

Each O
sub O
- O
process O
corresponds O
to O
much O
feasi- O
ble O
sub O
- O
problems O
: O
one O
- O
to O
- O
many O
mapping O
, O
ordering O
, O
and O
many O
- O
to O
- O
one O
mapping O
, O
respectively O
. O

Our O
experimental O
results O
show O
that O
AligNART B-MethodName
outperforms O
previous O
non O
- O
iterative O
NART O
models O
of O
explicit O
modality O
reduction O
on O
WMT14 B-DatasetName
En I-DatasetName
$ I-DatasetName
De I-DatasetName
and O
WMT16 B-DatasetName
Ro!En I-DatasetName
. O

AligNART B-MethodName
achieves O
per- O
formance O
comparable O
to O
that O
of O
the O
recent O
state- O
of O
- O
the O
- O
art O
non O
- O
iterative O
NART O
model O
on O
WMT14 B-DatasetName
En$De I-DatasetName
. O

We O
observe O
that O
the O
modality O
reduction O
in O
AligNART O
addresses O
the O
token O
repetition O
issue O
even O
without O
sequence O
- O
level O
knowledge O
distillation O
( O
Kim O
and O
Rush O
, O
2016 O
) O
. O

We O
also O
conduct O
quantita- O
tive O
and O
qualitative O
analyses O
on O
the O
effectiveness O
of O
alignment O
decomposition O
. O

2 O
Background O
Given O
a O
source O
sentence O
x O
= O
fx1;x2;:::;x O
Mgand O
its O
translation O
y O
= O
fy1;y2;:::;y O
Ng O
, O
ART O
models O
with O
encoder O
- O
decoder O
architecture O
are O
trained O
with O
chained O
target O
distributions O
and O
infer O
the O
target O
sen- O
tence O
autoregressively O
: O
p(yjx O
) O

= O
NY O
n=1p(ynjy O
< O
n;x O
): O
( O
1)At O
each O
decoding O
position O
n O
, O
the O
decoder O
of O
the O
model O
is O
conditioned O
with O
previous O
target O
tokens O
y O
< O
n O
= O
fy1;:::;y O
n 1 O
g O
, O
which O
is O
the O
key O
factor O
of O
performance O
in O
ART O
models O
. O

Previous O
target O
tokens O
reduce O
the O
target O
distribution O
modality O
and O
provide O
information O
about O
the O
target O
sentence O
. O

However O
, O
the O
autoregressive O
decoding O
scheme O
enforces O
the O
decoder O
to O
iterate O
Ntimes O
to O
complete O
the O
transla- O
tion O
and O
increases O
the O
translation O
time O
linearly O
with O
respect O
to O
the O
length O
of O
the O
target O
sentence O
. O

Non O
- O
iterative O
NART O
models O
( O
Gu O
et O
al O
. O
, O
2018 O
; O
Sun O
et O
al O
. O
, O
2019 O
; O
Sun O
and O
Yang O
, O
2020 O
) O
assume O
con- O
ditional O
independence O
between O
the O
target O
tokens O
to O
improve O
the O
translation O
speed O
: O
p(yjx O
) O
= O
p(Njx)NY O
n=1p(ynjx O
) O
; O
( O
2 O
) O
where O
N B-HyperparameterName
is O
the O
predicted B-HyperparameterName
target I-HyperparameterName
length I-HyperparameterName
to O
parallelize O
the O
decoding O
process O
. O

Non O
- O
iterative O
NART O
models O
provide O
only O
the O
length O
information O
of O
the O
target O
sentence O
to O
the O
decoder O
, O
which O
is O
insufﬁcient O
to O
address O
the O
multi O
- O
modality O
problem O
. O

3 O
AligNART B-MethodName
3.1 O
Model O
Overview O
Given O
the O
word O
alignments O
between O
the O
source O
and O
target O
sentences O
A2f0;1gNM O
, O
we O
factorize O
the O
task O
into O
( O
i)alignment O
estimation O
and O
( O
ii)transla- O
tion O
with O
aligned O
decoder O
inputs O
as O
follows O
: O
p(yjx O
) O
= O
p(Ajx)NY O
n=1p(ynjx;A O
) O
; O
( O
3 O
) O
where O
M B-HyperparameterName
and O
N B-HyperparameterName
are O
the B-HyperparameterName
lengths I-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
source I-HyperparameterName
and I-HyperparameterName
target I-HyperparameterName
sentences I-HyperparameterName
, O
respectively O
. O

Although O
we O
can O
also O
modify O
the O
negative O
log O
- O
likelihood O
loss O
to O
model O
dependencies O
between O
outputs O
such O
as O
connectionist O
temporal O
classiﬁcation O
( O
CTC O
) O
loss O
( O
Graves O
et O
al O
. O
, O
2006 O
) O
, O
we O
focus O
on O
the O
effect O
of O
the O
introduction O
of O
alignment O
as O
additional O
informa- O
tion O
. O

AligNART B-MethodName
is O
based O
on O
the O
encoder O
- O
decoder O
architecture O
, O
with O
an O
alignment O
estimation O
module O
called O
Aligner O
as O
depicted O
in O
Figure O
1a O
. O

The O
en- O
coder O
maps O
the O
embedding O
of O
the O
source O
tokens O
into O
hidden O
representations O
h O
= O
fh1;h2;:::;h O
Mg O
. O

Aligner O
constructs O
the O
aligned O
decoder O
inputs O
d= O
fd1;d2;:::;d O
Ngas O
follows O
: O
where O
rn B-HyperparameterName
is O
the O
number B-HyperparameterName
of I-HyperparameterName
non I-HyperparameterName
- I-HyperparameterName
zero I-HyperparameterName
elements I-HyperparameterName
in O
the O
n B-HyperparameterName
- O
th O
row O
of O
A. O

Given O
the O
aligned O
decoder O
in- O
puts O
, O
the O
decoder O
is O
guided O
to O
focus O
on O
a O
one O
- O
to- O
one O
translation O
from O
dn O
to O
yn O
. O

One O
- O
to O
- O
one O
mapping O
signiﬁcantly O
reduces O
the O
modality O
of O
the O
target O
dis- O
tribution O
. O

The O
key O
component O
of O
AligNART B-MethodName
, O
Aligner O
, O
mod- O
els O
a O
conditional O
distribution O
of O
alignments O
Agiven O
the O
source O
sentence O
xduring O
training O
, O
and O
aligns O
encoder O
outputs O
using O
the O
estimated O
alignments O
during O
inference O
, O
as O
depicted O
in O
Figure O
1b O
. O

The O
ground O
truth O
of O
the O
alignments O
is O
extracted O
using O
an O
external O
word O
alignment O
tool O
. O

However O
, O
align- O
ment O
estimation O
given O
only O
the O
source O
sentence O
is O
challenging O
since O
the O
alignment O
consists O
of O
two O
components O
related O
with O
target O
tokens O
: O
•The O
number O
of O
target O
tokens O
that O
correspond O
to O
each O
encoder O
output O
hm O
. O
•The O
positions O
of O
the O
target O
tokens O
to O
which O
hmcorresponds O
. O

The O
Aligner O
decomposes O
the O
alignment O
for O
effec- O
tive O
estimation O
, O
which O
is O
described O
in O
Section O
3.2 O
. O

3.2 O
Aligner O
To O
alleviate O
the O
alignment O
estimation O
problem O
, O
we O
start O
by O
factorizing O
the O
alignment O
process O
as O
shownin O
Figure O
1b O
. O

First O
, O
we O
copy O
each O
encoder O
output O
hmby O
the O
number O
of O
target O
tokens O
mapped O
to O
hm O
, O
which O
is O
denoted O
as O
cm O
= O
P O
nAn;m O
. O

Given O
the O
duplicated O
encoder O
outputs O
h0 O
, O
we O
have O
to O
predict O
the O
positions O
of O
target O
tokens O
to O
which O
each O
element O
inh0is O
mapped O
. O

We O
further O
decompose O
the O
remaining O
prediction O
process O
into O
permutation O
andgrouping O
, O
since O
non- O
iterative O
NART O
models O
have O
no O
information O
about O
the O
target O
length O
Nduring O
inference O
. O

In O
the O
per- O
mutation O
process O
, O
h' B-HyperparameterName
is O
re O
- O
ordered O
into O
d' B-HyperparameterName
such O
that O
elements O
corresponding O
to O
the O
same O
target O
token O
are O
placed O
adjacent O
to O
each O
other O
. O

In O
the O
group- O
ing O
process O
, O
each O
element O
in O
d' B-HyperparameterName
is O
clustered O
into O
Ngroups O
by O
predicting O
whether O
each O
element O
is O
mapped O
to O
the O
same O
target O
token O
as O
the O
previous O
element O
.rn O
= O
P O
mAn;m O
denotes O
the O
number B-HyperparameterName
of I-HyperparameterName
el- I-HyperparameterName
ements I-HyperparameterName
in I-HyperparameterName
the I-HyperparameterName
n I-HyperparameterName
- I-HyperparameterName
th I-HyperparameterName
group I-HyperparameterName
which O
is O
equivalent O
to O
rn O
in O
Equation O
4 O
. O

Finally O
, O
we O
can O
derive O
the O
decoder O
inputs O
d O
in O
Equation O
4 O
by O
averaging O
the O
elements O
in O
each O
group O
in O
d0 O
. O

In O
summary O
, O
we O
decompose O
the O
alignment O
estimation O
task O
into O
three O
sequential O
sub O
- O
tasks O
: O
duplication O
, O
permutation O
, O
and O
grouping O
. O

3.2.1 O
Alignment O
Decomposition O
As O
shown O
in O
Figure O
1b O
, O
we O
factorize O
the O
align- O
ment O
matrix O
Ainto O
duplication O
, O
permutation O
, O
and O

4grouping O
matrices O
that O
correspond O
to O
each O
pro- O

cess.h0 O
= O
fh1;1;:::;h O
1;c1;:::;h O
M;1;:::;d O
M;cMg O
denotes O
the O
duplicated O
encoder O
outputs O
where O
hi;jis O
thej O
- O
th O
copied O
element O
of O
hi O
. O

Similarly O
, O
d0 O
= O
fd1;1;:::;d O
1;r1;:::;d O
N;1;:::;d O
N;rNgdenotes O
the O
permuted O
encoder O
outputs O
where O
di;jis O
thej- O
th O
element O
in O
the O
i O
- O
th O
group O
. O

The B-HyperparameterName
number I-HyperparameterName
of I-HyperparameterName
non- I-HyperparameterName
zero I-HyperparameterName
elements I-HyperparameterName
in O
the O
alignment O
matrix O
is O
deﬁned O
as O
L B-HyperparameterName
= O
P O
mcm O
= O
P O
nrn O
. O

Duplication O
Matrix O
Aligner O
copies O
hmbycm O
to O
construct O
the O
duplicated O
encoder O
outputs O
h0with O
a O
duplication O
matrix O
D2f0;1gLM. O
LetCm O
= O
Pm O
i=1ciandC0= O
0 O
. O

Then O
, O
we O
can O
deﬁne O
Dusing O
cmas O
follows O
: O
Dl;m= O
( O
1ifCm 1 O
< O
lCm O
0else:(5 O
) O

We O
indexh0by O
the O
following O
rule O
: O

•For O
anyhm;iandhm;j(i O
< O
j O
) O
, O
which O
are O
matched O
to O
dxi;yianddxj;yj O
, O
respectively O
, O
xixjandyiyj O
. O

The O
duplication O
matrix O
Dcontains O
similar O
informa- O
tion O
to O
fertility O
( O
Gu O
et O
al O
. O
, O
2018 O
) O
. O

Permutation O
Matrix O
Aligner O
re O

-orders O
h0to O
constructd0with O
a O
permutation O
matrix O
P2 O
f0;1gLL. O
Since O
all O
the O
indexed O
elements O
in O
h0 O
andd0are O
distinct O
, O
the O
permutation O
matrix O
Pis O
uniquely O
deﬁned O
. O

Grouping O
Matrix O
Aligner O
ﬁnally O
aggregates O
d0 O
to O
construct O
d O
, O
the O
aligned O
decoder O
inputs O
, O
with O
a O
grouping O
matrix O
G2 O

f0;1gNL. O

LetRn= O

Pn O

i=1riandR0= O
0 O
. O

Then O
, O
Gcan O
be O
deﬁned O
using O
rnas O
follows O
: O

Gn;l= O
( O
1ifRn 1 O
< O
lRn O
0else:(6 O
) O

We O
indexd0by O
the O
following O
rule O
: O
•For O
anydn;ianddn;j(i O
< O
j O
) O
, O
which O
are O
matched O
to O
hxi;yiandhxj;yj O
, O
respectively O
, O
xixjandyiyj O
. O

We O
can O
derive O
the O
aligned O
decoder O
inputs O
by O
sepa- O
rately O
estimating O
the O
decomposed O
matrices O
D O
, O
P O
, O
andG O
, O
which O
approximately O
correspond O
to O
one O
- O
to- O
many O
mapping O
, O
ordering O
, O
and O
many O
- O
to O
- O
one O
map- O
ping O
, O
respectively O
. O

The O
decomposed O
matrices O
have O
an O
easily O
predictable O
form O
while O
recovering O
the O
complete O
alignment O
matrix.3.2.2 O

Training O
Aligner O
consists O
of O
three O
prediction O
sub O
- O
modules O
: O
duplication O
, O
permutation O
, O
and O
grouping O
predictors O
. O

Each O
of O
them O
estimates O
the O
decomposed O
alignment O
matrix O
as O
follows O
: O
p(Ajx O
) O

= O
p(Gjx;P;D O
) O
p(Pjx;D)p(Djx):(7 O
) O

The O
duplication O
predictor O
learns O
to O
classify O
the O
num- O
ber O
of O
copies O
of O
hm O
. O

The O
duplication O
loss O
is O
deﬁned O
as O
follows O
: O
LD= 1 O
MMX O
m=1logpm(cm O
) O
; O
( O
8) O
wherepmis O
the O
predicted O
probability O
distribution O
of O
the O
duplication O
at O
the O
position O
m. O
To O
discrimi- O
nate O
copied O
elements O
in O
h0 O
, O
we O
add O
copy O
position O
embedding O
tofhm;1;:::;h O
m;cmgfor O
the O
next O
two O
predictors O
. O

The O
permutation O
predictor O
takes O
the O
duplicated O
encoder O
outputs O
h0as O
inputs O
. O

We O
simplify O
the O
per- O
mutation O
prediction O
problem O
into O
a O
classiﬁcation O
of O
the O
re O
- O
ordered O
position O
. O

For O
the O
permutation O
loss O
, O
we O
minimize O
the O
KL B-MetricName
- I-MetricName
divergence I-MetricName
between O
the O
pre- O
dictionPpredand O
the O
ground O
truth O
PGT O
. O

LP= 1 O
LX O
iX O
jPGT O
i;jlogPpred O
i;j O
: O
( O
9 O
) O
Given O
the O
permuted O
encoder O
outputs O
, O
the O
grouping O
predictor O
conducts O
a O
binary O
classiﬁcation O
task O
of O
whetherd0 O
lis O
assigned O
to O
the O
same O
group O
as O
d0 O
l 1 O
. O

Let O
the O
label O
at O
the O
position O
lbegl O
. O

Then O
, O
we O
deﬁne O
glfromGas O
follows O
: O
gl= O
( O
1ifG;l O
= O
G;l 1andl>1 O
0else:(10 O
) O

The O
grouping O
loss O
is O
deﬁned O
as O
follows O
: O
LG= 1 O
LLX O
l=1logpl(gl O
) O
; O
( O
11 O
) O
whereplis O
the O
predicted O
probability O
distribution O
of O
the O
grouping O
predictor O
at O
position O
l. O

Our O
ﬁnal O
loss O
function O
is O
deﬁned O
as O
the O
sum O
of O
the O
negative O
log O
- O
likelihood O
based O
translation O
loss O
LTand O
alignment O
loss O
LA O
: O
L O
= O
LT+LA O
= O
LT+ O
LD+ O
LP+ O
LG;(12 O
) O
where O
we O
set O
= O
= O
= O
0.5 B-HyperparameterValue
for O
all O
the O
experi- O
ments O
. O

3.2.3 O
Inference O
During O
inference O
, O
Aligner O
sequentially O
predicts O
the O
duplication O
, O
permutation O
, O
and O
grouping O
matrices O
to O
compute O
the O
aligned O
decoder O
inputs O
das O
depicted O
in O
Figure O
1b O
. O

The O
duplication O
predictor O
in O
Aligner O
infers O
^cmat O
each O
position O
m O
; O
then O
, O
we O
can O
directly O
construct O
a O
duplication O
matrix O
^Dusing O
Equation O
5 O
. O

The O
permutation O
predictor O
predicts O
the O
distribution O
of O
the O
target O
position O
Ppred O
. O

We O
obtain O
a O
permuta- O
tion O
matrix O
^Pthat O
minimizes O
the O
KL B-MetricName
- I-MetricName
divergence I-MetricName
as O
follows O
: O
^P= B-HyperparameterName
arg O
min O
P( X O
iX O
jPi;jlogPpred O
i;j):(13 O
) O
We O
utilize O
the O
linear O
sum O
assignment O
problem O
solver O
provided O
by O
Jones O
et O
al O
. O
( O
2001 O
) O
to O
ﬁnd O
^P. B-HyperparameterName

The O
grouping O
predictor O
infers O
the O
binary O
predic- O
tions O
^gl O
from O
the O
permuted O
encoder O
outputs O
. O

We O
construct O
a O
grouping O
matrix O
^Gusing O
^gland O

Equa- O
tions O
6 O
and O
10 O
. O

With O
a O
predicted O
alignment O
matrix O
^A=^G^P^D O
, O
Aligner O
constructs O
the O
decoder O
inputs O
using O
Equation O
4 O
, O
and O
the O
decoder O
performs O
translation O
from O
the O
aligned O
inputs O
. O

3.2.4 O
Decoding O
Strategies O
For O
the O
re O
- O
scoring O
based O
decoding O
method O
, O
we O
se- O
lect O
candidates O
of O
alignments O
using O
the O
predicted O
distributions O
in O
the O
duplication O
and O
grouping O
pre- O
dictors O
. O

We O
identify O
m0positions O
in O
the O
outputs O
of O
the O
duplication O
predictor O
, O
where O
the O
probability O
of O
the O
predicted O
class O
is O
low O
. O

We O
then O
construct O
a O
2m0- O
candidate O
pool O
where O
the O
predictions O
in O
part O
of O
the O
m0positions O
are O
replaced O
with O
the O
second O
proba- O
ble O
class O
. O

Next O
, O
we O
identify O
the O
top- O
acandidates O
with O
the O
highest O
joint O
probabilities O
. O

Similarly O
, O
we O
construct O
a O
2l0 O
- O
candidate O
pool O
and O
identify O
bcandi- O
dates O
in O
the O
grouping O
predictor O
for O
the O
acandidates O
. O

Finally O
, O
we O
rank O
abtranslations O
for O
the O
alignments O
candidates O
using O
a O
teacher O
ART O
model O
and O
select O
the O
best O
translation O
among O
them O
. O

3.3 O
Architecture O
of O
AligNART B-MethodName

We O
use O
the O
deep O
- O
shallow O
( O
12 O
- O
1 O
for O
short O
) O
Trans- O
former O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
architecture O
( O
i.e. O
, O
12- O
layer O
encoder O
and O
1 O
- O
layer O
decoder O
) O
proposed O
by O
Kasai O
et O
al O
. O
( O
2020b O
) O
for O
two O
reasons O
. O

First O
, O
a O
deeper O
encoder O
assists O
Aligner O
to O
increase O
the O
estimation O
accuracy O
of O
the O
alignment O
matrix O
during O
inference O
. O

Second O
, O
the O
deep O
- O
shallow O
architecture O
improves O
the O
inference O
speed O
since O
the O
encoder O
layer O
has O
no O
cross O
- O
attention O
module O
compared O
to O
the O
decoderlayer O
. O

The O
architecture O
of O
the O
duplication O
, O
permu- O
tation O
, O
and O
grouping O
predictor O
is O
shown O
in O
the O
Ap- O
pendix O
. O

3.4 O
Alignment O
Score O
Filtering O
Some O
alignment O
tools O
such O
as O
GIZA++ O
( O
Och O
and O
Ney O
, O
2003 O
) O
provide O
an O
alignment O
score O
for O
each O
sentence O
pair O
as O
a O
default O
. O

Samples O
with O
low O
align- O
ment O
scores O
are O
more O
likely O
to O
contain O
noise O
caused O
by O
sentence O
pairs O
or O
alignment O
tools O
. O

For O
GIZA++ O
, O
we O
ﬁlter O
out O
a O
ﬁxed O
portion O
of O
samples O
with O
low O
alignment O
scores O
to O
ease O
the O
alignment O
estimation O
. O

Since O
the O
pair O
of O
long O
sentences O
tends O
to O
be O
aligned O
with O
a O
low O
score O
, O
we O
apply O
the O
same O
ﬁltering O
por- O
tion O
for O
each O
target O
sentence O
length O
. O

4 O
Experimental O
Setups O
4.1 O
Datasets O
and O
Preprocessing O
We O
evaluate O
our O
method O
on O
two O
translation O
datasets O
: O
WMT14 B-DatasetName
English I-DatasetName
- I-DatasetName
German I-DatasetName
( I-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
) I-DatasetName
and O
WMT16 B-DatasetName
English I-DatasetName
- I-DatasetName
Romanian I-DatasetName
( I-DatasetName
En I-DatasetName
- I-DatasetName
Ro I-DatasetName
) I-DatasetName
. O

WMT14 B-DatasetName
En- I-DatasetName
De I-DatasetName
/ O
WMT16 B-DatasetName
En I-DatasetName
- I-DatasetName
Ro I-DatasetName
datasets O
contain O
4.5M/610 O
K O
training O
pairs O
, O
respectively O
. O

For O
WMT14 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
dataset O
, O
we O
use O
preprocess- O
ing O
pipelines O
provided O
by O
fairseq1(Ott O
et O
al O
. O
, O
2019 O
) O
. O

For O
WMT16 O
En O
- O
Ro O
dataset O
, O
we O
use O
the O
prepro- O
cessed O
corpus O
provided O
by O
Lee O
et O
al O
. O

( O
2018 O
) O
. O

Pre- O
processed O
datasets O
share O
a O
vocabulary O
dictionary O
between O
the O
source O
and O
target O
languages O
. O

We O
use O
fast O
align O
( O
FA O
) O
( O
Dyer O
et O
al O
. O
, O
2013 O
) O
and O
GIZA++ O
( O
GZ O
) O
, O
which O
is O
known O
to O
be O
more O
accurate O
than O
fast O
align O
, O
as O
word O
alignment O
tools O
. O

All O
the O
corpus O
are O
passed O
to O
the O
alignment O
tools O
at O
the O
subword O
- O
level O
. O

We O
ﬁlter O
out O
samples O
where O
the O
maximum O
number O
of O
duplications O
exceed O
16 O
. O

We O
explain O
the O
details O
of O
the O
alignment O
processing O
in O
the O
Appendix O
. O

We O
use O
the O
sequence O
- O
level O
knowledge O
distillation O
method O
( O
KD O
) O
for O
the O
distillation O
set O
. O

Transformer O
ART O
models O
are O
trained O
to O
generate O
the O
distillation O
set O
for O
each O
translation O
direction O
. O

4.2 O
Models O
and O
Baselines O
We O
compare O
our O
model O
with O
several O
non O
- O
iterative O
NART O
baselines O
, O
and O
divide O
the O
non O
- O
iterative O
NART O
models O
into O
two O
types O
as O
aforementioned O
: O
implicit O
dependency O
modeling O
andexplicit O
modal- O
ity O
reduction O
( O
see O
Table O
1 O
) O
. O

We O
also O
train O
the O
ART O
models O
and O
deep O
- O
shallow O
NAT O
for O
the O
analysis O
. O

Our O
models O
are O
implemented O
based O
on O
fairseq O
. O

AligNART B-MethodName
is O
implemented O
based O
on O
the O
deep O
- O
shallow O
Transformer O
architecture O
. O

We O
set O
dmodel B-HyperparameterName
/ O
dhidden B-HyperparameterName
to O
512 B-HyperparameterValue
/ O
2048 B-HyperparameterValue
and O
the O
dropout B-HyperparameterName
rate I-HyperparameterName
to O
0.3 B-HyperparameterValue
. O

The O
number B-HyperparameterName
of I-HyperparameterName
heads I-HyperparameterName
in O
multi O
- O
head O
attention O
modules O
is O
8 B-HyperparameterValue
except O
for O
the O
last O
attention O
module O
of O
the O
permutation O
predictor O
which O
is O
1 B-HyperparameterValue
. O

We O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
approximately O
64 B-HyperparameterValue
K I-HyperparameterValue
tokens O
for O
all O
the O
models O
we O
implement O
. O

All O
these O
models O
we O
implement O
are O
trained O
for O
300 B-HyperparameterValue
K I-HyperparameterValue
/ O
50 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
on O
En- B-DatasetName
De I-DatasetName
/ O
En B-DatasetName
- I-DatasetName
Ro I-DatasetName
datasets O
, O
respectively O
. O

For O
AligNART B-MethodName
, O
we O
average O
5 B-HyperparameterValue
checkpoints O
with O
the O
highest O
valida- O
tion O
BLEU B-MetricName
scores O
in O
the O
20 B-HyperparameterValue
latest O
checkpoints O
. O

For O
optimization O
, O
we O
use O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
= O
( O
0.9 B-HyperparameterValue
; O
0.98 B-HyperparameterValue
) O
and O
= O
10-8 B-HyperparameterValue
. O

The O
learning B-HyperparameterName
rate I-HyperparameterName
scheduling O
follows O
that O
of O
Vaswani O
et O
al O
. O
( O
2017 O
) O
, O
starting O
from O
10-7 B-HyperparameterValue
and O
warms O
up O
to O
5e-4 B-HyperparameterValue
in O
10 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
. O

We O
use O
the O
label O
smoothing O
technique O
with O
ls B-HyperparameterName
= O
0.1 B-HyperparameterValue
for O
the O
target O
token O
distribution O
and O
each O
row O
of O
permuta- O
tion O
matrix O
. O

The O
translation O
latency O
is O
measured O
on O
an O
NVIDIA O
Tesla O
V100 O
GPU O
. O

5 O
Results O
5.1 O
Main O
Results O
Table O
1 O
shows O
the O
BLEU B-MetricName
scores O
, O
translation B-MetricName
latency I-MetricName
and O
speedup B-MetricName
on O
WMT14 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
and O
WMT16 B-DatasetName
En I-DatasetName
- I-DatasetName
Ro I-DatasetName
. O

In O
explicit O
modality O
reduction O
, O
AligNART B-MethodName
( I-MethodName
FA I-MethodName
) I-MethodName
achieves O
higher O
BLEU B-MetricName
scores O
than O
Distortion O
and O
ReorderNAT B-MethodName
, O
which O
utilize O
the O
same O
alignment O
tool O
, O
since O
we O
leverage O
the O
entire O
alignment O
information O
rather O
than O
partial O
information O
such O
as O
fertility O
or O
ordering O
. O

Moreover O
, O
AligNART B-MethodName
( I-MethodName
GZ I-MethodName
) I-MethodName
signiﬁcantly O
outperforms O
previous O
models O
for O
explicit O
modal- O
ity O
reduction O
except O
for O
SNAT B-MethodName
on O
En B-DatasetName
! I-DatasetName
Ro I-DatasetName
. O

In O
im- O
plicit O
dependency O
modeling O
, O
AligNART B-MethodName
( I-MethodName
GZ I-MethodName
) I-MethodName
out- O
performs O
Imputer O
and O
shows O
performance O
compara- O
ble O
to O
that O
of O
the O
state O
- O
of O
- O
the O
- O
art O
CTC O
- O
based O
model O
on O
En$De B-DatasetName
by O
simply O
augmenting O
Aligner O
module O
to O
deep O
- O
shallow O
NAT O
. O

In O
this O
study O
, O
we O
focus O
on O
introducing O
complete O
information O
in O
word O
align- O
ments O
; O
we O
do O
not O
modify O
the O
objective O
function O
, O
which O
can O
be O
explored O
in O
the O
future O
work O
. O

Table O
2 O
shows O
the O
BLEU B-MetricName
scores O
with O
re O
- O
scoring O
decoding O
strategies O
of O
the O
non O
- O
iterative O
NART O
mod- O
els O
. O

We O
set O
m0 B-HyperparameterName
= O
l0= B-HyperparameterName
4 B-HyperparameterValue
, O
a B-HyperparameterName
= O
4 B-HyperparameterValue
, O
and O
b B-HyperparameterName
= O
2 B-HyperparameterValue
for O
8 O
candidates O
. O

AligNART B-MethodName
outperforms O
the O
base- O
lines O
on O
En!De B-DatasetName
and O
Ro!En B-DatasetName
, O
and O
shows O
perfor- O
mance O
similar O
to O
that O
of O
GLAT B-MethodName
on O
De B-DatasetName
! I-DatasetName
En I-DatasetName
. O

In O
non O
- O
iterative O
NART O
for O
explicit O
modality O
reduc- O
tion O
, O
AligNART B-MethodName
shows O
the O
best O
performance O
on O
En$De B-DatasetName
and O
Ro!En B-DatasetName
. O

5.2 O
Analysis O
of O
Aligner O
Components O
In O
this O
section O
, O
we O
investigate O
the O
accuracy O
, O
exam- O
ple O
, O
and O
ablation O
results O
of O
Aligner O
components O
as O
shown O
in O
Table O
3 O
, O
4 O
, O
and O
5 O
, O
respectively O
. O

Note O
that O
we O
partially O
provide O
the O
ground O
truth O
D O
or O
P O
matrices O
during O
the O
accuracy O
measurement O
. O

Knowledge O
Distillation O
In O
Table O
3 O
, O
a O
com- O
parison O
of O
accuracy O
between O
raw O
and O
distilled O
datasets O
shows O
that O
KD O
signiﬁcantly O
decreases O
multi O
- O
modality O
of O
each O
component O
. O

After O
KD O
, O
Alig- O
NART O
shows O
marginally O
reduced O
accuracy O
on O
the O
raw O
dataset O
, O
but O
high O
prediction O
accuracy O
in O
each O
component O
on O
the O
distillation O
set O
, O
resulting O
in O
in- O
creased O
BLEU B-MetricName
scores O
. O

Alignment O
Tool O
Before O
KD O
, O
AligNART B-MethodName
using O
fast O
align O
and O
GIZA++ O
have O
accuracy O
bottlenecks O
in O
permutation O
and O
duplication O
predictors O
, O
respec- O
tively O
, O
as O
shown O
in O
Table O
3 O
. O

The O
results O
imply O
that O
the O
alignment O
tools O
have O
different O
degrees O
of O
multi- O
modality O
on O
the O
D O
, O
P O
, O
and O
G O
matrices O
, O
which O
can O
be O
explored O
in O
the O
future O
work O
. O

Qualitative O
Study O
Table O
4 O
shows O
an O
example O
of O
addressing O
the O
multi O
- O
modality O
problem O
. O

Deep- O
shallow O
NAT O
monotonically O
copies O
the O
encoder O
out- O
puts O
and O
suffers O
from O
repetition O
and O
omission O
prob- O
lems O
. O

AligNART B-MethodName
( I-MethodName
FA I-MethodName
) I-MethodName
does O
not O
show O
the O
inconsis- O
tency O
problems O
thanks O
to O
the O
well O
- O
aligned O
decoder O
inputs O
, O
which O
signiﬁcantly O
reduces O
the O
modality O
of O
the O
target O
distribution O
. O

We O
also O
conducted O
a O
case O
study O
on O
predicted O
alignments O
and O
their O
translations O
during O
re O
- O
scoring O
as O
shown O
in O
the O
Appendix O
. O

Ablation O
Study O
We O
conduct O
an O
analysis O
of O
align- O
ment O
estimation O
by O
ablating O
one O
of O
the O
predictors O
during O
inference O
. O

We O
ablate O
each O
module O
in O
Aligner O
by O
replacing O
the O
predicted O
matrix O
with O
an O
identical O
matrixI. O
The O
results O
in O
Table O
5 O
indicate O
that O
each O
module O
in O
Aligner O
properly O
estimates O
the O
decom- O
posed O
information O
in O
word O
alignments O
. O

However O
, O
there O
is O
an O
exception O
in O
GIZA++ O
where O
many O
- O
to- O
one O
mapping O
does O
not O
exist O
, O
resulting O
in O
perfor- O
mance O
equal O
to O
that O
without O
the O
grouping O
predic- O
tor O
. O

We O
observe O
that O
AligNART B-MethodName 
achieves O
BLEU B-MetricName
scores O
comparable O
to O
those O
of O
CTC B-MethodName
- O
based O
models O
on O
En$De B-DatasetName
even O
with O
the O
ground O
truth O
word O
align- O
ments O
of O
partial O
information O
. O

5.3 O
Analysis O
of O
Modality O
Reduction O
Effects O
To O
evaluate O
the O
modality O
reduction O
effects O
of O
Alig- B-MethodName 
NART I-MethodName
, O
we O
conducted O
experiments O
on O
two O
aspects O
: O
BLEU B-MetricName
score O
and O
token O
repetition O
ratio O
. O

Table O
6 O
shows O
the O
BLEU B-MetricName
scores O
on O
WMT14 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
. O

For O
En!De B-DatasetName
, O
AligNART B-MethodName
using O
fast O
align O
without O
KD O
achieves O
higher O
BLEU B-MetricName
scores O
than O
previous O
mod- O
els O
without O
KD O
and O
deep O
- O
shallow O
NAT O
with O
KD O
. O

The O
results O
indicate O
that O
our O
method O
is O
effective O
even O
without O
KD O
, O
which O
is O
known O
to O
decrease O
data O
complexity O
( O
Zhou O
et O
al O
. O
, O
2020a O
) O
. O

On O
the O
other O
hand O
, O
alignments O
from O
GIZA++ O
without O
KD O
are O
more O
complex O
for O
AligNART O
to O
learn O
, O
resulting O
in O
lower O
BLEU O
scores O
than O
deep O
- O
shallow O
NAT O
with O
KD O
. O

Ghazvininejad O
et O
al O
. O

( O
2020 O
) O
measured O
the O
token O
repetition O
ratio O
as O
a O
proxy O
for O
measuring O
multi- O
modality O
. O

The O
token O
repetition O
ratio O
represents O
the O
degree O
of O
the O
inconsistency O
problem O
. O

In O
Table O
7 O
, O
the O
token O
repetition O
ratio O
of O
AligNART O
is O
less O
than O
that O
of O
the O
CMLM O
- O
base O
( O
Ghazvininejad O
et O
al O
. O
, O
2019 O
) O
of O
5 O
iterations O
, O
AXE B-MethodName
, O
and O
GLAT B-MethodName
. O

We O
also O
observe O
that O
the O
decline O
in O
the O
token O
repetition O
ratio O
from O
Aligner O
is O
signiﬁcantly O
larger O
than O
that O
from O
KD O
. O

Combined O
with O
the O
results O
from O
Table O
6 O
, O
alignment O
information O
adequately O
alleviates O
the O
token O
repeti- O
tion O
issue O
even O
in O
the O
case O
where O
the O
BLEU B-MetricName
score O
is O
lower O
than O
that O
of O
deep O
- O
shallow O
NAT O
with O
KD O
. O

5.4 O
Ablation O
Study O
We O
conduct O
several O
extensive O
experiments O
to O
ana- O
lyze O
our O
method O
further O
as O
shown O
in O
Table O
8 O
and O
9 O
. O

Each O
of O
our O
method O
consistently O
improves O
the O
performance O
of O
AligNART B-MethodName
. O

Cross O
Attention O
As O
shown O
in O
Table O
8 O
, O
we O
ab- O
late O
the O
cross O
attention O
module O
in O
the O
decoder O
to O
observe O
the O
relationship O
between O
aligned O
decoder O
inputs O
and O
alignment O
learning O
of O
the O
cross O
atten- O
tion O
module O
. O

We O
train O
AligNART B-MethodName
and O
deep O
- O
shallow O
NAT O
without O
a O
cross O
attention O
module O
for O
compari- O
son O
. O

AligNART O
without O
the O
cross O
attention O
module O
has O
a O
smaller O
impact O
on O
the O
BLEU B-MetricName
score O
than O
the O
deep O
- O
shallow O
NAT O
. O

The O
cross O
attention O
module O
is O
known O
to O
learn O
alignments O
between O
source O
and O
tar- O
get O
tokens O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
, O
and O
the O
result O
implies O
that O
aligned O
decoder O
inputs O
signiﬁcantly O
ofﬂoad O
the O
role O
of O
the O
cross O
attention O
module O
. O

Deep O
- O
shallow O
Architecture O
Deep O
- O
shallow O
ar- O
chitecture O
heavily O
affects O
the O
BLEU B-MetricName
scores O
of O
Alig- B-MethodName
NART I-MethodName
as O
shown O
in O
Table O
8 O
. O

The O
results O
indicate O
that O
the O
deep O
encoder O
assists O
alignment O
estimation O
, O
whereas O
the O
shallow O
decoder O
with O
aligned O
inputs O
has O
a O
lower O
impact O
on O
performance O
degeneration O
. O

Alignment O
Score O
Filtering O
We O
investigate O
the O
trade O
- O
off O
between O
the O
alignment B-HyperparameterName
score I-HyperparameterName
ﬁltering I-HyperparameterName
ratio I-HyperparameterName
and O
BLEU B-MetricName
score O
using O
AligNART B-MethodName
( I-MethodName
GZ I-MethodName
) I-MethodName
presented O
in O
Table O
9 O
. O

Samples O
with O
low O
alignment O
scores O
are O
more O
likely O
to O
contain O
noise O
caused O
by O
distilled O
tar- O
gets O
or O
an O
alignment O
tool O
. O

We O
observe O
that O
ﬁltering O
out O
of O
5 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
samples O
improves O
the O
BLEU B-MetricName
score O
in O
both O
the O
directions O
. O

Surprisingly O
, O
increasing O
the O
ﬁltering O
ratio O
up O
to O
20 B-HyperparameterValue
% I-HyperparameterValue
preserves O
the O
performance O
thanks O
to O
the O
noise O
ﬁltering O
capability O
. O

6 O
Related O
Work O
6.1 O
Non O
- O
iterative O
NART O
After O
Gu O
et O
al O
. O
( O
2018 O
) O
proposed O
NAT O
, O
non O
- O
iterative O
NART O
has O
been O
investigated O
in O
various O
directions O
to O
maximize O
translation O
speed O
while O
maintaining O
translation O
quality O
. O

Shao O
et O
al O
. O
( O
2019 O
) O
, O
Shao O
et O
al O
. O
( O
2020 O
) O
, O
and O
Ghazvininejad O
et O
al O
. O
( O
2020 O
) O
address O
the O
limitations O
of O
conventional O
cross O
entropy O
based O
ob- O
jectives O
that O
overly O
penalize O
consistent O
predictions O
. O

Lee O
et O
al O
. O
( O
2018 O
) O
, O
Ma O
et O
al O
. O
( O
2019 O
) O
, O
Shu O
et O
al O
. O
( O
2020 O
) O
, O
and O
Lee O
et O
al O
. O
( O
2020 O
) O
introduce O
latent O
variables O
to O
model O
the O
complex O
dependencies O
between O
target O
to- O
kens O
. O

Saharia O
et O
al O
. O
( O
2020 O
) O
and O
Gu O
and O
Kong O
( O
2021 O
) O
apply O
CTC O
loss O
to O
the O
NMT O
domain O
. O

Qian O
et O
al O
. O
( O
2021 O
) O
provide O
target O
tokens O
to O
the O
decoder O
during O
training O
using O
the O
glancing O
sampling O
technique O
. O

6.2 O
Alignment O
in O
Parallel O
Generative O
Models O
In O
other O
domains O
, O
such O
as O
text O
- O
to O
- O
speech O
( O
Ren O
et O
al O
. O
, O
2019 O
; O
Kim O
et O
al O
. O
, O
2020 O
; O
Donahue O
et O
al O
. O
, O
2020 O
) O
, O
a O
common O
assumption O
is O
a O
monotonicity O
in O
the O
align- O
ments O
between O
text O
and O
speech O
. O

Given O
this O
assump- O
tion O
, O
only O
a O
duration O
predictor O
is O
required O
to O
alle- O
viate O
the O
length O
- O
mismatch O
problem O
between O
text O
and O
speech O
. O

On O
the O
other O
hand O
, O
modeling O
the O
align- O
ment O
in O
the O
NMT O
domain O
is O
challenging O
since O
the O
alignment O
contains O
additional O
ordering O
and O
group- O
ing O
information O
. O

Our O
method O
estimates O
an O
arbitrary O
alignment O
matrix O
using O
alignment O
decomposition O
.6.3 O

Improving O
NMT O
with O
Enhanced O
Information O
To O
alleviate O
the O
multi O
- O
modality O
problem O
of O
NART O
models O
, O
Gu O
et O
al O
. O
( O
2018 O
) O
, O
Akoury O
et O
al O
. O
( O
2019 O
) O
, O
Zhou O
et O
al O
. O
( O
2020b O
) O
, O
Ran O
et O
al O
. O
( O
2021 O
) O
, O
and O
Liu O
et O
al O
. O
( O
2021 O
) O
provide O
additional O
sentence O
information O
to O
the O
decoder O
. O

Alignment O
is O
considered O
as O
a O
major O
factor O
in O
machine O
translation O
( O
Li O
et O
al O
. O
, O
2007 O
; O
Zhang O
et O
al O
. O
, O
2017 O
) O
. O

Alkhouli O
et O
al O
. O
( O
2018 O
) O
decompose O
the O
ART O
model O
into O
alignment O
and O
lexical O
models O
. O

Song O
et O
al O
. O
( O
2020 O
) O
use O
the O
predicted O
alignment O
in O
ART O
models O
to O
constrain O
vocabulary O
candidates O
during O
decoding O
. O

However O
, O
the O
alignment O
estimation O
in O
NART O
is O
much O
challenging O
since O
the O
information O
of O
decoding O
outputs O
is O
limited O
. O

In O
NART B-MethodName
, O
Gu O
et O
al O
. O
( O
2018 O
) O
, O
Zhou O
et O
al O
. O
( O
2020b O
) O
, O
and O
Ran O
et O
al O
. O
( O
2021 O
) O
exploit O
partial O
information O
from O
the O
ground O
truth O
alignments O
. O

In O
contrast O
, O
we O
propose O
the O
alignment O
decomposition O
method O
for O
effective O
alignment O
esti- O
mation O
in O
NART O
where O
we O
leverage O
the O
complete O
alignment O
information O
. O

7 O
Conclusion O
and O
Future O
Work O

In O
this O
study O
, O
we O
leverage O
full O
alignment O
informa- O
tion O
to O
directly O
reduce O
the O
degree O
of O
the O
multi- O
modality O
in O
non O
- O
iterative O
NART O
and O
propose O
an O
alignment O
decomposition O
method O
for O
alignment O
estimation O
. O

AligNART B-MethodName
with O
GIZA++ O
shows O
per- O
formance O
comparable O
to O
that O
of O
the O
recent O
CTC- O
based O
implicit O
dependency O
modeling O
approach O
on O
WMT14 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
and O
modality O
reduction O
capability O
. O

However O
, O
we O
observe O
that O
AligNART O
depends O
on O
the O
quality O
of O
the O
ground O
truth O
word O
alignments O
, O
which O
can O
be O
studied O
in O
the O
future O
work O
. O

Further- O
more O
, O
we O
can O
study O
on O
the O
combination O
of O
Alig- O
NART O
and O
implicit O
dependency O
modeling O
meth- O
ods O
. O

Acknowledgement O
This O
work O
was O
supported O
by O
the O
National O
Research O
Foundation O
of O
Korea O
( O
NRF O
) O
grant O
funded O
by O
the O
Korea O
government O
( O
Ministry O
of O
Science O
and O
ICT O
) O
[ O
2018R1A2B3001628 O
] O
, O
the O
BK21 O
FOUR O
program O
of O
the O
Education O
and O
Research O
Program O
for O
Future O
ICT O
Pioneers O
, O
Seoul O
National O
University O
in O
2021 O
, O
AIRS O
Company O
in O
Hyundai O
& O
Kia O
Motor O
Company O
through O
HKMC O
- O
SNU O
AI O
Consortium O
Fund O
, O
and O
Kakao O
Enterprise O
. O

10References O
Nader O
Akoury O
, O
Kalpesh O
Krishna O
, O
and O
Mohit O
Iyyer O
. O

2019 O
. O

Syntactically O
supervised O
transformers O
for O
faster O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Com- O
putational O
Linguistics O
, O
pages O
1269–1281 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O

Tamer O
Alkhouli O
, O
Gabriel O
Bretschner O
, O
and O
Hermann O
Ney O
. O

2018 O
. O

On O
the O
alignment O
problem O
in O
multi O
- O
head O
attention O
- O
based O
neural O
machine O
translation O
. O

In O
Pro- O
ceedings O
of O
the O
Third O
Conference O
on O
Machine O
Trans- O
lation O
: O
Research O
Papers O
, O
pages O
177–185 O
, O
Brussels O
, O
Belgium O
. O

Association O
for O
Computational O
Linguis- O
tics O
. O

Dzmitry O
Bahdanau O
, O
Kyunghyun O
Cho O
, O
and O
Yoshua O
Ben- O
gio O
. O
2015 O
. O

Neural O
machine O
translation O
by O
jointly O
learning O
to O
align O
and O
translate O
. O

In O
3rd O
Inter- O
national O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2015 O
, O
San O
Diego O
, O
CA O
, O
USA O
, O
May O
7 O
- O
9 O
, O
2015 O
, O
Conference O
Track O
Proceedings O
. O

Peter O
F. O
Brown O
, O
Stephen O
A. O
Della O
Pietra O
, O
Vincent O
J. O
Della O
Pietra O
, O
and O
Robert O
L. O
Mercer O
. O

1993 O
. O

The O
math- O
ematics O
of O
statistical O
machine O
translation O
: O
Parameter O
estimation O
. O

Computational O
Linguistics O
, O
19(2):263 O
– O
311 O
. O

Jeff O
Donahue O
, O
Sander O
Dieleman O
, O
Mikolaj O
Binkowski O
, O
Erich O
Elsen O
, O
and O
Karen O
Simonyan O
. O

2020 O
. O

End O
- O
to- O
end O
adversarial O
text O
- O
to O
- O
speech O
. O

In O
International O
Con- O
ference O
on O
Learning O
Representations O
. O

Chris O
Dyer O
, O
Victor O
Chahuneau O
, O
and O
Noah O
A. O
Smith O
. O

2013 O
. O

A O
simple O
, O
fast O
, O
and O
effective O
reparameter- O
ization O
of O
IBM O
model O
2 O
. O

In O
Proceedings O
of O
the O
2013 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Hu- O
man O
Language O
Technologies O
, O
pages O
644–648 O
, O
At- O
lanta O
, O
Georgia O
. O

Association O
for O
Computational O
Lin- O
guistics O
. O

Marjan O
Ghazvininejad O
, O
Vladimir O
Karpukhin O
, O
Luke O
Zettlemoyer O
, O
and O
Omer O
Levy O
. O

2020 O
. O

Aligned O
cross O
entropy O
for O
non O
- O
autoregressive O
machine O
translation O
. O

InProceedings O
of O
the O
37th O
International O
Conference O
on O
Machine O
Learning O
, O
ICML O
2020 O
, O
13 O
- O
18 O
July O
2020 O
, O
Virtual O
Event O
, O
volume O
119 O
of O
Proceedings O
of O
Ma- O
chine O

Learning O
Research O
, O
pages O
3515–3523 O
. O

PMLR O
. O

Marjan O
Ghazvininejad O
, O
Omer O
Levy O
, O
Yinhan O
Liu O
, O
and O
Luke O
Zettlemoyer O
. O

2019 O
. O

Mask O
- O
predict O
: O
Parallel O
de- O
coding O
of O
conditional O
masked O
language O
models O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Lan- O
guage O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
6112 O
– O
6121 O
, O
Hong O
Kong O
, O
China O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Alex O
Graves O
, O
Santiago O
Fernández O
, O
Faustino O
J. O
Gomez O
, O
and O
Jürgen O
Schmidhuber O
. O

2006 O
. O

Connectionist O
tem- O
poral O
classiﬁcation O
: O
labelling O
unsegmented O
sequence O
data O
with O
recurrent O
neural O
networks O
. O

In O
MachineLearning O
, O
Proceedings O
of O
the O
Twenty O
- O
Third O
Interna- O
tional O
Conference O
( O
ICML O
2006 O
) O
, O
Pittsburgh O
, O
Penn- O
sylvania O
, O
USA O
, O
June O
25 O
- O
29 O
, O
2006 O
, O
volume O
148 O
of O
ACM O
International O
Conference O
Proceeding O
Series O
, O
pages O
369–376 O
. O

ACM O
. O

Jiatao O
Gu O
, O
James O
Bradbury O
, O
Caiming O
Xiong O
, O
Vic- O
tor O
O. O
K. O
Li O
, O
and O
Richard O
Socher O
. O

2018 O
. O

Non- O
autoregressive O
neural O
machine O
translation O
. O

In O
6th O
International O
Conference O
on O
Learning O
Representa- O
tions O
, O
ICLR O
2018 O
, O
Vancouver O
, O
BC O
, O
Canada O
, O
April O
30 O
- O
May O
3 O
, O
2018 O
, O
Conference O
Track O
Proceedings O
. O

Open- O
Review.net O
. O

Jiatao O
Gu O
and O
Xiang O
Kong O
. O
2021 O
. O

Fully O
non- O

autoregressive O
neural O
machine O
translation O
: O
Tricks O
of O
the O
trade O
. O

In O
Findings O
of O
the O
Association O
for O
Compu- O
tational O
Linguistics O
: O
ACL O
- O
IJCNLP O
2021 O
, O
pages O
120 O
– O
133 O
, O
Online O
. O

Association O
for O
Computational O
Linguis- O
tics O
. O

Junliang O
Guo O
, O
Linli O
Xu O
, O
and O
Enhong O
Chen O
. O

2020 O
. O

Jointly O
masked O
sequence O
- O
to O
- O
sequence O
model O
for O
non- O
autoregressive O
neural O
machine O
translation O
. O

In O
Pro- O
ceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Associa- O
tion O
for O
Computational O
Linguistics O
, O
pages O
376–385 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Eric O
Jones O
, O
Travis O
Oliphant O
, O
Pearu O
Peterson O
, O
et O
al O
. O
2001 O
. O

SciPy O
: O

Open O
source O
scientiﬁc O
tools O
for O
Python O
. O

Jungo O
Kasai O
, O
James O
Cross O
, O
Marjan O
Ghazvininejad O
, O
and O
Jiatao O
Gu O
. O
2020a O
. O

Non O
- O
autoregressive O
machine O
translation O
with O
disentangled O
context O
transformer O
. O

InProceedings O
of O
the O
37th O
International O
Conference O
on O
Machine O
Learning O
, O
ICML O
2020 O
, O
13 O
- O
18 O
July O
2020 O
, O
Virtual O
Event O
, O
volume O
119 O
of O
Proceedings O
of O
Ma- O
chine O

Learning O
Research O
, O
pages O
5144–5155 O
. O

PMLR O
. O

Jungo O
Kasai O
, O
Nikolaos O
Pappas O
, O
Hao O
Peng O
, O
James O
Cross O
, O
and O
Noah O
Smith O
. O

2020b O
. O

Deep O
encoder O
, O
shallow O
decoder O
: O
Reevaluating O
non O
- O
autoregressive O
machine O
translation O
. O

In O
International O
Conference O
on O
Learn- O
ing O
Representations O
. O

Jaehyeon O
Kim O
, O
Sungwon O
Kim O
, O
Jungil O
Kong O
, O
and O
Sun- O
groh O
Yoon O
. O

2020 O
. O

Glow O
- O
tts O
: O
A O
generative O
ﬂow O
for O
text O
- O
to O
- O
speech O
via O
monotonic O
alignment O
search O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
33 O
: O
Annual O
Conference O
on O
Neural O
Information O
Pro- O
cessing O
Systems O
2020 O
, O
NeurIPS O
2020 O
, O
December O
6- O
12 O
, O
2020 O
, O
virtual O
. O

Yoon O
Kim O
and O
Alexander O
M. O
Rush O
. O
2016 O
. O

Sequence- O
level O
knowledge O
distillation O
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natu- O
ral O
Language O
Processing O
, O
pages O
1317–1327 O
, O
Austin O
, O
Texas O
. O

Association O
for O
Computational O
Linguistics O
. O

Diederik O
P. O
Kingma O
and O
Jimmy O
Ba O
. O
2015 O
. O

Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O

In O
3rd O
Inter- O
national O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2015 O
, O
San O
Diego O
, O
CA O
, O
USA O
, O
May O
7 O
- O
9 O
, O
2015 O
, O
Conference O
Track O
Proceedings O
. O

11Jason O
Lee O
, O
Elman O
Mansimov O
, O
and O
Kyunghyun O
Cho O
. O
2018 O
. O

Deterministic O
non O
- O
autoregressive O
neural O
se- O
quence O
modeling O
by O
iterative O
reﬁnement O
. O

In O
Pro- O
ceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Meth- O
ods O
in O
Natural O
Language O
Processing O
, O
pages O
1173 O
– O
1182 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computa- O
tional O
Linguistics O
. O

Jason O
Lee O
, O
Raphael O
Shu O
, O
and O
Kyunghyun O
Cho O
. O
2020 O
. O

Iterative O
reﬁnement O
in O
the O
continuous O
space O
for O
non O
- O
autoregressive O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
1006–1015 O
, O
Online O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Chi O
- O
Ho O
Li O
, O
Minghui O
Li O
, O
Dongdong O
Zhang O
, O
Mu O
Li O
, O
Ming O
Zhou O
, O
and O
Yi O
Guan O
. O

2007 O
. O

A O
probabilistic O
approach O
to O
syntax O
- O
based O
reordering O
for O
statistical O
machine O
translation O
. O

In O
Proceedings O
of O
the O
45th O
An- O
nual O
Meeting O
of O
the O
Association O
of O
Computational O
Linguistics O
, O
pages O
720–727 O
, O
Prague O
, O
Czech O
Repub- O
lic O
. O

Association O
for O
Computational O
Linguistics O
. O

Ye O
Liu O
, O
Yao O
Wan O
, O
Jianguo O
Zhang O
, O
Wenting O
Zhao O
, O
and O
Philip O
Yu O
. O
2021 O
. O

Enriching O
non O
- O
autoregressive O
trans- O
former O
with O
syntactic O
and O
semantic O
structures O
for O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
the O
16th O
Conference O
of O
the O
European O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Main O
Volume O
, O
pages O
1235–1244 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Xuezhe O
Ma O
, O
Chunting O
Zhou O
, O
Xian O
Li O
, O
Graham O
Neu- O
big O
, O
and O
Eduard O
Hovy O
. O

2019 O
. O

FlowSeq O
: O
Non- O
autoregressive O
conditional O
sequence O
generation O
with O
generative O
ﬂow O
. O

In O
Proceedings O
of O
the O
2019 O
Con- O
ference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Confer- O
ence O
on O
Natural O
Language O
Processing O
( O
EMNLP- O
IJCNLP O
) O
, O
pages O
4282–4292 O
, O
Hong O
Kong O
, O
China O
. O

As- O
sociation O
for O
Computational O
Linguistics O
. O

Franz O
Josef O
Och O
and O
Hermann O
Ney O
. O

2003 O
. O

A O
systematic O
comparison O
of O
various O
statistical O
alignment O
models O
. O

Computational O
Linguistics O
, O
29(1):19–51 O
. O

Myle O
Ott O
, O
Sergey O
Edunov O
, O
Alexei O
Baevski O
, O
Angela O
Fan O
, O
Sam O
Gross O
, O
Nathan O
Ng O
, O
David O
Grangier O
, O
and O
Michael O
Auli O
. O

2019 O
. O

fairseq O
: O

A O
fast O
, O
extensible O
toolkit O
for O
sequence O
modeling O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chap- O
ter O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Demonstrations O
) O
, O
pages O
48–53 O
, O
Minneapolis O
, O
Min- O
nesota O
. O

Association O
for O
Computational O
Linguistics O
. O

Lihua O
Qian O
, O
Hao O
Zhou O
, O
Yu O
Bao O
, O
Mingxuan O
Wang O
, O
Lin O
Qiu O
, O
Weinan O
Zhang O
, O
Yong O
Yu O
, O
and O
Lei O
Li O
. O
2021 O
. O

Glancing O
transformer O
for O
non O
- O
autoregressive O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
the O
59th O
An- O
nual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Confer- O
ence O
on O
Natural O
Language O
Processing O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1993–2003 O
, O
Online O
. O

Associa- O
tion O
for O
Computational O
Linguistics O
. O

Qiu O
Ran O
, O
Yankai O
Lin O
, O
Peng O
Li O
, O
and O
Jie O
Zhou O
. O

2021 O
. O

Guiding O
non O
- O
autoregressive O
neural O
machine O
transla- O
tion O
decoding O
with O
reordering O
information O
. O

Proceed- O
ings O
of O
the O
AAAI O
Conference O
on O
Artiﬁcial O
Intelli- O
gence O
, O
35(15):13727–13735 O
. O

Yi O
Ren O
, O
Yangjun O
Ruan O
, O
Xu O
Tan O
, O
Tao O
Qin O
, O
Sheng O
Zhao O
, O
Zhou O
Zhao O
, O
and O
Tie O
- O
Yan O
Liu O
. O
2019 O
. O

Fastspeech O
: O
Fast O
, O
robust O
and O
controllable O
text O
to O
speech O
. O

In O
Ad- O
vances O
in O
Neural O
Information O
Processing O
Systems O
32 O
: O
Annual O
Conference O
on O
Neural O
Information O
Pro- O
cessing O
Systems O
2019 O
, O
NeurIPS O
2019 O
, O
December O
8 O
- O
14 O
, O
2019 O
, O
Vancouver O
, O
BC O
, O
Canada O
, O
pages O
3165 O
– O
3174 O
. O

Chitwan O
Saharia O
, O
William O
Chan O
, O
Saurabh O
Saxena O
, O
and O
Mohammad O
Norouzi O
. O
2020 O
. O

Non O
- O
autoregressive O
ma- O
chine O
translation O
with O
latent O
alignments O
. O

In O
Proceed- O
ings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
1098–1108 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Chenze O
Shao O
, O
Yang O
Feng O
, O
Jinchao O
Zhang O
, O
Fandong O
Meng O
, O
Xilin O
Chen O
, O
and O
Jie O
Zhou O
. O

2019 O
. O

Retrieving O
sequential O
information O
for O
non O
- O
autoregressive O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
the O
57th O
An- O
nual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
3013–3024 O
, O
Florence O
, O
Italy O
. O

Asso- O
ciation O
for O
Computational O
Linguistics O
. O

Chenze O
Shao O
, O
Jinchao O
Zhang O
, O
Yang O
Feng O
, O
Fandong O
Meng O
, O
and O
Jie O
Zhou O
. O

2020 O
. O

Minimizing O
the O
bag O
- O
of- O
ngrams O
difference O
for O
non O
- O
autoregressive O
neural O
ma- O
chine O
translation O
. O

In O
The O
Thirty O
- O
Fourth O
AAAI O
Con- O
ference O
on O
Artiﬁcial O
Intelligence O
, O
AAAI O
2020 O
, O
The O
Thirty O
- O
Second O
Innovative O
Applications O
of O
Artiﬁcial O
Intelligence O
Conference O
, O
IAAI O
2020 O
, O
The O
Tenth O
AAAI O
Symposium O
on O
Educational O
Advances O
in O
Artiﬁcial O
In- O
telligence O
, O
EAAI O
2020 O
, O
New O
York O
, O
NY O
, O
USA O
, O
Febru- O
ary O
7 O
- O
12 O
, O
2020 O
, O
pages O
198–205 O
. O

AAAI O
Press O
. O

Raphael O
Shu O
, O
Jason O
Lee O
, O
Hideki O
Nakayama O
, O
and O
Kyunghyun O
Cho O
. O
2020 O
. O

Latent O
- O
variable O
non- O
autoregressive O
neural O
machine O
translation O
with O
deter- O
ministic O
inference O
using O
a O
delta O
posterior O
. O

In O
Pro- O
ceedings O
of O
the O
AAAI O
Conference O
on O
Artiﬁcial O
Intel- O
ligence O
, O
volume O
34 O
, O
pages O
8846–8853 O
. O

Kai O
Song O
, O
Kun O
Wang O
, O
Heng O
Yu O
, O
Yue O
Zhang O
, O
Zhongqiang O
Huang O
, O
Weihua O
Luo O
, O
Xiangyu O
Duan O
, O
and O
Min O
Zhang O
. O

2020 O
. O

Alignment O
- O
enhanced O
trans- O
former O
for O
constraining O
nmt O
with O
pre O
- O
speciﬁed O
trans- O
lations O
. O

In O
AAAI O
, O
pages O
8886–8893 O
. O

Zhiqing O
Sun O
, O
Zhuohan O
Li O
, O
Haoqing O
Wang O
, O
Di O
He O
, O
Zi O
Lin O
, O
and O
Zhi O
- O
Hong O
Deng O
. O
2019 O
. O

Fast O
structured O
decoding O
for O
sequence O
models O
. O

In O
Advances O
in O
Neu- O
ral O
Information O
Processing O
Systems O
32 O
: O
Annual O
Con- O
ference O
on O
Neural O
Information O
Processing O
Systems O
2019 O
, O
NeurIPS O
2019 O
, O
December O
8 O
- O
14 O
, O
2019 O
, O
Vancou- O
ver O
, O
BC O
, O
Canada O
, O
pages O
3011–3020 O
. O

Zhiqing O
Sun O
and O
Yiming O
Yang O
. O

2020 O
. O

An O
EM O
approach O
to O
non O
- O
autoregressive O
conditional O
sequence O
genera- O

12tion O
. O

In O
Proceedings O
of O
the O
37th O
International O
Con- O
ference O
on O
Machine O
Learning O
, O
ICML O
2020 O
, O
13 O
- O
18 O
July O
2020 O
, O
Virtual O
Event O
, O
volume O
119 O
of O
Proceedings O
of O
Machine O
Learning O
Research O
, O
pages O
9249–9258 O
. O

PMLR O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N. O
Gomez O
, O
Lukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
Advances O
in O
Neural O
Information O
Pro- O
cessing O
Systems O
30 O
: O
Annual O
Conference O
on O
Neural O
Information O
Processing O
Systems O
2017 O
, O
December O
4- O
9 O
, O
2017 O
, O
Long O
Beach O
, O
CA O
, O
USA O
, O
pages O
5998–6008 O
. O

Jinchao O
Zhang O
, O
Mingxuan O
Wang O
, O
Qun O
Liu O
, O
and O
Jie O
Zhou O
. O
2017 O
. O

Incorporating O
word O
reordering O
knowl- O
edge O
into O
attention O
- O
based O
neural O
machine O
transla- O
tion O
. O

In O
Proceedings O
of O
the O
55th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Vol- O
ume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1524–1534 O
, O
Vancouver O
, O
Canada O
. O

Association O
for O
Computational O
Linguistics O
. O

Chunting O
Zhou O
, O
Jiatao O
Gu O
, O
and O
Graham O
Neubig O
. O
2020a O
. O

Understanding O
knowledge O
distillation O
in O
non- O
autoregressive O
machine O
translation O
. O

In O
8th O
Inter- O
national O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2020 O
, O
Addis O
Ababa O
, O
Ethiopia O
, O
April O
26 O
- O
30 O
, O
2020 O
. O

OpenReview.net O
. O

Long O
Zhou O
, O
Jiajun O
Zhang O
, O
Yang O
Zhao O
, O
and O
Chengqing O
Zong O
. O

2020b O
. O

Non O
- O
autoregressive O
neural O
machine O
translation O
with O
distortion O
model O
. O

In O
CCF O
Interna- O
tional O
Conference O
on O
Natural O
Language O
Processing O
and O
Chinese O
Computing O
, O
pages O
403–415 O
. O

Springer O
. O

Appendix O
A O
Mappings O
in O
Alignment O
In O
general O
, O
there O
are O
one O
- O
to O
- O
one O
, O
one O
- O
to O
- O
many O
, O
many O
- O
to O
- O
one O
, O
and O
many O
- O
to O
- O
many O
mappings O
exclud- O
ing O
zero O
- O
fertility O
and O
spurious O
word O
cases O
( O
see O
Fig- O
ure O
2 O
) O
. O

Distortion O
and O
ReorderNAT O
can O
not O
rep- O
resent O
many O
- O
to O
- O
one O
, O
many O
- O
to O
- O
many O
, O
and O
spurious O
word O
cases O
. O

The O
grouping O
predictor O
in O
AligNART B-MethodName
models O
many O
- O
to O
- O
one O
and O
many O
- O
to O
- O
many O
mappings O
. O

The O
addition O
of O
a O
spurious O
token O
, O
which O
is O
applied O
to O
AligNART B-MethodName
( I-MethodName
FA I-MethodName
) I-MethodName
, O
enables O
us O
to O
address O
the O
spuri- O
ous O
word O
case O
, O
which O
is O
explained O
in O
Section O
C.2 O
. O

During O
the O
experiments O
, O
we O
observe O
that O
the O
intro- O
duction O
of O
a O
spurious O
token O
degrades O
the O
perfor- O
mance O
for O
GIZA++ O
. O

We O
guess O
the O
reason O
of O
the O
degradation O
is O
that O
alignment O
matrix O
from O
GIZA++ O
contains O
more O
than O
two O
times O

as O
many O
empty O
rows O
as O
that O
of O
fast O
align O
on O
WMT14 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
. O
B O
Architecture O
of O
Aligner O

The O
duplication O
predictor O
and O
grouping O
predictor O
modules O
consist O
of O
a O
convolutional O
layer O
, O
ReLU O
ac- O
tivation O
, O
layer O
normalization O
, O
dropout O
, O
and O
a O
projec- O
tion O
layer O
, O
same O
as O
the O
phoneme O
duration O
predictor O
in O
FastSpeech O
( O
Ren O
et O
al O
. O
, O
2019 O
) O
, O
which O
is O
a O
parallel O
text O
- O
to O
- O
speech O
model O
. O

The O
permutation O
predictor O
in O
Aligner O
consists O
of O
three O
encoder O
layers O
: O
pre O
- O
network O
, O
query O
= O
key O
network O
, O
and O
single O
- O
head O
attention O
module O
for O
the O
outputs O
. O

Note O
that O
the O
outputs O
of O
the O
pre O
- O
network O
are O
passed O
to O
the O
query O
and O
key O
networks O
. O

To O
pre- O
vent O
the O
predicted O
permutation O
matrix O
from O
being O
an O
identity O
matrix O
, O
we O
apply O
a O
gate O
function O
to O
the O
last O
attention O
module O
in O
the O
permutation O
predictor O
to O
modulate O
the O
probabilities O
of O
un O
- O
permuted O
and O
permuted O
cases O
. O

We O
formulate O
the O
output O
of O
gated O
attention O
as O
follows O
: O
g=(Qu O
) O
( O
14 O
) O
Ppred O
= O
softmax O
( O
M+QKT O
) O
( O
15 O
) O
Ppred O
= O
Dg+ O
( O
I Dg)Ppred O
; O
( O
16 O
) O
where O
 O
is O
the O
sigmoid O
function O
and O
Q O
= O
K O
is O
the O
output O
of O
the O
query O
= O
key O
network O
, O
respectively O
. O

g B-HyperparameterName
is O
the B-HyperparameterName
probability I-HyperparameterName
of I-HyperparameterName
an I-HyperparameterName
un I-HyperparameterName
- I-HyperparameterName
permuted I-HyperparameterName
case I-HyperparameterName
. O

Mis O
a O
diagonal O
mask O
matrix O
, O
where O
the O
values O
of O
the O
di- O
agonal O
elements O
are O
 inf O
. O

Iis O
an O
identical O
matrix O
andDgis O
a O
diagonal O
matrix O
with O
gas O
the O
main O
diagonal O
. O

C O
Alignment O
Processing O
C.1 O
Word O
- O
to O
- O
subword O
Alignment O

To O
reduce O
the O
complexity O
of O
alignment O
, O
we O
further O
assume O
that O
the O
alignment O
process O
is O
conducted O
at O
the O
word O
- O
level O
. O

We O
decompose O
the O
alignment O
ma- O
trix O
into O
the O
source O
subword O
to O
source O
word O
matrix O
Sand O
the O
source O
word O
to O
target O
subword O
matrix O
Awsas O
depicted O
in O
Figure O
3 O
. O

Since O
Sis O
always O
given O
, O
Awsis O
the O
only O
target O
to O
be O
learned O
. O

First O
, O
we O
derive O
the O
source O
subword O
to O
target O
subword O
matrixAusing O
the O
alignment O
tool O
. O

Awsis O
achieved O
by O
clipping O
the O
maximum O
value O
of O
AS O
> O
to O
1.Aws O
reduces O
the O
search O
space O
because O
of O
the O
assumption O
that O
source O
tokens O
duplicate O
, O
permute O
, O
and O
group O
at O
the O
word O
- O
level O
. O

However O
, O
there O
is O
a O
trade O
- O
off O
be- O
tween O
the O
simplicity O
and O
resolution O
of O
information O
. O

The O
recovered O
source O
subword O
to O
target O
subword O
matrixAwsSloses O
the O
subword O
- O
level O
information O
as O
shown O
in O
the O
rightmost O
matrix O
in O
Figure O
3 O
. O

C.2 O
Filling O
Null O
Rows O
in O
Alignment O
Matrix O

The O
output O
of O
the O
alignment O
tool O
usually O
contains O
empty O
rows O
which O
means O
that O
no O
aligned O
source O
token O
exists O
for O
certain O
target O
tokens O
. O

We O
select O
two O
strategies O
to O
ﬁll O
the O
null O
rows O
: O
( O
i)copy O
the O
alignment O
from O
the O
previous O
target O
token O
, O
or O
( O
ii O
) O
introduce O
a O
special O
spurious O
token O
. O

For O
the O
second O
strategy O
, O
we O
concatenate O
a O
special O
spurious O
token O
at O
the O
end O
of O
the O
source O
sentence O
. O

If O
the O
current O
and O
previous O
target O
tokens O
belong O
to O
the O
same O
word O
, O
we O
follow O
( O
i O
) O
. O

The O
remaining O
target O
tokens O
of O
the O
null O
alignment O
are O
aligned O
to O
the O
spurious O
token O
. O

C.3 O
Details O
of O
Alignment O
Tool O
Conﬁguration O
Forfast O
align O
, O
we O
follow O
the O
default O
setting O
for O
for- O
ward O
/ O
backward O
directions O
and O
obtain O
symmetrized O
alignment O
with O
the O
grow O
- O
diag-ﬁnal O
- O
and O
option O
. O

We O
apply O
the O
word O
- O
to O
- O
subword O
alignment O
technique O
andspurious O
token O
strategy O
for O
null O
alignments O
. O

For O
GIZA++ O
, O
we O
apply O
the O
word O
- O
to O
- O
subword O
align- O
ment O
technique O
and O
copy O
the O
alignment O
from O
the O
previous O
target O
token O
for O
null O
alignment O
. O

We O
set O
the O
alignment B-HyperparameterName
score I-HyperparameterName
ﬁltering I-HyperparameterName
ratio I-HyperparameterName
to O
5 B-HyperparameterValue
% I-HyperparameterValue
. O

D O
Case O
Study O
To O
analyze O
various O
alignments O
and O
their O
transla- O
tions O
during O
re O
- O
scoring O
decoding O
, O
we O
conduct O
acase O
study O
on O
WMT14 B-DatasetName
De I-DatasetName
! I-DatasetName
En I-DatasetName
validation O
set O
as O
shown O
in O
Figure O
4 O
. O

The O
two O
translations O
have O
differ- O
ent O
orderings O
: O
the O
telescope O
’s O
tasks O
andthe O
tasks O
of O
the O
telescope O
. O

In O
this O
sample O
, O
we O
observe O
that O
Alig- B-MethodName
NART I-MethodName
( O
i) O
can O
capture O
non O
- O
diagonal O
alignments O
, O
( O
ii)models O
multiple O
alignments O
, O
and O
( O
iii)trans- O
lates O
corresponding O
to O
the O
given O
alignments O
. O

Zero B-TaskName
- I-TaskName
Shot I-TaskName
Cross I-TaskName
- I-TaskName
Lingual I-TaskName
Transfer I-TaskName
of O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName
with O
Multilingual B-MethodName
Pretrained I-MethodName
Encoders I-MethodName

Guanhua O
Chen1 O
, O
Shuming O
Ma2 O
, O
Yun O
Chen3y O
, O
Li O
Dong2 O
Dongdong O
Zhang2 O
, O
Jia O
Pan1 O
, O
Wenping O
Wang4;1 O
, O
Furu O
Wei2 O

1The O
University O
of O
Hong O
Kong;2Microsoft O
Research O
3Shanghai O
University O
of O
Finance O
and O
Economics;4Texas O
A&M O
University O
{ O
ghchen,jpan,wenping}@cs.hku.hk O
, O
yunchen@sufe.edu.cn O
, O
{ O
shumma O
, O
lidong1 O
, O
dozhang O
, O

fuwei}@microsoft.com O
Abstract O
Previous O
work O
mainly O
focuses O
on O
improving O
cross O
- O
lingual O
transfer O
for O
NLU B-TaskName
tasks O
with O
a O
multilingual O
pretrained O
encoder O
( O
MPE O
) O
, O
or O
im- O
proving O
the O
performance O
on O
supervised O
ma- O
chine O
translation O
with O
BERT O
. O

However O
, O
it O
is O
under O
- O
explored O
that O
whether O
the O
MPE O
can O
help O
to O
facilitate O
the O
cross O
- O
lingual O
transferability O
of O
NMT B-TaskName
model O
. O

In O
this O
paper O
, O
we O
focus O
on O
a O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
task O
in O
NMT B-TaskName
. O

In O
this O
task O
, O
the O
NMT O
model O
is O
trained O
with O
parallel O
dataset O
of O
only O
one O
language O
pair O
and O
an O
off O
- O
the O
- O
shelf O
MPE O
, O
then O
it O
is O
directly O
tested O
on O
zero O
- O
shot O
language O
pairs O
. O

We O
pro- O
pose O
SixT B-MethodName
, O
a O
simple O
yet O
effective O
model O
for O
this O
task O
. O

SixT B-MethodName
leverages O
the O
MPE O
with O
a O
two O
- O
stage O
training O
schedule O
and O
gets O
further O
improvement O
with O
a O
position O
disentangled O
en- O
coder O
and O
a O
capacity O
- O
enhanced O
decoder O
. O

Using O
this O
method O
, O
SixT B-MethodName
signiﬁcantly O
outperforms O
mBART B-MethodName
, O
a O
pretrained O
multilingual O
encoder- O
decoder O
model O
explicitly O
designed O
for O
NMT B-TaskName
, O
with O
an O
average O
improvement O
of O
7.1 B-MetricValue
BLEU B-MetricName
on O
zero O
- O
shot O
any O
- O
to O
- O
English O
test O
sets O
across O
14 O
source O
languages O
. O

Furthermore O
, O
with O
much O
less O
training O
computation O
cost O
and O
training O
data O
, O
our O
model O
achieves O
better O
performance O
on O
15 O
any O
- O
to O
- O
English O
test O
sets O
than O
CRISS B-MethodName
and O
m2m-100 B-MethodName
, O
two O
strong O
multilingual O
NMT B-TaskName
base- O
lines O
. O

1 O
Introduction O
Multilingual O
pretrained O
encoders O
( O
MPE O
) O
such O
as O
mBERT B-MethodName
( O
Wu O
and O
Dredze O
, O
2019 O
) O
, O
XLM B-MethodName
( O
Con- O
neau O
and O
Lample O
, O
2019 O
) O
, O
and O
XLM B-MethodName
- I-MethodName
R I-MethodName
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
have O
shown O
remarkably O
strong O
re- O
sults O
on O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
mainly O
for O
natural B-TaskName
language I-TaskName
understanding I-TaskName
( O
NLU B-TaskName
) O
tasks O
, O
including O
named B-TaskName
entity I-TaskName
recognition I-TaskName
( O
NER B-TaskName
) O
, O
ques- B-TaskName
tion I-TaskName
answering I-TaskName
( O
QA B-TaskName
) O
and O
natural B-TaskName
language I-TaskName
infer- I-TaskName
ence I-TaskName
( O
NLI B-TaskName
) O
. O

These O
methods O
jointly O
train O
a O
Trans- O
former O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
encoder O
to O
perform O
masked O
language O
modeling O
task O
in O
multiple O
lan- O
guages O
. O

The O
pretrained O
model O
is O
then O
ﬁne O
- O
tuned O
on O
a O
downstream O
NLU B-TaskName
task O
using O
labeled O
data O
in O
a O
single O
language O
and O
evaluated O
on O
the O
same O
task O
in O
other O
languages O
. O

With O
this O
pretraining O
and O
ﬁne- O
tuning O
approach O
, O
the O
MPE O
is O
able O
to O
generalize O
to O
other O
languages O
that O
even O
do O
not O
have O
labeled O
data O
. O

Given O
that O
MPE O
has O
achieved O
great O
success O
in O
cross- B-TaskName
lingual I-TaskName
NLU I-TaskName
tasks O
, O
a O
question O
worthy O
of O
research O
is O
how O
to O
perform O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
in O
the O
NMT B-TaskName
task O
by O
leveraging O
the O
MPE B-MethodName
. O

Some O
work O
( O
Zhu O
et O
al O
. O
, O
2020 O
; O
Yang O
et O
al O
. O
, O
2020 O
; O
Weng O
et O

al O
. O
, O
2020 O
; O
Imamura O
and O
Sumita O
, O
2019 O
) O
explores O
approaches O
to O
improve O
NMT O
performance O
by O
in- O
corporating O
monolingual O
pretrained O
Transformer O
encoder O
such O
as O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

How- O
ever O
, O
simply O
replacing O
the O
monolingual O
pretrained O
encoder O
in O
previous O
studies O
with O
MPE O
does O
not O
work O
well O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
of O
NMT B-TaskName
( O
see O
baselines O
in O
Table O
2 O
) O
. O

Others O
propose O
to O
ﬁne O
- O
tune O
the O
encoder O
- O
decoder O
- O
based O
multilingual O
pretrained O
model O
for O
cross O
- O
lingual O
transfer O
of O
NMT O
( O
Liu O
et O
al O
. O
, O
2020 O
; O
Lin O
et O
al O
. O
, O
2020 O
) O
. O

It O
is O
still O
unclear O
how O
to O
conduct O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
for O
NMT B-TaskName
model O
with O
existing O
multilingual O
pretrained O
encoders O
such O
as O
XLM B-MethodName
- I-MethodName
R. I-MethodName

In O
this O
paper O
, O
we O
focus O
on O
a O
Zero B-TaskName
- I-TaskName
shot I-TaskName
cross- I-TaskName
lingual I-TaskName
( I-TaskName
X I-TaskName
) I-TaskName
NMT I-TaskName
Transfer I-TaskName
task O
( O
ZeXT B-TaskName
, O
see O
Fig- O
ure O
1 O
) O
, O
which O
aims O
at O
translating O
multiple O
unseen O
languages O
by O
leveraging O
an O
MPE O
. O

Different O
from O
unsupervised O
or O
multilingual O
NMT O
, O
only O
an O
MPE O
and O
parallel O
dataset O
of O
one O
language O
pair O
such O
as O
German O
- O
English O
are O
available O
in O
this O
task O
. O

The O
trained O
model O
is O
directly O
tested O
on O
many O
- O
to O
- O
one O
test O
sets O
in O
a O
zero O
- O
shot O
manner O
. O

We O
propose O
a O
Simple B-MethodName
cross I-MethodName
- I-MethodName
lingual I-MethodName
( I-MethodName
X) I-MethodName
Transfer I-MethodName
NMT I-MethodName
model I-MethodName
( O
SixT B-MethodName
) O
which O
can O
directly O
translates O
languages O
unseen O
during O
supervised O
training O
. O

We O
initialize O
the O
encoder O
and O
decoder O
embeddings O
of O
SixT B-MethodName
with O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
propose O
a O
two O
- O
stage O
training O
schedule O
that O
trades O
off O
between O
super- O
vised O
performance O
and O
transferability O
. O

At O
the O
ﬁrst O
stage O
, O
we O
only O
train O
the O
decoder O
layers O
, O
while O
at O
the O
second O
stage O
, O
all O
model O
parameters O
are O
jointly O
optimized O
except O
the O
encoder O
embedding O
. O

We O
fur- O
ther O
improve O
the O
model O
by O
introducing O
a O
position O
disentangled O
encoder O
and O
a O
capacity O
- O
enhanced O
de- O
coder O
. O

The O
position O
disentangled O
encoder O
enhances O
cross O
- O
lingual O
transferability O
by O
removing O
residual O
connection O
in O
one O
of O
the O
encoder O
layers O
and O
mak- O
ing O
the O
encoder O
outputs O
more O
language O
- O
agnostic O
. O

The O
capacity O
- O
enhanced O
decoder O
leverages O
a O
bigger O
decoder O
than O
vanilla O
Transformer O
base O
model O
to O
fully O
utilize O
the O
labelled O
dataset O
. O

Although O
trained O
with O
only O
one O
language O
pair O
, O
the O
SixT B-MethodName
model O
alle- O
viates O
the O
effect O
of O
‘ O
catastrophic O
forgetting O
’ O
( O
Serra O
et O
al O
. O
, O
2018 O
) O
and O
can O
be O
transferred O
to O
unseen O
lan- O
guages O
. O

SixT B-MethodName
signiﬁcantly O
outperforms O
mBART B-MethodName
with O
an O
average O
improvement O
of O
7.1 B-MetricValue
BLEU B-MetricName
on O
zero- B-TaskName
shot I-TaskName
any I-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
English I-TaskName
translation I-TaskName
across O
14 O
source O
languages O
. O

Furthermore O
, O
with O
much O
less O
training O
computation O
cost O
and O
training O
data O
, O
the O
SixT B-MethodName
model O
gets O
better O
performance O
on O
15 O
any O
- O
to O
- O
English O
test O
sets O
than O
CRISS B-MethodName
and O
m2m-100 B-MethodName
, O
two O
strong O
multi- O
lingual O
NMT O
baselines.1 O
2 O
Problem O
Statement O

The O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
NMT I-TaskName
transfer I-TaskName
task O
( O
ZeXT B-TaskName
) O
explores O
approaches O
to O
enhance O
the O
cross- O
lingual O
transferability O
of O
NMT O
model O
. O

Given O
an O
MPE O
and O
parallel O
dataset O
of O
a O
language O
pair O
ls O
- O
to- O
lt O
, O
where O
ls O
and O
lt O
are O
supported O
by O
the O
MPE O
, O
we O
aim O
to O
train O
an O
NMT O
model O
that O
can O
be O
transferred O
to O
multiple O
unseen O
language O
pairs O
li O
z O
- O
to O
- O
lt O
, O
where O
li O
z6 O
= O
lsandli O
zis O
supported O
by O
the O
MPE O
. O

The O
learned O
NMT O
model O
is O
directly O
tested O
between O
the O
un- O
seen O
language O
pairs O
li O
z O
- O
to O
- O
ltin O
a O
zero O
- O
shot O
manner O
. O

Different O
from O
multilingual B-TaskName
NMT I-TaskName
( O
Johnson O
et O
al O
. O
, O
2017 O
) O
, O
unsupervised B-TaskName
NMT I-TaskName
( O
Lample O
et O
al O
. O
, O
2018 O
) O
or O
zero B-TaskName
- I-TaskName
resource I-TaskName
NMT I-TaskName
through O
pivoting O
( O
Chen O
et O
al O
. O
, O
2017 O
, O
2018 O
) O
, O
neither O
the O
parallel O
nor O
monolingual O
data O
in O
the O
language O
li O
zis O
directly O
accessible O
in O
the O
ZeXT B-TaskName
task O
. O

The O
model O
has O
to O
rely O
on O
the O
off- O
the O
- O
shelf O
MPE O
to O
translate O
from O
language O
li O
z. O

The O
challenge O
to O
this O
task O
is O
how O
to O
leverage O
an O
MPE O
for O
machine O
translation O
while O
preserving O
its O
cross- O
lingual O
transferability O
. O

In O
this O
paper O
, O
we O
utilize O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
which O
is O
jointly O
trained O
on O
100languages O
, O
as O
the O
off O
- O
the O
- O
shelf O
MPE O
. O

The O
ZeXT B-TaskName
task O
calls O
for O
approaches O
to O
efﬁciently O
build O
a O
many O
- O
to O
- O
one O
NMT O
model O
that O
can O
translate O
from O
100languages O
supported O
by O
XLM B-MethodName
- I-MethodName
R I-MethodName
with O
par- O
allel O
dataset O
of O
only O
one O
language O
pair O
. O

The O
trained O
model O
could O
be O
useful O
for O
translating O
resource O
- O
poor O
languages O
. O

It O
can O
further O
extend O
to O
scenarios O
where O
datasets O
of O
more O
language O
pairs O
are O
available O
. O

In O
addition O
, O
while O
currently O
the O
cross O
- O
lingual O
transfer- O
ability O
of O
different O
MPEs O
is O
mainly O
evaluated O
on O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NLU O
tasks O
, O
the O
ZeXT B-TaskName
task O
provides O
a O
new O
perspective O
for O
the O
evaluation O
, O
which O
can O
hopefully O
facilitate O
the O
research O
on O
MPEs O
. O

3 O
Approach O
3.1 O
Initialization O
and O
Fine O
- O
tuning O
Strategy O
For O
downstream O
tasks O
like O
cross O
- O
lingual O
NLI O
/ O
QA O
, O
only O
an O
output O
layer O
is O
added O
to O
the O
pretrained O
en- O
coder O
at O
the O
ﬁne O
- O
tuning O
stage O
. O

In O
contrast O
, O
an O
en- O
tire O
decoder O
is O
added O
on O
top O
of O
the O
MPE O
when O
the O
model O
is O
adapted O
to O
NMT O
task O
. O

The O
conven- O
tional O
strategy O
that O
ﬁne O
- O
tunes O
all O
parameters O
re- O
duces O
the O
cross O
- O
lingual O
transferability O
in O
the O
pre- O
trained O
encoder O
due O
to O
the O
catastrophic O
forgetting O
effect O
. O

Therefore O
, O
we O
make O
an O
empirical O
explo- O
ration O
on O
how O
to O
initialize O
and O
ﬁne O
- O
tune O
the O
NMT O
model O
with O
an O
MPE O
. O

The O
NMT O
model O
can O
be O
di- O
vided O
into O
four O
parts O
in O
our O
method O
: O
encoder O
em- O
bedding O
, O
encoder O
layers O
, O
decoder O
embedding O
, O
and O
decoder O
layers O
. O

With O
an O
MPE O
, O
each O
part O
can O
be O
trained O
with O
one O
of O
the O
following O
methods O
, O
namely O
, O
Rand O
: O
randomly O
initialized O
and O
trained O
; O
Fix O
: O
initialized O
from O
the O
MPE O
and O
ﬁxed O
; O
FT O
: O
initialized O
from O
the O
MPE O
and O
trained O
. O

We O
compare O
different O
ﬁne O
- O
tuning O
strategies O
for O
these O
modules O
in O
a O
greedy O
manner O
. O

Starting O
from O
vanilla O
Transformer O
where O
all O
parts O
are O
randomly O
initialized O
, O
we O
explore O
the O
best O
training O
method O
for O
the O
encoder O
embedding O
, O
the O
encoder O
layers O
, O
the O
decoder O
embedding O
, O
and O
the O
decoder O
layers O
, O
se- O
quentially O
. O

The O
details O
of O
experimental O
settings O
are O
in O
the O
Section O
4.1 O
. O

From O
the O
results O
shown O
in O
Ta- O
ble O
1 O
, O
we O
observe O
that O
it O
is O
the O
best O
to O
initialize O
the O
encoder O
embedding O
, O
the O
encoder O
layers O
and O
the O
decoder O
embedding O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
keep O
their O
parameters O
frozen O
, O
while O
randomly O
initializing O
the O
decoder O
layers O
( O
see O
Figure O
2 O
) O
. O

More O
discussions O
are O
in O
the O
Section O
4.2 O
. O

Two O
- O
stage O
training O
Since O
we O
freeze O
the O
encoder O
and O
only O
train O
the O
decoder O
layers O
, O
the O
model O
is O
able O
to O
perform O
translation O
while O
preserving O
the O
trans- O
ferability O
of O
the O
encoder O
. O

However O
, O
freezing O
most O
of O
the O
parameters O
limits O
the O
capacity O
of O
the O
NMT O
model O
, O
especially O
when O
the O
training O
data O
goes O
large O
. O

Therefore O
, O
we O
propose O
a O
second O
training O
stage O
to O
further O
improve O
the O
translation O
performance O
by O
jointly O
ﬁne O
- O
tuning O
all O
parameters O
except O
encoder O
embedding O
of O
the O
NMT.2Since O
the O
decoder O
has O
been O
well O
adapted O
to O
the O
encoder O
at O
the O
ﬁrst O
stage O
, O
we O
expect O
the O
model O
can O
be O
slightly O
ﬁne O
- O
tuned O
to O
improve O
the O
translation O
capacity O
without O
losing O
the O
2According O
to O
our O
preliminary O
experiment O
, O
the O
average O
BLEU O
is O
0.2 O
lower O
when O
the O
encoder O
embedding O
is O
also O
learned O
at O
the O
second O
stage O
. O

Besides O
, O
freezing O
encoder O
embed- O
ding O
leads O
to O
higher O
computational O
efﬁciency.transferability O
of O
the O
encoder O
. O

3.2 O
Model O
The O
training O
strategy O
and O
generalization O
objective O
of O
our O
model O
are O
different O
from O
vanilla O
Transformer O
. O

This O
motivates O
us O
to O
propose O
a O
new O
model O
that O
can O
further O
improve O
on O
zero B-TaskName
- I-TaskName
shot I-TaskName
translations I-TaskName
. O

The O
proposed O
model O
consists O
of O
a O
position O
disentangled O
encoder O
and O
a O
capacity O
- O
enhanced O
decoder O
, O
which O
aims O
at O
enhancing O
the O
cross O
- O
lingual O
transferability O
of O
the O
encoder O
and O
fully O
utilizing O
the O
labelled O
data O
, O
respectively O
. O

Position O
disentangled O
encoder O

The O
representa- O
tions O
from O
XLM B-MethodName
- I-MethodName
R I-MethodName
initialized O
encoder O
have O
a O
strong O
positional O
correspondence O
to O
the O
source O
sentence O
. O

The O
word O
order O
information O
inside O
is O
language O
- O
speciﬁc O
and O
may O
hinder O
the O
cross O
- O
lingual O
transfer O
from O
supervised O
source O
language O
to O
unseen O
languages O
. O

Inspired O
by O
Liu O
et O
al O
. O
( O
2021 O
) O
, O
we O
pro- O
pose O
to O
relax O
this O
structural O
constraint O
and O
make O
the O
encoder O
outputs O
less O
position- O
and O
language- O
speciﬁc O
. O

More O
speciﬁcally O
, O
at O
the O
second O
stage O
, O
we O
remove O
the O
residual O
connection O
after O
the O
self- O
attention O
sublayer O
in O
one O
of O
the O
encoder O
layers O
i O
during O
training O
and O
inference. O

The O
other O
encoder O
layers O
remain O
the O
same O
. O

The O
hidden O
states O
in O
this O
ithencoder O
layer O
are O
calculated O
as O
the O
following O
pseudo O
code O
: O
1h[i O
] O
= O
SelfAttn(h[i-1 O
] O
) O
2h[i O
] O
= O
LayerNorm(h[i O
] O
) O

# O
No O
residual O
connection O
here O
3h[i O
] O
= O
h[i O
] O
+ O
LayerNorm(FFN(h[i O
] O
) O
) O

where O
SelfAttn O
is O
the O
encoder O
self O
- O
attention O
sublayer O
, O
FFN O
is O
the O
feed O
- O
forward O
sublayer O
and O
LayerNorm O
is O
the O
layer O
normalization O
. O

Liu O
et O
al O
. O

( O
2021 O
) O
aim O
at O
training O
a O
language O
- O
agnostic O
encoder O
for O
NMT O
using O
parallel O
corpus O
from O
scratch O
. O

Com- O
pared O
with O
them O
, O
our O
method O
shows O
that O
it O
’s O
pos- O
sible O
to O
make O
a O
pretrained O
multilingual O
encoder O
more O
language O
- O
agnostic O
by O
relaxing O
the O
position O
constraint O
during O
ﬁne O
- O
tuning O
. O

Capacity O
- O
enhanced O
decoder O
Some O
previous O
work O
( O
Zhu O
et O
al O
. O
, O
2020 O
; O
Yang O
et O
al O
. O
, O
2020 O
) O
incorpo- O
rates O
BERT O
into O
NMT O
and O
conﬁgures O
the O
decoder O
size O
as O
Vaswani O
et O

al O
. O
( O
2017 O
) O
. O

For O
example O
, O
to O
train O
an O
NMT O
on O
Europarl B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
training O
dataset O
, O
the O
default O
decoder O
conﬁguration O
is O
Transformer O
base O
( O
Gu O
et O
al O
. O
, O
2018 O
; O
Currey O
et O
al O
. O
, O
2020 O
) O
. O

However O
, O
our O
model O
relies O
more O
on O
the O
decoder O
to O
learn O
from O
the O
labeled O
data O
, O
as O
the O
encoder O
is O
mainly O
responsi- O
ble O
for O
cross O
- O
lingual O
transfer O
. O

This O
is O
also O
reﬂected O
in O
our O
training O
strategy O
: O
at O
the O
ﬁrst O
stage O
only O
the O
decoder O
parameters O
are O
optimized O
, O
while O
at O
the O
sec- O
ond O
stage O
the O
encoder O
is O
only O
slightly O
ﬁne O
- O
tuned O
to O
preserve O
its O
transferability O
. O

Therefore O
, O
the O
model O
ca- O
pacity O
of O
SixT B-MethodName
is O
smaller O
than O
vanilla O
Transformer O
with O
the O
same O
size O
. O

We O
propose O
to O
apply O
a O
capacity- O
enhanced O
decoder O
that O
has O
larger O
dimension O
of O
feed O
forward O
network O
, O
more O
layers O
and O
more O
attention O
heads O
at O
both O
the O
ﬁrst O
and O
second O
training O
stages O
. O

The O
improvement O
brought O
by O
the O
big O
decoder O
is O
not O
simply O
because O
of O
more O
model O
parameters O
. O

More O
discussions O
are O
in O
the O
Section O
4.2 O
. O

4 O
Experiments O
4.1 O
Setup O
Dataset O
We O
focus O
on O
the O
any O
- O
to O
- O
English O
transla- O
tions O
for O
the O
ZeXT B-TaskName
task O
. O

The O
Europarl B-DatasetName
- I-DatasetName
v7 I-DatasetName
German I-DatasetName
and I-DatasetName
English I-DatasetName
is O
used O
as O
training O
set O
. O

We O
evaluate O
the O
cross O
- O
lingual O
transfer O
abilities O
of O
NMT O
mod- O
els O
on O
a O
variety O
of O
languages O
from O
different O
lan- O
guage O
groups4 O
: O
German O
group O
( O
De O
, O
Nl O
) O
, O
Romance O
group O
( O
Es O
, O
It O
, O
Ro O
) O
, O
Uralic O
and O
Baltic O
group O
( O
Et O
, O
Fi O
, O
Lv O
) O
, O
Indo O
- O
Aryan O
group O
( O
Hi O
, O
Ne O
) O
and O
Chinese O
( O
Zh O
) O
. O

A O
concatenation O
of O
Fr O
- O
En O
and O
Cs O
- O
En O
validation O
dataset O
which O
are O
from O
different O
language O
groups O
is O
used O
as O
validation O
dataset O
for O
all O
any O
- O
to O
- O
English O
translation O
tasks O
. O

The O
details O
of O
the O
datasets O
are O
in O
the O
appendix O
. O

Note O
that O
none O
of O
the O
monolingual O
dataset O
of O
the O
tested O
source O
languages O
is O
available O
in O
all O
experiments O
. O

Model O
settings O
We O
use O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
base O
model O
as O
the O
off O
- O
the O
- O
shelf O
MPE O
. O

The O
model O
is O
imple- O
mented O
on O
fairseq O
toolkit O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
. O

We O
set O
Transformer O
encoder O
the O
same O
size O
as O
the O
XLM- B-MethodName
R I-MethodName
base O
model O
. O

For O
the O
decoder O
, O
we O
use O
the O
same O
hyper O
- O
parameter O
setting O
as O
the O
encoder O
. O

We O
de- O
note O
model O
with O
such O
conﬁguration O
as O
SixT B-MethodName
and O
use O
this O
conﬁguration O
for O
our O
NMT O
models O
through O
the O
paper O
unless O
otherwise O
stated O
. O

The O
encoder- O
decoder O
attention O
modules O
are O
randomly O
initialized O
. O

We O
remove O
the O
residual O
connection O
at O
the O
11 O
- O
th O
( O
penultimate O
) O
encoder O
layer O
, O
which O
is O
selected O
on O
the O
validation O
dataset O
. O

For O
the O
empirical O
exploration O
in O
Table O
1 O
, O
we O
use O
two O
model O
conﬁgurations O
. O

For O
Strategy O
( O
1)–(7 O
) O
where O
decoder O
layers O
are O
trained O
from O
scratch O
, O
we O
use O
a O
smaller O
decoder O
denoted O
as O
BaseDec B-MethodName
. O

This O
model O
conﬁguration O
is O
denoted O
as O
SixT B-MethodName
small O
. O

For O
the O
rest O
strategies O
, O
we O
follow O
the O
conﬁguration O
of O
SixT B-MethodName
and O
denote O
its O
decoder O
as O
BigDec B-MethodName
. O

Table O
12 O
in O
Appendix O
presents O
the O
details O
of O
different O
model O
conﬁgurations O
. O

Training O
and O
evaluation O
The O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
1= O
0.9 B-HyperparameterValue
and O
2= O
0.98 B-HyperparameterValue
is O
used O
for O
training O
. O

We O
use O
label B-HyperparameterName
smoothing I-HyperparameterName
with O
value O
0.1 B-HyperparameterValue
. O

The O
learning B-HyperparameterName
rate I-HyperparameterName
is O
0.0005 B-HyperparameterValue
and O
warmup B-HyperparameterName
step I-HyperparameterName
is O
4000 B-HyperparameterValue
at O
the O
ﬁrst O
stage O
. O

For O
the O
second O
stage O
, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
as O
0.0001 B-HyperparameterValue
and O
do O
not O
use O
warmup O
. O

All O
the O
drop B-HyperparameterName
- I-HyperparameterName
out I-HyperparameterName
probabilities I-HyperparameterName
are O
set O
to O
0.3 B-HyperparameterValue
. O

We O
use O
eight O
GPUs O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
set O
as O
4096 B-HyperparameterValue
tokens O
per O
GPU O
. O

Maximum B-HyperparameterName
updates I-HyperparameterName
number I-HyperparameterName
is O
200k B-HyperparameterValue
for O
the O
ﬁrst O
stage O
and O
30k B-HyperparameterValue
for O
the O
second O
stage O
. O

We O
use O
beam O
search O
( O
beam B-HyperparameterName
size I-HyperparameterName
is O
5 B-HyperparameterValue
) O
and O
do O
not O
tune O
length O
penalty O
. O

We O
evaluate O
the O
results O
with O
sacrebleu B-MetricName
. O

If O
not O
speciﬁed O
, O
the O
best O
checkpoint O
is O
selected O
by O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
performance O
on O
the O
validation O
set O
for O
all O
experiments O
. O

We O
refer O
the O
reader O
to O
Section O
B O
in O
Appendix O
for O
more O
training O
details O
. O

Baselines O
We O
compare O
our O
model O
with O
vanilla B-MethodName
Transformer I-MethodName
and O
ﬁve O
conventional O
methods O
to O
ap- O
ply O
pretrained O
Transformer O
encoder O
on O
NMT O
task O
. O

The O
pretrained O
encoders O
in O
these O
methods O
are O
re- O
placed O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
base O
for O
fair O
comparison O
. O

Vanilla B-MethodName
Transformer I-MethodName
. O
The O
encoder O
is O
with O
the O
same O
size O
of O
XLM B-MethodName
- I-MethodName
R I-MethodName
base O
, O
the O
decoder O
uses O
the O
size O
of O
BaseDec B-MethodName
. O

All O
model O
parameters O
are O
randomly O
initialized O
. O

+XLM B-MethodName
- I-MethodName
R I-MethodName
ﬁne I-MethodName
- I-MethodName
tune I-MethodName
encoder I-MethodName
( O
Conneau O
and O
Lample O
, O
2019 O
) O
. O
The O
encoder O
is O
initialized O
with O
XLM B-MethodName
- I-MethodName
R. I-MethodName
All O
parameters O
are O
trained O
. O

+XLM B-MethodName
- I-MethodName
R I-MethodName
ﬁne I-MethodName
- I-MethodName
tune I-MethodName
all I-MethodName
( O
Conneau O
and O
Lample O
, O
2019 O
) O
. O
All O
parameters O
except O
those O
of O
cross O
at- O
tention O
module O
are O
initialized O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
and I-MethodName
directly O
ﬁne O
- O
tuned O
. O

+XLM B-MethodName
- I-MethodName
R I-MethodName
as I-MethodName
encoder I-MethodName
embedding I-MethodName
( O
Zhu O
et O
al O
. O
, O
2020 O
) O
. O
The O
XLM B-MethodName
- I-MethodName
R I-MethodName
output O
is O
leveraged O
as O
the O
en- O
coder O
input O
of O
the O
NMT O
. O
The O
XLM B-MethodName
- I-MethodName
R I-MethodName
model O
is O
ﬁxed O
during O
training O
. O

+Recycle B-MethodName
XLM I-MethodName
- I-MethodName
R I-MethodName
for I-MethodName
NMT I-MethodName
( O
Imamura O
and O
Sumita O
, O
2019 O
) O
. O
The O
method O
initializes O
the O
encoder O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
only O
trains O
decoder O
at O
the O
ﬁrst O
step O
. O
Then O
all O
are O
trained O
at O
the O
second O
step O
. O

XLM B-MethodName
- I-MethodName
R I-MethodName
fused I-MethodName
model I-MethodName
( O
Zhu O
et O
al O
. O
, O
2020 O
) O
. O
The O
XLM B-MethodName
- I-MethodName
R I-MethodName
output O
is O
fused O
into O
encoder O
and O
decoder O
separately O
with O
attention O
mechanism O
. O

The O
encoder O
embedding O
is O
initialized O
from O
XLM B-MethodName
- I-MethodName
R I-MethodName
to I-MethodName
facilitate O
transfer O
. O

The O
parameters O
of O
XLM B-MethodName
- I-MethodName
R I-MethodName
are I-MethodName
frozen O
during O
training O
. O

4.2 O
Results O
The O
results O
of O
the O
empirical O
exploration O
in O
the O
Sec- O
tion O
3.1 O
are O
shown O
in O
Table O
1 O
. O

Since O
Strategy O
( O
8) O
– O
( O
9 O
) O
use O
a O
larger O
decoder O
than O
the O
rest O
ones O
, O
we O
add O
Strategy O
( O
10 O
) O
whose O
decoder O
size O
is O
the O
same O
as O
Strategy O
( O
8)–(9 O
) O
for O
fair O
comparison O
. O

Overall O
, O
we O
observe O
that O
it O
is O
best O
to O
use O
a O
big O
decoder O
and O
initialize O
the O
decoder O
embedding O
and O
all O
encoder O
parameters O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
and O
to O
train O
the O
decoder O
layers O
from O
scratch O
( O
Strategy O
( O
10 O
) O
) O
. O

To O
verify O
the O
effect O
of O
a O
capacity O
enhanced O
de- O
coder O
in O
the O
ZeXT B-TaskName
task O
, O
we O
train O
vanilla B-MethodName
Trans- I-MethodName
former I-MethodName
with O
the O
same O
size O
of O
Strategy O
( O
7 O
) O
( O
with O
BaseDec B-MethodName
) O
and O
Strategy O
( O
10 O
) O
( O
with O
BigDec B-MethodName
) O
using O
the O
same O
training O
corpus.6The O
vanilla O
Transformer O
model O
with O
BaseDec B-MethodName
and O
BigDec B-MethodName
obtains O
a O
BLEU B-MetricName
score O
of O
23.5 B-MetricValue
and O
22.9 B-MetricValue
on O
the O
De B-DatasetName
- I-DatasetName
En I-DatasetName
test O
set O
, O
respec- O
tively O
. O

The O
big O
decoder O
improves O
the O
performance O
of O
SixT B-MethodName
, O
but O
fails O
to O
improve O
that O
of O
vanilla B-MethodName
Trans- I-MethodName
former I-MethodName
. O

This O
proves O
the O
effectiveness O
of O
BigDec B-MethodName
to O
improve O
the O
zero B-TaskName
- I-TaskName
shot I-TaskName
translation I-TaskName
performance O
of O
our O
model O
. O

Table O
2 O
illustrates O
the O
performance O
of O
the O
pro- O
posed O
SixT B-MethodName
comparing O
with O
the O
baselines O
. O

SixT B-MethodName
gets O
18.3 B-HyperparameterValue
average O
BLEU B-MetricValue
and O
improves O
over O
the O
best O
baseline O
by O
5.4 B-HyperparameterValue
average O
BLEU B-MetricName
, O
showing O
that O
SixT B-MethodName
successfully O
learns O
to O
translate O
while O
preserv- O
ing O
the O
cross O
- O
lingual O
transferability O
of O
XLM B-MethodName
- I-MethodName
R. I-MethodName

For O
all O
language O
pairs O
, O
SixT B-MethodName
obtains O
better O
transferring O
scores O
. O

In O
contrast O
, O
vanilla B-MethodName
Transformer I-MethodName
can O
hardly O
transfer O
and O
the O
other O
baselines O
do O
not O
well O
transfer O
to O
the O
distant O
languages O
. O

In O
addition O
to O
zero O
- O
shot O
performance O
, O
SixT B-MethodName
also O
achieves O
the O
best O
result O
on O
De O
- O
En O
test O
set O
. O

Note O
that O
the O
best O
checkpoint O
is O
se- O
lected O
with O
zero O
- O
shot O
validation O
set O
for O
all O
methods O
. O

Previous O
work O
( O
Conneau O
et O
al O
. O
, O
2020 O
; O
Hu O
et O
al O
. O
, O
2020 O
) O
mainly O
uses O
XLM B-MethodName
- I-MethodName
R I-MethodName
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
trans- I-TaskName
fer O
on O
NLU O
tasks O
. O

The O
experiments O
demonstrate O
that O
XLM B-MethodName
- I-MethodName
R I-MethodName
can O
be O
also O
utilized O
for O
zero B-TaskName
- I-TaskName
shot I-TaskName
neu- I-TaskName
ral I-TaskName
machine I-TaskName
translation I-TaskName
if O
it O
is O
ﬁne O
- O
tuned O
properly O
. O

We O
leave O
the O
exploration O
of O
cross O
- O
lingual O
transfer O
using O
XLM O
- O
R O
for O
other O
NLG O
tasks O
as O
the O
future O
work O
. O

4.3 O
Ablation O
Study O
We O
conduct O
an O
ablation O
study O
with O
the O
proposed O
SixT B-MethodName
on O
the O
Europarl B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
training O
set O
, O
as O
shown O
in O
Table O
3 O
. O

Overall O
, O
SixT B-MethodName
obtains O
the O
best O
zero B-TaskName
- I-TaskName
shot I-TaskName
translation I-TaskName
results O
, O
demonstrating O
the O
importance O
of O
all O
three O
components O
. O

From O
the O
results O
of O
( O
1 O
) O
to O
( O
3 O
) O
, O
TwoStage B-MethodName
and O
BigDec B-MethodName
along O
improve O
the O
zero- B-TaskName
shot I-TaskName
translation I-TaskName
performance O
by O
0.8 B-MetricValue
and O
0.4 B-MetricValue
average O
BLEU B-MetricName
over O
( O
1 O
) O
, O
respectively O
. O

However O
, O
combining O
them O
together O
brings O
a O
signiﬁcant O
improvement O
of O
2.6 B-MetricValue
average O
BLEU B-MetricName
over O
( O
1 O
) O
. O

This O
indicates O
that O
TwoStage B-MethodName
and O
BigDec O 
are O
complementary O
to O
each O
other O
, O
thus O
it O
is O
important O
to O
use O
them O
together O
. O

The O
results O
of O
( O
6)!(5 O
) O
conﬁrms O
our O
claim O
: O
without O
using O
BigDec O
, O
the O
performance O
of O
SixT B-MethodName
drops O
by O
1.8 B-MetricValue
average O
BLEU B-MetricName
. O

We O
also O
observe O
that O
the O
super- O
vised O
task O
( O
De B-DatasetName
- I-DatasetName
En I-DatasetName
) O
improves O
with O
TwoStage B-MethodName
and O
BigDec B-MethodName
( O
from O
results O
of O
( O
1 O
) O
to O
( O
4 O
) O
) O
while O
degrades O
with O
Resdrop B-MethodName
( O
see O
results O
of O
( O
2 O
) O
! O
( O
5 O
) O
and O
( O
4)!(6 O
) O
) O
. O

This O
is O
expected O
since O
Resdrop B-MethodName
helps O
to O
build O
a O
more O
language O
- O
agnostic O
encoder O
. O

Although O
Res- B-MethodName
drop I-MethodName
degrades O
supervised O
performance O
, O
it O
improves O
zero B-TaskName
- I-TaskName
shot I-TaskName
translation I-TaskName
. O

The O
zero O
- O
shot O
performance O
is O
related O
with O
both O
supervised O
performance O
and O
model O
transferability O
. O

By O
either O
enhancing O
the O
su- O
pervised O
performance O
( O
with O
TwoStage B-MethodName
and O
BigDec B-MethodName
) O
or O
the O
model O
transferability O
( O
with O
Resdrop B-MethodName
) O
, O
the O
overall O
performance O
of O
zero O
- O
shot O
translation O
can O
be O
improved. O

5 O
Analysis O
Comparison O
with O
multilingual O
NMT O
In O
this O
part O
, O
we O
compare O
SixT B-MethodName
with O
mBART B-MethodName
( O
Liu O
et O
al O
. O
, O
2020 O
) O
, O
CRISS B-MethodName
( O
Tran O
et O
al O
. O
, O
2020 O
) O
and O
m2m-100 B-MethodName
( O
Fan O
et O
al O
. O
, O
2020 O
) O
on O
any B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
English I-TaskName
test O
sets O
. O

mBART B-MethodName
is O
a O
strong O
pretrained O
multilingual O
encoder- O
decoder O
based O
Transformer B-MethodName
explicitly O
designed O
for O
NMT B-TaskName
. O

We O
follow O
their O
setting O
and O
directly O
ﬁne O
- O
tune O
all O
model O
parameters O
on O
WMT19 B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
training O
set O
. O

CRISS B-MethodName
and O
m2m-100 B-MethodName
are O
the O
state O
- O
of O
- O
the O
- O
art O
unsupervised O
and O
supervised O
multilingual O
NMT O
models O
, O
respectively O
. O

The O
CRISS B-MethodName
model O
is O
initial- O
ized O
with O
the O
mBART B-MethodName
model O
and O
iteratively O
ﬁne- O
tuned O
on O
1.8 O
billion O
sentences O
covering O
90 O
language O
pairs O
. O

m2m-100 B-MethodName
is O
trained O
with O
7.5 O
billion O
parallel O
sentences O
across O
2200 O
translation O
directions O
. O

The O
results O
of O
CRISS B-MethodName
and O
m2m-100 B-MethodName
are O
listed O
as O
ref- O
erence O
, O
because O
CRISS B-MethodName
and O
m2m-100 B-MethodName
are O
many- O
to O
- O
many O
NMT O
models O
whose O
performance O
may O
degrade O
due O
to O
the O
competitions O
among O
different O
target O
languages O
( O
Aharoni O
et O
al O
. O
, O
2019 O
; O
Zhang O
et O
al O
. O
, O
2020 O
) O
, O
while O
SixT B-MethodName
is O
a O
many B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
one I-TaskName
NMT I-TaskName
model O
. O

The O
ofﬁcial O
m2m-100 B-MethodName
model O
has O
three O
sizes O
: O
small O
( O
418 B-HyperparameterValue
M I-HyperparameterValue
parameters B-HyperparameterName
) O
, O
base O
( O
1.2B B-HyperparameterValue
parameters B-HyperparameterName
) O
and O
large O
( O
12B B-HyperparameterValue
parameters B-HyperparameterName
) O
. O

The O
results O
of O
m2m-100 B-MethodName
( I-MethodName
small I-MethodName
) I-MethodName
model O
are O
reported O
. O

To O
compare O
with O
these O
models O
, O
we O
train O
a O
many- O
to O
- O
one O
SixT B-MethodName
large I-MethodName
model O
with O
WMT19 B-DatasetName
German- I-DatasetName
English I-DatasetName
training O
data O
, O
which O
only O
consists O
of O
41 O
million O
sentences O
pairs O
. O

It O
only O
requires O
a O
pre- O
trained O
XLM B-MethodName
- I-MethodName
R I-MethodName
large I-MethodName
model I-MethodName
and O
do O
not O
contain O
any O
data O
in O
other O
languages O
. O

We O
remove O
the O
residual O
connection O
after O
the O
self O
- O
attention O
sublayer O
of O
the O
23 B-HyperparameterValue
- I-HyperparameterValue
th I-HyperparameterValue
( O
penultimate O
) O
encoder O
layer O
. O

The O
dataset O
and O
model O
conﬁguration O
details O
are O
in O
Table O
9 O
and O
12 O
in O
the O
appendix O
. O

From O
the O
results O
in O
Table O
4 O
, O
the O
SixT B-MethodName
large I-MethodName
model O
is O
signiﬁcantly O
better O
than O
mBART B-MethodName
and O
slightly O
bet- O
ter O
than O
CRISS B-MethodName
and O
m2m-100 B-MethodName
. O

The O
averaged O
BLEU B-MetricName
across O
all O
languages O
is O
7.1 B-MetricValue
, O
0.5 B-MetricValue
and O
1.4 B-MetricValue
higher O
than O
mBART B-MethodName
, O
CRISS B-MethodName
and O
m2m-1007 B-MethodName
, O
respectively O
. O

The O
SixT B-MethodName
model O
has O
larger O
model O
size O
, O
nevertheless O
, O
the O
results O
of O
SixT B-MethodName
are O
impressive O
given O
that O
SixT B-MethodName
does O
not O
use O
any O
monolingual O
or O
parallel O
texts O
ex- O
cept O
German B-DatasetName
- I-DatasetName
English I-DatasetName
training O
data O
. O

The O
perfor- O
mance O
gain O
over O
mBART B-MethodName
shows O
that O
with O
proper O
ﬁne O
- O
tuning O
strategy O
, O
the O
pretrained O
multilingual O
en- O
coder O
has O
better O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
ability O
on O
NMT O
tasks O
. O

In O
addition O
, O
with O
large O
- O
scale O
German- O
English O
parallel O
data O
, O
the O
SixT B-MethodName
model O
transfers O
well O
to O
distant O
resource O
- O
poor O
languages O
like O
Ne O
and O
Si O
, O
which O
indicates O
a O
promising O
approach O
to O
translate O
resource O
- O
poor O
languages O
. O

The O
SixT B-MethodName
performance O
might O
be O
further O
improved O
with O
the O
data O
of O
more O
languages O
pairs O
. O

We O
leave O
this O
as O
future O
work O
. O

Language O
transfer O
v.s. O
language O
distance O
In O
this O
part O
, O
we O
explore O
the O
relationship O
between O
the O
cross O
- O
lingual O
transfer O
performance O
and O
the O
lan- O
guage O
distance O
. O

We O
train O
the O
SixT B-MethodName
models O
on O
dif- O
ferent O
supervised O
language O
pairs O
including O
De O
- O
En O
, O
Es O
- O
En O
, O
Fi O
- O
En O
, O
Hi O
- O
En O
and O
Zh O
- O
En O
, O
and O
then O
directly O
apply O
them O
to O
all O
test O
sets O
, O
as O
seen O
in O
Table O
5. O

We O
observe O
that O
the O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
gener- O
ally O
works O
better O
when O
the O
SixT B-MethodName
model O
is O
trained O
on O
source O
languages O
in O
the O
same O
language O
family O
. O

The O
performance O
on O
Ko B-DatasetName
- I-DatasetName
En I-DatasetName
is O
one O
exception O
, O
where O
Hi B-DatasetName
- I-DatasetName
En I-DatasetName
achieves O
the O
best O
transfer O
performance O
. O

We O
also O
notice O
that O
the O
vocabulary O
overlapping O
( O
even O
character O
overlapping O
) O
between O
Hindi O
and O
Korean O
is O
low O
, O
showing O
that O
signiﬁcant O
vocabulary O
sharing O
is O
not O
a O
requirement O
for O
effective O
transfer O
. O

When O
trained O
on O
3.5 O
million O
Hi O
- O
En O
sentence O
pairs O
, O
SixT B-MethodName
obtains O
promising O
results O
on O
the O
Ne O
- O
En O
and O
Si O
- O
En O
translation O
, O
with O
a O
BLEU B-MetricName
score O
of O
16.7 B-MetricValue
and O
9.6 B-MetricValue
, O
respectively O
. O

As O
comparison O
, O
The O
vanilla B-MethodName
Trans- I-MethodName
former I-MethodName
supervised O
with O
FLoRes B-DatasetName
training O
set O
only O
receives O
14.5 B-MetricValue
and O
7.2 B-MetricValue
BLEU B-MetricName
score O
( O
Liu O
et O
al O
. O
, O
2020 O
) O
on O
the O
same O
test O
sets O
. O

Therefore O
, O
another O
approach O
to O
translate O
resource O
- O
poor O
languages O
is O
to O
train O
SixT B-MethodName
on O
similar O
high O
- O
resource O
language O
pairs O
. O

As O
a O
comparison O
, O
we O
train O
vanilla B-MethodName
Transformer I-MethodName
conﬁgured O
as O
Transformer B-MethodName 
big I-MethodName
without O
MPE O
ini- O
tialization O
with O
the O
same O
training O
sets O
and O
valida- O
tion O
sets O
. O

The O
poor O
zero O
- O
shot O
cross O
- O
lingual O
perfor- O
mance O
of O
vanilla B-MethodName
Transformer I-MethodName
indicates O
that O
the O
XLM O
- O
R O
initialized O
encoder O
is O
essential O
and O
can O
pro- O
duce O
language O
- O
agnostic O
representations O
. O

Performance O
on O
the O
supervised O
language O
pair O
To O
study O
whether O
the O
SixT B-MethodName
model O
gains O
the O
cross- O
lingual O
transfer O
ability O
at O
the O
cost O
of O
performance O
degradation O
on O
the O
supervised O
language O
pair O
, O
we O
compare O
the O
vanilla B-MethodName
Transformer I-MethodName
big I-MethodName
model O
and O
SixT B-MethodName
model O
on O
the O
supervised O
translation O
task O
. O

The O
performance O
of O
SixT B-MethodName
is O
lower O
than O
that O
of O
vanilla B-MethodName
Transformer I-MethodName
when O
more O
than O
20 O
M O
parallel O
sentences O
are O
available O
, O
but O
it O
gets O
better O
perfor- O
mance O
with O
fewer O
parallel O
sentences O
. O

The O
Hindi- O
to O
- O
English O
is O
an O
exception O
where O
SixT B-MethodName
has O
lower O
BLEU B-MetricName
. O

When O
large O
amount O
of O
bi O
- O
text O
data O
is O
given O
, O
the O
SixT B-MethodName
model O
size O
is O
expected O
to O
be O
increased O
to O
fully O
digest O
the O
bi O
- O
text O
. O

For O
example O
, O
if O
we O
re- O
place O
SixT B-MethodName
with O
SixT B-MethodName
large I-MethodName
and O
train O
SixT B-MethodName
large I-MethodName
on O
WMT19 B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
, O
we O
get O
33.8 B-MetricValue
BLEU B-MetricName
on O
De B-DatasetName
- I-DatasetName
En I-DatasetName
test O
set O
( O
see O
Table O
4 O
) O
, O
which O
is O
comparable O
of O
33.7 B-MetricValue
BLEU B-MetricName
obtained O
by O
vanilla B-MethodName
Transformer I-MethodName
. O

Performance O
vs. O
training O
corpus O
size O
To O
exam- O
ine O
the O
relationship O
between O
cross O
- O
lingual O
transfer O
ability O
and O
training O
data O
size O
, O
we O
compare O
the O
zero- O
shot O
BLEU B-MetricName
scores O
of O
SixT O
models B-MethodName
trained O
on O
Eu- B-DatasetName
roparl I-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
and O
WMT19 B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
. O

The O
results O
are O
shown O
in O
Table O
7 O
. O

It O
shows O
that O
increasing O
train- O
ing O
data O
size O
can O
consistently O
improve O
the O
zero- B-TaskName
shot I-TaskName
translation I-TaskName
performance O
. O

For O
instance O
, O
SixT B-MethodName
trained O
with O
WMT19 B-DatasetName
improves O
over O
SixT B-MethodName
trained O
with O
Europarl B-DatasetName
- I-DatasetName
v7 I-DatasetName
by O
3.4 B-MetricValue
average O
BLEU B-MetricName
. O

Performance O
with O
other O
target O
language O
To O
build O
many O
- O
to O
- O
one O
NMT O
model O
with O
other O
target O
language O
, O
we O
train O
two O
SixT B-MethodName
models O
on O
WMT16 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
and O
WMT19 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
, O
respectively O
. O

We O
use O
Fi O
- O
De O
as O
validation O
language O
pair O
and O
Fr O
/ O
Cs O
/ O
Ru O
/ O
Nl- O
De O
as O
test O
language O
pairs O
. O

From O
the O
results O
shown O
in O
Table O
8 O
, O
SixT B-MethodName
can O
obtain O
reasonable O
transferring O
scores O
to O
unseen O
source O
languages O
when O
target O
lan- O
guage O
is O
not O
English O
. O

Again O
, O
the O
results O
conﬁrm O
that O
the O
cross O
- O
lingual O
transfer O
ability O
improves O
with O
larger O
training O
data O
. O

6 O
Related O
Work O
Zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
learning I-TaskName

Mul- O
tilingual O
pretrained O
models O
, O
such O
as O
mBERT B-MethodName
( O
Wu O
and O
Dredze O
, O
2019 O
) O
, O
XLM B-MethodName
- I-MethodName
R I-MethodName
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
, O
mBART B-MethodName
( O
Liu O
et O
al O
. O
, O
2020 O
) O
, O
and O
mT5 B-MethodName
( O
Xue O
et O
al O
. O
, O
2021 O
) O
, O
have O
achieved O
success O
on O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross- I-TaskName
lingual I-TaskName
transfer I-TaskName
for O
various O
NLP O
tasks O
. O

The O
models O
are O
pretrained O
on O
large O
- O
scale O
multilingual O
corpora O
with O
a O
shared O
vocabulary O
. O

After O
pretrained O
, O
it O
is O
ﬁne O
- O
tuned O
on O
labeled O
data O
of O
downstream O
tasks O
in O
one O
language O
and O
directly O
tested O
in O
other O
lan- O
guages O
in O
a O
zero O
- O
shot O
manner O
. O

While O
multilingual O
pretrained O
models O
with O
encoder O
- O
decoder O
- O
based O
ar- O
chitecture O
( O
Liu O
et O
al O
. O
, O
2020 O
; O
Chi O
et O
al O
. O
, O
2020 O
) O
work O
well O
on O
cross O
- O
lingual O
transfer O
for O
NLG O
tasks O
, O
multi- O
lingual O
pretrained O
encoders O
( O
Wu O
and O
Dredze O
, O
2019 O
; O
Conneau O
and O
Lample O
, O
2019 O
; O
Conneau O
et O
al O
. O
, O
2020 O
) O
are O
mainly O
applied O
to O
cross O
- O
lingual O
NLU O
tasks O
( O
Hu O
et O
al O
. O
, O
2020 O
) O
. O

In O
this O
work O
, O
we O
explore O
how O
to O
ﬁne O
- O
tune O
an O
off O
- O
the O
- O
shelf O
multilingual O
pretrained O
encoder O
for O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
in O
neu- O
ral O
machine O
translation O
, O
a O
typical O
NLG O
task O
. O

Pretrained O
models O
for O
NMT O
Some O
previous O
works O
( O
Imamura O
and O
Sumita O
, O
2019 O
; O
Conneau O
and O
Lample O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2020 O
; O
Weng O
et O

al O
. O
, O
2020 O
; O
Ma O
et O
al O
. O
, O
2020 O
; O
Zhu O
et O
al O
. O
, O
2020 O
) O
explore O
meth- O
ods O
to O
integrate O
pretrained O
language O
encoders O
into O
the O
NMT O
model O
to O
improve O
supervised O
translation O
performance O
. O

For O
instance O
, O
Zhu O
et O
al O
. O

( O
2020 O
) O
pro- O
pose O
BERT O
- O
fused O
model O
, O
in O
which O
they O
ﬁrst O
use O
BERT O
to O
extract O
representations O
for O
an O
input O
sen- O
tence O
, O
and O
then O
fuses O
the O
representations O
into O
both O
the O
encoder O
and O
decoder O
via O
the O
attention O
mech- O
anism O
. O

Another O
line O
of O
works O
( O
Liu O
et O
al O
. O
, O
2020 O
; O
Song O
et O
al O
. O
, O
2019 O
; O
Lin O
et O
al O
. O
, O
2020 O
) O
propose O
novel O
encoder O
- O
decoder O
- O
based O
multilingual O
pretrained O
lan- O
guage O
models O
and O
ﬁne O
- O
tune O
such O
models O
for O
NMT O
. O

For O
example O
, O
Liu O
et O
al O
. O
( O
2020 O
) O
propose O
mBART B-MethodName
, O
an O
encoder O
- O
decoder O
- O
based O
Transformer O
explicitly O
de- O
signs O
for O
NMT O
and O
demonstrate O
that O
mBART B-MethodName
can O
be O
ﬁne O
- O
tuned O
for O
supervised O
and O
zero B-TaskName
- I-TaskName
shot I-TaskName
NMT I-TaskName
. O

Different O
from O
them O
, O
we O
leverage O
MPE O
for O
zero- B-TaskName
shot I-TaskName
translation I-TaskName
instead O
of O
supervised O
translation O
. O

Among O
the O
previous O
works O
, O
Wei O
et O
al O
. O
( O
2021 O
) O
is O
the O
most O
similar O
with O
ours O
. O

They O
ﬁne O
- O
tune O
their O
MPE O
on O
NMT O
with O
a O
two O
- O
stage O
strategy O
. O

However O
, O
their O
work O
focuses O
on O
improving O
the O
MPE O
for O
a O
more O
universal O
representation O
across O
languages O
and O
lacks O
in O
- O
depth O
study O
of O
cross O
- O
lingual O
NMT O
. O

In O
contrast O
, O
we O
aim O
at O
leveraging O
an O
MPE O
for O
machine O
trans- O
lation O
while O
preserving O
its O
ability O
of O
cross O
- O
lingual O
transfer O
. O

7 O
Conclusion O
In O
this O
paper O
, O
we O
focus O
on O
the O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross- I-TaskName
lingual I-TaskName
NMT I-TaskName
transfer I-TaskName
( O
ZeXT B-TaskName
) O
task O
which O
aims O
at O
leveraging O
an O
MPE O
for O
machine O
translation O
while O
preserving O
its O
ability O
of O
cross O
- O
lingual O
transfer O
. O

In O
this O
task O
, O
only O
a O
multilingual O
pretrained O
encoder O
such O
as O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
one O
parallel O
dataset O
such O
asGerman O
- O
English O
are O
available O
. O

We O
propose O
SixT B-MethodName
for O
this O
task O
, O
which O
enables O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
for I-TaskName
NMT I-TaskName
by O
making O
full O
use O
of O
the O
la- O
belled O
data O
and O
enhancing O
the O
transferability O
of O
XLM B-MethodName
- I-MethodName
R. I-MethodName

Extensive O
experiments O
demonstrate O
the O
effectiveness O
of O
SixT. O
In O
particular O
, O
SixT B-MethodName
outper- O
forms O
mBART B-MethodName
, O
a O
pretrained O
encoder O
- O
decoder O
- O
based O
model O
explicitly O
designed O
for O
NMT O
. O

It O
also O
gets O
bet- O
ter O
performance O
than O
CRISS B-MethodName
and O
m2m-100 B-MethodName
, O
two O
strong O
multilingual O
NMT O
models O
, O
on O
15 O
any O
- O
to- O

English O
test O
sets O
with O
less O
training O
data O
and O
training O
computation O
cost O
. O

Acknowledgements O
This O
project O
was O
supported O
by O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No O
. O
62106138 O
) O
and O
Shanghai O
Sailing O
Program O
( O
No O
. O
21YF1412100 O
) O
. O

Wenping O
Wang O
and O
Jia O
Pan O
acknowledge O
the O
sup- O
port O
from O
Centre O
for O
Transformative O
Garment O
Pro- O
duction O
. O

We O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
feedbacks O
on O
this O
work O
. O

References O
Roee O
Aharoni O
, O
Melvin O
Johnson O
, O
and O
Orhan O
Firat O
. O
2019 O
. O
Massively O
multilingual O
neural O
machine O
translation O
. O
InProceedings O
of O
NAACL O
, O
pages O
3874–3884 O
. O

Yun O
Chen O
, O
Yang O
Liu O
, O
Yong O
Cheng O
, O
and O
Victor O
OK O
Li O
. O
2017 O
. O
A O
teacher O
- O
student O
framework O
for O
zero- O
resource O
neural O
machine O
translation O
. O
In O
Proceedings O
of O
ACL O
, O
pages O
1925–1935 O
. O

Yun O
Chen O
, O
Yang O
Liu O
, O
and O
Victor O
OK O
Li O
. O
2018 O
. O
Zero- O
resource O
neural O
machine O
translation O
with O
multi- O
agent O
communication O
game O
. O
In O
Thirty O
- O
Second O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
. O

Zewen O
Chi O
, O
Li O
Dong O
, O
Furu O
Wei O
, O
Wenhui O
Wang O
, O
Xian- O
Ling O
Mao O
, O
and O
Heyan O
Huang O
. O
2020 O
. O
Cross O
- O
lingual O
natural O
language O
generation O
via O
pre O
- O
training O
. O
In O
Pro- O
ceedings O
of O
the O
AAAI O
Conference O
on O
Artiﬁcial O
Intel- O
ligence O
, O
volume O
34 O
, O
pages O
7570–7577 O
. O

Alexis O
Conneau O
, O
Kartikay O
Khandelwal O
, O
Naman O
Goyal O
, O
Vishrav O
Chaudhary O
, O
Guillaume O
Wenzek O
, O
Francisco O
Guzmán O
, O
Edouard O
Grave O
, O
Myle O
Ott O
, O
Luke O
Zettle- O
moyer O
, O
and O
Veselin O
Stoyanov O
. O

2020 O
. O

Unsupervised O
cross O
- O
lingual O
representation O
learning O
at O
scale O
. O

In O
Proceedings O
of O
ACL O
, O
pages O
8440–8451 O
, O
Online O
. O

Alexis O
Conneau O
and O
Guillaume O
Lample O
. O

2019 O
. O
Cross- O

lingual O
language O
model O
pretraining O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
pages O
7059–7069 O
. O

Anna O
Currey O
, O
Prashant O
Mathur O
, O
and O
Georgiana O
Dinu O
. O
2020 O
. O

Distilling O
multiple O
domains O
for O
neural O
ma- O
chine O
translation O
. O

In O
Proceedings O
of O
EMNLP O
, O
pages O
4500–4511 O
. O

24Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
under- O
standing O
. O

In O
Proceedings O
of O
NAACL O
, O
pages O
4171 O
– O
4186 O
. O

Angela O
Fan O
, O
Shruti O
Bhosale O
, O
Holger O
Schwenk O
, O
Zhiyi O
Ma O
, O
Ahmed O
El O
- O
Kishky O
, O
Siddharth O
Goyal O
, O
Man- O
deep O
Baines O
, O
Onur O
Celebi O
, O
Guillaume O
Wenzek O
, O
Vishrav O
Chaudhary O
, O
Naman O
Goyal O
, O
Tom O
Birch O
, O
Vi- O
taliy O
Liptchinsky O
, O
Sergey O
Edunov O
, O
Edouard O
Grave O
, O
Michael O
Auli O
, O
and O
Armand O
Joulin O
. O

2020 O
. O

Be- O

yond O
english O
- O
centric O
multilingual O
machine O
transla- O
tion O
. O

arXiv O
preprint O
arXiv:2010.11125 O
. O

Jiatao O
Gu O
, O
Yong O
Wang O
, O
Yun O
Chen O
, O
Victor O
O. O
K. O
Li O
, O
and O
Kyunghyun O
Cho O
. O
2018 O
. O

Meta O
- O
learning O
for O
low- O
resource O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
EMNLP O
, O
pages O
3622–3631 O
. O

Junjie O
Hu O
, O
Sebastian O
Ruder O
, O
Aditya O
Siddhant O
, O
Gra- O
ham O
Neubig O
, O
Orhan O
Firat O
, O
and O
Melvin O
Johnson O
. O

2020 O
. O

XTREME O
: O

A O
massively O
multilingual O
multi- O
task O
benchmark O
for O
evaluating O
cross O
- O
lingual O
gen- O
eralisation O
. O

In O
Proceedings O
of O
the O
37th O
Interna- O
tional O
Conference O
on O
Machine O
Learning O
, O
volume O
119 O
, O
pages O
4411–4421 O
. O

Kenji O
Imamura O
and O
Eiichiro O
Sumita O
. O

2019 O
. O

Recycling O
a O
pre O
- O
trained O
BERT O
encoder O
for O
neural O
machine O
trans- O
lation O
. O

In O
Proceedings O
of O
the O
3rd O
Workshop O
on O
Neu- O
ral O
Generation O
and O
Translation O
, O
pages O
23–31 O
. O

Melvin O
Johnson O
, O
Mike O
Schuster O
, O
Quoc O
V O
Le O
, O
Maxim O
Krikun O
, O
Yonghui O
Wu O
, O
Zhifeng O
Chen O
, O
Nikhil O
Thorat O
, O
Fernanda O
Viégas O
, O
Martin O
Wattenberg O
, O
Greg O
Corrado O
, O
et O
al O
. O
2017 O
. O

Google O
’s O
multilingual O
neural O
machine O
translation O
system O
: O
Enabling O
zero O
- O
shot O
translation O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
5:339–351 O
. O

Diederik O
P. O
Kingma O
and O
Jimmy O
Ba O
. O
2015 O
. O

Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O

In O
Proceedings O
of O
ICLR O
. O

Taku O
Kudo O
. O

2018 O
. O

Subword O
regularization O
: O
Improving O
neural O
network O
translation O
models O
with O
multiple O
sub- O
word O
candidates O
. O

In O
Proceedings O
of O
ACL O
, O
pages O
66 O
– O
75 O
. O

Guillaume O
Lample O
, O
Myle O
Ott O
, O
Alexis O
Conneau O
, O
Lu- O
dovic O
Denoyer O
, O
and O
Marc’Aurelio O
Ranzato O
. O

2018 O
. O

Phrase O
- O
based O
& O
neural O
unsupervised O
machine O
trans- O
lation O
. O

In O
Proceedings O
of O
EMNLP O
. O

Zehui O
Lin O
, O
Xiao O
Pan O
, O
Mingxuan O
Wang O
, O
Xipeng O
Qiu O
, O
Jiangtao O
Feng O
, O
Hao O
Zhou O
, O
and O
Lei O
Li O
. O
2020 O
. O

Pre- O
training O
multilingual O
neural O
machine O
translation O
by O
leveraging O
alignment O
information O
. O

In O
Proceed- O
ings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
2649–2663 O
. O

Danni O
Liu O
, O
Jan O
Niehues O
, O
James O
Cross O
, O
Francisco O
Guzmán O
, O
and O
Xian O
Li O
. O
2021 O
. O

Improving O
zero O
- O
shot O
translation O
by O
disentangling O
positional O
information O
. O

InProceedings O
of O
ACL O
, O
pages O
1259–1273.Yinhan O

Liu O
, O
Jiatao O
Gu O
, O
Naman O
Goyal O
, O
Xian O
Li O
, O
Sergey O
Edunov O
, O
Marjan O
Ghazvininejad O
, O
Mike O
Lewis O
, O
and O
Luke O
Zettlemoyer O
. O

2020 O
. O

Multilingual O
denoising O
pre O
- O
training O
for O
neural O
machine O
translation O
. O

arXiv O
preprint O
arXiv:2001.08210 O
. O

Shuming O
Ma O
, O
Jian O
Yang O
, O
Haoyang O
Huang O
, O
Zewen O
Chi O
, O
Li O
Dong O
, O
Dongdong O
Zhang O
, O
Hany O
Hassan O
Awadalla O
, O
Alexandre O
Muzio O
, O
Akiko O
Eriguchi O
, O
Saksham O
Sing- O
hal O
, O
Xia O
Song O
, O
Arul O
Menezes O
, O
and O
Furu O
Wei O
. O
2020 O
. O

Xlm O
- O
t O
: O
Scaling O
up O
multilingual O
machine O
translation O
with O
pretrained O
cross O
- O
lingual O
transformer O
encoders O
. O

arXiv O
preprint O
arXiv:2012.15547 O
. O

Myle O
Ott O
, O
Sergey O
Edunov O
, O
Alexei O
Baevski O
, O
Angela O
Fan O
, O
Sam O
Gross O
, O
Nathan O
Ng O
, O
David O
Grangier O
, O
and O
Michael O
Auli O
. O

2019 O
. O

fairseq O
: O

A O
fast O
, O
extensible O
toolkit O
for O
sequence O
modeling O
. O

In O
Proceedings O
of O
NAACL O
. O

Joan O
Serra O
, O
Didac O
Suris O
, O
Marius O
Miron O
, O
and O
Alexandros O
Karatzoglou O
. O

2018 O
. O

Overcoming O
catastrophic O
forget- O
ting O
with O
hard O
attention O
to O
the O
task O
. O

In O
Proceedings O
of O
ICML O
, O
volume O
80 O
, O
pages O
4548–4557 O
. O

Kaitao O
Song O
, O
Xu O
Tan O
, O
Tao O
Qin O
, O
Jianfeng O
Lu O
, O
and O
Tie- O
Yan O
Liu O
. O
2019 O
. O

Mass O
: O

Masked O
sequence O
to O
se- O
quence O
pre O
- O
training O
for O
language O
generation O
. O

In O
In- O
ternational O
Conference O
on O
Machine O
Learning O
, O
pages O
5926–5936 O
. O

Chau O
Tran O
, O
Yuqing O
Tang O
, O
Xian O
Li O
, O
and O
Jiatao O
Gu O
. O
2020 O
. O

Cross O
- O
lingual O
retrieval O
for O
iterative O
self O
- O
supervised O
training O
. O

In O
Proceedings O
of O
NeurIPS O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
Lukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
NeurIPS O
, O
pages O
5998–6008 O
. O

Xiangpeng O
Wei O
, O
Yue O
Hu O
, O
Rongxiang O
Weng O
, O
Luxi O
Xing O
, O
Heng O
Yu O
, O
and O
Weihua O
Luo O
. O
2021 O
. O

On O
learning O
uni- O
versal O
representations O
across O
languages O
. O

Proceed- O
ings O
of O
ICLR O
. O

Rongxiang O
Weng O
, O
Heng O
Yu O
, O
Shujian O
Huang O
, O
Shanbo O
Cheng O
, O
and O
Weihua O
Luo O
. O
2020 O
. O

Acquiring O
knowl- O
edge O
from O
pre O
- O
trained O
model O
to O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
AAAI O
, O
pages O
9266 O
– O
9273 O
. O

Shijie O
Wu O
and O
Mark O
Dredze O
. O

2019 O
. O

Beto O
, O
bentz O
, O
becas O
: O
The O
surprising O
cross O
- O
lingual O
effectiveness O
of O
bert O
. O

In O
Proceedings O
of O
EMNLP O
- O
IJCNLP O
, O
pages O
833–844 O
. O

Linting O
Xue O
, O
Noah O
Constant O
, O
Adam O
Roberts O
, O
Mi- O
hir O
Kale O
, O
Rami O
Al O
- O
Rfou O
, O
Aditya O
Siddhant O
, O
Aditya O
Barua O
, O
and O
Colin O
Raffel O
. O
2021 O
. O

mT5 O
: O

A O
massively O
multilingual O
pre O
- O
trained O
text O
- O
to O
- O
text O
transformer O
. O

In O
Proceedings O
of O
NAACL O
, O
pages O
483–498 O
. O

Jiacheng O
Yang O
, O
Mingxuan O
Wang O
, O
Hao O
Zhou O
, O
Chengqi O
Zhao O
, O
Weinan O
Zhang O
, O
Yong O
Yu O
, O
and O
Lei O
Li O
. O
2020 O
. O

Towards O
making O
the O
most O
of O
bert O
in O
neural O
machine O
translation O
. O

In O
Proceedings O
of O
AAAI O
, O
pages O
9378 O
– O
9385 O
. O

25Biao O
Zhang O
, O
Philip O
Williams O
, O
Ivan O
Titov O
, O
and O
Rico O
Sen- O
nrich O
. O

2020 O
. O

Improving O
massively O
multilingual O
neu- O
ral O
machine O
translation O
and O
zero O
- O
shot O
translation O
. O

In O
Proceedings O
of O
ACL O
, O
pages O
1628–1639 O
. O

Jinhua O
Zhu O
, O
Yingce O
Xia O
, O
Lijun O
Wu O
, O
Di O
He O
, O
Tao O
Qin O
, O
Wengang O
Zhou O
, O
Houqiang O
Li O
, O
and O
Tieyan O
Liu O
. O
2020 O
. O

Incorporating O
bert O
into O
neural O
machine O
translation O
. O

InProceedings O
of O
ICLR O
. O

A O
Dataset O

The O
dataset O
is O
from O
WMT O
translation O
task O
, O
CCAligned O
corpus11 O
, O
WAT21 O
translation O
task12 O
, O
Flores O
test O
set13and O
Tatoeba O
test O
sets14 O
. O

We O
use O
the O
ﬁrst O
20 O
M O
sentence O
pairs O
in O
the O
Es O
- O
En O
CCAligned O
corpus O
as O
training O
set O
. O

For O
experiments O
of O
Table O
5 O
, O
the O
validation O
set O
for O
De O
- O
En O
, O
Es O
- O
En O
and O
Fi O
- O
En O
are O
the O
concatnation O
of O
Fr O
- O
En O
and O
Cs O
- O
En O
validation O
set O
. O

We O
use O
Ta O
- O
En O
and O
Zh O
- O
En O
as O
the O
validation O
set O
for O
Hi O
- O
En O
and O
Zh O
- O
En O
, O
respectively O
. O

More O
details O
are O
in O
Table O
9 O
to O
Table O
11 O
. O

To O
be O
compatible O
with O
XLM O
- O
R O
model O
, O
all O
texts O
are O
tokenized O
with O
the O
same O
XLM O
- O
R O
sentencepiece O
( O
Kudo O
, O
2018 O
) O
model O
. O

The O
< O
bos O
> O
token O
is O
added O
at O
the O
beginning O
of O
each O
source O
sentence O
while O
< O
eos O
> O
token O
is O
appended O
at O
the O
end O
when O
the O
NMT O
model O
initializes O
encoder O
with O
XLM O
- O
R. O

The O
source O
sentence O
length O
is O
limited O
within O
512 O
tokens O
. O

B O
Model O
and O
Training O
Details O

The O
encoder O
of O
SixT B-MethodName
is O
the O
same O
size O
of O
XLM O
- O
R O
model O
. O

We O
compare O
models O
with O
different O
decoder O
conﬁgurations O
in O
the O
paper O
, O
the O
details O
are O
in O
the O
Table O
12 O
. O

For O
all O
models O
, O
the O
dimension O
of O
decoder O
hidden O
states O
equals O
that O
of O
encoder O
hidden O
states O
. O

The O
number O
of O
attentionO
heads O
is O
set O
as O
16 O
for O
the O
decoder O
of O
SixT O
large O
model O
, O
so O
that O
the O
dimension O
of O
hidden O
states O
can O
be O
divided O
by O
the O
number O
of O
attention O
heads O
. O

We O
use O
separate O
encoder O
and O
de- O
coder O
embeddings O
. O

We O
tie O
the O
decoder O
input O
and O
output O
embeddings O
. O

The O
source O
vocabulary O
uses O
the O
same O
250k O
vocabulary O
of O
XLM O
- O
R O
, O
while O
the O
target O
vocabulary O
is O
generated O
from O
the O
training O
corpus O
. O

All O
experiments O
are O
done O
with O
8 O
GPUs O
. O

We O
compare O
SixT O
large O
with O
CRISS O
, O
m2m-100 O
and O
mBART O
in O
the O
Table O
4 O
. O

We O
use O
the O
ofﬁcial O
model O
checkpoints O
of O
mBART15 O
(611 O
M O
parame- O
ters O
) O
, O
CRISS16O
(680 O
M O
parameters O
) O
and O
m2m-100 O

Mediators O
in O
Determining O
what O
Processing O
BERT B-MethodName
Performs O
First O
Aviv O
Slobodkin O

Leshem O
Choshen O
Omri O
Abend O
School O
of O
Computer O
Science O
and O
Engineering O
The O
Hebrew O
University O
of O
Jerusalem O
{ O
aviv.slobodkin,leshem.choshen,omri.abend}@mail.huji.ac.il O
Abstract O
Probing O
neural O
models O
for O
the O
ability O
to O
per- O
form O
downstream O
tasks O
using O
their O
activation O
patterns O
is O
often O
used O
to O
localize O
what O
parts O
of O
the O
network O
specialize O
in O
performing O
what O
tasks O
. O

However O
, O
little O
work O
addressed O
poten- O
tial O
mediating O
factors O
in O
such O
comparisons O
. O

As O
a O
test O
- O
case O
mediating O
factor O
, O
we O
consider O
the O
prediction O
’s O
context O
length O
, O
namely O
the O
length O
of O
the O
span O
whose O
processing O
is O
minimally O
re- O
quired O
to O
perform O
the O
prediction O
. O

We O
show O
that O
not O
controlling O
for O
context O
length O
may O
lead O
to O
contradictory O
conclusions O
as O
to O
the O
local- O
ization O
patterns O
of O
the O
network O
, O
depending O
on O
the O
distribution O
of O
the O
probing O
dataset O
. O

Indeed O
, O
when O
probing O
BERT B-MethodName
with O
seven O
tasks O
, O
we O
ﬁnd O
that O
it O
is O
possible O
to O
get O
196 O
different O
rankings O
between O
them O
when O
manipulating O
the O
distribu- O
tion O
of O
context O
lengths O
in O
the O
probing O
dataset O
. O

We O
conclude O
by O
presenting O
best O
practices O
for O
conducting O
such O
comparisons O
in O
the O
future.1 O
1 O
Introduction O
The O
strong O
performance O
of O
end O
- O
to O
- O
end O
models O
and O
the O
difﬁculty O
in O
understanding O
their O
inner O
work- O
ings O
has O
led O
to O
extensive O
research O
aimed O
at O
inter- O
preting O
their O
behavior O
( O
Li O
et O
al O
. O
, O
2016 O
; O

Yosinski O
et O
al O
. O
, O
2015 O
; O
Karpathy O
et O
al O
. O
, O
2015 O
) O
. O

This O
notion O
has O
led O
researchers O
to O
investigate O
the O
behavioral O
traits O
of O
networks O
in O
general O
( O
Li O
et O
al O
. O
, O
2015 O
; O
Haco- O
hen O
et O
al O
. O
, O
2020 O
) O
and O
representative O
architectures O
in O
particular O
( O
Schlichtkrull O
et O
al O
. O
, O
2020 O
) O
. O

Within O
NLP B-TaskName
, O
Transformer O
- O
based O
pretrained O
embeddings O
are O
the O
basis O
for O
many O
tasks O
, O
which O
underscores O
the O
impor- O
tance O
in O
interpreting O
their O
behavior O
( O
Belinkov O
et O
al O
. O
, O
2020 O
) O
, O
and O
especially O
the O
behavior O
of O
BERT B-MethodName
( O
De- O
vlin O
et O

al O
. O
, O
2019 O
; O
Rogers O
et O
al O
. O
, O
2020 O
) O
, O
perhaps O
the O
most O
widely O
used O
of O
Transformer O
- O
based O
models O
. O

In O
this O
work O
, O
we O
analyze O
the O
common O
approach O
ofprobing O
( O
§ O
2 O
) O
, O
used O
to O
localize O
where O
“ O
knowledge O
” O
1The O
code O
is O
available O
at O
https://github.com/ O
lovodkin93 O
/ O
BERT B-MethodName
- O
context O
- O
distance O
.of O

particular O
tasks O
is O
encoded O
; O
localization O
is O
often O
carried O
out O
in O
terms O
of O
the O
layers O
most O
responsible O
for O
the O
task O
at O
hand O
( O
c.f O
. O
Tenney O
et O
al O
. O
, O
2019b O
) O
. O

Vari- O
ous O
works O
( O
Tenney O
et O
al O
. O
, O
2019a O
; O
Peters O
et O
al O
. O
, O
2018 O
; O
Blevins O
et O
al O
. O
, O
2018 O
) O
showed O
that O
some O
tasks O
are O
processed O
in O
lower O
levels O
than O
others O
. O

We O
examine O
the O
extent O
to O
which O
potential O
me- O
diating O
factors O
may O
account O
for O
observed O
trends O
and O
show O
that O
varying O
some O
mediating O
factors O
( O
see O
§ O
2 O
) O
may O
diminish O
, O
or O
even O
reverse O
, O
the O
conclusions O
made O
by O
Tenney O
et O
al O
. O

( O
T19 O
; O
2019a O
) O
. O

Speciﬁcally O
, O
despite O
reafﬁrming O
T19 O
’s O
experimental O
ﬁndings O
, O
we O
contest O
T19 O
’s O
interpretation O
of O
the O
results O
, O
namely O
that O
the O
processing O
carried O
out O
by O
BERT B-MethodName
parallels O
the O
classical O
NLP B-TaskName
pipeline O
. O

Indeed O
, O
T19 O
concludes O
that O
lexical O
tasks O
( O
POS B-TaskName
tagging I-TaskName
) O
are O
performed O
by O
the O
lower O
layers O
, O
followed O
by O
syntactic B-TaskName
tasks I-TaskName
, O
whereas O
more O
semantic O
tasks O
are O
performed O
later O
on O
. O

This O
analysis O
rests O
on O
the O
assumption O
that O
the O
nature O
of O
the O
task O
( O
lexical O
, O
syntactic O
, O
or O
semantic O
) O
is O
the O
driving O
force O
that O
determines O
what O
layer O
per- O
forms O
what O
analysis O
. O

We O
show O
that O
other O
factors O
should O
be O
weighed O
in O
as O
well O
. O

Speciﬁcally O
, O
we O
show O
that O
manipulating O
the O
distribution O
of O
examples O
in O
the O
probing O
dataset O
can O
lead O
to O
a O
variety O
of O
different O
conclusions O
as O
to O
what O
tasks O
are O
performed O
ﬁrst O
. O

We O
argue O
that O
potential O
mediators O
must O
be O
con- O
sidered O
when O
comparing O
tasks O
, O
and O
focus O
on O
one O
such O
mediator O
– O
the O
context O
length O
, O
which O
we O
de- O
ﬁne O
as O
the O
number O
of O
tokens O
whose O
processing O
is O
minimally O
required O
to O
perform O
the O
prediction O
. O

We O
operationalize O
this O
notion O
by O
deﬁning O
it O
as O
the O
max- O
imal O
distance O
between O
any O
two O
tokens O
for O
which O
a O
label O
is O
predicted O
. O

This O
amounts O
to O
the O
span O
length O
in O
tasks O
that O
involve O
a O
single O
span O
( O
e.g. O
, O
NER B-TaskName
) O
, O
and O
to O
the O
dependency O
length O
in O
tasks O
that O
address O
the O
relation O
between O
two O
spans O
. O

See O
§ O
2 O
. O

Our O
motiva- O
tion O
for O
considering O
context O
length O
as O
a O
mediator O
is O
grounded O
in O
previous O
work O
that O
presented O
the O
difﬁculty O
posed O
by O
long O
- O
distance O
dependencies O
in O
various O
NLP B-TaskName
tasks O
( O
Xu O
et O
al O
. O
, O
2009 O
; O
Sennrich O
, O
2017 O
) O
, O

87and O
particularly O
in O
previous O
work O
that O
indicated O
the O
Transformers O
’ O
difﬁculty O
to O
generalize O
across O
dif- O

ferent O
dependency O
lengths O
( O
Choshen O
and O
Abend O
, O
2019 O
) O
. O

We O
show O
that O
in O
some O
of O
the O
cases O
where O
one O
task O
seems O
to O
be O
better O
predicted O
by O
a O
higher O
layer O
than O
another O
task O
, O
controlling O
for O
context O
length O
may O
reverse O
that O
order O
. O

Indeed O
we O
show O
that O
196 O
different O
rankings O
between O
the O
seven O
tasks O
explored O
in O
T19 O
may O
be O
obtained O
with O
a O
suitable O
distribution O
over O
the O
probing O
datasets O
, O
namely O
196 O
different O
ways O
to O
rank O
the O
tasks O
according O
to O
their O
expected O
layer O
. O

Moreover O
, O
our O
results O
show O
that O
when O
context O
length O
is O
not O
taken O
into O
account O
, O
one O
task O
( O
e.g. O
, O
dependency B-TaskName
parsing I-TaskName
) O
may O
seem O
to O
be O
processed O
at O
a O
higher O
layer O
than O
another O
( O
e.g. O
, O
NER B-TaskName
) O
, O
when O
its O
expected O
layer O
( O
see O
§ O
2 O
) O
is O
, O
in O
fact O
, O
lower O
for O
all O
ranges O
of O
context O
lengths O
( O
§ O
3.1.1 O
) O
. O

2 O
Background O
We O
begin O
by O
laying O
out O
the O
terminology O
and O
methodology O
we O
will O
use O
in O
the O
paper O
. O

Edge O
Probing O
. O

Edge O
probing O
is O
the O
method O
of O
training O
a O
classiﬁer O
for O
a O
given O
task O
on O
different O
parts O
of O
the O
network O
( O
without O
ﬁne O
- O
tuning O
) O
. O

Suc- O
cess O
in O
classiﬁcation O
is O
interpreted O
as O
evidence O
that O
the O
required O
features O
for O
classiﬁcation O
are O
some- O
how O
encoded O
in O
the O
examined O
part O
and O
are O
sufﬁ- O
ciently O
easy O
to O
extract O
. O

In O
our O
experiments O
, O
we O
follow O
T19 O
and O
probe O
BERT B-MethodName
with O
Named O
Entity O
Recognition O
( O
NER B-TaskName
) O
, O
a O
constituent O
- O
based O
task O
( O
clas- O
sifying O
Non O
- O
terminals O
- O
Non O
- O
term O
. O
) O
, O
Semantic O
Role O
Labeling O
( O
SRL B-TaskName
) O
, O
Co B-TaskName
- I-TaskName
reference I-TaskName
( O
Co B-TaskName
- I-TaskName
ref I-TaskName
. O
) O
, O
Semantic O
Proto O
- O
Roles O
( O
SPR B-TaskName
; O
Reisinger O
et O
al O
. O
, O
2015 O
) O
, O
Relation O
Classiﬁcation O
( O
RC B-TaskName
) O
and O
the O
Stanford O
Dependency B-TaskName
Parsing I-TaskName
( O
Dep B-TaskName
. O
; O
de O
Marneffe O
et O
al O
. O
, O
2006 O
) O
. O

Causal O
considerations O
in O
interpreting O
probing O
results O
were O
also O
emphasized O
by O
several O
recent O
works O
( O
e.g. O
, O
Kaushik O
et O
al O
. O
, O
2020 O
; O

Vig O
et O
al O
. O
, O
2020 O
; O
Elazar O
et O
al O
. O
, O
2021 O
) O
. O

Localization O
by O
Expected O
Layer O
. O

The O
expected O
layer O
metric O
( O
which O
we O
will O
henceforth O
refer O
to O
it O
as O
Elayer B-MetricName
) O
of O
T19 O
assesses O
which O
layer O
in O
BERT B-MethodName
is O
most O
needed O
for O
prediction O
: O
a O
probing B-MetricName
classiﬁer I-MetricName
P I-MetricName
is O
trained O
on O
the O
lowest O
llayers O
. O

Then O
, O
a O
dif- O
ferential O
score O
(l)is O
computed O
, O
which O
indicates O
the O
performance O
gain O
when O
taking O
into O
account O
one O
additional O
layer O
: O
(l)=Score O
( O
P(l)) Score O
( O
P(l 1))(1)Once O
all O
thef(l)g12 O
l=1are O
computed O
, O
we O
may O
compute O
Elayer B-MetricName
: O
Elayer B-MetricName
] O
= O
P12 O
l=1l(l O
) O
P12 O
l=1(l)(2 O
) O
Therefore O
, O
unlike O
standard O
edge O
probing O
, O
which O
is O
performed O
on O
each O
layer O
individually O
, O
computing O
Elayer B-MetricName
takes O
into O
account O
all O
layers O
up O
to O
a O
given O
l. O
Mediation O
Analysis O
. O

Each O
of O
the O
explored O
tasks O
classiﬁes O
one O
or O
two O
input O
sub O
- O
spans O
. O

In O
both O
cases O
, O
we O
deﬁne O
the O
context O
length O
to O
be O
the O
distance O

be- O
tween O
the O
earliest O
and O
latest O
span O
index O
. O

Namely O
, O
for O
tasks O
with O
two O
spans O
( O
e.g. O
, O
SPR B-TaskName
) O
, O
span O
1=[i1,j1 O
] O
andspan O
2=[i2,j2 O
] O
, O
where O
span O
1appears O
before O
span O
2 O
, O
the O
context O
length O
is O
j2 O
- O
i1 O
, O
whereas O
for O
tasks O
with O
just O
one O
span O
( O
e.g. O
, O
NER B-TaskName
) O
, O
span O
1=[i1,j1 O
] O
, O
it O
is O
j1 O
- O
i1 O
. O

In O
order O
to O
examine O
the O
effect O
of O
context O
length O
on O
Elayer B-MetricName
, O
we O
model O
it O
as O
a O
mediating O
factor O
, O
namely O
as O
an O
intermediate O
variable O
that O
( O
partly O
) O
ex- O
plains O
the O
relationship O
between O
two O
other O
variables O
( O
in O
this O
work O
, O
a O
task O
and O
its O
Elayer B-MetricName
) O
. O

See O
Figure O
1 O
. O

We O
bin O
each O
task O
’s O
test O
set O
into O
non O
- O
overlapping O
bins O
, O
according O
to O
their O
context O
length O
ranges O
. O

We O
use O
the O
notation O
‘ O
i O
- O
j O
’ O
to O
denote O
the O
bin O
of O
context O
lengths O
in O
the O
range O

[ O
i O
, O
j O
] O
. O

For O
example O
, O
the O
sec- O
ond O
bin O
would O
be O
’ O
3 O
- O
5 O
’ O
, O
denoting O
context O
lengths O
3 O
, O
4 O
, O
and O
5 O
. O

In O
addition O
, O
given O
a O
speciﬁc O
task O
, O
two O
possible O
approaches O
exist O
to O
examine O
the O
media- O
tion O
effect O
of O
context O
length O
on O
the O
task O
’s O
Elayer B-MetricName
. O

The O
ﬁrst O
one O
bins O
all O
the O
task O
’s O
data O
into O
sub O
- O
sets O
, O
in O
advance O
. O

Then O
, O
this O
approach O
ﬁne O
- O
tunes O
over O
each O
subset O
separately O
. O

Alternatively O
, O
the O
second O
approach O
ﬁne O
- O
tunes O
over O
the O
whole O
dataset O
, O
binning O
only O
during O
the O
test O
phase O
. O

We O
follow O
the O
latter O
ap- O
proach O
, O
as O
it O
is O
more O
computationally O
efﬁcient O
. O

T O
C O
Elayer B-MetricName
Figure O
1 O
: O
The O
relationship O
we O
stipulate O
between O
the O
task O
, O
the O
context O
length O
, O
and O
Elayer B-MetricName
. O

We O
use O
two O
ran- O
dom O
variables O
: O
Tis O
the O
task O
, O
which O
can O
be O
any O
of O
the O
seven O
tasks O
we O
observe O
and O
Cis O
the O
context O
length O
. O

Interestingly O
, O
in O
§ O
3.1.1 O
, O
we O
encounter O
a O
spe- O
cial O
edge O
case O
, O
where O
the O
aggregated O
average O
( O
i.e. O
, O
Elayer B-MetricName
) O
of O
one O
task O
is O
higher O
than O
another O
, O
whereas O

88 O
in O
each O
sub O
- O
set O
( O
by O
a O
given O
context O
length O
) O
it O
is O
lower O
. O

This O
may O
occur O
when O
the O
weight O
of O
the O
sub O
- O
sets O
differs O
between O
the O
two O
aggregations O
. O

3 O
Experiments O
We O
hypothesize O
that O
the O
context O
length O
is O
a O
medi- O
ating O
factor O
in O
the O
Elayer B-MetricName
of O
a O
task O
. O

In O
order O
to O
test O
this O
hypothesis O
, O
we O
run O
the O
following O
experiments O
, O
aiming O
at O
isolating O
the O
context O
length O
. O

We O
use O
the O
SPR1 B-DatasetName
dataset O
( O
Reisinger O
et O
al O
. O
, O
2015 O
) O
to O
probe O
SPR B-TaskName
, O
the O
English O
Web O
Treebank O
for O
the O
Dep B-TaskName
. O
task O
( O
Silveira O
et O
al O
. O
, O
2014 O
) O
, O
the O
SemEval O
2010 O
Task O
8 O
for O
the O
RC B-TaskName
task O
( O
Hendrickx O
et O
al O
. O
, O
2009 O
) O
, O
and O
the O
OntoNotes B-DatasetName
5.0 I-DatasetName
dataset O
( O
Weischedel O
et O
al O
. O
, O
2013 O
) O
for O
the O
other O
tasks O
. O

Conﬁgurations O
follow O
the O
defaults O
in O
the O
Jiant O
toolkit O
implementation O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
. O

In O
addition O
, O
we O
work O
with O
the O
BERT- O
base O
model O
. O

3.1 O
The O
Effect O
on O
Elayer B-MetricName
First O
, O
we O
wish O
to O
conﬁrm O
that O
context O
length O
indeed O
affects O
Elayer B-MetricName
and O
that O
the O
task O
is O
not O
a O
sole O
contrib- O
utor O
to O
this O
. O

Given O
a O
task O
and O
a O
threshold O
thr O
, O
we O
compile O
a O
dataset O
for O
the O
task O
containing O
the O
sub- O
set O
of O
examples O
with O
context O
lengths O
shorter O
than O
thr O
, O
and O
use O
it O
to O
compute O
Elayer B-MetricName
. O

We O
do O
it O
for O
all O
tasks O
and O
for O
every O
integer O
threshold O
between O
0 O
and O
a O
maximal O
threshold O
, O
which O
is O
selected O
separately O
for O
each O
task O
to O
ensure O
that O
at O
least O
2000 O
instances O
remain O
in O
the O
last O
bin O
. O

We O
ﬁnd O
that O
context O
length O
plays O
an O
important O
role O
in O
the O
difference O
between O
the O
expected O
layers O
( O
Figure O
2 O
) O
. O

Most O
notably O
, O
the O
Co B-TaskName
- I-TaskName
ref I-TaskName
. O
, O
SRL B-TaskName
, O
Dep B-TaskName
. O
, O
and O
RC B-TaskName
tasks O
’ O
Elayer B-MetricName
increases O
when O
increasing O
the O
threshold O
. O

Next O
, O
we O
divide O
the O
data O
into O
smaller O
bins O
of O
non O
- O
overlapping O
context O
length O
ranges O
, O
in O
order O
to O
control O
for O
the O
inﬂuence O
of O
the O
context O
lengths O
on O
the O
expected O
layers O
of O
the O
tasks O
. O

We O
compute O
Elayer B-MetricName
for O
sub O
- O
sets O
of O
similar O
lengths O
. O

In O
choos- O
ing O
the O
size O
of O
each O
such O
range O
, O
we O
try O
to O
balance O
between O
informativeness O
( O
narrower O
ranges O
) O
and O
re- O
liability O
( O
having O
enough O
examples O
in O
each O
range O
, O
so O
as O
to O
reduce O
noise O
) O
. O

We O
ﬁnd O
that O
the O
narrowest O
range O
width O
that O
retains O
at O
least O
1 O
% O
of O
the O
examples O
in O
each O
bin O
is O
3 O
. O

We O
thus O
divide O
the O
dataset O
for O
each O
task O
into O
context O
length O
ranges O
of O
width O
3 O
, O
until O
the O
maximal O
threshold O
is O
reached O
. O

Higher O
context O
lengths O
are O
lumped O
into O
an O
additional O
bin O
. O

Figure O
2 O
: O
Elayer B-MetricName
as O
a O
function O
of O
a O
threshold O
on O
the O
context O
length O
. O

For O
each O
such O
threshold O
thr(x O
- O
axis O
) O
, O
Elayer B-MetricName
( O
y O
- O
axis O
) O
is O
computed O
based O
only O
on O
the O
examples O
with O
context O
length O
no O
longer O
than O
thr O
. O

3.1.1 O
Manipulating O
the O
Context O
Length O
Distribution O
: O
An O
Extreme O
Case O
. O

We O
begin O
by O
examining O
two O
speciﬁc O
tasks O
: O

Dep B-TaskName
. O
and O
NER B-TaskName
, O
and O
their O
Elayer B-MetricName
for O
each O
context O
length O
’s O
range O
. O

We O
then O
consider O
, O
for O
simplicity O
, O
a O
case O
where O
all O
the O
context O
lengths O
of O
Dep B-TaskName
. O
are O
of O
length O
9 O
+ O
, O
while O
those O
of O
NER B-TaskName
are O
in O
the O
range O
of O
3 O
- O
5 O
( O
Figure O
3 O
) O
. O

We O
see O
that O
when O
controlling O
for O
context O
length O
, O
Dep B-TaskName
. O
is O
computed O
in O
a O
lower O
layer O
than O
NER B-TaskName
, O
regardless O
of O
the O
range O
. O

However O
, O
depending O
on O
the O
distribution O
of O
context O
lengths O
in O
the O
probing B-DatasetName
dataset I-DatasetName
, O
the O
outcome O
may O
be O
completely O
different O
, O
with O
Dep B-TaskName
. O

being O
processed O
in O
higher O
layers O
( O
for O
a O
similar O
example O
of O
a O
different O
task O
- O
pair O
, O
see O
§ O
A.1 O
) O
. O

These O
results O
indicate O
that O
the O
results O
of O
T19 O
do O
not O
necessarily O
indicate O
that O
BERT B-MethodName
is O
performing O
a O
pipeline O
of O
computations O
( O
as O
is O
commonly O
asserted O
, O
see O
e.g. O
, O
T19 O
and O
Blevins O
et O
al O
. O

( O
2018 O
) O
) O
, O
and O
that O
mediating O
factors O
need O
to O
be O
taken O
into O
account O
when O
interpreting O
Elayer B-MetricName
. O

Figure O
3 O
: O
Elayer B-MetricName
of O
NER B-TaskName
and O
Dep B-TaskName
. O

for O
different O
context O
length O
ranges O
( O
4 O
left O
blue O
and O
yellow O
pairs O
) O
, O
and O
their O
Elayer B-MetricName
when O
all O
instances O
of O
NER B-TaskName
are O
of O
context O
length O
l2[3;5]and O
all O
those O
of O
Dep B-TaskName
. O

are O
of O
context O
length O
l9(rightmost O
green O
and O
red O
pair O
) O
. O

While O
for O
every O
context O
length O
range O
, O
NER B-TaskName
’s O
Elayer B-MetricName
is O
bigger O
than O
that O
of O
Dep B-TaskName
. O
, O
for O
some O
context O
length O
distribution O
that O
order O
may O
be O
reversed O
. O

893.2 O
Imposing O
Similar O
Length O
Distributions O
In O
the O
previous O
section O
, O
we O
observed O
that O
one O
task O
can O
be O
both O
higher O
and O
lower O
than O
another O
. O

That O
depends O
on O
the O
distribution O
of O
context O
lengths O
in O
the O
probing B-DatasetName
dataset I-DatasetName
. O

We O
next O
ask O
whether O
such O
a O
" O
paradox O
" O
arises O
in O
experiments O
when O
imposing O
the O
same O
context O
length O
distributions O
on O
the O
two O
tasks O
. O

Following O
Pearl O
( O
2001 O
) O
, O
we O
employ O
mediation O
analysis O
and O
speciﬁcally O
concentrate O
on O
the O
Natural O
Direct O
Effect O
( O
NDE O
) O
, O
which O
is O
the O
difference O
be- O
tween O
two O
of O
the O
observed O
dependent O
variables O
( O
in O
our O
case O
Elayer B-MetricName
) O
, O
when O
ﬁxing O
the O
mediator O
. O

In O
our O
case O
, O
the O
NDE O
is O
the O
difference O
between O
the O
Elayer B-MetricName
of O
two O
tasks O
, O
while O
forcing O
the O
same O
context O
length O
distribution O
on O
both O
. O

For O
convenience O
, O
we O
force O
the O
distribution O
of O
one O
of O
the O
examined O
tasks O
( O
for O
more O
details O
, O
see O
§ O
A.2 O
) O
, O
but O
any O
distribution O
is O
applicable O
. O

In O
general O
, O
the O
equation O
for O
computing O
the O
NDE O
of O
taskst1andt2 O
, O
with O
the O
context O
length O
distribution O
oft1imposed O
on O
both O
, O
is O
: O
NDE O
t1t2 O
= O
X O
c[E[ljC O
= O
c O
; O

T O
= O
t2 O
] O
 E[ljC O
= O
c O
; O
T O
= O
t1]]P(C O
= O
cjT O
= O
t1)(3 O
) O
where O
T O
is O
a O
random O
variable O
of O
the O
tasks O
, O
and O
C O
is O
a O
random O
variable O
of O
the O
context O
length O
. O

We O
apply O
NDE O
twice O
for O
every O
pair O
of O
tasks O
( O
once O
for O
each O
task O
’s O
context O
length O
distribution O
) O
. O

We O
then O
compare O
the O
results O
to O
the O
difference O
be- O

tween O
the O
tasks O
’ O
expected O
layers O
where O
each O
task O
keeps O
its O
original O
context O
length O
distribution O
( O
un- O
mediated O
) O
. O

Results O
( O
Figure O
4 O
) O
show O
that O
the O
differ- O
ence O
could O
be O
more O
than O
50 O
times O
larger O
( O
change O
of O
1.24 O
in O
absolute O
value O
) O
or O
decrease O
by O
86 O
% O
( O
0.73 O
in O
absolute O
value O
) O
. O

In O
some O
cases O
the O
order O
of O
the O
two O
tasks O
is O
reversed O
, O
namely O
, O
the O
task O
that O
is O
lower O
with O
one O
distribution O
becomes O
higher O
with O
another O
. O

This O
shows O
that O
even O
among O
our O
examined O
set O
of O
seven O
tasks O
, O
the O
effect O
of O
potential O
mediators O
can O
not O
be O
ignored O
. O

For O
more O
results O
, O
see O
§ O
A.3 O
. O

3.2.1 O
Controlling O
for O
Context O
Length O
After O
observing O
that O
the O
distribution O
of O
context O
length O
in O
the O
probing B-DatasetName
dataset I-DatasetName
may O
affect O
the O
relative O
order O
of O
the O
expected O
layers O
, O
we O
propose O
a O
more O
de- O
tailed O
and O
accurate O
method O
to O
compare O
the O
expected O
layers O
, O
which O
does O
not O
rely O
on O
a O
speciﬁc O
length O
dis- O
tribution O
. O

We O
do O
so O
by O
plotting O
the O
controlled O
effect O
, O
namely O
Elayer B-MetricName
for O
each O
range O
separately O
. O

Our O
results O
( O
Figure O
5 O
) O
allow O
computing O
the O
range O
of O
possible O
expected O
layers O
for O
a O
task O
, O
that O
may O
re- O
sult O
from O
taking O
any O
context O
length O
distribution O
Figure O
4 O
: O
Difference O
between O
unmediated O
Elayer B-MetricName
and O
NDE O
for O
NER B-TaskName
and O
Co B-TaskName
- I-TaskName
ref I-TaskName
. O

( O
left O
) O
; O
NER B-TaskName
and O
RC B-TaskName
( O
mid- O
dle O
) O
; O
and O
SPR B-TaskName
and O
RC B-TaskName

( O
right O
) O
. O

The O
employed O
context O
length O
distributions O
( O
as O
part O
of O
the O
NDE O
calculations O
) O
are O
of O
Co B-TaskName
- I-TaskName
ref I-TaskName
. O
, O
NER B-TaskName
and O
SPR B-TaskName
, O
respectively O
. O

( O
Figure O
6 O
) O
. O

The O
ﬁgure O
shows O
the O
wide O
range O
of O
possible O
relative O
behaviors O
of O
Elayer B-MetricName
for O
task O
- O
pairs O
: O
from O
notable O
to O
negligible O
difference O
in O
expected O
layers O
( O
e.g. O
, O
SRL B-TaskName
and O
Co B-TaskName
- I-TaskName
ref I-TaskName
. O
) O
, O
to O
pairs O
whose O
or- O
dering O
of O
expected O
layers O
may O
be O
reversed O
( O
i.e. O
, O
overlapping O
ranges O
, O
such O
as O
with O
SPR B-TaskName
and O
RC B-TaskName
) O
. O

In O
fact O
, O
by O
taking O
into O
account O
every O
possible O
combi- O
nation O
of O
context O
length O
distribution O
for O
each O
of O
the O
tasks O
, O
we O
get O
as O
many O
as O
196 O
possible O
rankings O
of O
the O
seven O
tasks O
according O
to O
their O
Elayer B-MetricName
. O

One O
such O
possible O
order O
is O
, O
for O
example O
, O
Non O
- O
term O
. O

< O

Dep B-TaskName
. O
< O

SRL B-TaskName
< O
RC B-TaskName
< O

NER B-TaskName
< O
Co B-TaskName
- I-TaskName
ref I-TaskName
. O

< O

SPR B-TaskName
. O

We O
elaborate O
on O
this O
in O
§ O
A.4 O
. O

To O
recap O
, O
we O
ﬁnd O
that O
the O
difference O
in O
Elayer B-MetricName
between O
some O
tasks O
may O
considerably O
change O
and O
their O
order O
may O
reverse O
, O
depending O
on O
the O
context O
length O
. O

This O
ﬁnding O
lends O
further O
support O
to O
our O
claim O
that O
mediators O
should O
be O
taken O
into O
account O
. O

Figure O
5 O
: O
Expected O
layers O
of O
all O
seven O
tasks O
as O
a O
func- O
tion O
of O
context O
length O
range O
. O

4 O
Conclusion O
We O
showed O
that O
when O
performing O
edge O
probing O
to O
identify O
what O
layers O
are O
responsible O
for O
addressing O
what O
tasks O
, O
it O
is O
imperative O
to O
take O
into O
account O
potential O
mediators O
, O
as O
they O
may O
be O
responsible O

90 O
Figure O
6 O
: O
The O
range O
of O
possible O
expected O
layers O
when O
varying O
context O
length O
, O
for O
each O
of O
the O
seven O
tasks O
. O

for O
much O
of O
the O
observed O
effect O
. O

Speciﬁcally O
, O
we O
showed O
that O
context O
length O
has O
a O
signiﬁcant O
impact O
on O
a O
task O
’s O
Elayer B-MetricName
. O

Our O
analysis O
shows O
the O
wide O
range O
of O
relative O
orderings O
of O
the O
expected O
layers O
for O
different O
tasks O
when O
assuming O
different O
con- O
text O
length O
distributions O
; O
from O
extreme O
edge O
cases O
, O
like O
the O
one O
we O
observed O
in O
§ O
3.1.1 O
, O
to O
more O
com- O
mon O
, O
but O
potentially O
misleading O
ones O
, O
where O
the O
difference O
between O
expected O
layers O
may O
dramati- O
cally O
increase O
or O
decrease O
depending O
on O
the O
context O
length O
distribution O
. O

Most O
importantly O
, O
it O
shows O
that O
by O
manipulating O
the O
context O
length O
distribution O
, O
we O
may O
get O
a O
wide O
range O
of O
outcomes O
. O

Our O
work O
suggests O
that O
mediating O
factors O
should O
be O
taken O
into O
account O
when O
basing O
analysis O
on O
the O
Elayer B-MetricName
. O

On O
a O
broader O
note O
, O
alternative O
hypotheses O
should O
be O
considered O
, O
before O
limiting O
oneself O
to O
a O
single O
interpretation O
. O

Future O
work O
will O
consider O
the O
effect O
of O
other O
me- O
diating O
factors O
. O

The O
two O
methods O
we O
used O
, O
NDE O
and O
controlled O
effect O
, O
can O
be O
used O
to O
examine O
the O
impact O
of O
other O
mediating O
factors O
and O
should O
be O
adopted O
as O
part O
of O
the O
ﬁeld O
’s O
basic O
analysis O
toolkit O
( O
cf O
. O

Feder O
et O

al O
. O
, O
2020 O
; O
Vig O
et O
al O
. O
, O
2020 O
) O
. O

NDE O
should O
be O
used O
when O
several O
effects O
are O
examined O
simultaneously O
, O
as O
it O
facilitates O
the O
assessment O
of O
their O
effect O
on O
the O
tasks O
’ O
complexity O
. O

It O
is O
also O
ad- O
visable O
to O
use O
NDE O
when O
a O
more O
practical O
examina- O
tion O
is O
required O
, O
i.e. O
, O
when O
distributions O
of O
the O
medi- O
ators O
are O
given O
empirically O
, O
as O
it O
is O
easier O
to O
derive O
the O
mediating O
factors O
’ O
impact O
using O
this O
method O
. O

In O
contrast O
, O
the O
controlled O
effect O
method O
should O
be O
used O
when O
examining O
the O
effects O
of O
two O
vari- O
ables O
( O
e.g. O
, O
tasks O
and O
mediating O
factors O
) O
or O
when O
comparing O
several O
tasks O
with O
one O
mediating O
effect O
. O

Acknowledgements O
This O
work O
was O
supported O
by O
the O
Israel O
Science O
Foundation O
( O
grant O
no O
. O
929/17 O
) O
. O

We O
would O
also O
like O
to O
thank O
Amir O
Feder O
for O
his O
very O
insightful O
feedback O
on O
our O
paper O
. O

References O
Yonatan O
Belinkov O
, O
Nadir O
Durrani O
, O
Fahim O
Dalvi O
, O
Has- O
san O
Sajjad O
, O
and O
James O
Glass O
. O
2020 O
. O

On O
the O
linguistic O
representational O
power O
of O
neural O
machine O
translation O
models O
. O

Computational O
Linguistics O
, O
46(1):1–52 O
. O

Terra O
Blevins O
, O
Omer O
Levy O
, O
and O
Luke O
Zettlemoyer O
. O

2018 O
. O

Deep O
RNNs O
encode O
soft O
hierarchical O
syntax O
. O

In O
Pro- O
ceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Associa- O
tion O
for O
Computational O
Linguistics O
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
14–19 O
, O
Melbourne O
, O
Australia O
. O

Asso- O
ciation O
for O
Computational O
Linguistics O
. O

Leshem O
Choshen O
and O
Omri O
Abend O
. O

2019 O
. O

Automat- O
ically O
extracting O
challenge O
sets O
for O
non O
- O
local O
phe- O
nomena O
in O
neural O
machine O
translation O
. O

In O
Proceed- O
ings O
of O
the O
23rd O
Conference O
on O
Computational O
Nat- O
ural O
Language O
Learning O
( O
CoNLL O
) O
, O
pages O
291–303 O
, O
Hong O
Kong O
, O
China O
. O

Association O
for O
Computational O
Linguistics O
. O

Marie O
- O
Catherine O
de O
Marneffe O
, O
Bill O
MacCartney O
, O
and O
Christopher O
D. O
Manning O
. O

2006 O
. O

Generating O
typed O
dependency B-TaskName
parses I-TaskName
from O
phrase O
structure O
parses O
. O

In O
Proceedings O
of O
the O
Fifth O
International O
Conference O
on O
Language O
Resources O
and O
Evaluation O
( O
LREC’06 O
) O
, O
Genoa O
, O
Italy O
. O

European O
Language O
Resources O
Associ- O
ation O
( O
ELRA O
) O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT B-MethodName
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
under- O
standing O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
, O
Minneapolis O
, O
Minnesota O
. O

Associ- O
ation O
for O
Computational O
Linguistics O
. O

Yanai O
Elazar O
, O
Shauli O
Ravfogel O
, O
Alon O
Jacovi O
, O
and O
Yoav O
Goldberg O
. O
2021 O
. O

Amnesic O
probing O
: O
Behavioral O
ex- O
planation O
with O
amnesic O
counterfactuals O
. O

Transac- O
tions O
of O
the O
Association O
for O
Computational O
Linguis- O
tics O
, O
9:160–175 O
. O

Amir O
Feder O
, O
Nadav O
Oved O
, O
Uri O
Shalit O
, O
and O
Roi O
Reichart O
. O

2020 O
. O

Causalm O
: O
Causal O
model O
explanation O
through O
counterfactual O
language O
models O
. O

Guy O
Hacohen O
, O
Leshem O
Choshen O
, O
and O
D. O
Weinshall O
. O
2020 O
. O

Let O
’s O
agree O
to O
agree O
: O
Neural O
networks O
share O
classiﬁcation O
order O
on O
real O
datasets O
. O

International O
Conference O
of O
Machine O
Learning O
. O

91Iris O
Hendrickx O
, O
Su O
Nam O
Kim O
, O
Zornitsa O
Kozareva O
, O
Preslav O
Nakov O
, O
Diarmuid O
Ó O
Séaghdha O
, O
Sebastian O
Padó O
, O
Marco O
Pennacchiotti O
, O
Lorenza O
Romano O
, O
and O
Stan O
Szpakowicz O
. O
2009 O
. O

SemEval-2010 O
task O
8 O
: O
Multi O
- O
way O
classiﬁcation O
of O
semantic O
relations O
be- O
tween O
pairs O
of O
nominals O
. O

In O
Proceedings O
of O
the O
Workshop O
on O
Semantic O
Evaluations O
: O
Recent O
Achieve- O
ments O
and O
Future O
Directions O
( O
SEW-2009 O
) O
, O
pages O
94 O
– O
99 O
, O
Boulder O
, O
Colorado O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Andrej O
Karpathy O
, O
Justin O
Johnson O
, O
and O
Li O
Fei O
- O
Fei O
. O
2015 O
. O

Visualizing O
and O
understanding O
recurrent O
networks O
. O

Divyansh O
Kaushik O
, O
Eduard O
Hovy O
, O
and O
Zachary O
Lipton O
. O

2020 O
. O

Learning O
the O
difference O
that O
makes O
a O
differ- O
ence O
with O
counterfactually O
- O
augmented O
data O
. O

In O
Inter- O
national O
Conference O
on O
Learning O
Representations O
. O

Jiwei O
Li O
, O
Xinlei O
Chen O
, O
Eduard O
Hovy O
, O
and O
Dan O
Jurafsky O
. O
2016 O
. O

Visualizing O
and O
understanding O
neural O
models O
in O
NLP B-TaskName
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Tech- O
nologies O
, O
pages O
681–691 O
, O
San O
Diego O
, O
California O
. O

As- O
sociation O
for O
Computational O
Linguistics O
. O

Y O
. O

Li O
, O
J. O
Yosinski O
, O
J. O
Clune O
, O
H. O
Lipson O
, O
and O
J. O
Hopcroft O
. O

2015 O
. O

Convergent O
learning O
: O
Do O
different O
neu- O
ral O
networks O
learn O
the O
same O
representations O
? O

In O
FE@NIPS O
. O

Judea O
Pearl O
. O

2001 O
. O

Direct O
and O
indirect O
effects O
. O

In O
Pro- O
ceedings O
of O
the O
17th O
Conference O
in O
Uncertainty O
in O
Artiﬁcial O
Intelligence O
, O
UAI O
’ O
01 O
, O
page O
411–420 O
, O
San O
Francisco O
, O
CA O
, O
USA O
. O

Morgan O
Kaufmann O
Publishers O
Inc. O

Matthew O
Peters O
, O
Mark O
Neumann O
, O
Luke O
Zettlemoyer O
, O
and O
Wen O
- O
tau O
Yih O
. O
2018 O
. O

Dissecting O
contextual O
word O
embeddings O
: O
Architecture O
and O
representation O
. O

InProceedings O
of O
the O
2018 O
Conference O
on O
Em- O
pirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1499–1509 O
, O
Brussels O
, O
Belgium O
. O

Association O
for O
Computational O
Linguistics O
. O

Drew O
Reisinger O
, O
Rachel O
Rudinger O
, O
Francis O
Ferraro O
, O
Craig O
Harman O
, O
Kyle O
Rawlins O
, O
and O
Benjamin O
Van O
Durme O
. O

2015 O
. O

Semantic O
proto O
- O
roles O
. O

Transac- O
tions O
of O
the O
Association O
for O
Computational O
Linguis- O
tics O
, O
3:475–488 O
. O

Anna O
Rogers O
, O
Olga O
Kovaleva O
, O
and O
Anna O
Rumshisky O
. O

2020 O
. O

A O
primer O
in O
bertology O
: O
What O
we O
know O
about O
how O
bert O
works O
. O

M. O
Schlichtkrull O
, O
Nicola O
De O
Cao O
, O
and O
Ivan O
Titov O
. O

2020 O
. O

Interpreting O
graph O
neural O
networks O
for O
nlp O
with O
dif- O
ferentiable O
edge O
masking O
. O

Rico O
Sennrich O
. O

2017 O
. O

How O
grammatical O
is O
character- O
level O
neural O
machine O
translation O
? O

assessing O
MT O
qual- O
ity O
with O
contrastive O
translation O
pairs O
. O

In O
Proceed- O
ings O
of O
the O
15th O
Conference O
of O
the O
European O
Chap- O
ter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Volume O
2 O
, O
Short O
Papers O
, O
pages O
376–382 O
, O
Valencia O
, O
Spain O
. O

Association O
for O
Computational O
Linguistics O
. O

Natalia O
Silveira O
, O
Timothy O
Dozat O
, O
Marie O
- O
Catherine O
de O
Marneffe O
, O
Samuel O
Bowman O
, O
Miriam O
Connor O
, O
John O
Bauer O
, O
and O
Chris O
Manning O
. O

2014 O
. O

A O
gold O
stan- O
dard O
dependency O
corpus O
for O
English O
. O

In O
Proceedings O
of O
the O
Ninth O
International O
Conference O
on O
Language O
Resources O
and O
Evaluation O
( O
LREC’14 O
) O
, O
pages O
2897 O
– O
2904 O
, O
Reykjavik O
, O
Iceland O
. O

European O
Language O
Re- O
sources O
Association O
( O
ELRA O
) O
. O

Ian O
Tenney O
, O
Dipanjan O
Das O
, O
and O
Ellie O
Pavlick O
. O
2019a O
. O

BERT B-MethodName
rediscovers O
the O
classical O
NLP B-TaskName
pipeline O
. O

In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Asso- O
ciation O
for O
Computational O
Linguistics O
, O
pages O
4593 O
– O
4601 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O

Ian O
Tenney O
, O
Patrick O
Xia O
, O
Berlin O
Chen O
, O
Alex O
Wang O
, O
Adam O
Poliak O
, O
R O
Thomas O
McCoy O
, O
Najoung O
Kim O
, O
Benjamin O
Van O
Durme O
, O
Sam O
Bowman O
, O
Dipanjan O
Das O
, O
and O
Ellie O
Pavlick O
. O
2019b O
. O

What O
do O
you O
learn O
from O
context O
? O

probing O
for O
sentence O
structure O
in O
contextu- O
alized O
word O
representations O
. O

In O
International O
Con- O
ference O
on O
Learning O
Representations O
. O

Jesse O
Vig O
, O
Sebastian O
Gehrmann O
, O
Yonatan O
Belinkov O
, O
Sharon O
Qian O
, O
Daniel O
Nevo O
, O
Yaron O
Singer O
, O
and O
Stuart O
Shieber O
. O

2020 O
. O

Causal O
mediation O
analysis O
for O
inter- O
preting O
neural O
nlp O
: O
The O
case O
of O
gender O
bias O
. O

Alex O
Wang O
, O
Ian O
F. O
Tenney O
, O
Yada O
Pruksachatkun O
, O
Katherin O
Yu O
, O
Jan O
Hula O
, O
Patrick O
Xia O
, O
Raghu O
Pappa- O
gari O
, O
Shuning O
Jin O
, O
R. O
Thomas O
McCoy O
, O
Roma O
Pa- O
tel O
, O
Yinghui O
Huang O
, O
Jason O
Phang O
, O
Edouard O
Grave O
, O
Najoung O
Kim O
, O
Phu O
Mon O
Htut O
, O
Thibault O
F’evry O
, O
Berlin O
Chen O
, O
Nikita O
Nangia O
, O
Haokun O
Liu O
, O
, O
An- O
had O
Mohananey O
, O
Shikha O
Bordia O
, O
Ellie O
Pavlick O
, O
and O
Samuel O
R. O
Bowman O
. O

2019 O
. O

jiant O
1.0 O
: O
A O
software O
toolkit O
for O
research O
on O
general O
- O
purpose O
text O
under- O
standing O
models O
. O

http://jiant.info/ O
. O

Ralph O
Weischedel O
, O
Martha O
Palmer O
, O
Mitchell O
Marcus O
, O
Eduard O
Hovy O
, O
Sameer O
Pradhan O
, O
Lance O
Ramshaw O
, O
Ni- O
anwen O
Xue O
, O
Ann O
Taylor O
, O
Jeff O
Kaufman O
, O
Michelle O
Franchini O
, O
et O
al O
. O
2013 O
. O

Ontonotes O
release O
5.0 O
ldc2013t19 O
. O

Linguistic O
Data O
Consortium O
, O
Philadel- O
phia O
, O
PA O
, O
23 O
. O

Peng O
Xu O
, O
Jaeho O
Kang O
, O
Michael O
Ringgaard O
, O
and O
Franz O
Och O
. O
2009 O
. O

Using O
a O
dependency B-TaskName
parser I-TaskName
to O
improve O
smt O
for O
subject O
- O
object O
- O
verb O
languages O
. O

In O
Proceed- O
ings O
of O
Human O
Language O
Technologies O
: O
The O
2009 O
Annual O
Conference O
of O
the O
North O
American O
Chap- O
ter O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
NAACL O
’ O
09 O
, O
page O
245–253 O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O

Jason O
Yosinski O
, O
Jeff O
Clune O
, O
Anh O
Mai O
Nguyen O
, O
Thomas O
J. O
Fuchs O
, O
and O
Hod O
Lipson O
. O
2015 O
. O

Under- O

standing O
neural O
networks O
through O
deep O
visualization O
. O

CoRR O
, O
abs/1506.06579 O
. O

92A O
Appendix O
A.1 O
Additional O
Example O
of O
the O
Extreme O
Case O
We O
show O
another O
example O
of O
a O
task O
- O
pair O
that O
, O
under O
certain O
distributions O
of O
context O
lengths O
, O
exhibits O
similar O
behavior O
to O
that O
observed O
in O
the O
edge O
case O
described O
in O
§ O
3.1.1 O
( O
ﬁgure O
7 O
) O
. O

Figure O
7 O
: O
Elayer B-MetricName
of O
SRL B-TaskName
and O
Non O
- O
term O
. O

for O
different O
context O
length O
ranges O
( O
4 O
left O
blue O
and O
yellow O
pairs O
) O
, O
and O
their O
Elayer B-MetricName
when O
all O
instances O
of O
SRL B-TaskName
are O
of O
context O
length O
l2[0;2]and O
all O
those O
of O
Non O
- O
term O
. O

are O
of O
con- O
text O
length O
l9(rightmost O
green O
and O
red O
pair O
) O
. O

While O
for O
every O
context O
length O
range O
, O
SRL B-TaskName
’s O
Elayer B-MetricName
is O
bigger O
than O
that O
of O
Non O
- O
term O
. O
, O
for O
some O
context O
length O
distri- O
bution O
that O
order O
may O
be O
reversed O
. O

A.2 O
Context O
Length O
Distribution O
A O
lot O
of O
our O
work O
deals O
with O
possible O
context O
length O
distributions O
, O
normalizing O
distribution O
, O
and O
accounting O
for O
the O
distribution O
. O

We O
provide O
here O
the O
actual O
distributions O
which O
are O
the O
underlying O
property O
controlling O
the O
seen O
effects O
. O

We O
provide O
data O
on O
the O
percentage O
of O
examples O
in O
each O
context O
length O
range O
for O
each O
task O
( O
ﬁgure O
8) O
. O

Figure O
8 O
: O
Percentage O
of O
examples O
as O
a O
function O
of O
con- O
text O
length O
range O
, O
for O
each O
of O
the O
7 O
tasks O
( O
see O
legend O
) O
. O

A.3 O
NDE O
vs. O
Unmediated O
Difference O
for O
All O
Task O
- O
Pairs O
For O
every O
task O
- O
pair O
, O
we O
compare O
the O
unmediated O
Elayer B-MetricName
difference O
with O
the O
pair O
’s O
NDE O
. O

Figure O
9 O
presents O
this O
comparison O
for O
each O
task O
- O
pair O
, O
withthe O
distribution O
of O
one O
of O
the O
pair O
’s O
tasks O
being O
applied O
in O
the O
NDE O
calculations O
, O
for O
each O
task O
- O
pair O
. O

A.4 O
Extreme O
Elayer B-MetricName
Differences O
Based O
on O
ﬁgure O
6 O
, O
we O
compute O
the O
extreme O
Elayer B-MetricName
differences O
of O
each O
task O
- O
pair O
. O

Namely O
, O
for O
each O
such O
pair O
, O
we O
juxtapose O
the O
difference O
between O
the O
maximal O
possible O
Elayer B-MetricName
of O
the O
ﬁrst O
task O
and O
the O
minimal O
Elayer B-MetricName
of O
the O
second O
one O
with O
the O
opposite O
case O
( O
the O
difference O
between O
the O
minimal O
possible O
Elayer B-MetricName
of O
the O
ﬁrst O
task O
and O
the O
maximal O
Elayer B-MetricName
of O
the O
second O
one O
) O
. O

Our O
results O
can O
be O
seen O
in O
ﬁgure O
10 O
. O

93 O
Figure O
9 O
: O
Difference O
between O
unmediated O
Elayer B-MetricName
and O
NDE O
for O
every O
task O
- O
pair O
. O

The O
employed O
context O
length O
distributions O
( O
as O
part O
of O
the O
NDE O
calculations O
) O
are O
, O
from O
left O
to O
right O
, O
of O
NER B-TaskName
, O
SRL B-TaskName
, O
Dep B-TaskName
. O
, O
Non O
- O
term O
. O
, O
SRL B-TaskName
, O
Co B-TaskName
- I-TaskName
ref I-TaskName
. O
, O
Dep B-TaskName
. O
, O
Non O
- O
term O
. O
, O
SRL B-TaskName
, O
Non O
- O
term O
. O
, O
SPR B-TaskName
, O
SRL B-TaskName
, O
SPR B-TaskName
, O
SPR B-TaskName
, O
Non O
- O
term O
. O
, O
SRL B-TaskName
, O
RC B-TaskName
, O
NER B-TaskName
, O
Non O
- O
term O
. O
, O
Dep B-TaskName
. O
and O
SRL B-TaskName
. O

Figure O
10 O
: O
Difference O
between O
the O
minimal O
possible O
expected O
layer O
of O
the O
left O
task O
and O
the O
maximal O
possible O
expected O
layer O
of O
the O
right O
task O
( O
blue O
- O
see O
legend O
) O
, O
and O
vice O
- O
versa O
( O
yellow O
- O
see O
legend O
) O
, O
for O
every O
task O
- O
pair O
. O

UniTranSeR B-MethodName
: O
A O
Uniﬁed O
Transformer O
Semantic O
Representation O
Framework O
for O
Multimodal B-MethodName
Task I-MethodName
- I-MethodName
Oriented I-MethodName
Dialog I-MethodName
Systems I-MethodName
Zhiyuan O
Ma1 O
, O
Jianjun O
Li1 O
, O
Guohui O
Li1 O
, O
Yongjing O
Cheng2 O
1Huazhong O
University O
of O
Science O
and O
Technology O
( O
HUST O
) O
, O
China O
2National O
University O
of O
Defense O
Technology O
( O
NUDT O
) O
, O
China O
{ O
zhiyuanma,jianjunli,guohuili}@hust.edu.cn O
davidcheng1001@163.com O

Abstract O
As O
a O
more O
natural O
and O
intelligent O
interac- O
tion O
manner O
, O
multimodal O
task O
- O
oriented O
dia- O
log O
system O
recently O
has O
received O
great O
atten- O
tion O
and O
many O
remarkable O
progresses O
have O
been O
achieved O
. O

Nevertheless O
, O
almost O
all O
ex- O
isting O
studies O
follow O
the O
pipeline O
to O
ﬁrst O
learn O
intra O
- O
modal O
features O
separately O
and O
then O
conduct O
simple O
feature O
concatenation O
or O
attention O
- O
based O
feature O
fusion O
to O
generate O
re- O
sponses O
, O
which O
hampers O
them O
from O
learning O
inter O
- O
modal O
interactions O
and O
conducting O
cross- O
modal O
feature O
alignment O
for O
generating O
more O
intention O
- O
aware O
responses O
. O

To O
address O
these O
issues O
, O
we O
propose O
UniTranSeR B-MethodName
, O
a O
Uni O
ﬁed O
Transformer O
Se O
mantic O
R O
epresentation O
frame- O
work O
with O
feature O
alignment O
and O
intention O
rea- O

soning O
for O
multimodal O
dialog O
systems O
. O

Specif- O

ically O
, O
we O
ﬁrst O
embed O
the O
multimodal O
features O
into O
a O
uniﬁed O
Transformer O
semantic O
space O
to O
prompt O
inter O
- O
modal O
interactions O
, O
and O
then O
de- O
vise O
a O
feature O
alignment O
and O
intention O
reason- O
ing O
( O
FAIR O
) O
layer O
to O
perform O
cross O
- O
modal O
en- O
tity O
alignment O
and O
ﬁne O
- O
grained O
key O
- O
value O
rea- O
soning O
, O
so O
as O
to O
effectively O
identify O
user O
’s O
in- O
tention O
for O
generating O
more O
accurate O
responses O
. O

Experimental O
results O
verify O
the O
effectiveness O
of O
UniTranSeR B-MethodName
, O
showing O
that O
it O
signiﬁcantly O
outperforms O
state O
- O
of O
- O
the O
- O
art O
approaches O
on O
the O
representative O
MMD B-DatasetName
dataset O
. O

1 O
Introduction O
The O
multimodal O
task O
- O
oriented O
dialog O
systems O
are O
designed O
to O
help O
users O
achieve O
speciﬁc O
goals O
such O
as O
clothing O
recommendation O
or O
restaurant O
reserva- O
tion O
, O
which O
is O
in O
growing O
demand O
in O
the O
current O
business O
environment O
. O

As O
a O
leading O
study O
, O
Saha O
et O
al O
. O

( O
2018 O
) O
released O
a O
multimodal B-DatasetName
dialog I-DatasetName
dataset I-DatasetName
( O
MMD B-DatasetName 
) O
in O
the O
online O
retail O
domain O
. O

Based O
on O
such O
a O
benchmark O
dataset O
, O
many O
multimodal O
dialog O
mod- O
els O
incorporating O
domain O
knowledge O
have O
recently O
been O
proposed O
( O
Chauhan O
et O
al O
. O
, O
2019 O
; O
Zhang O
et O

al O
. O
, O

2019 O
, O
2021 O
) O
, O
which O
basically O
exploit O
taxonomy- O
based O
method O
( O
Liao O
et O
al O
. O
, O
2018 O
; O
Cui O
et O

al O
. O
, O
2019 O
) O
or O
attention O
- O
based O
method O
( O
Nie O
et O
al O
. O
, O
2019 O
; O
He O
et O
al O
. O
, O
2020 O
) O
to O
incorporate O
knowledge O
base O
( O
KB O
) O
information O
for O
better O
performance O
. O

Though O
achieving O
remarkable O
progress O
, O
existing O
multimodal O
task O
- O
oriented O
dialog O
systems O
still O
suf- O
fer O
from O
the O
following O
three O
limitations O
. O

Firstly O
, O
prior O
models O
only O
learn O
the O
intra O
- O
modal O
features O
( O
including O
textual O
features O
, O
visual O
features O
and O
do- O
main O
knowledge O
) O
separately O
before O
fusing O
them O
. O

Since O
these O
multimodal O
cues O
in O
general O
can O
enhance O
and O
complement O
each O
other O
, O
projecting O
them O
into O
a O
uniﬁed O
semantic O
space O
to O
learn O
the O
inter O
- O
modal O
features O
, O
with O
no O
doubt O
, O
can O
help O
improve O
the O
abil- O
ities O
of O
natural O
language O
understanding O
, O
which O
in O
turn O
will O
beneﬁt O
the O
response B-TaskName
generation I-TaskName
. O

Sec- O
ondly O
, O
prior O
models O
only O
conduct O
simple O
feature O
concatenation O
( O
Saha O
et O
al O
. O
, O
2018 O
; O
Nie O
et O
al O
. O
, O
2019 O
) O
or O
attention O
- O
based O
feature O
fusion O
( O
Cui O
et O

ter O
acquiring O
intra O
- O
modal O
representations O
, O
but O
with- O
out O
learning O
ﬁne O
- O
grained O
alignment O
between O
differ- O
ent O
modalities O
before O
fusion O
, O
which O
is O
not O
favorable O
to O
query O
knowledge O
for O
accurate O
multimodal B-TaskName
re- I-TaskName
sponse I-TaskName
generation I-TaskName
. O

Take O
the O
dialog O
in O
Figure O
1 O
as O
an O
example O
, O
when O
answering O
the O
user O
’s O
query O
on O
similar O
style O
of O
jackets O
, O
the O
model O
is O
expected O
to O
align O
the O
word O
“ O
jackets O
” O
with O
the O
corresponding O
vi- O
sual O
features O
for O
proper O
semantic O
complement O
and O
entity O
enhancement O
. O

Thirdly O
, O
prior O
models O
basi- O
cally O
lack O
the O
capability O
of O
entity O
- O
level O
reasoning O
, O
which O
prevents O
them O
from O
performing O
reasoning O
over O
crucial O
entities O
to O
guide O
intention O
- O
aware O
re- O
sponse O
generation O
. O

For O
example O
, O
in O
Figure O
1 O
, O
when O
the O
user O
asks O
“ O
show O
some O
similar O
jackets O
in O
black O
color O
” O
, O
the O
chatbot O
is O
expected O
to O
properly O
explore O
the O
pivot O
attribute O
“ O
black O
” O
that O
connects O
the O
start O
query O
cue O
“ O
jackets O
” O
with O
the O
target O
recommended O
product O
images O
. O

Speciﬁcally O
, O
the O
model O
needs O
to O
perform O
a O
2 O
- O
hop O
reasoning O
over O
triples O
( O
jacket_q O
, O
attribute O
, O
black_v O
) O
and(black_q O
, O
image O
, O
jacket_v O
) O
and O
obtain O
the O
intended O
4images O
. O

To O
address O
the O
aforementioned O
limitations O
, O
we O
propose O
a O
Uniﬁed B-MethodName
Transformer I-MethodName
Semantic I-MethodName
Repre- I-MethodName
sentation I-MethodName
framework O
with O
feature O
alignment O
and O
intention O
reasoning O
, O
UniTranSeR B-MethodName
for O
short O
. O

Specif- O

ically O
, O
to O
address O
the O
ﬁrst O
limitation O
, O
we O
stand O
on O
the O
shoulder O
of O
Vision O
- O
and O
- O
Language O
Pre O
- O
training O
( O
VLP O
) O
methods O
( O
Lu O
et O
al O
. O
, O
2019 O
; O
Li O
et O

al O
. O
, O
2019 O
; O
Chen O
et O
al O
. O
, O
2020 O
; O
Li O
et O
al O
. O
, O
2021 O
) O
to O
propose O
a O
uniﬁed O
- O
modal O
Transformer O
encoder O
, O
which O
is O
used O
to O
project O
all O
the O
multimodal O
features O
into O
a O
uniﬁed O
semantic O
space O
to O
prompt O
inter O
- O
modality O
interac- O
tions O
, O
with O
the O
objective O
of O
learning O
better O
repre- O
sentations O
. O

Based O
on O
the O
uniﬁed O
encoder O
, O
we O
fur- O
ther O
address O
the O
second O
limitation O
by O
designing O
a O
feature O
alignment O
module O
to O
perform O
cross O
- O
modal O
feature O
alignment O
. O

Finally O
, O
to O
address O
the O
third O
limitation O
, O
we O
devise O
a O
ﬁne O
- O
grained O
intention O
rea- O
soning O
module O
for O
capturing O
users O
’ O
real O
intentions O
, O
by O
leveraging O
a O
key O
- O
value O
attention O
based O
memory O
mechanism O
to O
perform O
multi O
- O
hop O
knowledge O
query O
for O
generating O
text O
or O
image O
responses O
. O

We O
conduct O
experiments O
on O
MMD B-DatasetName
, O
one O
of O
the O
most O
inﬂuential O
benchmark O
datasets O
for O
multimodal B-TaskName
dialog I-TaskName
generation I-TaskName
. O

We O
follow O
the O
mainstream O
eval- O
uation O
script O
of O
dialog B-TaskName
generation I-TaskName
and O
demonstrate O
that O
UniTranSeR B-MethodName
signiﬁcantly O
outperforms O
the O
cur- O
rent O
state O
- O
of O
- O
the O
- O
art O
baselines O
. O

Ablation O
study O
also O
shows O
the O
efﬁcacy O
of O
each O
component O
in O
improving O
the O
performance O
of O
dialog B-TaskName
generation I-TaskName
, O
and O
a O
furthercase O
study O
reveals O
that O
our O
model O
can O
effectively O
perform O
ﬁne O
- O
grained O
token O
- O
level O
feature O
alignment O
for O
multimodal B-TaskName
dialog I-TaskName
generation I-TaskName
. O

2 O
Related O
Work O
2.1 O

Unimodal O
Dialog O
Systems O
Recent O
years O
has O
witnessed O
the O
remarkable O
success O
in O
textual O
dialog O
systems O
, O
which O
can O
be O
roughly O
divided O
into O
two O
categories O
: O
open O
- O
domain O
conver- O
sations O
with O
casual O
chi O
- O
chat O
( O
Song O
et O
al O
. O
, O
2020 O
; O
Gangal O
et O
al O
. O
, O
2021 O
; O

Chan O
et O
al O
. O
, O
2021 O
; O
Yang O
et O
al O
. O
, O
2021 O
) O
and O
task O
- O
oriented O
dialog O
systems O
( O
Pei O
et O
al O
. O
, O
2021 O
; O
Santra O
et O
al O
. O
, O
2021 O
; O
Wang O
et O
al O
. O
, O
2021 O
; O
Mi O
et O
al O
. O
, O
2021 O
; O
Madotto O
et O
al O
. O
, O
2021 O
; O
Gou O
et O
al O
. O
, O
2021 O
; O
Raghu O
et O
al O
. O
, O
2021 O
) O
, O
which O
are O
designed O
to O
help O
users O
achieve O
speciﬁc O
goals O
. O

Early O
efforts O
mainly O
adopt O
a O
sequence O
- O
to O
- O
sequence O
( O
Seq2Seq O
) O
architec- O
ture O
, O
but O
can O
not O
work O
well O
in O
KB O
retrieval O
and O
rea- O
soning O
. O

To O
alleviate O
this O
problem O
, O
copy O
mecha- O
nism O
( O
Eric O
and O
Manning O
, O
2017 O
) O
have O
been O
adopted O
and O
many O
memory O
augmented O
Seq2Seq O
models O
have O
been O
proposed O
( O
Bordes O
et O
al O
. O
, O
2017 O
; O

Wen O
et O
al O
. O
, O
2018 O
; O
Madotto O
et O
al O
. O
, O
2018 O
; O
Wu O
et O
al O
. O
, O
2019 O
; O
Reddy O
et O
al O
. O
, O
2019 O
; O
Qin O
et O
al O
. O
, O
2019 O
; O
Wang O
et O
al O
. O
, O
2020 O
; O
Qin O
et O
al O
. O
, O
2020 O
) O
, O
which O
achieve O
promising O
results O
. O

2.2 O
Multimodal O
Dialog O
Systems O
With O
the O
ﬂourishing O
of O
social O
media O
platforms O
, O
massive O
amounts O
of O
multimedia O
data O
are O
gener- O
ated O
daily O
, O
which O
poses O
great O
demand O
for O
mul- O
timodal O
dialog O
systems O
. O

However O
, O
due O
to O
the O
lack O
of O
large O
- O
scale O
multimodal O
dialog O
datasets O
, O
re- O
searches O
in O
this O
domain O
have O
been O
limited O
. O

To O
this O
end O
, O
Saha O
et O
al O
. O

( O
2018 O
) O
provided O
a O
vertical O
re- O
tail O
domain O
dataset O
MMD B-DatasetName
to O
promote O
the O
research O
and O
proposed O
a O
multimodal B-MethodName
hierarchical I-MethodName
encoder- I-MethodName
decoder I-MethodName
model I-MethodName
( O
MHRED B-MethodName
) O
as O
a O
baseline O
. O

Based O
on O
MHRED B-MethodName
, O
Liao O
et O
al O
. O

( O
2018 O
) O
incorporated O
the O
style O
tips O
into O
a O
knowledge B-MethodName
- I-MethodName
aware I-MethodName
multimodal I-MethodName
di- I-MethodName
alog I-MethodName
model I-MethodName
( O
KMD B-MethodName
) O
. O

Cui O
et O

al O
. O

( O
2019 O
) O
designed O
a O
user B-MethodName
attention I-MethodName
- I-MethodName
guided I-MethodName
multimodal I-MethodName
dialog I-MethodName
system I-MethodName
( O
UMD B-MethodName
) O
by O
additionally O
considering O
the O
hierarchi- O
cal O
product O
taxonomy O
and O
user O
’s O
attention O
to O
prod- O
ucts O
. O

Chauhan O
et O
al O
. O

( O
2019 O
) O
introduced O
an O
ordi- B-MethodName
nal I-MethodName
and I-MethodName
attribute I-MethodName
aware I-MethodName
multimodal I-MethodName
dialog I-MethodName
system I-MethodName
( O
OAM B-MethodName
) O
by O
employing O
a O
novel O
position O
and O
attribute O
aware O
attention O
mechanism O
. O

Later O
, O
Nie O
et O
al O
. O

( O
2019 O
) O
proposed O
a O
multimodal B-MethodName
dialog I-MethodName
system I-MethodName
with I-MethodName
adaptive I-MethodName
decoders I-MethodName
( O
MAGIC B-MethodName
) O
, O
which O
can O
incorporate O
differ- O
ent O
forms O
of O
domain O
knowledge O
to O
generate O
differ- O
ent O
kinds O
of O
responses O
. O

Recently O
, O
combining O
with104 O

Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
He O
et O
al O
. O

( O
2020 O
) O
advanced O
a O
multimodal B-MethodName
dialog I-MethodName
system I-MethodName
via O
capturing O
context O
- O
aware O
dependencies O
of O
semantic O
elements O
( O
MATE B-MethodName
) O
for O
textual B-TaskName
response I-TaskName
generation I-TaskName
. O

Most O
existing O
multimodal O
dialog O
systems O
learn O
intra O
- O
modal O
features O
separately O
for O
later O
feature O
con- O
catenation O
or O
fusion O
. O

Different O
from O
them O
, O
our O
pro- O
posed O
UniTranSeR B-MethodName
can O
project O
all O
the O
multimodal O
features O
into O
a O
uniﬁed O
semantic O
space O
to O
perform O
ﬁne O
- O
grained O
feature O
alignment O
and O
intention O
rea- O
soning O
, O
which O
can O
lead O
to O
more O
accurate O
responses O
. O

Vision O
- O
and O
- O
Language O
Pre O
- O
training O
( O
VLP O
) O
( O
Lu O
et O
al O
. O
, O
2019 O
; O
Li O
et O
al O
. O
, O
2021 O
) O
is O
another O
line O
of O
research O
relevant O
to O
our O
work O
, O
but O
different O
from O
ours O
in O
that O
it O
focuses O
more O
on O
boosting O
the O
performance O
of O
representation O
learning O
, O
while O
the O
multimodal O
dia- O
log O
systems O
focus O
more O
on O
multi O
- O
turn O
multimodal O
interaction O
between O
users O
and O
agents O
. O

3 O
Methodology O
The O
proposed O
UniTranSeR B-MethodName
mainly O
comprises O
three O
parts O
: O
Uniﬁed O
- O
modal O
Transformer O
Semantic O
( O
UTS O
) O
encoder O
( O
Sec O
. O
3.1 O
) O
, O
Feature O
Alignment O
and O
Inten- O
tion O
Reasoning O
( O
FAIR O
) O
layer O
( O
Sec O
. O
3.2 O
) O
, O
and O
Hi- O
erarchical O
Transformer O
Response O
( O
HTR O
) O
decoder O
( O
Sec O
. O
3.3 O
) O
, O
as O
shown O
in O
Figure O
2 O
. O

We O
deﬁne O
the O
multimodal B-TaskName
dialog I-TaskName
generation I-TaskName
task O
as O
gener- O
ating O
the O
most O
likely O
response O
sequence O
Y= O
fy1;y2;;yngand O
selecting O
top- O
kmost O
matched O
images O
, O
giving O
multimodal O
context O
utterances O
U= O
fu1;u2;:::;ujUjgand O

multimodal O
knowledge O
base O
Bas O
inputs O
. O

The O
probability O
of O
a O
textual O
response O
can O
be O
formally O
deﬁned O
as O
, O
P(YjU;B O
) O

= O
nY O
t=1P(ytjy1;:::;yt 1;U;B O
) O
( O
1)whereytrepresents O
the O
current O
token O
decoded O
by O
the O
HTR O
decoder O
. O

The O
UTS O
encoder O
is O
used O
to O
project O
all O
the O
mul- O
timodal O
features O
into O
a O
uniﬁed O
vector O
space O
for O
inter O
- O
modal O
interactions O
, O
while O
the O
FAIR O
layer O
is O
designed O
to O
align O
cross O
- O
modal O
hidden O
features O
, O
with O
textual O
features O
and O
visual O
features O
from O
previous O
UTS O
encoder O
as O
inputs O
. O

Similar O
to O
MAGIC B-MethodName
( O
Nie O
et O
al O
. O
, O
2019 O
) O
, O
our O
HTR O
decoder O
is O
designed O
to O
de- O
code O
three O
types O
of O
responses O
: O
general O
responses O
that O
refer O
to O
the O
highly O
frequent O
responses O
( O
e.g. O
, O
courtesy O
greetings O
) O
in O
the O
conversation O
, O
such O
as O
“ O
How O
can O
I O
help O
you O
? O
” O
; O
intention O
- O
aware O
responses O
that O
refer O
to O
the O
task O
- O
oriented O
utterances O
, O
such O
as O
“ O
Found O
some O
similar O
black O
leather O
- O
jackets O
for O
you O
” O
; O
and O
multimodal O
responses O
that O
refer O
to O
the O
intention- O
aware O
responses O
with O
image O
output O
. O

The O
response O
type O
is O
determined O
by O
a O
query O
vector O
Qfrom O
the O
FAIR O
layer O
, O
in O
which O
an O
intention O
classiﬁer O
is O
trained O
to O
decide O
which O
kind O
of O
response O
should O
be O
given O
out O
. O

3.1 O
UTS O
Encoder O
We O
ﬁrst O
use O
a O
text O
embedder O
and O
an O
image O
embed- O
der O
to O
extract O
textual O
features O
and O
visual O
features O
, O
respectively O
, O
and O
extract O
informative O
features O
from O
external O
knowledge O
by O
utilizing O
both O
text O
and O
image O
embedders O
. O

Afterwards O
, O
we O
feed O
these O
three O
kinds O
of O
features O
into O
a O
uniﬁed O
Transformer O
encoder O
for O
uniﬁed O
- O
modal O
semantic O
representation O
learning O
. O

Text O
Embedder O
. O

To O
learn O
textual O
intra O
- O
modal O
features O
, O
we O
use O
a O
BERT O
tokenizer O
to O
split O
the O
in- O
put O
sentence O
into O
words O
and O
exploit O
a O
single O
trans- O
former O
layer O
to O
obtain O
these O
words O
’ O
initial O
embed- O
dings O
. O

Note O
the O
self O
- O
attention O
mechanism O
in O
Trans- O
former O
is O
order O
- O
less O
. O

So O
, O
it O
is O
necessary O
to O
encode O
the O
words O
’ O
position O
as O
additional O
inputs O
. O

The O
ﬁnal105 O

representation O
for O
each O
word O
is O
derived O
via O
sum- O
ming O
up O
its O
word O
embedding O
and O
position O
embed- O
ding O
, O
followed O
by O
a O
layer O
normalization O
( O
LN O
) O
layer O
. O

Image O
Embedder O
. O

To O
learn O
visual O
intra O
- O
modal O
features O
, O
we O
use O
a O
contour O
slicer O
to O
cut O
the O
input O
images O
into O
patches O
and O
exploit O
ResNet-50 O
( O
He O
et O
al O
. O
, O
2016 O
) O
to O
extract O
these O
patches O
’ O
visual O
fea- O
tures O
. O

We O
notice O
that O
people O
usually O
focus O
on O
four O
parts O
of O
a O
clothing O
image O
: O
head O
, O
upper O
body O
, O
lower O
body O
, O
and O
feet O
, O
so O
we O
intuitively O
use O
an O
equal O
- O
height O
mode O
to O
slice O
an O
image O
into O
four O
patches O
, O
which O
efﬁciently O
solves O
the O
problem O
of O
region O
feature O
extraction O
, O
without O
using O
com- O
plex O
target O
detection O
networks O
such O
as O
Faster O
R- O
CNN O
( O
Ren O
et O
al O
. O
, O
2015 O
) O
. O

Then O
, O
we O
feed O
the O
patches O
into O
ResNet-50 O
to O
get O
the O
patches O
’ O
initial O
embed- O
dings O
. O

Similarly O
, O
we O
also O
encode O
the O
position O
features O
for O
each O
patch O
via O
a O
4 O
- O
dimensional O
vec- O
tor[image O
_ O
index;patch O

_ O
index;width;height O
] O
. O

Both O
visual O
and O
position O
features O
are O
then O
fed O
through O
a O
fully O
- O
connected O
( O
FC O
) O
layer O
, O
to O
be O
pro- O
jected O
into O
the O
same O
embedding O
space O
. O

The O
ﬁnal O
visual O
embedding O
for O
each O
patch O
is O
obtained O
by O
ﬁrst O
summing O
up O
the O
two O
FC O
outputs O
, O
and O
then O
passing O
them O
through O
an O
LN O
layer O
. O

Knowledge O
Embedder O
. O

To O
integrate O
informa- O
tive O
features O
from O
external O
knowledge1into O
the O
task O
- O
oriented O
dialog O
, O
we O
equip O
the O
product O
knowl- O
edge O
base O
for O
each O
utterance O
through O
searching O
a O
fashion O
item O
table O
provided O
by O
MMD B-DatasetName
. O

We O
then O
treat O
these O
searched O
knowledge O
entries O
into O
the O
same O
triplet O
format O
, O
i.e. O
, O
( O
product O
, O
match O
, O
product O
) O
, O
( O
product O
, O
attribute O
, O
value O
) O
, O
( O
product O
, O
celebrity O
, O
pas- O
sion_score O
) O
. O

Next O
, O
for O
the O
text O
and O
image O
elements O
of O
these O
triples O
, O
we O
use O
the O
text O
and O
image O
embed- O
ders O
to O
obtain O
their O
respective O
representations O
. O

Uniﬁed O
Transformer O
Encoder O
. O

After O
obtaining O
the O
multimodal O
initial O
embeddings O
, O
denoted O
as O
ht O
, O
hvandhkrespectively O
, O
we O
project O
them O
into O
a O
uniﬁed O
semantic O
space O
to O
obtain O
interactive O
repre- O
sentations O
by O
using O
a O
uniﬁed O
Transformer O
encoder O
. O

Speciﬁcally O
, O
in O
each O
utterance O
, O
the O
textual O
features O
, O
visual O
features O
and O
informative O
features O
correspond O
toltokens O
with O
“ O
[ O
TXT O
] O
” O
, O
4 O
tokens2with O
“ O
[ O
IMG O
] O
” O
and O
4 O
tokens3with O
“ O
[ O
KNG O
] O
” O
. O

In O
order O
to O
integrate O

dialog O
history O
of O
previous O
rounds O
, O
we O
initialize O
the O
current O

[ O
CLS]pby O
using O
the O
representation O
of O
the O
previous O
round O
[ O
CLS]p 1 O
. O

The O
output O
hidden O
state O
representations O
can O
then O
be O
phrased O
as O
: O
Hp O
= O
f  O

[ O
CLS]p 1hp O
t[TXT O
] O
hp O
v[IMG O
] O
hp O
k[KNG] O
( O
2 O
) O
wheref()denotes O
the O
Transformer O
encoder O
, O
Hp O
0 O
denotes O
the O
hidden O
state O
representation O
of O
the O
cur- O
rent O
round O

[ O
CLS]p O
, O
which O
is O
regarded O
as O
the O
con- O

textual O
semantic O
vector O
of O
the O
entire O
utterance O
in O
this O
round O
, O
Hp O
1 O
: O
ldenotes O
the O
representations O
for O
the O
text O
sequence O
, O
Hp O
l+1 O
: O
l+4denotes O
the O
representations O
for O
the O
patch O
sequence O
, O
and O
Hp O
l+5 O
: O
l+8denotes O
the O
representations O
for O
knowledge O
entries O
. O

Note O
the O
su- O
perscriptpis O
omitted O
for O
simplicity O
if O
no O
confusion O
occurs O
in O
the O
following O
discussion O
. O

To O
obtain O
better O
representations O
, O
we O
introduce O
the O
Masked O
Language O
Modeling O
( O
MLM O
) O
loss O
and O
Masked O
Patch O
Modeling O
( O
MPM O
) O
loss O
to O
train O
them O
. O

We O
denote O
the O
input O
words O
as O
w O
= O
fw1;:::;wlg O
, O
the O
image O
patches O
as O
v O

= O
fv1;:::;v O
4 O
g O
, O
the O
knowl- O
edge O
elements O
as O
k O
= O
fk1;:::;k O
4 O
g O
, O
and O
the O
mask O
indices O
asm2NL O
, O
where O
Nis O
the O
natural O
numbers O
andLis O
the O
length O
of O
masked O
tokens O
. O

In O
MLM O
, O
we O
randomly O
mask O
out O
the O
input O
words O
with O
a O
probabil- O
ity O
of O
15 O
% O
, O
and O
replace O
the O
masked O
ones O
wmwith O
a O
special O
token O
“ O
[ O
MASK O
] O
” O
, O
as O
illustrated O
in O
Figure O
3 O
. O

The O
goal O
is O
to O
predict O
these O
masked O
words O
by O
atten- O
tively O
integrating O
the O
information O
of O
their O
surround- O
ing O
wordswnm O
, O
image O
patches O
vand O
knowledge O
elementsk O
, O
by O
minimizing O
the O
following O
loss O
: O
LMLM( O
) O
= O
 E(w;v;k O
) O

UlogP  O
wmjwnm;v;k O
( O
3 O
) O
Similar O
to O
MLM O
, O
in O
MPM O
, O
we O
also O
randomly O
mask O
out O
the O
image O
patches O
and O
use O
zeros O
tensor O
to O
re- O
place O
them O
, O
as O
shown O
in O
Figure O
3 O
. O

Unlike O
textual O
words O
that O
can O
be O
categorized O
as O
discrete O
labels O
, O
visual O
features O
are O
high O
- O
dimensional O
and O
continu- O
ous O
tensors O
, O
thus O
can O
not O
be O
supervised O
via O
a O
nega- O
tive O
log O
- O
likelihood O
loss O
. O

Following O
UNITER B-MethodName
( O
Chen O
et O
al O
. O
, O
2020 O
) O
, O
we O
built O
the O
MPM O
loss O
as O
: O
LMPM( O
) O
= O
E(w;v;k O
) O

Ug  O
vmjvnm;w;k O
( O
4 O
) O
wherevmare O
masked O
image O
patches O
and O
vnmare O
remaining O
patches O
. O

Note O
here O
gis O
deﬁned O
as O
an106 O

L2 O
regression O
function O
, O
where O
g  O
vmjvnm;w;k O
= O
LX O
i=1 O



f O
v(i O
) O
m O
 hv(i O
) O
m O



2 O
2 O
( O
5 O
) O
3.2 O
The O
FAIR O
Layer O
To O
align O
the O
cross O
- O
modal O
features O
for O
accurate O
in- O
tention O
classiﬁcation O
and O
knowledge O
query O
, O
we O
de- O
vise O
a O
feature O
alignment O
and O
intention O
reasoning O
( O
FAIR O
) O
layer O
. O

In O
feature O
alignment O
, O
we O
use O
Image- O

Text O
Matching O
( O
ITM O
) O
and O
Word O
- O
Patch O
Alignment4 O
( O
WPA O
) O
to O
conduct O
a O
two O
- O
level O
alignment O
. O

That O
is O
, O
ITM O
is O
used O
to O
align O
text O
and O
image O
in O
sentence- O
level O
, O
while O
WPA O
is O
used O
to O
align O
each O
split O
word O
and O
each O
sliced O
patch O
in O
token O
- O
level O
. O

In O
intention O
reasoning O
, O
we O
fuse O
f([CLS O
] O
) O
and O
aligned O
entities O
’ O
hidden O
state O
representations O
to O
obtain O
a O
query O
vec- O
torQ O
, O
which O
is O
then O
used O
for O
intention O
classiﬁcation O
and O
knowledge O
query O
. O

3.2.1 O
Feature O
Alignment O
Image O
- O
Text O
Matching O
( O
ITM O
) O
. O

In O
ITM O
, O
we O
use O
the O
outputf([CLS O
] O
) O
of O
the O
uniﬁed O
Transformer O
encoder O
to O
compute O
the O
match O
probability O
of O
the O
sampled O
pair O
. O

Speciﬁcally O
, O
we O
feed O
f([CLS O
] O
) O
into O
an O
FC O
layer O
and O
a O
sigmoid O
function O
to O
predict O
a O
probability O
score O
P(w;v O
) O
, O
which O
is O
between O
0and O
1 O
. O

During O
training O
, O
we O
sample O
a O
positive O
or O
negative O
pair(w;v)from O
the O
dataset O
Dat O
each O
step O
. O

The O
negative O
pair O
is O
created O
by O
randomly O
replacing O
the O
image O
or O
text O
in O
the O
same O
batch O
. O

We O
employ O
a O
binary O
cross O
- O
entropy O
loss O
for O
optimization O
: O
LITM( O
) O

= O
 E(w;v)D[ylogP(w;v)+ O
( O
1 y O
) O
log O
( O
1 P(w;v))](6 O
) O
whereyis O
a O
binary O
truth O
label O
. O

Note O
here O
we O
only O
use O
ITM O
to O
train O
image O
- O
text O
pairs O
but O
without O
con- O
sidering O
the O
knowledge O
vector O
, O
because O
it O
has O
al- O

ready O
matched O
the O
textual O
sequence O
when O
being O
searched O
out O
. O

Word O
- O
Patch O
Alignment O
( O
WPA O
) O
. O

For O
more O
ﬁne- O
grained O
alignment O
between O
each O
word O
and O
image O
patch O
, O
we O
introduce O
a O
WPA O
technology O
, O
which O
is O
used O
to O
train O
the O
consistency O
and O
exclusiveness O
be- O
tween O
these O
cross O
- O
modal O
features O
to O
prompt O
align- O
ment O
. O

We O
use O
a O
WPA O
loss O
to O
supervise O
the O
process O
, O

which O
is O
deﬁned O
as O
: O
LWPA( O
) O
= O
 Xl O
i=1X4 O
j=1Tij O

( O
wi;vj)(7 O
) O
where O

denotes O
the O
cos()similarity O
function O
, O
T2Rl4is O
a O
ground O
truth O
table O
and O
each O
Tij2 O
T O
is O
a O
binary O
label O
0 O
or O
1 O
. O

During O
training O
, O
we O
sample O
positive O
or O
negative O
pairs O
( O
wi;vj)from O
each O
multi- O
modal O
utterance O
to O
construct O
a O
probability O
table O
, O
as O
shown O
in O
Figure O
2 O
. O

The O
above O
loss O
function O
LWPA O
is O
then O
used O
to O
update O
the O
parameters O
. O

During O
inference O
, O
we O
continue O
to O
fuse O
aligned O
entities O
’ O
hid- O
den O
state O
representation O
and O
f([CLS O
] O
) O
to O
obtain O
a O
uniﬁed O
query O
vector O
Q O
, O
which O
contains O
multimodal O
query O
information O
with O
entity O
enhancement O
, O
and O
will O
be O
used O
for O
subsequent O
intention O
reasoning O
. O

3.2.2 O
Intention O
Reasoning O
Intention O
Classify O
( O
IC O
) O
. O

Given O
the O
query O
vector O
Q O
, O
this O
component O
aims O
to O
understand O
the O
users O
’ O
intention O
and O
thereafter O
determine O
which O
type O
of O
response O
should O
be O
generated O
. O

To O
be O
clear O
, O
there O
are O
a O
total O
of O
17types O
labeled O
in O
the O
MMD B-DatasetName
dataset O
, O
and O
each O
user O
’s O
utterance O
is O
labeled O
with O
a O
speciﬁc O
intention O
type O
. O

Following O
MAGIC B-MethodName
, O
we O
customize O
the O
type O
of O
response O
speciﬁcally O
for O
each O
intention O
, O
as O
shown O
in O
Table O
1 O
. O

Subsequently O
, O
we O
leverage O
an O
MLP O
layer O
to O
predict O
Q O
’s O
probability O
distribution O
and O
select O
the O
highest O
probability O
to O
generate O
a O
re- O
sponse O
. O

Besides O
, O
a O
cross O
- O
entropy O
loss O
is O
applied O
to O
optimizing O
the O
intention O
classiﬁer O
: O
LIC( O
) O

= O
XjUj O
i=1X17 O
j=1I O
ijlogP(IijjQ)(8 O
) O
whereP(IijjQ)denotes O
the O
probability O
of O
being O
predicted O
as O
intention O
Iij O
, O
andI O
ijis O
a O
ground O
truth O
label O
. O

The O
intention O
classiﬁer O
is O
trained O
by O
the O
loss O
functionLIC()to O
update O
parameter O
 O
, O
and O
ﬁnally O
outputs O
a O
reliable O
intention O
prediction O
result O

Iin O
the O
inference O
phase O
. O

Knowledge O
Query O
( O
KQ O
) O
. O

Given O
the O
predicted O
intention O
result O
I O
, O
this O
component O
ﬁrst O
determines O
whether O
knowledge O
query O
is O
required O
based O
on O
Ta- O
ble O
1 O
. O

If O
required O
, O
we O
adopt O
a O
key O
- O
value O
mem- O
ory O
mechanism O
to O
query O
all O
embedded O
knowledge O
triples5 O
. O

Speciﬁcally O
, O
these O
embedded O
knowledge O
triples O
are O
divided O
into O
key O
parts O
and O
value O
parts O
, O
which O
are O
respectively O
denoted O
as O
vector O
Kand O
vector O
V. O
Note O
here O
Kis O
obtained O
through O
a O
linear O

fusion O
of O
the O
embedded O
head O
- O
entities O
and O
relations O
. O

The O
knowledge O
query O
process O
is O
as O
follows O
: O

i= O
Softmax  O
QTKi O
( O
9 O
) O
VT O
= O
XjMj O
i=1 O

iVi O
( O
10 O
) O
where O

idenotes O
the O
attentive O
probability O
score O
for O
Ki O
, O
jMjis O
the O
number O
of O
knowledge O
triples O
, O
and O
VTis O
a O
weighted O
sum O
of O
Vi O
, O
which O
will O
be O
used O
for O
textual O
decoding O
in O
an O
intention O
- O
aware O
response O
. O

Multi O
- O
hop O
Recommend O
( O
MR O
) O
. O

Given O
the O
pre- O
dicted O
intention O
result O
Iand O
one O
- O
hop O
query O
re- O

sultVT O
, O
this O
component O

ﬁrst O
needs O
to O
determine O
whether O
an O
image O
recommendation O
is O
required O
based O
on O
Table O
1 O
. O

If O
required O
, O
we O
continue O
to O
use O
VTas O
a O
query O
vector O
to O
perform O
another O
hop O
query O
over O
the O
entire O
knowledge O
base O
, O
which O
implies O
that O
the O
product O
images O
will O
be O
recommended O
, O
if O
the O
key O
parts O
of O
their O
corresponding O
triples O
have O
high O
similarity O
to O
VT O
. O

Speciﬁcally O
, O

i= O
Softmax  O
VT O
TKi O
( O
11 O
) O
After O
deriving O

i O
, O
we O
use O
VI O
= O
fqig O
, O
an O
image O
pointer O
vector O
, O
to O
select O
images O
with O
top O

ifor O
recommendation O
, O
where O
qi=1;ifVi=11512 O
0;otherwise(12 O
) O

and11512is O
a O
column O
vector O
with O
each O
element O
equal O
to O
1 O
, O
which O
denotes O
for O
the O
special O
token O
[ O
URL O
] O
of O
the O
image O
’s O
link O
. O

Note O
here O
512 B-HyperparameterValue
is O
the O
embedding B-HyperparameterName
size I-HyperparameterName
in O
our O
uniﬁed O
Transformer O
encoder O
. O

It O
is O
not O
difﬁcult O
to O
see O
that O
UniTranSeR B-MethodName
can O
extend O
the O
above O
one O
- O
hop O
knowledge O
query O
to O
multi O
- O
hop O
by O
iteratively O
performing O
attention O
- O
based O
key O
- O
value O
reasoning O
and O
ultimately O
achieve O
multi O
- O
hop O
image O
recommendation O
. O

3.3 O
HTR O
Decoder O
As O
mentioned O
earlier O
, O
we O
used O
a O
hierarchy O
mech- O
anism O
to O
decode O
different O
types O
of O
response O
se- O
quences O
, O
including O
general O
responses O
, O
intention- O
aware O
responses O
and O
multimodal O
responses O
. O

They O
share O
the O
same O
uni O
- O
directional O
Transformer O
layer O
, O
but O
the O
semantic O
representations O
fed O
to O
this O
de- O
coder O
are O
different O
. O

Speciﬁcally O
, O
for O
general O
re- O
sponses O
, O
we O
just O
take O
the O
sentence O
- O
level O
represen- O
tationsf([CLS O
] O
) O
as O
input O
. O

For O
intention O
- O
aware O
re- O
sponses O
, O
we O
take O
the O
concatenation O
of O
f([CLS O
] O
) O
and O
attentive O
vector O
VTfollowed O
by O
an O
FC O
layer O
as O
input O
. O

For O
multimodal O
responses O
, O
we O
take O
the O
input O
for O
the O
intention O
- O
aware O
responses O
, O
as O
well O
as O
VI O
, O
the O
image O
pointer O
vector O
, O
as O
input O
. O

4 O
Experimental O
Setup O
4.1 O
Datasets O
and O
Metrics O
To O
evaluate O
the O
performance O
of O
UniTranSeR B-MethodName
, O
we O
conduct O
experiments O
on O
the O
widely O
- O
used O
bench- O
mark O
dataset O
MMD B-DatasetName
contributed O
by O
Saha O
et O
al O
. O

( O
2018 O
) O
. O

The O
MMD B-DatasetName
dataset O
consists O
of O
over O
150k O
conversations O
between O
users O
and O
chatbots O
in O
the O
retail O
domain O
, O
and O
each O
conversation O
describes O
a O
complete O
online O
shopping O
process O
. O

During O
the O
con- O
versations O
, O
the O
user O
proposes O
his O
/ O
her O
requirements O
in O
multimodal O
utterances O
and O
the O
chatbot O
introduces O
different O
products O
step O
by O
step O
until O
they O
make O
a O
deal O
. O

In O
our O
experiments O
, O
we O
follow O
Nie O
et O
al O
. O

( O
2019 O
) O
to O
partition O
MMD B-DatasetName
. O

The O
statistics O
the O
dataset O
after O
partition O
are O
presented O
in O
Table O
2 O
, O
and O
more O
detailed O
statistics O
can O
be O
found O
in O
Appendix O
A.4 O
. O

Following O
several O
previous O
work O
( O
Nie O
et O
al O
. O
, O
2019 O
; O
He O
et O
al O
. O
, O
2020 O
; O
Zhang O
et O
al O
. O
, O
2021 O
) O
, O
we O
use O
Bleu- B-MetricName
n I-MetricName
, O
Nist B-MetricName
and O
Recall@ B-MetricName
kto I-MetricName
evaluate O
our O
model O
over O
two O
basic O
tasks O
separately O
, O
i.e. O
, O
text O
task O
and O
image O
task O
. O

For O
the O
text O
task O
, O
we O
employ O
the O
proposed O
HTR O
decoder O
to O
produce O
all O
general O
responses O
and O
intention O
- O
aware O
responses O
. O

As O
the O
length O
of O
20.07 O
% O
target O
responses O
in O
MMD B-DatasetName
is O
less O
than4 O
, O
such O
as O
“ O
Hello O
! O
” O

and“Thanks O
a O
lot O
! O
” O
, O
we O
follow O
Nie O
et O
al O
. O

( O
2019 O
) O
to O
calculate O
Bleu- B-MetricName
n I-MetricName
by O 

varying O
n B-HyperparameterName
from O
1 B-HyperparameterValue
to O
4 B-HyperparameterValue
. O

Note O
higher O
Bleu O
and O
Nist O
scores O
indicate O
that O
more O
n O
- O
gram O
overlaps O
exist O
between O
the O
predicted O
and O
target O
responses O
, O
and O
hence O
are O
more O
favorable O
. O

For O
the O
image O
task O
, O
we O
adopt O
Recall@ B-MetricName
kto I-MetricName
evaluate O
the O
efﬁcacy O
of O
image O
response O
, O
where O
k B-HyperparameterName
is O
varied O
from O
1 B-MetricValue
to O
3 B-MetricValue
. O

Note O
the O
image O
response O
is O
correct O
only O
if O
the O
positive O
image O
is O
recommended O
in O
the O
top- O
kproduct O
images O
. O

4.2 O
Baselines O
We O
compare O
our O
model O
with O
the O
following O
state O
- O
of- O
the O
- O
art O
baselines O
. O

•MHRED B-MethodName
( O
Saha O
et O
al O
. O
, O
2018)6is O
the O
ﬁrst O
base- O
line O
work O
to O
integrate O
the O
visual O
features O
into O
a O
hierarchical O
encoder O
- O
decoder O
model O
for O
their O
constructed O
MMD B-DatasetName
dataset O
. O

•KMD B-MethodName
( O
Liao O
et O
al O
. O
, O
2018 O
) O
incorporates O
the O
style O
tips O
into O
the O
memory O
augmented O
neural O
model O
and O
adopts O
deep O
reinforcement O
learning O
to O
boost O
the O
performance O
. O

•UMD B-MethodName
( O
Cui O
et O
al O
. O
, O
2019)7proposes O
a O
user O
attention O
- O
guided O
multimodal O
dialog O
system O
by O
considerring O
the O
hierarchical O
product O
taxon- O
omy O
and O
the O
user O
’s O
attention O
to O
products O
. O

•OAM B-MethodName
( O
Chauhan O
et O
al O
. O
, O
2019 O
) O
proposes O
a O
novel O
ordinal O
and O
attribute O
aware O
attention O
mecha- O
nism O
for O
multimodal B-TaskName
dialog I-TaskName
generation I-TaskName
. O

•MAGIC B-MethodName
( O
Nie O
et O
al O
. O
, O
2019)8adopts O
the O
adap- O
tive O
decoders O
with O
intention O
understanding O
to O
explicitly O
generate O
three O
types O
of O
responses O
. O

•MATE B-MethodName
( O
He O
et O
al O
. O
, O
2020)9utilizes O
a O
multi- O
modal O
element O
- O
level O
encoder O
to O
integrate O
dia- O
log O
context O
and O
leverages O
a O
knowledge O
- O
aware O
two O
- O
stage O
decoder O
for O
response B-TaskName
generation I-TaskName
, O
and O
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
. O

6https://github.com/amritasaha1812/MMD_Code O
7https://github.com/ChenTsuei/UMD O
8https://acmmultimedia.wixsite.com/magic O
. O

9https://github.com/githwd2016/MATE/tree/dev4.3 O
Implementation O
Details O
Following O
Saha O
et O
al O
. O

( O
2018 O
) O
and O
Nie O
et O
al O
. O

( O
2019 O
) O
, O
we O
utilize O
two O
- O
turn O
utterances O
prior O
to O
the O
target O
response O
as O
the O
context O
, O
and O
set O
the O
vocabulary O
size O
to26;422 O
. O

In O
our O
trainings O
, O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
set O
to O
64 B-HyperparameterValue
, O
learning B-HyperparameterName
rate I-HyperparameterName
is O
set O
to O
1e 4 B-HyperparameterValue
and O

the O
max O
number O
of O
training B-HyperparameterName
epoches I-HyperparameterName
is O
set O
to O
1e4 B-HyperparameterValue
. O

Adam O
optimizer O
is O
used O
to O
optimize O
all O
models O
. O

All O
experiments O
are O
conducted O
with O
PyTorch O
. O

More O
details O
about O
hyper- O
parameter O
settings O
can O
be O
found O
in O
Appendix O
A.1 O
. O

5 O
Evaluation O
Results O
5.1 O
Response O
Quality O
Evaluation O
Automatic O
Evaluation O
Following O
KMD B-MethodName
, O
UMD B-MethodName
and O
MAGIC B-MethodName
, O
we O
evaluate O
model O
performance O
auto- O
matically O
from O
two O
aspects O
: O
text O
response O
and O
im- O
age O
response O
. O

From O
the O
results O
in O
Table O
3 O
, O
we O
can O
observe O
that O
our O
model O
UniTranSeR B-MethodName
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
both O
tasks O
. O

Speciﬁ- O
cally O
, O
in O
text O
task O
, O
UniTranSeR B-MethodName
exhibits O
the O
highest O
Bleu B-MetricName
- I-MetricName
n I-MetricName
with O
varying O
n B-HyperparameterName
from O
1 B-HyperparameterValue
to O
4 B-HyperparameterValue
compared O
with O
other O
baselines O
, O
indicating O
that O
our O
model O
can O
gen- O
erate O
responses O
closer O
to O
the O
golden O
ones O
. O

More- O
over O
, O
our O
model O
outperforms O
MATE B-MethodName
, O
a O
recent O
model O
that O
can O
capture O
context O
- O
aware O
dependencies O
of O
se- O
mantic O
elements O
, O
by O
26.3% B-MetricValue
in O
Bleu-4 B-MetricName
score O
, O
which O
veriﬁes O
the O
effectiveness O
of O
our O
model O
in O
learning O
cross O
- O
modal O
feature O
alignment O
and O
conduct O
inten- O
tion O
reasoning O
to O
generate O
more O
accurate O
and O
infor- O
mative O
responses O
. O

In O
image O
task O
, O
an O
extremely O
dif- O
ﬁcult O
performance O
improvement O
can O
be O
observed O
, O
which O
further O
veriﬁes O
the O
superiority O
of O
our O
model O
. O

Human O
Evaluation O

The O
human O
evaluation O
mainly O
focuses O
on O
four O
aspects O
: O
ﬂuency O
, O
relevance O
, O
correctness O
, O
and O
informativeness O
, O
which O
are O
all O
im- O
portant O
for O
task O
- O
oriented O
dialogue O
systems O
( O
Cui O
et O
al O
. O
, O
2019 O
; O
Nie O
et O
al O
. O
, O
2019 O
; O
He O
et O
al O
. O
, O
2020 O
) O
. O

We O
ﬁrst O
randomly O
selected O
200dialogs O
from O
the O
MMD B-DatasetName
datasets O
, O
and O
used O
different O
models O
to O
generate O
re- O
sponses O
, O
including O
UMD B-MethodName
, O
OAM B-MethodName
, O
MAGIC B-MethodName
, O
MATE B-MethodName

and O
UniTranSeR. B-MethodName
Then O
, O
we O
hired O
human O
experts O
to O
score O
the O
responses O
and O
golden O
responses O
in O
blind O
review O
on O
a O
scale O
from O
1to5 O
, O
which O
simulated O
a O
real O
- O
life O
multimodal O
task O
- O
oriented O
conversation O
scenario O
. O

By O
calculating O
the O
average O
score O
of O
the O
above O
metrics O
, O
we O
obtained O
the O
ﬁnal O
manual O
evalua- O
tion O
results O
, O
as O
shown O
in O
Table O
4 O
. O

It O
can O
be O
observed O
that O
UniTranSeR B-MethodName
consistently O
outperforms O
the O
other O
four O
models O
on O
all O
metrics O
, O
which O
is O
in O
line O
with O
the O
results O
of O
automatic O
evaluation O
. O

5.2 O
Ablation O
Study O
In O
this O
part O
, O
we O
perform O
ablation O
experiments O
to O
evaluate O
the O
effectiveness O
of O
each O
component O
. O

We O
focus O
on O
ﬁve O
crucial O
components O
and O
set O
them O
ac- O
cordingly O
: O
1 O
) O
w/o O
UTS O
Encoder O
denotes O
that O
we O
use O
a O
BiGRU O
to O
replace O
the O
uniﬁed O
- O
modal O
Transformer O
encoder O
for O
multimodal O
encoding O
; O
2 O
) O
w/o O
HTR O
De- O
coder O
denotes O
that O
we O
use O
a O
Uni O
- O
directional O
GRU O
to O
replace O
the O
hierarchical O
Transformer O
decoder O
for O
response B-TaskName
generation I-TaskName
; O
3 O
) O
w/o O
ITM O
denotes O
that O
we O
remove O
theLITMloss O
to O
make O
the O
parameters O
not O
updated O
; O
4 O
) O
w/o O
WPA O
denotes O
that O
we O
remove O
the O
LWPA O
loss O
and O
just O
regard O
the O
sentence O
- O
level O
rep- O
resentationf([CLS O
] O
) O
as O
query O
vector O
Qto O
query O
knowledge O
; O
5 O
) O
w/o O
IR O
Module O
denotes O
that O
we O
re- O
move O
the O
IC O
and O
KQ O
components O
and O
just O
adopt O
the O
context O
vector O
f([CLS O
] O
) O
to O
generate O
responses10 O
; O
From O
Table O
5 O
, O
we O
can O
observe O
that O
removing O
each O
component O
will O
result O
in O
a O
performance O
degrada- O
tion O
. O

Speciﬁcally O
, O
w/o O
IR O
Module O
causes O
54.96 B-MetricValue
% I-MetricValue
drops O
in O
Bleu-4 B-MetricName
score O
and O
54.18 B-MetricValue
% I-MetricValue
drops O
in O
Nist B-MetricName

score O
, O
which O
veriﬁes O
the O
great O
efﬁcacy O
of O
intention O
classify O
and O
knowledge O
query O
components O
. O

More- O
over O
, O
w/o O
WPA O
, O
w/o O
ITM O
and O
w/o O
UTS O
Encoder O
respectively O
cause O
28.54 B-MetricValue
% I-MetricValue
, O
20.48 B-MetricValue
% I-MetricValue
and O
14.37 B-MetricValue
% I-MetricValue
drops O
in O
Nist B-MetricName
score O
, O
which O
further O
demonstrates O
the O
effectiveness O
of O
cross O
- O
modal O
feature O
alignment O
and O
uniﬁed O
- O
modal O
semantic O
encoding O
. O

5.3 O
Case O
Study O
and O
Visualization O
To O
better O
illustrate O
the O
advantage O
of O
our O
model O
and O
understand O
what O
the O
feature O
alignment O
module O
has O
learned O
, O
we O
visualize O
several O
examples O
of O
text O
- O
to- O
image O
attention O
, O
as O
shown O
in O
Figure O
4 O
. O

It O
can O
be O
ob- O
served O
that O
our O
model O
is O
able O
to O
capture O
ﬁne O
- O
grained O
entity O
alignment O
between O
different O
modalities O
. O

The O
reason O
may O
be O
that O
: O
1 O
) O
We O
adopt O
a O
uniﬁed O
- O
modal O
Transformer O
semantic O
encoder O
, O
which O
enables O
to O
map O
different O
modalities O
of O
semantic O
cues O
into O
a O
same O
vector O
space O
to O
prompt O
inter O
- O
modality O
inter- O
actions O
for O
better O
representations O
; O
2 O
) O
Based O
on O
the O
obtained O
representations O
, O
the O
WPA O
technology O
can O
help O
supervise O
ﬁne O
- O
grained O
word O
- O
patch O
alignment O
, O
which O
is O
beneﬁcial O
to O
identifying O
user O
’s O
real O
inten- O
tion O
and O
generate O
more O
intention O
- O
aware O
responses O
. O

6 O
Conclusion O
In O
this O
paper O
, O
we O
propose O
a O
Uniﬁed B-MethodName
Transformer I-MethodName
Semantic I-MethodName
Representation I-MethodName
framework O
with O
feature O
alignment O
and O
intention O
reasoning O
, O
referred O
to O
Uni- B-MethodName
TranSeR. I-MethodName

Speciﬁcally O
, O
we O
project O
the O
multimodal O
features O
into O
a O
uniﬁed O
semantic O
space O
by O
utilizing O
a O
Transformer O
encoder O
to O
prompt O
inter O
- O
modal O
inter- O
actions O
. O

We O
further O
design O
a O
feature O
alignment O
and O
intention O
reasoning O
layer O
to O
conduct O
cross O
- O
modal O
feature O
alignment O
and O
ﬁne O
- O
grained O
intention O
rea-110 O

soning O
, O
with O
the O
objective O
of O
generating O
more O
accu- O
rate O
and O
intention O
- O
aware O
responses O
. O

Experiments O
on O
the O
representative O
MMD B-DatasetName
dataset O
demonstrate O
the O
effectiveness O
and O
superior O
performance O
of O
our O
UniTranSeR B-MethodName
model O
in O
both O
automatic O
and O
human O
evaluation O
. O

References O
Antoine O
Bordes O
, O
Y O
- O
Lan O
Boureau O
, O
and O
Jason O
Weston O
. O

2017 O
. O

Learning O
end O
- O
to O
- O
end O
goal O
- O
oriented O
dialog O
. O

In5th O
International O
Conference O
on O
Learning O
Rep- O
resentations O
, O
ICLR O
2017 O
, O
Toulon O
, O
France O
, O
April O
24- O
26 O
, O
2017 O
, O
Conference O
Track O
Proceedings O
. O

OpenRe- O
view.net O
. O

Zhangming O
Chan O
, O
Lemao O
Liu O
, O
Juntao O
Li O
, O
Haisong O
Zhang O
, O
Dongyan O
Zhao O
, O
Shuming O
Shi O
, O
and O
Rui O
Yan O
. O
2021 O
. O

Enhancing O
the O
open O
- O
domain O
dialogue O
eval- O
uation O
in O
latent O
space O
. O

In O
Findings O
of O
the O
Associ- O
ation O
for O
Computational O
Linguistics O
: O
ACL O
/ O
IJCNLP O
2021 O
, O
Online O
Event O
, O
August O
1 O
- O
6 O
, O
2021 O
, O
volume O
ACL O
/ O
IJCNLP O
2021 O
of O
Findings O
of O
ACL O
, O
pages O
4889 O
– O
4900 O
. O

Association O
for O
Computational O
Linguistics O
. O

Hardik O
Chauhan O
, O
Mauajama O
Firdaus O
, O
Asif O
Ekbal O
, O
and O
Pushpak O
Bhattacharyya O
. O

2019 O
. O

Ordinal O
and O
attribute O
aware O
response B-TaskName
generation I-TaskName
in O
a O
multimodal O
dialogue O
system O
. O

In O
Proceedings O
of O
the O
57th O
Conference O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
ACL O
2019 O
, O
Florence O
, O
Italy O
, O
July O
28- O
August O
2 O
, O
2019 O
, O
Vol- O
ume O
1 O
: O
Long O
Papers O
, O
pages O
5437–5447 O
. O

Association O
for O
Computational O
Linguistics O
. O

Yen O
- O
Chun O
Chen O
, O
Linjie O
Li O
, O
Licheng O
Yu O
, O
Ahmed O
El O
Kholy O
, O
Faisal O
Ahmed O
, O
Zhe O
Gan O
, O
Yu O
Cheng O
, O
and O
Jingjing O
Liu O
. O
2020 O
. O

UNITER O
: O
universal O
image O
- O
text O
representation O
learning O
. O

In O
Computer O
Vision O
- O
ECCV O
2020 O
- O
16th O
European O
Conference O
, O
Glasgow O
, O
UK O
, O
Au- O
gust O
23 O
- O
28 O
, O
2020 O
, O
Proceedings O
, O
Part O
XXX O
, O
volume O
12375 O
of O
Lecture O
Notes O
in O
Computer O
Science O
, O
pages O
104–120 O
. O

Springer O
. O

Chen O
Cui O
, O
Wenjie O
Wang O
, O
Xuemeng O
Song O
, O
Minlie O
Huang O
, O
Xin O
- O
Shun O
Xu O
, O
and O
Liqiang O
Nie O
. O
2019 O
. O

User O
attention O
- O
guided O
multimodal O
dialog O
systems O
. O

In O
Pro- O
ceedings O
of O
the O
42nd O
International O
ACM O
SIGIR O
Con- O
ference O
on O
Research O
and O
Development O
in O
Informa- O
tion O
Retrieval O
, O
SIGIR O
2019 O
, O
Paris O
, O
France O
, O
July O
21- O
25 O
, O
2019 O
, O
pages O
445–454 O
. O
ACM O
. O

Mihail O
Eric O
and O
Christopher O
D. O
Manning O
. O

2017 O
. O

A O
copy O
- O
augmented O
sequence O
- O
to O
- O
sequence O
architecture O
gives O
good O
performance O
on O
task O
- O
oriented O
dialogue O
. O

InProceedings O
of O
the O
15th O
Conference O
of O
the O
Euro- O
pean O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
EACL O
2017 O
, O
Valencia O
, O
Spain O
, O
April O
3 O
- O
7 O
, O
2017 O
, O
Volume O
2 O
: O
Short O
Papers O
, O
pages O
468–473 O
. O

As- O
sociation O
for O
Computational O
Linguistics O
. O

Varun O
Gangal O
, O
Harsh O
Jhamtani O
, O
Eduard O
H. O
Hovy O
, O
and O
Taylor O
Berg O
- O
Kirkpatrick O
. O
2021 O
. O

Improving O
auto- O
mated O
evaluation O
of O
open O
domain O
dialog O
via O
diversereference O
augmentation O
. O

In O
Findings O
of O
the O
Associ- O
ation O
for O
Computational O
Linguistics O
: O
ACL O
/ O
IJCNLP O
2021 O
, O
Online O
Event O
, O
August O
1 O
- O
6 O
, O
2021 O
, O
volume O
ACL O
/ O
IJCNLP O
2021 O
of O
Findings O
of O
ACL O
, O
pages O
4079 O
– O
4090 O
. O

Association O
for O
Computational O
Linguistics O
. O

Yanjie O
Gou O
, O
Yinjie O
Lei O
, O
Lingqiao O
Liu O
, O
Yong O
Dai O
, O
and O
Chunxu O
Shen O
. O
2021 O
. O

Contextualize O
knowledge O
bases O
with O
transformer O
for O
end O
- O
to O
- O
end O
task O
- O
oriented O
dialogue O
systems O
. O

In O
Proceedings O
of O
the O
2021 O
Con- O
ference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
EMNLP O
2021 O
, O
Virtual O
Event O
/ O
Punta O
Cana O
, O
Dominican O
Republic O
, O
7 O
- O
11 O
November O
, O
2021 O
, O
pages O
4300–4310 O
. O

Association O
for O
Computational O
Linguistics O
. O

Kaiming O
He O
, O
Xiangyu O
Zhang O
, O
Shaoqing O
Ren O
, O
and O
Jian O
Sun O
. O
2016 O
. O

Deep O
residual O
learning O
for O
image O
recog- O
nition O
. O

In O
2016 O
IEEE O
Conference O
on O
Computer O
Vi- O
sion O
and O
Pattern O
Recognition O
, O
CVPR O
2016 O
, O
Las O
Ve- O
gas O
, O
NV O
, O
USA O
, O
June O
27 O
- O
30 O
, O
2016 O
, O
pages O
770–778 O
. O

IEEE O
Computer O
Society O
. O

Weidong O
He O
, O
Zhi O
Li O
, O
Dongcai O
Lu O
, O
Enhong O
Chen O
, O
Tong O
Xu O
, O
Baoxing O
Huai O
, O
and O
Jing O
Yuan O
. O

2020 O
. O

Multi- O
modal O
dialogue O
systems O
via O
capturing O
context O
- O
aware O
dependencies O
of O
semantic O
elements O
. O

In O
MM O
’ O
20 O
: O
The O
28th O
ACM O
International O
Conference O
on O
Multi- O
media O
, O
Virtual O
Event O
/ O
Seattle O
, O
WA O
, O
USA O
, O
October O
12 O
- O
16 O
, O
2020 O
, O
pages O
2755–2764 O
. O
ACM O
. O

Liunian O
Harold O
Li O
, O
Mark O
Yatskar O
, O
Da O
Yin O
, O
Cho O
- O
Jui O
Hsieh O
, O
and O
Kai O
- O
Wei O
Chang O
. O

2019 O
. O

Visualbert O
: O

A O
simple O
and O
performant O
baseline O
for O
vision O
and O
lan- O
guage O
. O

CoRR O
, O
abs/1908.03557 O
. O

Wei O
Li O
, O
Can O
Gao O
, O
Guocheng O
Niu O
, O
Xinyan O
Xiao O
, O
Hao O
Liu O
, O
Jiachen O
Liu O
, O
Hua O
Wu O
, O
and O
Haifeng O
Wang O
. O

2021 O
. O

UNIMO O
: O
towards O
uniﬁed O
- O
modal O
understand- O
ing O
and O
generation O
via O
cross O
- O
modal O
contrastive O
learn- O
ing O
. O

In O
Proceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
ACL O
/ O
IJCNLP O
2021 O
, O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
Virtual O
Event O
, O
August O
1 O
- O
6 O
, O
2021 O
, O
pages O
2592–2607 O
. O

Association O
for O
Computational O
Linguistics O
. O

Lizi O
Liao O
, O
Yunshan O
Ma O
, O
Xiangnan O
He O
, O
Richang O
Hong O
, O
and O
Tat O
- O
Seng O
Chua O
. O

2018 O
. O

Knowledge O
- O
aware O
multi- O
modal O
dialogue O
systems O
. O

In O
2018 O
ACM O
Multimedia O
Conference O
on O
Multimedia O
Conference O
, O
MM O
2018 O
, O
Seoul O
, O
Republic O
of O
Korea O
, O
October O
22 O
- O
26 O
, O
2018 O
, O
pages O
801–809 O
. O
ACM O
. O

Jiasen O
Lu O
, O
Dhruv O
Batra O
, O
Devi O
Parikh O
, O
and O
Stefan O
Lee O
. O
2019 O
. O

Vilbert O
: O
Pretraining O
task O
- O
agnostic O
visi- O
olinguistic O
representations O
for O
vision O
- O
and O
- O
language O
tasks O
. O

In O
Advances O
in O
Neural O
Information O
Process- O
ing O
Systems O
32 O
: O
Annual O
Conference O
on O
Neural O
Infor- O
mation O
Processing O
Systems O
2019 O
, O
NeurIPS O
2019 O
, O
De- O
cember O
8 O
- O
14 O
, O
2019 O
, O
Vancouver O
, O
BC O
, O
Canada O
, O
pages O
13–23 O
. O

Andrea O
Madotto O
, O
Zhaojiang O
Lin O
, O
Zhenpeng O
Zhou O
, O
Se- O
ungwhan O
Moon O
, O
Paul O
A. O
Crook O
, O
Bing O
Liu O
, O
Zhou O
Yu,111 O

Eunjoon O
Cho O
, O
Pascale O
Fung O
, O
and O
Zhiguang O
Wang O
. O
2021 O
. O

Continual O
learning O
in O
task O
- O
oriented O
dialogue O
systems O
. O

In O
Proceedings O
of O
the O
2021 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
EMNLP O
2021 O
, O
Virtual O
Event O
/ O
Punta O
Cana O
, O
Domini- O
can O
Republic O
, O
7 O
- O
11 O
November O
, O
2021 O
, O
pages O
7452 O
– O
7467 O
. O

Association O
for O
Computational O
Linguistics O
. O

Andrea O
Madotto O
, O
Chien O
- O
Sheng O
Wu O
, O
and O
Pascale O
Fung O
. O

2018 O
. O

Mem2seq O
: O
Effectively O
incorporating O
knowl- O
edge O
bases O
into O
end O
- O
to O
- O
end O
task O
- O
oriented O
dialog O
sys- O
tems O
. O

In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
ACL O
2018 O
, O
Melbourne O
, O
Australia O
, O
July O
15 O
- O
20 O
, O
2018 O
, O
Vol- O
ume O
1 O
: O
Long O
Papers O
, O
pages O
1468–1478 O
. O

Association O
for O
Computational O
Linguistics O
. O

Fei O
Mi O
, O
Wanhao O
Zhou O
, O
Lingjing O
Kong O
, O
Fengyu O
Cai O
, O
Minlie O
Huang O
, O
and O
Boi O
Faltings O
. O
2021 O
. O

Self O
- O
training O
improves O
pre O
- O
training O
for O
few O
- O
shot O
learning O
in O
task- O
oriented O
dialog O
systems O
. O

In O
Proceedings O
of O
the O
2021 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Lan- O
guage O
Processing O
, O
EMNLP O
2021 O
, O
Virtual O
Event O
/ O
Punta O
Cana O
, O
Dominican O
Republic O
, O
7 O
- O
11 O
November O
, O
2021 O
, O
pages O
1887–1898 O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Liqiang O
Nie O
, O
Wenjie O
Wang O
, O
Richang O
Hong O
, O
Meng O
Wang O
, O
and O
Qi O
Tian O
. O

2019 O
. O

Multimodal O
dialog O
sys- O
tem O
: O
Generating O
responses O
via O
adaptive O
decoders O
. O

In O
Proceedings O
of O
the O
27th O
ACM O
International O
Confer- O

ence O
on O
Multimedia O
, O
MM O
2019 O
, O
Nice O
, O
France O
, O
Octo- O
ber O
21 O
- O
25 O
, O
2019 O
, O
pages O
1098–1106 O
. O

ACM O
. O

Jiahuan O
Pei O
, O
Pengjie O
Ren O
, O
and O
Maarten O
de O
Rijke O
. O
2021 O
. O

A O
cooperative O
memory O
network O
for O
personalized O
task O
- O
oriented O
dialogue O
systems O
with O
incomplete O
user O
proﬁles O
. O

In O
WWW O
’ O
21 O
: O
The O
Web O
Conference O
2021 O
, O
Virtual O
Event O
/ O
Ljubljana O
, O
Slovenia O
, O
April O
19 O
- O
23 O
, O
2021 O
, O
pages O
1552–1561 O
. O

ACM O
/ O
IW3C2 O
. O

Libo O
Qin O
, O
Yijia O
Liu O
, O
Wanxiang O
Che O
, O
Haoyang O
Wen O
, O
Yangming O
Li O
, O
and O
Ting O
Liu O
. O

2019 O
. O

Entity O
- O
consistent O
end O
- O
to O
- O
end O
task O
- O
oriented O
dialogue O
system O
with O
KB O
retriever O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Nat- O
ural O
Language O
Processing O
, O
EMNLP O
- O
IJCNLP O
2019 O
, O
Hong O
Kong O
, O
China O
, O
November O
3 O
- O
7 O
, O
2019 O
, O
pages O
133 O
– O
142 O
. O

Association O
for O
Computational O
Linguistics O
. O

Libo O
Qin O
, O
Xiao O
Xu O
, O
Wanxiang O
Che O
, O
Yue O
Zhang O
, O
and O
Ting O
Liu O
. O
2020 O
. O

Dynamic O
fusion O
network O
for O
multi- O
domain O
end O
- O
to O
- O
end O
task O
- O
oriented O
dialog O
. O

In O
Proceed- O
ings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
ACL O
2020 O
, O
Online O
, O
July O
5 O
- O
10 O
, O
2020 O
, O
pages O
6344–6354 O
. O
Association O
for O
Computational O
Linguistics O
. O

Dinesh O
Raghu O
, O
Atishya O
Jain O
, O
Mausam O
, O
and O
Sachindra O
Joshi O
. O
2021 O
. O

Constraint O
based O
knowledge O
base O
distil- O
lation O
in O
end O
- O
to O
- O
end O
task O
oriented O
dialogs O
. O

In O
Find- O
ings O
of O
the O
Association O
for O
Computational O
Linguis- O

tics O
: O
ACL O
/ O
IJCNLP O
2021 O
, O
Online O
Event O
, O
August O
1- O
6 O
, O
2021 O
, O
volume O
ACL O
/ O
IJCNLP O
2021 O
of O
Findings O
ofACL O
, O
pages O
5051–5061 O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Revanth O
Reddy O
, O
Danish O
Contractor O
, O
Dinesh O
Raghu O
, O
and O
Sachindra O
Joshi O
. O

2019 O
. O

Multi O
- O
level O
memory O
for O
task O
oriented O
dialogs O
. O

In O
Proceedings O
of O
the O
2019 O
Con- O
ference O
of O
the O
North O
American O
Chapter O
of O
the O
Asso- O
ciation O
for O
Computational O
Linguistics O
: O
Human O
Lan- O
guage O
Technologies O
, O
NAACL O
- O
HLT O
2019 O
, O
Minneapo- O
lis O
, O
MN O
, O
USA O
, O
June O
2 O
- O
7 O
, O
2019 O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
3744–3754 O
. O

Association O
for O
Computational O
Linguistics O
. O

Shaoqing O
Ren O
, O
Kaiming O
He O
, O
Ross O
B. O
Girshick O
, O
and O
Jian O
Sun O
. O
2015 O
. O

Faster O
R O
- O
CNN O
: O
towards O
real O
- O
time O
object O
detection O
with O
region O
proposal O
networks O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
28 O
: O
Annual O
Conference O
on O
Neural O
Information O
Pro- O
cessing O
Systems O
2015 O
, O
December O
7 O
- O
12 O
, O
2015 O
, O
Mon- O
treal O
, O
Quebec O
, O
Canada O
, O
pages O
91–99 O
. O

Amrita O
Saha O
, O
Mitesh O
M. O
Khapra O
, O
and O
Karthik O
Sankara- O
narayanan O
. O

2018 O
. O

Towards O
building O
large O
scale O
mul- O
timodal O
domain O
- O
aware O
conversation O
systems O
. O

In O
Pro- O
ceedings O
of O
the O
Thirty O
- O
Second O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
( O
AAAI-18 O
) O
, O
the O
30th O
innova- O
tive O
Applications O
of O
Artiﬁcial O
Intelligence O
( O
IAAI-18 O
) O
, O
and O
the O
8th O
AAAI O
Symposium O
on O
Educational O
Ad- O
vances O
in O
Artiﬁcial O
Intelligence O
( O
EAAI-18 O
) O
, O
New O
Or- O
leans O
, O
Louisiana O
, O
USA O
, O
February O
2 O
- O
7 O
, O
2018 O
, O
pages O
696–704 O
. O

AAAI O
Press O
. O

Bishal O
Santra O
, O
Potnuru O
Anusha O
, O
and O
Pawan O
Goyal O
. O
2021 O
. O

Hierarchical O
transformer O
for O
task O
oriented O
dia- O
log O
systems O
. O

In O
Proceedings O
of O
the O
2021 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
NAACL O
- O
HLT O
2021 O
, O
Online O
, O
June O
6 O
- O
11 O
, O
2021 O
, O
pages O
5649–5658 O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Haoyu O
Song O
, O
Yan O
Wang O
, O
Wei O
- O
Nan O
Zhang O
, O
Zhengyu O
Zhao O
, O
Ting O
Liu O
, O
and O
Xiaojiang O
Liu O
. O
2020 O
. O

Proﬁle O
consistency O
identiﬁcation O
for O
open O
- O
domain O
dialogue O
agents O
. O

In O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Process- O
ing O
, O
EMNLP O
2020 O
, O
Online O
, O
November O
16 O
- O
20 O
, O
2020 O
, O
pages O
6651–6662 O
. O

Association O
for O
Computational O
Linguistics O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N. O
Gomez O
, O
Lukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
Advances O
in O
Neural O
Information O
Pro- O
cessing O
Systems O
30 O
: O
Annual O
Conference O
on O
Neural O
Information O
Processing O
Systems O
2017 O
, O
December O
4- O
9 O
, O
2017 O
, O
Long O
Beach O
, O
CA O
, O
USA O
, O
pages O
5998–6008 O
. O

Jian O
Wang O
, O
Junhao O
Liu O
, O
Wei O
Bi O
, O
Xiaojiang O
Liu O
, O
Ke- O
jing O
He O
, O
Ruifeng O
Xu O
, O
and O
Min O
Yang O
. O

2020 O
. O

Dual O
dynamic O
memory O
network O
for O
end O
- O
to O
- O
end O
multi O
- O
turn O
task O
- O
oriented O
dialog O
systems O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
COLING O
2020 O
, O
Barcelona O
, O
Spain O
( O
On- O
line O
) O
, O
December O
8 O
- O
13 O
, O
2020 O
, O
pages O
4100–4110 O
. O
Inter- O
national O
Committee O
on O
Computational O
Linguistics.112 O

Jianhong O
Wang O
, O
Yuan O
Zhang O
, O
Tae O
- O
Kyun O
Kim O
, O
and O
Yun- O
jie O
Gu O
. O
2021 O
. O

Modelling O
hierarchical O
structure O
be- O
tween O
dialogue O
policy O
and O
natural O
language O
genera- O
tor O
with O
option O
framework O
for O
task O
- O
oriented O
dialogue O
system O
. O

In O
9th O
International O
Conference O
on O
Learn- O
ing O
Representations O
, O
ICLR O
2021 O
, O
Virtual O
Event O
, O
Aus- O
tria O
, O
May O
3 O
- O
7 O
, O
2021 O
. O

OpenReview.net O
. O

Haoyang O
Wen O
, O
Yijia O
Liu O
, O
Wanxiang O
Che O
, O
Libo O
Qin O
, O
and O
Ting O
Liu O
. O

2018 O
. O

Sequence O
- O
to O
- O
sequence O
learning O
for O
task O
- O
oriented O
dialogue O
with O
dialogue O
state O
repre- O
sentation O
. O

In O
Proceedings O
of O
the O
27th O
International O
Conference O
on O
Computational O
Linguistics O
, O
COLING O
2018 O
, O
Santa O
Fe O
, O
New O
Mexico O
, O
USA O
, O
August O
20 O
- O
26 O
, O
2018 O
, O
pages O
3781–3792 O
. O

Association O
for O
Computa- O
tional O
Linguistics O
. O

Chien O
- O
Sheng O
Wu O
, O
Richard O
Socher O
, O
and O
Caiming O
Xiong O
. O

2019 O
. O

Global O
- O
to O
- O
local O
memory O
pointer O
networks O
for O
task O
- O
oriented O
dialogue O
. O

In O
7th O
International O
Confer- O
ence O
on O
Learning O
Representations O
, O
ICLR O
2019 O
, O
New O
Orleans O
, O
LA O
, O
USA O
, O
May O
6 O
- O
9 O
, O
2019 O
. O

OpenReview.net O
. O

Ze O
Yang O
, O
Wei O
Wu O
, O
Huang O
Hu O
, O

Can O
Xu O
, O
Wei O
Wang O
, O
and O
Zhoujun O
Li O
. O

2021 O
. O

Open O
domain O
dialogue B-TaskName
genera- I-TaskName
tion I-TaskName
with O
latent O
images O
. O

In O
Thirty O
- O
Fifth O
AAAI O
Con- O
ference O
on O
Artiﬁcial O
Intelligence O
, O
AAAI O
2021 O
, O
Thirty- O
Third O
Conference O
on O
Innovative O
Applications O
of O
Ar- O
tiﬁcial O
Intelligence O
, O
IAAI O
2021 O
, O
The O
Eleventh O
Sym- O
posium O
on O
Educational O
Advances O
in O
Artiﬁcial O
Intel- O

ligence O
, O
EAAI O
2021 O
, O
Virtual O
Event O
, O
February O
2 O
- O
9 O
, O
2021 O
, O
pages O
14239–14247 O
. O

AAAI O
Press O
. O

Haoyu O
Zhang O
, O
Meng O
Liu O
, O
Zan O
Gao O
, O
Xiaoqiang O
Lei O
, O
Yin- O
glong O
Wang O
, O
and O
Liqiang O
Nie O
. O
2021 O
. O

Multimodal O
di- O
alog O
system O
: O
Relational O
graph O
- O
based O
context O
- O
aware O
question O
understanding O
. O

In O
MM O
’ O
21 O
: O
ACM O
Multime- O
dia O
Conference O
, O
Virtual O
Event O
, O
China O
, O
October O
20 O
- O
24 O
, O
2021 O
, O
pages O
695–703 O
. O
ACM O
. O

Zheng O
Zhang O
, O
Lizi O
Liao O
, O
Minlie O
Huang O
, O
Xiaoyan O
Zhu O
, O
and O
Tat O
- O
Seng O
Chua O
. O
2019 O
. O

Neural O
multimodal O
be- O
lief O
tracker O
with O
adaptive O
attention O
for O
dialogue O
sys- O
tems O
. O

In O
The O
World O
Wide O
Web O
Conference O
, O
WWW O
2019 O
, O
San O
Francisco O
, O
CA O
, O
USA O
, O
May O
13 O
- O
17 O
, O
2019 O
, O
pages O
2401–2412 O
. O

ACM.113 O

A O
Appendices O
A.1 O
Hyperparameters O
Setting O
The O
hyperparameters O
used O
for O
MMD O
dataset O
are O
shown O
in O
Table O
6 O
. O

A.2 O
Description O
of O
Special O
Tokens O

The O
special O
tokens O
used O
in O
our O
experiments O
are O
shown O
in O
Table O
7 O
. O

A.3 O
Loss O
Function O
Our O
total O
loss O
function O
LTotal O
comprises O
three O
parts O
: O
UTS O
encoder O
loss O
LE O
, O
FAIR O
layer O
loss O
LFand O
HTR O
decoder O
lossLD O
, O
which O
can O
be O
calculated O
as O
follows O
: O

LTotal O
= O

ELE+ O

FLF+ O

DLD O
( O
13 O
) O
where O

E B-HyperparameterName
, O

F B-HyperparameterName
and O

D B-HyperparameterName
are O
hyperparameters O
, O
and O
are O
initialized O
equally O
, O
i.e. O
, O
0.33 B-HyperparameterValue
, O 
0.33 B-HyperparameterValue
and O 
0.33 B-HyperparameterValue
. O

Then O
, O
we O
tune O
them O
on O
the O
veriﬁcation O
set O
to O
obtain O
a O
better O
weight O
setting O
of O
0.30 B-HyperparameterValue
, O 
0.35 B-HyperparameterValue
and O 
0.35 B-HyperparameterValue
. O

The O
UTS O
encoder O
loss O
LEcontains O
two O
parts O
: O
LMLM O
andLMPM O
, O
LE O
= O
LMLM O

+ O
LMPM O
( O
14 O
) O
the O
FAIR O
layer O
loss O
contains O
three O
parts O
: O
LITM O
, O
LWPA O
andLIC O
: O

LF O
= O
LITM+LWPA O
+ O
LIC O
( O
15 O
) O
and O
the O
HTR O
decoder O
loss O
is O
divided O
into O
two O
types O
: O
the O
textual O
decoding O
loss O
LTXT O
for O
text O
task O
and O
image O
recommend O
loss O
LIMGfor O
image O
task O
, O
which O
is O
consistent O
with O
previous O
work O
( O
Nie O
et O
al O
. O
, O
2019 O
) O
. O

LD O
= O
LTXT+LIMG O
( O
16) O 

A.4 O
Dateset O
Statistics O
A O
detailed O
statistics O
of O
the O
MMD B-DatasetName
dataset O
is O
pre- O
sented O
in O
Table O
8 O
. O

A.5 O
Error O
Analysis O
To O
better O
understand O
the O
limitations O
of O
our O
model O
, O
we O
conduct O
an O
error O
analysis O
on O
UniTranSeR. B-MethodName

We O
randomly O
select O
100responses O
generated O
by O
Uni- B-MethodName
TranSeR I-MethodName
that O
achieve O
low O
human O
evaluation O
scores O
in O
the O
test O
set O
of O
MMD B-DatasetName
. O

We O
report O
several O
reasons O
for O
the O
low O
scores O
, O
which O
can O
roughly O
be O
classiﬁed O
into O
four O
categories O
. O

( O
1 O
) O
KB O
information O
in O
the O
generated O
responses O
is O
incorrect O
( O
38 O
% O
) O
, O
especially O
when O
the O
corresponding O
equipped O
knowledge O
base O
is O
large O
and O
complex O
. O

( O
2 O
) O
The O
sentence O
structure O
of O
the O
generated O
responses O
is O
incorrect O
and O
there O
are O
serious O
grammatical O
and O
semantic O
errors O
( O
24 O
% O
) O
. O

( O
3 O
) O
The O
model O
makes O
incomplete O
response O
when O
there O
are O
multiple O
intentions O
contained O
in O
users O
’ O
ut- O
terances O
( O
21 O
% O
) O
. O

( O
4 O
) O
The O
model O
selects O
incorrect O
product O
images O
since O
different O
products O
have O
simi- O
lar O
attributes O
( O
17%). O

Minh O
- O
Thang O
Luong O
. O
Addressing O
the O
Rare O
Word O
Problem O
in O
Machine O
Translation O
. O
Presented O
at O
the O
Workshop O
on O
Statistical O
Machine O
Translation O
, O
Edinburgh O
, O
UK O
, O
June O
2010 O
. O


 O
The O
rare O
word O
problem O
is O
a O
challenge O
in O
machine O
translation O
when O
a O
word O
is O
infrequent O
in O
a O
parallel O
corpus O
. O


 O
Addressing O
the O
challenge O
of O
translating O
words O
that O
are O
infrequent O
in O
a O
parallel O
corpus O
is O
known O
as O
the O
rare O
word O
problem O
. O

Abstract O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName
( O
NMT B-TaskName
) O
is O
a O
new O
approach O
to O
machine B-TaskName
translation I-TaskName
that O
has O
shown O
promising O
results O
that O
are O
comparable O
to O
traditional O
approaches O
. O

Our O
experiments O
on O
the O
WMT’14 B-DatasetName
English I-DatasetName
to I-DatasetName
French I-DatasetName
translation O
task O
show O
that O
this O
method O
provides O
a O
substantial O
improvement O
of O
up O
to O
2.8 B-MetricValue
BLEU B-MetricName
points O
over O
an O
equivalent O
NMT O
system O
that O
does O
not O
use O
this O
technique O
. O

Our O
NMT O
system O
is O
the O
first O
to O
surpass O
the O
best O
result O
achieved O
on O
a O
contest O
task O
, O
with O
37.5 B-MetricValue
BLEU B-MetricName
points O
. O

NMT B-TaskName
is O
a O
novel O
approach O
to O
MT B-TaskName
that O
has O
achieved O
promising O
results O
. O

We O
propose O
and O
implement O
a O
novel O
approach O
to O
address O
the O
rare O
word O
problem O
of O
NMTs B-TaskName
, O
which O
is O
motivated O
by O
the O
strengths O
of O
standard O
phrase O
- O
based O
systems O
. O

This O
approach O
provides O
an O
improvement O
of O
up O
to O
2.8 B-MetricValue
points O
over O
an O
equivalent O
NMT O
system O
that O
does O
not O
use O
this O
technique O
, O
on O
the O
English B-DatasetName
to I-DatasetName
French I-DatasetName
WMT’14 I-DatasetName
translation O
task O
, O
if O
the O
vocabulary O
is O
relatively O
small O
. O

In O
addition O
, O
our O
system O
is O
the O
first O
NMT O
to O
outperform O
the O
winner O
of O
a O
task O
. O

, O
sn O
, O
to O
a O
target O
sentence O
, O
t1 O
, O
. O
. O
. O
, O
tm O
, O
where O
n O
and O
m O
can O
be O
different O
. O


 O
2 O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName
A O
neural O
machine O
translation O
system O
is O
any O
neural O
network O
that O
maps O
a O
source O
sentence O
to O
a O
target O
sentence O
, O
where O
the O
number O
of O
words O
in O
the O
source O
sentence O
can O
be O
different O
than O
the O
number O
of O
words O
in O
the O
target O
sentence O
. O

3.1 O
Copyable B-MethodName
Model O
In O
this O
approach O
, O
we O
represent O
various O
unknown O
words O
in O
the O
source O
and O
target O
language O
using O
multiple O
tokens O
, O
instead O
of O
only O
one O
unk O
token O
. O

The O
copyable B-MethodName
model O
is O
limited O
by O
its O
inability O
to O
translate O
unknown O
target O
words O
that O
are O
aligned O
to O
known O
words O
in O
the O
source O
sentence O
. O

Our O
notation O
is O
illustrated O
in O
Figure O
3.3 O
Positional B-MethodName
Unknown I-MethodName
Model O
( O
PosUnk B-MethodName
) O
. O

The O
main O
weakness O
of O
the O
PosAll B-MethodName
model O
is O
that O
it O
increases O
the O
length O
of O
the O
target O
sentence O
by O
two O
. O

, O
0 O
) O
to O
predict O
the O
tth O
tokentoken O


 O
Our O
positional B-MethodName
un- I-MethodName
known I-MethodName
model O
uses O
unkpos O
dtokens O
to O
predict O
the O
tth O
tokentoken O
, O
which O
motivates O
us O
to O
keep O
going O
. O

The O
PosAll B-MethodName
model O
, O
despite O
being O
trained O
on O
more O
examples O
of O
words O
and O
their O
alignments O
, O
might O
not O
learn O
better O
alignments O
because O
it O
has O
a O
slower O
speed O
. O

We O
evaluate O
the O
effectiveness O
of O
our O
OOV O
models O
on O
the O
translation O
task O
WMT’14 B-DatasetName
English I-DatasetName
- I-DatasetName
to I-DatasetName
- I-DatasetName
French I-DatasetName
. O

The O
BLEU B-MetricName
metric O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
is O
used O
to O
measure O
translation O
quality O
on O
the O
new- B-DatasetName
stest2014 I-DatasetName
test O
set O
, O
which O
consists O
of O
3003 O
sentences O
. O

The O
proposed O
method O
in O
Axelrod O
et O
al O
. O
selected O
the O
12 O
M O
subset O
from O
the O
full O
WMT’14 B-DatasetName
parallel O
corpora O
. O

We O
train O
multi O
- O
layer O
deep O
LSTMs O
, O
each O
of O
which O
has O
1000 B-HyperparameterValue
cells O
and O
1000 B-HyperparameterValue
dimensional O
embeddings O
. O

Our O
hyperparameters O
can O
be O
summarized O
as O
follows O
: O
( O
a O
) O
the O
weights O
are O
initialized O
uniformly O
in O
[ O
-0.1 O
, O
0.1 O
] O
for O
two O
- O
layer O
models O
and O
[ O
-0.01 O
, O
0.01 O
] O
for O
four O
- O
layer O
models O
, O
( O
b O
) O
SGD O
has O
a O
ﬁxed O
momentum O
of O
0.9 O
, O
( O
c O
) O
we O
train O
for O
8 O
epochs O
( O
after O
5 O
epochs O
, O
we O
begin O
to O
halve O
the O
learning O
rate O
every O
0.5 O
epoch O
) O
, O
( O
d O
) O
the O
batch O
size O
is O
128 O
, O
and O
( O
e O
) O
we O
rescale O
the O
normalized O
gradient O
to O
ensure O
that O
its O
norm O
does O
not O
exceed O
10 O
( O
Pascanu O
et O
al O
. O
, O
2012 O
) O
. O

( O
b O
) O
raw O
, O
case O
- O
sensitive O
translations O
to O
be O
com- O
parable O
with O
systems O
that O
did O
not O
use O
true- O
cas- O
ing O
. O


 O
We O
report O
BLEU B-MetricName
scores O
based O
on O
both O
detokenized O
translations O
( O
WMT'14 O
style O
) O
and O
raw O
, O
case O
- O
sensitive O
translations O
, O
to O
be O
comparable O
with O
results O
reported O
on O
the O
WMT O
website O
and O
with O
systems O
that O
did O
not O
use O
true O
- O
casing O
. O

The O
best O
system O
currently O
available O
( O
Durrani O
et O
al O
. O
, O
2014 O
) O
achieves O
a O
score O
of O
35.8 B-MetricValue
on O
the O
newstest2014 B-DatasetName
test O
set O
for O
the O
English O
to O
French O
language O
pair O
( O
see O
Table O
2 O
) O
. O

The O
performance O
is O
37.0 B-MetricValue
points O
out O
of O
a O
possible O
tokenized B-MetricName
BLEU I-MetricName
points O
( O
see O
Table O
1 O
) O
. O

The O
results O
in O
Table O
1 O
demonstrate O
that O
our O
unknown O
word O
translation O
technique O
( O
in O
particular O
, O
the O
PosUnk B-MethodName
model O
) O
significantly O
improves O
the O
translation O
quality O
for O
both O
the O
individual O
( O
non O
- O
ensemble O
) O
LSTM O
models O
and O
the O
ensemble O
models O
. O
For O
40K O
- O
word O
vocabularies O
, O
the O
performance O
gains O
are O
in O
the O
range O
of O
2.3 B-MetricValue
- I-MetricValue
2.8 I-MetricValue
to O
BLEU B-MetricName
points O
. O

Although O
the O
performance O
gains O
are O
not O
as O
great O
as O
with O
a O
larger O
vocabulary O
, O
our O
technique O
can O
still O
provide O
a O
significant O
improvement O
of O
1.6 B-MetricValue
- I-MetricValue
1.9 I-MetricValue
to O
BLEU B-MetricName
points O
. O

The O
usefulness O
of O
the O
PosUnk B-MethodName
model O
is O
directly O
related O
to O
the O
NMT O
's O
ability O
to O
correctly O
identify O
the O
corresponding O
word O
in O
the O
source O
sentence O
for O
a O
given O
OOV O
target O
word O
. O

Our O
result O
of O
BLEU B-MetricName
outperforms O
all O
other O
NMT O
systems O
by O
a O
large O
margin O
, O
which O
is O
more O
important O
because O
it O
establishes O
a O
new O
record O
on O
the O
WMT’14 B-DatasetName
English I-DatasetName
to I-DatasetName
French I-DatasetName
translation O
. O

We O
also O
look O
at O
how O
depth O
affects O
LSTM O
architectures O
and O
show O
that O
there O
is O
a O
strong O
relationship O
between O
perplexities O
and O
BLEU B-MetricName
scores O
. O

Sutskever O
et O
al O
. O
, O
2014 O


 O
Sutskever O
et O
al O
. O
, O
2014 O


 O
A. O
Sutskever O
et O
al O
. O
, O
2014 O

 O
B. O
Sutskever O
et O
al O
. O
, O
2014 O

 O
C. O
Sutskever O
et O
al O
. O
, O
2014 O

 O
D. O
Sutskever O
et O
al O
. O
, O
2014 O

 O
E. O
Sutskever O
et O
al O
. O
, O
2014 O

 O
F. O
Sutskever O
et O
al O
. O
, O
2014 O

 O
G. O
Sutskever O
et O
al O
. O
, O
2014 O


 O
0 O
. O
Sutskever O
et O
al O
. O
, O
2014 O

The O
neural O
machine O
translation O
system O
is O
challenged O
by O
Rare B-TaskName
word I-TaskName
translation I-TaskName
, O
as O
shown O
in O
Figure O
5 O
. O

When O
we O
use O
our O
unknown O
word O
translation O
technique O
( O
purple O
curve O
) O
, O
we O
are O
able O
to O
significantly O
improve O
the O
translation O
quality O
of O
our O
NMT O
system O
. O
This O
is O
especially O
true O
for O
the O
last O
group O
of O
500 O
sentences O
, O
which O
have O
the O
greatest O
proportion O
of O
OOV O
words O
in O
the O
test O
set O
. O
We O
were O
able O
to O
increase O
the O
BLEU B-MetricName
score O
of O
our O
system O
by O
4.8 B-MetricValue
BLEU B-MetricName
points O
. O

We O
examine O
the O
effect O
of O
different O
rare O
word O
models O
on O
the O
rate O
of O
unknown O
words O
being O
aligned O
on O
both O
the O
input O
and O
target O
side O
. O
Models O
Positional B-MethodName
All I-MethodName
( O
PosAll B-MethodName
) O
predict O
the O
aligned O
source O
positions O
for O
every O
target O
word O
, O
while O
Model O
Positional B-MethodName
Unknown I-MethodName
( O
PosUnk B-MethodName
) O
only O
predicts O
aligned O
source O
positions O
for O
unknown O
target O
words O
. O
It O
is O
also O
interesting O
to O
measure O
the O
improvement O
obtained O
when O
no O
alignment O
information O
is O
used O
during O
training O
. O

Since O
we O
do O
n't O
know O
where O
the O
words O
align O
, O
we O
just O
assume O
that O
the O
ith O
unknown O
word O
on O
the O
target O
sentence O
aligns O
to O
the O
ith O
unknown O
word O
on O
the O
source O
sentence O
. O

The O
NoAlign B-MethodName
model O
with O
a O
simple O
mono O
- O
tone O
alignment O
assumption O
results O
in O
a O
modest O
gain O
of O
0.8 B-MetricValue
BLEU B-MetricName
points O
. O

If O
we O
train O
the O
model O
to O
predict O
the O
alignment O
, O
the O
Copyable B-MethodName
model O
will O
offer O
a O
slightly O
better O
gain O
than O
the O
1.0 B-MetricValue
or O
BLEU B-MetricName
model O
. O

This O
means O
that O
the O
NoAlign B-MethodName
model O
might O
not O
improve O
as O
much O
as O
the O
Copyable B-MethodName
model O
when O
applied O
to O
harder O
language O
pairs O
. O

The O
positional O
models O
( O
PosAll B-MethodName
and O
PosUnk B-MethodName
) O
improve O
translation O
performance O
by O
more O
than O
2 O
points O
. O

The O
model O
's O
output O
words O
can O
only O
be O
aligned O
with O
unknown O
input O
words O
, O
which O
is O
a O
big O
limitation O
. O

The O
PosUnk B-MethodName
model O
outperforms O
the O
PosAll B-MethodName
model O
in O
translation O
quality O
, O
which O
suggests O
that O
the O
LSTM O
is O
easier O
to O
train O
on O
shorter O
sequences O
. O

The O
sentence O
is O
saying O
that O
the O
PosUnk B-MethodName
models O
were O
trained O
with O
different O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
, O
3 B-HyperparameterValue
, O
4 B-HyperparameterValue
, O
and O
6 B-HyperparameterValue
. O

The O
gain O
obtained O
by O
the O
PosUnk B-MethodName
model O
increases O
as O
the O
overall O
accuracy O
of O
the O
model O
increases O
, O
which O
suggests O
that O
larger O
models O
are O
more O
accurate O
at O
identifying O
the O
correct O
source O
word O
. O

We O
also O
observe O
that O
, O
on O
average O
, O
each O
extra O
LSTM O
layer O
provides O
a O
roughly O
1.0 B-MetricValue
BLEU B-MetricName
point O
improvement O
, O
as O
demonstrated O
in O
Figure O
7 O
. O

I O
'm O
perplexed O
by O
the O
string O
of O
letters O
and O
numbers O
. O

We O
find O
it O
interesting O
to O
observe O
a O
strong O
correlation O
between O
the O
perplexity O
( O
our O
training O
objective O
) O
and O
translation O
quality O
as O
measured O
by O
BLEU B-MetricName
. O

In O
Figure O
8 O
, O
we O
compare O
the O
performance O
of O
a O
4 O
- O
layer O
LSTM O
using O
both O
perplexity O
and O
BLEU B-MetricName
scores O
at O
different O
points O
during O
training O
. O

On O
average O
, O
reducing O
perplexity O
by O
0.5 O
leads O
to O
roughly O
a O
1 O
point O
improvement O
. O

We O
present O
three O
sample O
translations O
of O
our O
best O
system O
in O
Table O
3 O
. O

Our O
technique O
significantly O
improves O
various O
NMT O
systems O
by O
up O
to O
BLEU B-MetricName
points O
, O
regardless O
of O
the O
architecture O
. O

Most O
importantly O
, O
we O
have O
established O
the O
first O
NMT O
system O
that O
outperformed O
the O
best O
MT O
system O
by O
BLEU B-MetricName
points O
on O
a O
WMT’14 B-DatasetName
contest O
dataset O
. O

Relation B-TaskName
extraction I-TaskName
is O
a O
popular O
tool O
for O
finding O
unknown O
relational O
facts O
from O
text O
. O

The O
results O
of O
our O
experiment O
show O
that O
our O
model O
can O
take O
advantage O
of O
multilingual O
texts O
and O
achieve O
significant O
improvements O
on O
re- B-TaskName
lation I-TaskName
extraction I-TaskName
compared O
to O
the O
baseline O
. O

Since O
existing O
knowledge O
bases O
are O
far O
from O
complete O
and O
new O
facts O
are O
growing O
infinitely O
, O
many O
works O
have O
been O
devoted O
to O
automated O
extraction O
of O
novel O
facts O
from O
various O
Web O
resources O
, O
where O
relation B-TaskName
extraction I-TaskName
( O
RE B-TaskName
) O
from O
plain O
texts O
is O
one O
of O
the O
most O
important O
knowledge O
sources O
. O

The O
most O
promising O
approach O
for O
relation B-TaskName
extraction I-TaskName
is O
distant O
supervision O
, O
which O
can O
automatically O
generate O
training O
instances O
by O
aligning O
knowledge O
bases O
and O
texts O
to O
address O
the O
issue O
of O
lacking O
supervised O
data O
( O
Mintz O
et O
al O
. O
, O
2009 O
; O
Riedel O
et O
al O
. O
, O
2010 O
; O
Hoffmann O
et O
al O
. O
, O
2011 O
; O
Surdeanu O
et O
al O
. O
, O
2012 O
) O
. O

The O
2016 O
study O
further O
employed O
a O
sentence O
- O
level O
attention O
mechanism O
in O
neural B-TaskName
relation I-TaskName
extraction I-TaskName
and O
achieved O
state O
- O
of O
- O
the O
- O
art O
performance O
. O

It O
is O
not O
easy O
to O
find O
and O
learn O
these O
relational O
patterns O
from O
complicated O
sentences O
for O
re- B-TaskName
lation I-TaskName
extraction I-TaskName
. O

The O
fact O
that O
patterns O
are O
consistent O
across O
languages O
provides O
us O
with O
additional O
clues O
that O
can O
help O
us O
learn O
the O
patterns O
for O
relation B-TaskName
extraction I-TaskName
. O

We O
propose O
Multi B-MethodName
- I-MethodName
lingual I-MethodName
Attention I-MethodName
- I-MethodName
based I-MethodName
Neural I-MethodName
Relation I-MethodName
Extraction I-MethodName
( O
MNRE B-MethodName
) O
while O
taking O
full O
consideration O
of O
these O
issues O
. O

As O
an O
important O
approach O
to O
enriching O
KBs O
, O
relation B-TaskName
extraction I-TaskName
has O
attracted O
many O
research O
interests O
. O

ABCDEFG O
typically O
classifies O
each O
entity O
pair O
into O
various O
relation O
types O
. O

al O
, O
2015 O
) O
, O
and O
graph O
- O
based O
model O
( O
Li O
et O
. O
al O
, O
2015 O
) O
. O


 O
Many O
works O
have O
been O
invested O
in O
relation B-TaskName
extraction I-TaskName
, O
including O
kernel O
- O
based O
model O
( O
Zelenko O
et O
al O
. O
, O
2003 O
) O
, O
embedding O
- O
based O
model O
( O
Gormley O
et O
. O
al O
, O
2015 O
) O
, O
and O
graph O
- O
based O
model O
( O
Li O
et O
. O
al O
, O
2015 O
) O
. O

Hoffmann O
et O
al O
. O
proposed O
a O
model O
for O
distant O
supervision O
for O
relation B-TaskName
extraction I-TaskName
as O
a O
multi O
- O
instance O
single O
- O
label O
learning O
problem O
. O

As O
far O
as O
we O
know O
, O
this O
is O
the O
first O
time O
anyone O
has O
tried O
to O
multi B-TaskName
- I-TaskName
lingual I-TaskName
neural I-TaskName
relation I-TaskName
extraction I-TaskName
. O

The O
scope O
of O
multi O
- O
lingual O
analysis O
has O
been O
widely O
considered O
in O
many O
tasks O
, O
such O
as O
sentiment O
analysis O
, O
cross O
- O
lingual O
document O
summarization O
, O
information O
retrieval O
in O
Web O
search O
, O
and O
so O
on O
. O

We O
describe O
our O
proposed O
MNRE B-MethodName
framework O
in O
detail O
in O
this O
section O
. O

MNRE B-MethodName
's O
key O
motivation O
is O
that O
the O
relation O
patterns O
in O
sentences O
of O
different O
languages O
should O
be O
substantially O
consistent O
for O
each O
relational O
fact O
, O
and O
MNRE B-MethodName
can O
utilize O
the O
pattern O
consistency O
and O
complementarity O
among O
languages O
to O
achieve O
better O
results O
for O
relation B-TaskName
ex- I-TaskName
traction I-TaskName
. O

There O
are O
two O
main O
components O
to O
the O
MNRE B-MethodName
framework O
: O
1 O
. O
Sentence O
Encoder O
. O

This O
sentence O
is O
saying O
that O
the O
MNRE B-MethodName
system O
further O
processes O
information O
by O
taking O
into O
account O
the O
importance O
of O
each O
piece O
of O
information O
( O
weighted O
attentions O
) O
and O
creating O
a O
global O
representation O
for O
relation O
prediction O
. O

First O
, O
the O
convolutional O
layer O
extracts O
local O
features O
by O
sliding O
a O
window O
of O
length O
l B-HyperparameterName
over O
the O
sentence O
and O
performing O
a O
convolution O
within O
each O
sliding O
window O
. O

3.2 O
Instead O
of O
using O
max O
pooling O
, O
we O
can O
use O
piecewise O
max O
pooling O
, O
which O
is O
a O
variation O
of O
CNN O
, O
to O
better O
capture O
the O
relation O
patterns O
in O
the O
input O
sentence.3.2 O
Multi O
- O
lingual O
AttentionTo O
exploit O
the O
information O
of O
the O
sentences O
from O
all O
languages O
, O
our O
model O
adopts O
two O
kinds O
of O
attention O
mechanisms O
, O
including O
: O
( O
1 O
) O
the O
mono O
- O
lingual O
attention O
which O
selects O
the O
informative O
sentences O
within O
one O
language O
and O
( O
2 O
) O
the O
cross O
- O
lingual O
attention O
which O
measures O
the O
pattern O
consistency O
among O
languages O
. O

To O
address O
the O
issue O
of O
incorrect O
labeling O
in O
distant O
supervision O
, O
we O
follow O
the O
idea O
of O
sentence O
- O
level O
attention O
( O
Lin O
et O
al O
. O
, O
2016 O
) O
and O
set O
mono O
- O
lingual O
attention O
for O
MNRE B-MethodName
. O

We O
propose O
cross O
- O
lingual O
attention O
for O
neural B-TaskName
relation I-TaskName
extraction I-TaskName
in O
addition O
to O
mono O
- O
lingual O
attention O
in O
order O
to O
better O
take O
advantage O
of O
multi O
- O
lingual O
data O
. O

Here O
we O
introduce O
the O
details O
of O
our O
MNRE B-MethodName
framework O
for O
learning O
and O
optimization O
. O

We O
create O
a O
new O
dataset O
to O
evaluate O
our O
framework O
that O
extracts O
relations O
between O
languages O
. O

The O
experiments O
in O
this O
study O
focus O
on O
relation B-TaskName
extraction I-TaskName
from O
two O
languages O
, O
without O
losing O
generality O
. O

The O
evaluation O
method O
assumes O
that O
if O
a O
RE O
system O
accurately O
ﬁnds O
more O
relational O
facts O
in O
KBs O
from O
the O
testing O
set O
, O
it O
will O
achieve O
better O
performance O
for O
rela- B-TaskName
tion I-TaskName
extraction I-TaskName
. O

The O
held O
- O
out O
evaluation O
is O
a O
quick O
way O
to O
measure O
RE B-TaskName
performance O
without O
having O
to O
do O
a O
time O
- O
consuming O
human O
evaluation O
. O

We O
report O
the O
precision B-MetricName
/ O
recall B-MetricName
curves O
as O
the O
evaluation O
metric O
in O
our O
experiments O
. O

We O
tune O
the O
parameters O
of O
our O
MNRE B-MethodName
framework O
using O
a O
grid O
search O
on O
the O
validation O
set O
. O

We O
set O
the O
iteration B-HyperparameterName
number O
to O
15 B-HyperparameterValue
for O
all O
training O
data O
. O

We O
select O
the O
CNN O
proposed O
in O
( O
Zeng O
et O
al O
. O
, O
2014 O
) O
and O
implement O
it O
ourselves O
, O
achieving O
comparable O
results O
to O
what O
the O
authors O
reported O
on O
their O
experimental O
dataset O
NYT104 B-DatasetName
. O

We O
compare O
the O
performance O
of O
our O
framework O
with O
the O
[ B-MethodName
P]CNN I-MethodName
model O
trained O
with O
only O
English O
data O
( O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
) O
, O
only O
Chinese O
data O
( O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
) O
, O
a O
joint O
model O
( O
[ B-MethodName
P]CNN+joint I-MethodName
) O
which O
predicts O
using O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
and O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
jointly O
, O
and O
another O
joint O
model O
with O
shared O
embeddings O
( O
[ B-MethodName
P]CNN+share I-MethodName
) O
which O
trains O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
and O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
with O
common O
relation O
embedding O
matrices O
. O

2 O
. O
We O
have O
the O
following O
observations O
: O


 O
1 O
) O
Both O
[ B-MethodName
P]CNN+joint I-MethodName
and O
[ B-MethodName
P]CNN+share I-MethodName
achieve O
better O
performances O
as O
compared O
to O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
and O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
. O

Even O
though O
CNN+share B-MethodName
has O
similar O
performance O
to O
CNN+joint B-MethodName
, O
it O
is O
a O
bit O
worse O
when O
recall B-MetricName
ranges O
from O
0.1 B-MetricValue
to O
0.2 B-MetricValue
. O

PCNN+share B-MethodName
does O
not O
perform O
as O
well O
as O
PCNN+joint B-MethodName
for O
most O
of O
the O
recall B-MetricName
range O
. O

The O
MNRE B-MethodName
model O
is O
more O
precise O
than O
the O
[ B-MethodName
P]CNN+joint I-MethodName
and O
[ B-MethodName
P]CNN+share I-MethodName
models O
over O
the O
entire O
range O
of O
recall O
. O

By O
testing O
different O
parameter O
combinations O
for O
these O
baseline O
models O
, O
we O
can O
see O
that O
neither O
[ B-MethodName
P]CNN+joint I-MethodName
or O
[ B-MethodName
P]CNN+share I-MethodName
can O
match O
the O
results O
of O
MNRE B-MethodName
, O
even O
when O
the O
size O
of O
the O
output O
layer O
is O
increased O
. O

Our O
proposed O
MNRE B-MethodName
model O
can O
successfully O
improve O
multi B-TaskName
- I-TaskName
lingual I-TaskName
relation I-TaskName
ex- I-TaskName
traction I-TaskName
by O
considering O
pattern O
consistency O
among O
languages O
, O
which O
is O
contrary O
to O
the O
current O
belief O
. O

The O
sentence O
shows O
that O
four O
relations O
have O
the O
highest O
and O
lowest O
Chinese O
- O
to O
- O
English O
and O
English O
- O
to O
- O
Chinese O
attention O
weights O
, O
respectively O
, O
with O
respect O
to O
the O
PlaceOfBirth O
relation O
in O
MNRE B-MethodName
. O

We O
also O
show O
the O
attention O
weights O
from O
CNN+Zh B-MethodName
and O
CNN+En B-MethodName
for O
comparison O
. O

We O
find O
from O
the O
table O
that O
all O
four O
sentences O
express O
the O
fact O
that O
Barzun O
was O
born O
in O
France O
, O
but O
the O
first O
and O
third O
sentences O
contain O
much O
more O
noisy O
information O
that O
may O
confuse O
RE B-TaskName
systems O
. O

By O
paying O
attention O
to O
how O
sentence O
patterns O
match O
up O
between O
two O
languages O
, O
MNRE B-MethodName
can O
more O
easily O
identify O
which O
of O
the O
second O
and O
fourth O
sentences O
unambiguously O
express O
the O
PlaceOfBirth O
relation O
, O
compared O
to O
CNN+Zh B-MethodName
and O
CNN+En B-MethodName
. O

We O
compare O
the O
following O
methods O
to O
demonstrate O
the O
effectiveness O
of O
considering O
pattern O
complementarity O
among O
languages O
: O
MNRE B-MethodName
for I-MethodName
English I-MethodName
( O
MNRE B-MethodName
- I-MethodName
En I-MethodName
) O
and O
MNRE B-MethodName
for I-MethodName
Chinese I-MethodName
( O
MNRE B-MethodName
- I-MethodName
Zh I-MethodName
) O
which O
only O
use O
the O
monolingual O
vectors O
to O
predict O
relations O
, O
and O
[ O
P]CNN O
- O
En O
and O
[ O
P]CNN O
- O
Zh O
models O
. O

We O
can O
see O
from O
the O
graph O
that O
MNRE B-MethodName
- I-MethodName
En I-MethodName
and O
MNRE B-MethodName
- I-MethodName
Zh I-MethodName
are O
better O
than O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
and O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
in O
almost O
the O
entire O
range O
of O
recall B-MetricName
. O

[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
, O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
, O
MNRE B-MethodName
- I-MethodName
En I-MethodName
, O
and O
MNRE B-MethodName
- I-MethodName
Zh I-MethodName
can O
all O
be O
jointly O
trained O
through O
multi O
- O
lingual O
attention O
, O
although O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
En I-MethodName
underperforms O
compared O
to O
[ B-MethodName
P]CNN I-MethodName
- I-MethodName
Zh I-MethodName
and O
MNRE B-MethodName
- I-MethodName
En I-MethodName
is O
comparable O
to O
MNRE B-MethodName
- I-MethodName
Zh I-MethodName
. O

Table O
5 O
provides O
more O
detailed O
results O
for O
some O
specific O
relations O
where O
the O
training O
instances O
are O
unbalanced O
on O
the O
English O
and O
Chinese O
sides O
. O

We O
can O
see O
from O
the O
table O
that O
, O
for O
the O
relation O
Contains O
, O
CNN B-MethodName
- I-MethodName
En I-MethodName
performs O
much O
worse O
than O
CNN B-MethodName
- I-MethodName
Zh I-MethodName
due O
to O
the O
lack O
of O
training O
data O
. O
This O
is O
because O
there O
are O
only O
1/7 O
as O
many O
English O
training O
instances O
as O
there O
are O
Chinese O
ones O
. O

Even O
though O
they O
used O
different O
training O
methods O
, O
MNRE(CNN)- B-MethodName
En I-MethodName
and O
MNRE(CNN)-Zh B-MethodName
both O
performed O
well O
, O
with O
MNRE(CNN)- B-MethodName
En I-MethodName
slightly O
outperforming O
MNRE(CNN)-Zh B-MethodName
. O

Even O
CNN B-MethodName
- I-MethodName
Zh I-MethodName
can O
not O
accurately O
predict O
results O
for O
the O
HeadquartersLoca- O
tion O
relation O
when O
the O
number O
of O
Chinese O
training O
instances O
is O
only O
1/9of O
the O
English O
ones O
. O

One O
possible O
reason O
for O
this O
is O
that O
the O
training O
data O
for O
this O
relation O
is O
insufficient O
- O
there O
are O
only O
210 O
Chinese O
examples O
. O

Similarly O
, O
by O
training O
together O
with O
multi O
- O
lingual O
attention O
, O
MNRE(CNN)-En B-MethodName
and O
MNRE(CNN)-Zh B-MethodName
both O
achieve O
promising O
results O
. O

For O
relations O
that O
are O
not O
unbalanced O
in O
terms O
of O
the O
number O
of O
Father O
and O
Country- O
Of O
- O
Citizenship O
sentences O
in O
English O
and O
Chinese O
, O
our O
MNRE B-MethodName
can O
still O
improve O
the O
performance O
of O
relation O
extraction O
on O
both O
English O
and O
Chinese O
sides O
. O

We O
compare O
the O
performance O
of O
MNRE B-MethodName
using O
both O
matrices O
with O
those O
only O
using O
M O
( O
MNRE B-MethodName
- I-MethodName
M I-MethodName
) O
and O
only O
using O
R O
( O
MNRE B-MethodName
- I-MethodName
R I-MethodName
) O
to O
measure O
the O
effect O
of O
the O
two O
relation O
matrices O
. O

We O
can O
see O
from O
the O
graph O
that O
MNRE B-MethodName
- I-MethodName
M I-MethodName
performs O
much O
worse O
than O
both O
MNRE B-MethodName
- I-MethodName
R I-MethodName
and O
MNRE B-MethodName
. O

MNRE(CNN)-R B-MethodName
and O
MNRE(CNN B-MethodName
) I-MethodName
have O
similar O
performance O
when O
the O
recall O
is O
low O
. O

However O
, O
there O
is O
a O
sharp O
decline O
when O
recall B-MetricName
reaches O
0.25 B-MetricValue
. O

We O
should O
combine O
both O
MandR O
together O
for O
multi B-TaskName
- I-TaskName
lingual I-TaskName
relation I-TaskName
extraction I-TaskName
, O
as O
proposed O
in O
our O
MNRE B-MethodName
framework O
. O

We O
introduce O
a O
framework O
that O
takes O
pattern O
consistency O
and O
complementarity O
among O
multiple O
languages O
into O
consideration O
. O

Our O
framework O
is O
effective O
at O
modeling O
relation O
patterns O
among O
languages O
and O
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
, O
as O
shown O
by O
our O
evaluation O
on O
the O
multi B-TaskName
- I-TaskName
lingual I-TaskName
relation I-TaskName
extrac- I-TaskName
tion I-TaskName
task O
. O

In O
this O
paper O
, O
we O
only O
consider O
sentence O
- O
level O
multi O
- O
lingual O
attention O
for O
relation B-TaskName
extraction I-TaskName
. O
However O
, O
we O
believe O
that O
this O
approach O
can O
be O
extended O
to O
document O
- O
level O
or O
even O
paragraph O
- O
level O
attention O
. O

This O
means O
that O
attention O
at O
the O
word O
level O
in O
multiple O
languages O
can O
help O
improve O
multi B-TaskName
- I-TaskName
lingual I-TaskName
relation I-TaskName
extraction I-TaskName
. O

Our O
future O
work O
will O
explore O
the O
effectiveness O
of O
word O
- O
level O
multi O
- O
lingual O
attention O
for O
relation B-TaskName
extraction I-TaskName
. O

This O
paper O
focuses O
on O
two O
languages O
, O
English O
and O
Chinese O
, O
which O
can O
be O
flexibly O
implemented O
in O
the O
scenario O
of O
multiple O
languages O
. O

We O
will O
extend O
MNRE B-MethodName
to O
more O
languages O
in O
the O
future O
and O
explore O
its O
significance O
. O

Probabilistic B-MethodName
FastText I-MethodName
is O
a O
multi O
- O
sense O
word O
embedding O
that O
can O
be O
used O
to O
improve O
the O
accuracy O
of O
your O
word O
embeddings O
. O

We O
introduce O
Probabilistic B-MethodName
FastText I-MethodName
, O
a O
new O
model O
for O
word O
embeddings O
that O
can O
capture O
multiple O
word O
senses O
, O
sub O
- O
word O
structure O
, O
and O
uncertainty O
information O
. O

Probabilistic B-MethodName
FastText I-MethodName
outperforms O
both O
F B-MethodName
ASTTEXT I-MethodName
and O
dictionary B-MethodName
- I-MethodName
level I-MethodName
probabilistic I-MethodName
embeddings I-MethodName
on O
several O
word O
- O
similarity O
benchmarks O
, O
including O
English B-DatasetName
RareWord I-DatasetName
and O
foreign O
language O
datasets O
. O

The O
search O
has O
been O
focused O
on O
finding O
vector O
word O
embeddings O
, O
such O
as O
W B-MethodName
ORD2VEC I-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
, O
where O
words O
with O
similar O
meanings O
are O
mapped O
to O
points O
that O
are O
close O
to O
each O
other O
in O
a O
vector O
space O
. O

Bojanowski O
et O
al O
. O
, O
2016 O
) O
F B-MethodName
ASTTEXT I-MethodName
is O
the O
current O
best O
practice O
for O
character O
- O
level O
embeddings O
. O

In O
the O
sentence O
F B-MethodName
ASTTEXT I-MethodName
, O
each O
word O
is O
represented O
by O
a O
sum O
of O
vectors O
, O
with O
each O
vector O
representing O
an O
n O
- O
gram O
. O

In O
this O
paper O
, O
we O
propose O
a O
method O
( O
PFT B-MethodName
) O
that O
provides O
probabilistic O
character O
- O
level O
representations O
of O
words O
. O

The O
sentence O
states O
that O
PFT B-MethodName
can O
model O
rare O
words O
, O
uncertainty O
information O
, O
hierarchical O
representations O
, O
and O
multiple O
word O
senses O
. O

We O
represent O
each O
word O
with O
a O
Gaussian O
or O
a O
Gaussian O
mixture O
density O
, O
which O
we O
name O
PFT B-MethodName
- I-MethodName
G I-MethodName
and O
PFT B-MethodName
- I-MethodName
GM I-MethodName
, O
respectively O
. O

We O
also O
develop O
an O
efficient O
energy O
- O
based O
max O
- O
margin O
training O
procedure O
for O
PFT B-MethodName
. O

We O
compare O
our O
model O
to O
existing O
density O
word O
embeddings O
F B-MethodName
ASTTEXT I-MethodName
, O
W2 B-MethodName
G I-MethodName
, O
and O
W2GM B-MethodName
. O

Our O
models O
are O
able O
to O
extract O
high O
- O
quality O
semantics O
based O
on O
multiple O
word O
- O
similarity O
benchmarks O
, O
including O
the O
rare B-DatasetName
word I-DatasetName
dataset O
. O

We O
average O
a O
weighted O
improvement O
of O
3.7% B-MetricValue
over O
F B-MethodName
ASTTEXT I-MethodName
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
and O
3.1 B-MetricValue
% I-MetricValue
over O
the O
dictionary B-MethodName
- I-MethodName
level I-MethodName
density I-MethodName
- I-MethodName
based I-MethodName
models I-MethodName
. O

Our O
models O
can O
be O
used O
on O
foreign O
languages O
without O
changing O
any O
hyperparameters O
, O
and O
we O
see O
good O
performance O
, O
outperforming O
F B-MethodName
AST- I-MethodName
TEXT I-MethodName
on O
many O
foreign O
word O
similarity O
datasets O
. O

Our O
models O
outperform O
a O
recent O
density O
embedding O
model O
on O
a O
benchmark O
that O
measures O
the O
ability O
to O
separate O
different O
word O
meanings O
. O

The O
W B-MethodName
ORD2VEC I-MethodName
method O
, O
which O
uses O
a O
log O
- O
linear O
model O
and O
negative O
sampling O
approach O
to O
extract O
rich O
semantics O
from O
text O
, O
was O
developed O
in O
2013 O
. O

Another O
popular O
approach O
, O
G B-MethodName
LOVE I-MethodName
, O
involves O
learning O
word O
embeddings O
by O
factorizing O
co O
- O
occurrence O
matrices O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

The O
sentence O
means O
that O
the O
2014 O
version O
of O
the O
software O
uses O
an O
external O
dataset O
called O
WORDNET B-DatasetName
to O
improve O
its O
ability O
to O
identify O
the O
meaning O
of O
words O
. O

We O
compare O
our O
multimodal O
embeddings O
to O
these O
models O
in O
Section O
4.3 O
. O

We O
represent O
each O
word O
with O
a O
Gaussian O
mixture O
of O
K B-HyperparameterName
Gaussian O
components O
. O

This O
sentence O
states O
that O
the O
structure O
of O
F B-MethodName
ASTTEXT I-MethodName
is O
similar O
to O
that O
of O
the O
author O
's O
study O
; O
however O
, O
the O
author O
's O
study O
uses O
single O
- O
prototype O
determinis- O
tic O
embeddings O
, O
while O
F B-MethodName
ASTTEXT I-MethodName
uses O
a O
multi O
- O
prototype O
probabilistic O
embedding O
. O

Figure O
1b O
and O
1c O
depict O
our O
models O
Gaussian B-MethodName
probabilistic I-MethodName
F I-MethodName
ASTTEXT I-MethodName
and O
PFT- B-MethodName
G I-MethodName
, O
as O
well O
as O
Gaussian B-MethodName
mixture I-MethodName
probabilistic I-MethodName
F I-MethodName
ASTTEXT I-MethodName
and O
PFT B-MethodName
- I-MethodName
GM I-MethodName
. O

If O
words O
are O
represented O
by O
distribution O
functions O
, O
we O
use O
the O
generalized O
dot O
product O
in O
Hilbert O
space O
, O
which O
is O
called O
the O
expected B-MetricName
likelihood I-MetricName
kernel I-MetricName
. O

This O
loss O
function O
, O
in O
combination O
with O
the O
Gaussian O
mixture O
model O
, O
can O
extract O
multiple O
senses O
of O
words O
. O

The O
term O
scale B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
in- I-HyperparameterName
verse I-HyperparameterName
covariance I-HyperparameterName
is O
in O
Equation O
3 O
. O

We O
generate O
a O
context O
word O
for O
a O
given O
word O
by O
picking O
a O
nearby O
word O
within O
a O
context O
window O
. O

We O
have O
proposed O
a O
model O
which O
combines O
the O
flexibility O
of O
subword O
structure O
with O
the O
density O
embedding O
approach O
. O

Our O
quantitative O
evaluation O
in O
Section O
4.3 O
demonstrates O
strong O
performance O
compared O
to O
the O
baseline O
models O
F B-MethodName
ASTTEXT I-MethodName
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
and O
the O
dictionary O
- O
level O
Gaussian O
( O
W2 O
G O
) O
( O
Vilnis O
and O
McCallum O
, O
2014 O
) O
and O
Gaussian O
mixture O
embed- O
dings O
( O
Athiwaratkun O
and O
Wilson O
, O
2017 O
) O
( O
W2GM O
) O
. O

The O
sentence O
is O
saying O
that O
English O
uses O
a O
combination O
of O
UKWAC B-DatasetName
and O
W B-DatasetName
ACKY I-DatasetName
PEDIA I-DatasetName
, O
which O
consists O
of O
3.376 O
billion O
words O
. O

We O
use O
three O
text O
corpora O
consisting O
of O
1.634 O
, O
1.716 O
, O
and O
1.955 O
billion O
words O
respectively O
. O
These O
corpora O
are O
in O
French O
, O
German O
, O
and O
Italian O
( O
Baroni O
et O
al O
. O
, O
2009 O
) O
. O

We O
have O
dictionaries O
with O
the O
following O
word O
counts O
: O
1:3 O
, O
2:7 O
, O
and O
1:4 O
million O
words O
for O
F B-DatasetName
RWAC I-DatasetName
, O
DEWAC B-DatasetName
, O
and O
ITWAC B-DatasetName
. O

The O
adjustable O
parameters O
in O
our O
models O
are O
loss B-HyperparameterName
margin I-HyperparameterName
m B-HyperparameterName
in O
Equation O
4 O
and O
scale B-HyperparameterName
. O

We O
search O
for O
the O
optimal O
hyperparameters O
in O
a O
grid O
of O
m B-HyperparameterName
2f O
0.01 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
1 B-HyperparameterValue
, O
10 B-HyperparameterValue
, O
100 B-HyperparameterValue
, O
g O
and O
. O

5*10-3/1 B-HyperparameterValue
, O
10-3/1 B-HyperparameterValue
, O
2*10-4/1 B-HyperparameterValue
, O
1*10-4/1 B-HyperparameterValue
can O
be O
found O
in O
our O
English O
corpus O
. O

The O
scale O
of O
the O
loss O
function O
affects O
how O
we O
adjust O
the O
learn- B-HyperparameterName
ing I-HyperparameterName
rate I-HyperparameterName
. O

The O
learning B-HyperparameterName
rates I-HyperparameterName
are O
particularly O
used O
. O

The O
sentence O
means O
that O
there O
are O
seven O
letters O
in O
the O
alphabet O
, O
with O
A O
being O
the O
first O
letter O
, O
G O
being O
the O
seventh O
letter O
, O
and O
0 O
, O
1 O
, O
2 O
being O
the O
eighth O
, O
ninth O
, O
and O
tenth O
letters O
, O
respectively O
. O

The O
other O
hyperparameters O
that O
are O
ﬁxed O
include O
K B-HyperparameterName
= O
2 B-HyperparameterValue
, O
con- B-HyperparameterName
text I-HyperparameterName
window I-HyperparameterName
length I-HyperparameterName
` O
= O
10 B-HyperparameterValue
and O
subsampling B-HyperparameterName
threshold I-HyperparameterName
t B-HyperparameterName
= O
10-5 B-HyperparameterValue
. O

We O
use O
n O
- O
grams O
where O
the O
setup O
is O
similar O
to O
F B-MethodName
AST I-MethodName
- I-MethodName
TEXT I-MethodName
. O

To O
estimate O
the O
mean O
vectors O
, O
3 B-HyperparameterValue
, O
4 B-HyperparameterValue
, O
5 B-HyperparameterValue
, O
and O
6 B-HyperparameterValue
are O
used O
. O

We O
evaluate O
our O
word O
embeddings O
on O
several O
standard O
datasets O
, O
including O
SL-999 B-DatasetName
( O
Hill O
et O
al O
. O
, O
2014 O
) O
, O
WS-353 B-DatasetName
( O
Finkelstein O
et O
al O
. O
, O
2002 O
) O
, O
MEN-3k B-DatasetName
( O
Bruni O
et O
al O
. O
, O
2014 O
) O
, O
MC-30 B-DatasetName
( O
Miller O
and O
Charles O
, O
1991 O
) O
, O
RG-65 B-DatasetName
( O
Rubenstein O
and O
Goodenough O
, O
1965 O
) O
, O
YP-130 B-DatasetName
( O
Yang O
and O
Powers O
, O
2006 O
) O
, O
MTurk(-287,-771 B-DatasetName
) I-DatasetName
( O
Radinsky O
et O
al O
. O
, O
2011 O
; O
Halawi O
et O
al O
. O
, O
2012 O
) O
, O
and O
RW-2k B-DatasetName
( O
Luong O
et O
al O
. O
, O
2013 O
) O
. O

We O
note O
that O
the O
dataset O
RW B-DatasetName
focuses O
more O
on O
infrequent O
words O
and O
SimLex-999 B-DatasetName
focuses O
on O
the O
similarity O
of O
words O
rather O
than O
relatedness O
. O

We O
compare O
PFT B-MethodName
- I-MethodName
GM I-MethodName
to O
other O
multi O
- O
prototype O
embeddings O
in O
the O
literature O
using O
. O

A O
word O
similar O
to O
SCWS B-DatasetName
was O
found O
in O
Huang O
et O
al O
. O
, O
2012 O
. O

We O
calculate O
the O
Spearman O
correlation O
coefficient O
between O
the O
labels O
and O
our O
scores O
generated O
by O
the O
embeddings O
. O

The O
Spearman B-MetricName
corre- I-MetricName
lation I-MetricName
is O
a O
correlation O
measure O
that O
ranks O
how O
well O
the O
scores O
describe O
the O
true O
labels O
. O

This O
sentence O
can O
be O
rewritten O
as O
: O
In O
the O
Gaussian O
case O
, O
this O
score O
reduces O
to O
a O
cosine O
similarity O
( O
K B-HyperparameterName
= O
1 B-HyperparameterValue
) O
. O

We O
compare O
our O
models O
with O
the O
dictionary- B-MethodName
level I-MethodName
Gaussian I-MethodName
and I-MethodName
Gaussian I-MethodName
mixture I-MethodName
embed- I-MethodName
dings I-MethodName
in O
Table O
2 O
, O
with O
50 O
- O
dimensional O
and O
300 O
- O
dimensional O
mean O
vectors O
. O

The O
results O
for O
W2G B-MethodName
and O
W2GM B-MethodName
in O
50 O
dimensions O
are O
taken O
directly O
from O
Athiwaratkun O
and O
Wilson O
's O
2017 O
study O
. O

We O
compare O
the O
public O
code3 O
to O
train O
the O
300- O
dimensional O
W2G B-MethodName
and O
W2GM B-MethodName
models O
against O
the O
publicly O
available O
F B-MethodName
ASTTEXT I-MethodName
model4 O
. O

We O
calculate O
Spearman B-MetricName
’s I-MetricName
correlations I-MetricName
for O
each O
word O
similarity O
dataset O
. O

Our O
PFT B-MethodName
- I-MethodName
GM I-MethodName
model O
outperforms O
both O
FASTTEXT B-MethodName
and O
the O
dictionary O
- O
level O
embeddings O
W2G B-MethodName
and O
W2GM B-MethodName
, O
achieving O
the O
highest O
average O
score O
among O
all O
competing O
models O
. O

Our O
unimodal O
model O
outperforms O
the O
dictionary O
- O
level O
counterparts O
W2 B-MethodName
G I-MethodName
and O
F B-MethodName
ASTTEXT I-MethodName
. O

The O
model O
W2GM B-MethodName
appears O
to O
be O
stronger O
than O
PFT B-MethodName
- I-MethodName
GM I-MethodName
, O
based O
on O
the O
results O
in O
Table O
2 O
. O

However O
, O
the O
datasets O
that O
W2GM B-MethodName
performs O
better O
than O
PFT B-MethodName
- I-MethodName
GM I-MethodName
often O
have O
small O
sizes O
, O
such O
as O
MC-30 B-DatasetName
or O
RG-65 B-DatasetName
, O
where O
the O
Spearman B-MetricName
’s I-MetricName
correlations I-MetricName
are O
more O
subject O
to O
noise O
. O

In O
general O
, O
PFT B-MethodName
- I-MethodName
GM I-MethodName
achieves O
better O
results O
than O
W2GM B-DatasetName
in O
300 O
and O
50 O
dimensional O
models O
, O
as O
measured O
by O
3.1% B-MetricValue
and O
8.7% B-MetricValue
. O

In O
addition O
, O
PFT B-MethodName
- I-MethodName
G I-MethodName
and O
PFT B-MethodName
- I-MethodName
GM I-MethodName
outperform O
F B-MethodName
ASTTEXT I-MethodName
by O
1.2% B-MetricValue
and O
3.7% B-MetricValue
, O
respectively O
. O

In O
Table O
3 O
, O
we O
compare O
the O
50- O
and O
300 O
- O
dimensional O
PFT B-MethodName
- I-MethodName
GM I-MethodName
models O
against O
the O
multi O
- O
prototype O
embeddings O
described O
in O
Section O
2 O
and O
the O
existing O
multimodal O
density O
embeddings O
W2GM B-MethodName
. O

The O
word O
similarity O
dataset O
SCWS B-DatasetName
( O
Huang O
et O
al O
. O
, O
2012 O
) O
can O
be O
used O
to O
distinguish O
senses O
by O
containing O
words O
with O
potentially O
many O
meanings O
. O

We O
use O
the O
maximum B-MetricName
similarity I-MetricName
score O
( O
denoted O
as O
M B-MetricName
AXSIM I-MetricName
) O
to O
calculate O
the O
results O
. O

The O
AVESIM B-MetricName
denotes O
the O
average B-MetricName
of I-MetricName
the I-MetricName
similarity I-MetricName
scores I-MetricName
, O
not O
the O
maximum O
. O

We O
outperform O
the O
dictionary O
- O
based O
density O
embeddings O
W2GM B-MethodName
in O
both O
50 O
and O
300 O
dimensions O
, O
demonstrating O
the O
benefits O
of O
subword O
in- O
. O

We O
use O
Italian O
models O
Italian B-DatasetName
W I-DatasetName
ORDSIM353 I-DatasetName
and O
Ital- B-DatasetName
ian I-DatasetName
S I-DatasetName
IMLEX-999 I-DatasetName
( O
Leviant O
and O
Reichart O
, O
2015 O
) O
, O
German O
models O
GUR350 B-DatasetName
and O
GUR65 B-DatasetName
( O
Gurevych O
, O
2005 O
) O
, O
and O
French O
model O
W B-DatasetName
ORD- I-DatasetName
SIM353 I-DatasetName
( O
Finkelstein O
et O
al O
. O
, O
2002 O
) O
. O

We O
use O
the O
results O
reported O
in O
the O
F B-MethodName
ASTTEXT I-MethodName
publication O
( O
Bojanowski O
et O
al O
. O
, O
2016 O
) O
for O
datasets O
GUR350 B-DatasetName
and O
GUR65 B-DatasetName
. O

We O
train O
F B-MethodName
ASTTEXT I-MethodName
models O
for O
comparison O
using O
the O
public O
code5on O
our O
text O
corpuses O
for O
other O
datasets O
. O

We O
also O
train O
dictionary O
- O
level O
models O
W2 B-MethodName
G I-MethodName
and O
W2GM B-MethodName
for O
comparison O
. O

Table O
4 O
presents O
the O
results O
of O
our O
models O
for O
the O
Spearman B-MetricName
’s I-MetricName
correlation I-MetricName
metrics O
. O

We O
are O
better O
than O
F B-MethodName
ASTTEXT I-MethodName
on O
many O
word O
similarity O
tests O
. O

Our O
results O
are O
significantly O
better O
than O
the O
dictionary O
- O
based O
models O
W2G B-MethodName
and O
W2GM B-MethodName
. O

We O
think O
that O
W2G B-MethodName
and O
W2GM B-MethodName
could O
do O
better O
than O
what O
has O
been O
reported O
so O
far O
, O
if O
the O
words O
were O
pre O
- O
processed O
to O
account O
for O
special O
characters O
like O
accents O
. O

It O
is O
possible O
to O
train O
our O
approach O
with O
a O
mixture O
of O
components O
; O
however O
, O
Athiwaratkun O
and O
Wilson O
( O
2017 O
) O
observe O
that O
dictionary O
- O
level O
Gaussian O
mixtures O
with O
do O
not O
overall O
improve O
word O
similarity O
results O
, O
even O
though O
these O
mixtures O
can O
discover O
distinct O
senses O
for O
certain O
words O
. O

K B-HyperparameterName
and O
> B-HyperparameterValue
2 I-HyperparameterValue
allow O
for O
more O
flexibility O
than O
K B-HyperparameterName
and O
2 B-HyperparameterValue
, O
but O
most O
words O
can O
be O
accurately O
modeled O
with O
a O
mixture O
of O
two O
Gaussians O
. O
This O
leads O
to O
K B-HyperparameterName
and O
2 B-HyperparameterValue
, O
which O
is O
a O
good O
balance O
between O
flexibility O
and O
Occam O
's O
razor O
. O

Even O
a O
model O
with O
a O
single O
meaning O
often O
learns O
richer O
representations O
than O
a O
model O
with O
multiple O
meanings O
. O

We O
observe O
that O
our O
model O
with O
more O
components O
can O
capture O
more O
meanings O
. O

For O
example O
, O
the O
word O
pairs O
( O
" O
cell O
" O
, O
" O
jail O
" O
) O
and O
( O
" O
cell O
" O
, O
" O
biology O
" O
) O
and O
( O
" O
cell O
" O
, O
" O
phone O
" O
) O
will O
all O
have O
positive O
similarity O
scores O
based O
on O
the O
1 B-HyperparameterValue
model O
. O

Other O
work O
that O
needs O
to O
be O
done O
involves O
training O
PFT B-MethodName
on O
many O
languages O
. O

The O
2018 O
Association O
for O
Computational O
Linguistics O
A O
Deep B-MethodName
Generative I-MethodName
Model I-MethodName
studied O
vowel O
formant O
typology O
. O

In O
our O
work O
, O
we O
propose O
a O
system O
of O
vowel O
typology O
, O
which O
classifies O
the O
vowels O
a O
language O
contains O
. O

The O
novel O
generative B-MethodName
probability I-MethodName
model I-MethodName
was O
used O
to O
report O
results O
based O
on O
a O
corpus O
of O
233 O
languages O
. O

In O
this O
work O
, O
we O
provide O
a O
more O
formal O
treatment O
of O
the O
subject O
by O
deriving O
a O
typology O
of O
vowel O
inventory O
. O

We O
propose O
a O
Bayesian O
generative B-MethodName
model I-MethodName
of O
vowel O
inventories O
, O
where O
each O
language O
's O
inven- O


 O
tory O
is O
modelled O
as O
a O
distinct O
multinomial O
distribution O
. O

Cotterell O
and O
Eisner O
( O
2017 O
) O
proposed O
that O
a O
determinantal O
point O
process O
( O
DPP B-MethodName
) O
can O
be O
used O
to O
over O
a O
universal O
inventory O
of O
53 O
symbolic O
( O
IPA O
) O
vowels O
. O

A O
language O
- O
specific O
inventory O
of O
vowel O
phonemes O
is O
a O
set O
of O
vowel O
sounds O
that O
are O
specific O
to O
a O
particular O
language O
. O

In O
this O
paper O
, O
we O
state O
that O
a O
language O
draws O
its O
inventory O
from O
a O
larger O
set O
using O
a O
DPP B-MethodName
. O

The O
reason O
to O
use O
a O
DPP B-MethodName
in O
both O
cases O
is O
that O
it O
prefers O
relatively O
diverse O
inventories O
that O
have O
individual O
elements O
that O
are O
relatively O
quantal O
. O

We O
assume O
that O
language O
lscript O
consists O
of O
a O
subset O
of O
the O
available O
symbols O
, O
which O
we O
call O
n. O

5 O
Determinantal O
Point O
Processes O
: O
Before O
delving O
into O
our O
generative B-MethodName
model I-MethodName
, O
we O
briefly O
review O
the O
technical O
background O
used O
by O
Cotterell O
and O
Eisner O
( O
2017 O
) O
. O

A O
probability O
distribution O
over O
the O
subsets O
of O
a O
fixed O
ground O
set O
of O
size O
N O
is O
called O
an O
DPP B-MethodName
. O
It O
is O
usually O
parameterized O
by O
a O
positive O
semi O
- O
definite O
matrix O
L O
∈ O
RN×N O
, O
meaning O
that O
it O
is O
given O
as O
an O
L O
- O
ensemble O
( O
Borodin O
and O
Rains O
, O
2005 O
) O
. O
The O
probability O
of O
a O
subset O
V O
⊆ O
V O
is O
given O
by O
p(V)∝det(LV O
) O
, O
where O
LV O
is O
the O
submatrix O
of O
L O
corresponding O
to O
the O
rows O
and O
columns O
associated O
with O
the O
subset O
V O
⊆ O
V. O
The O
entry O
Lij O
, O
where O
i O
≠ O
j O
, O
has O
the O
effect O
of O
describing O
the O
similarity O
between O
the O
elements O
v_i O
and O
v_j O
( O
both O
in O
V)—an O
ingredient O
needed O
to O
model O
dispersion O
. O

The O
joint O
likelihood O
of O
Mvowel O
systems O
under O
our O
deep O
generative B-MethodName
probability I-MethodName
model I-MethodName
for O
continuous O
- O
space O
vowel O
inventories O
is O
shown O
in O
Figure O
2 O
. O

Factor O
3 O
takes O
into O
account O
the O
prior O
probability O
of O
¯V(a O
/ O
lscript O
) O
under O
the O
DPP B-MethodName
, O
and O
factor O
4 O
is O
a O
likelihood O
term O
that O
looks O
at O
the O
probability O
of O
the O
related O
pronunciations O
. O

Since O
L O
is O
positive O
definite O
and O
not O
merely O
positive O
semidefinite O
, O
it O
can O
be O
used O
to O
parameterize O
a O
distribution O
over O
V. O
Indeed O
, O
this O
distribution O
will O
assign O
positive O
probability O
to O
any O
subset O
of O
V. O
As O
previously O
noted O
, O
this O
distribution O
does O
not O
define O
a O
distribution O
over O
an O
infinite O
set O
, O
e.g. O
the O
powerset O
of O
R2 O
, O
as O
does O
recent O
work O
on O
continuous O
DPPs O
( O
Affandi O
et O
al O
. O
, O
2013 O
) O
. O

We O
are O
now O
able O
to O
explain O
our O
view O
of O
vowel O
types O
in O
continuous O
space O
. O

We O
sample O
a O
diverse O
subset O
of O
the O
phones O
for O
each O
language O
via O
a O
single O
draw O
from O
a O
matrix O
parameterized O
by O
L. O

This O
means O
that O
the O
language O
contains O
the O
phone O
, O
even O
though O
the O
size O
of O
the O
inventory O
was O
chosen O
by O
the O
DPP B-MethodName
. O

Let O
νθ O
be O
a O
perceptron O
νθ(˜ O
vi O
) O
where O
depth-2 B-HyperparameterName
multi I-HyperparameterName
- I-HyperparameterName
layer I-HyperparameterName
is O
an O
input O
vector O
. O

This O
will O
be O
true O
as O
long O
as O
W1 O
and O
W2 O
are O
square O
matrices O
of O
full O
- O
rank O
and O
we O
choose O
a O
smooth O
, O
invertible O
activation O
function O
, O
such O
as O
tanh B-MethodName
. O

This O
technique O
allows O
us O
to O
use O
a O
discrete O
rather O
than O
a O
continuous O
DPP B-MethodName
over O
the O
R2space O
, O
which O
is O
why O
a O
neural O
network O
is O
beneficial O
. O

We O
use O
the O
MAP B-MethodName
- I-MethodName
EM I-MethodName
model O
( O
Dempster O
et O
al O
. O
, O
1977 O
) O
to O
learn O
and O
make O
inferences O
. O

Inference O
in O
our O
model O
is O
difficult O
to O
calculate O
, O
even O
when O
the O
phones O
µ1 O
, O
... O
, O
µNare O
known O
. O

This O
sentence O
is O
saying O
that O
the O
phone O
inventory O
for O
a O
language O
is O
determined O
by O
a O
, O
and O
that O
they O
are O
using O
a O
method O
related O
to O
Gibbs O
sampling O
to O
do O
this O
. O

It O
sweeps O
S= B-HyperparameterName
5 B-HyperparameterValue
times O
2Taken O
from O
V O
olkovs O
and O
Zemel O
( O
2012 O
, O
3.1).42 O
. O


 O
This O
sentence O
states O
that O
something O
sweeps O
S= B-HyperparameterName
5 B-HyperparameterValue
times O
2 O
. O

We O
are O
also O
interested O
in O
automatically O
choosing O
the O
number O
of O
phones O
N O
, O
for O
which O
we O
take O
the O
Poisson O
rate O
parameter O
λ= B-HyperparameterName
100 B-HyperparameterValue
. O

We O
use O
the O
reversible O
- O
jump O
MCMC B-MethodName
( O
Green O
, O
1995 O
) O
, O
resampling O
at O
the O
start O
of O
every O
E O
- O
step O
, O
to O
achieve O
this O
goal O
. O

We O
achieved O
this O
by O
training O
the O
diffeomorphism O
parameters O
θ O
, O
the O
means O
µiof O
the O
Gaussian O
phones O
, O
and O
the O
parameters O
of O
the O
focalization O
kernel O
F. O

Our O
work O
's O
key O
technical O
innovation O
lies O
in O
incorporating O
a O
DPP B-MethodName
into O
a O
generative B-MethodName
model I-MethodName
of O
vowel O
formants O
— O
a O
continuous O
- O
valued O
quantity O
. O

The O
role O
of O
the O
DPP B-MethodName
is O
to O
model O
the O
linguistic O
principle O
of O
dispersion O
. O
We O
may O
cripple O
this O
portion O
of O
our O
model O
by O
forcing O
K O
to O
be O
a O
diagonal O
kernel O
, O
which O
means O
that O
Kij O
= O
0 O
for O
i O
≠ O
j. O

In O
this O
case O
, O
the O
DPP B-MethodName
becomes O
a O
Bernoulli O
Point O
Process O
( O
BPP O
) O
- O
a O
special O
case O
of O
the O
DPP B-MethodName
. O

The O
second O
baseline O
is O
Removing B-TaskName
the I-TaskName
neural I-TaskName
network I-TaskName
vth O
. O

Our O
dataset O
only O
contains O
50 O
IPA O
symbols O
, O
so O
the O
baseline O
is O
only O
reported O
for O
N= B-HyperparameterName
50 B-HyperparameterValue
. O

In O
metric O
DPP B-MethodName
, O
the O
addition O
of O
νθBPP O
and O
νθDPP O
minus O
νθSup O
results O
in O
a O
new O
value O
. O

The O
lower O
the O
Cross B-MetricName
- I-MetricName
entropy I-MetricName
score O
in O
nats O
per O
language O
and O
the O
lower O
the O
expected O
Euclidean O
- O
distance O
error O
of O
the O
cloze O
prediction O
, O
the O
better O
the O
results O
. O

We O
compare O
the O
case O
N= B-HyperparameterName
50 B-HyperparameterValue
against O
our O
supervised O
baseline O
. O

The O
N= B-HyperparameterName
57 B-HyperparameterValue
row O
is O
the O
case O
where O
we O
allowed O
N O
to O
fluctuate O
during O
inference O
using O
reversible O
- O
jump O
MCMC B-MethodName
. O
This O
was O
the O
value O
of O
N O
that O
was O
selected O
at O
the O
final O
EM O
iteration O
. O

The O
sampler O
's O
inference O
is O
based O
on O
the O
observed O
vowels O
( O
the O
likelihood O
) O
and O
the O
focalization O
- O
dispersion O
preferences O
of O
the O
DPP B-MethodName
( O
the O
prior O
) O
. O

A O
graph O
showing O
the O
phone O
inventories O
for O
all O
training O
languages O
, O
with O
each O
phone O
color O
- O
coded O
according O
to O
the O
inferred O
phones O
( O
N= B-HyperparameterName
50 B-HyperparameterValue
) O
. O

Our O
dif O
- O
feomorphism O
constraint O
requires O
that O
the O
number O
of O
observed O
formants O
is O
the O
same O
for O
each B-HyperparameterName
layer I-HyperparameterName
and O
two B-HyperparameterValue
hidden I-HyperparameterValue
units I-HyperparameterValue
. O

We O
consider O
N O
phones O
to O
be O
fluctuating O
with O
a O
reversible O
jump O
MCMC B-MethodName
( O
see O
footnote O
1 O
) O
. O

We O
train O
for O
100 B-HyperparameterName
iterations I-HyperparameterName
of O
EM O
, O
taking O
S=5 O
samples O
at O
each O
E O
- O
step O
. O

At O
every O
M O
- O
step O
, O
we O
run O
50 B-HyperparameterName
iterations I-HyperparameterName
for O
the O
focalization O
NN O
and O
also O
for O
the O
diffeomorphism O
NN O
. O

Our O
DPP B-MethodName
model O
is O
an O
improvement O
over O
the O
baselines O
. O

We O
also O
observe O
that O
as O
we O
increase O
the O
number O
of O
phones O
, O
the O
role O
of O
the O
DPP B-MethodName
becomes O
more O
important O
. O

Our O
approach O
is O
different O
in O
that O
we O
construct O
a O
model O
, O
whose O
parameters O
we O
learn O
from O
data O
. O

We O
presented O
a O
set O
of O
measured O
pairs O
consisting O
of O
( O
F1,F2 O
) O
. O

We O
believe O
that O
this O
is O
a O
necessary O
step O
in O
the O
development O
of O
generative B-MethodName
probability I-MethodName
model I-MethodName
that O
can O
help O
explain O
the O
distribution O
of O
the O
world O
’s O
languages O
. O

Roy O
is O
a O
very O
smart O
boy O
. O

Roy O
is O
a O
very O
smart O
boy O
. O

The O
deep O
inventories O
of O
vowels O
allows O
for O
a O
great O
deal O
of O
diversity O
in O
vocalization O
. O

The O
Cloze B-MetricName
procedure O
is O
a O
new O
tool O
for O
measuring O
readability O
. O

At O
the O
International O
Conference O
on O
Machine O
Learning O
, O
pages O
1105 O
- O
1112 O
, O
researchers O
presented O
their O
findings O
on O
how O
machine O
learning O
can O
improve O
various O
aspects O
of O
life O
. O

kruszewski@fb.com O


 O
The O
emergence O
of O
number O
and O
syntax O
units O
in O
LSTM B-TaskName
language O
models O
is O
a O
result O
of O
the O
cognitive O
neuroimaging O
unit O
's O
research O
at O
the O
NeuroSpin O
center O
in O
France O
. O

Recent O
work O
has O
shown O
that O
a O
model O
trained O
on O
a O
generic O
language O
modeling O
objective O
captures O
syntax O
- O
sensitive O
generalizations O
, O
such O
as O
long O
- O
distance O
number O
agreement O
. O

We O
provide O
a O
detailed O
study O
of O
how O
individual O
neurons O
keep O
track O
of O
numbers O
in O
the O
LSTMs B-TaskName
network O
. O

Our O
conclusion O
is O
that O
LSTMs B-TaskName
are O
, O
to O
some O
extent O
, O
using O
genuinely O
syntactic O
processing O
mechanisms O
, O
which O
could O
lead O
to O
a O
more O
general O
understanding O
of O
grammatical O
encoding O
in O
LSTMs B-TaskName
. O

In O
recent O
years O
, O
recurrent O
neural O
networks O
( O
RNNs O
) O
have O
been O
successfully O
applied O
to O
a O
variety O
of O
tasks O
, O
especially O
those O
involving O
long O
- O
term O
dependencies O
. O

So O
far O
, O
this O
debate O
has O
mostly O
been O
based O
on O
" O
behavioral O
" O
evidence O
: O
The O
LSTM B-TaskName
has O
been O
treated O
as O
a O
black O
box O
, O
and O
its O
capabilities O
have O
been O
indirectly O
inferred O
from O
its O
performance O
on O
linguistic O
tasks O
. O

In O
this O
study O
, O
we O
took O
an O
approach O
inspired O
by O
neuroscience O
that O
investigates O
the O
inner O
dynamics O
of O
an O
LSTM B-TaskName
language O
model O
to O
understand O
how O
it O
accomplishes O
the O
task O
of O
agreement O
. O

We O
found O
that O
the O
LSTM B-TaskName
had O
specialized O
two O
“ O
grand- O
mother O
” O
cells O
( O
Bowers O
, O
2009 O
) O
to O
carry O
number O
features O
from O
the O
subject O
to O
the O
verb O
across O
the O
intervening O
material O
. O
Interestingly O
, O
the O
LSTM B-TaskName
also O
specialized O
two O
“ O
grand- O
mother O
” O
cells O
to O
carry O
number O
features O
from O
the O
subject O
to O
the O
verb O
across O
the O
intervening O
material O
. O

Our O
analysis O
provides O
evidence O
that O
LSTMs B-TaskName
, O
which O
is O
trained O
on O
unannotated O
corpus O
data O
and O
lacks O
significant O
linguistic O
priors O
, O
can O
learn O
to O
perform O
structure O
- O
dependent O
linguistic O
operations O
. O

This O
suggests O
that O
mechanisms O
such O
as O
those O
implemented O
in O
LSTMs B-TaskName
, O
which O
take O
in O
raw O
linguistic O
input O
and O
store O
generic O
information O
, O
may O
be O
enough O
to O
induce O
the O
formation O
of O
complex O
grammatical O
rules O
. O

In O
2018 O
, O
al O
. O
carried O
out O
a O
study O
that O
showed O
that O
LSTM B-TaskName
language O
models O
perform O
almost O
as O
well O
as O
humans O
on O
the O
agreement O
task O
. O

Gulordava O
and O
colleagues O
provided O
some O
evidence O
that O
the O
LSTMs B-TaskName
are O
relying O
on O
genuine O
syntactic O
generalizations O
, O
but O
Kuncoro O
et O
al O
. O
( O
2018a O
) O
and O
Linzen O
and O
Leonard O
( O
2018 O
) O
suggested O
that O
the O
LSTM B-TaskName
achievements O
can O
, O
at O
least O
in O
part O
, O
be O
accounted O
for O
by O
superficial O
heuristics O
( O
e.g. O
, O
“ O
percolate O
the O
number O
of O
the O
first O
noun O
in O
a O
sentence O
” O
) O
. O

Kementchedjhieva O
and O
Lopez O
found O
a O
character O
- O
level O
RNN B-TaskName
to O
track O
morpheme O
boundaries O
in O
a O
single O
cell O
. O

We O
are O
not O
aware O
of O
any O
other O
studies O
that O
systematically O
characterize O
the O
processing O
of O
a O
linguistic O
phenomenon O
at O
the O
level O
of O
cell O
dynamics O
, O
as O
is O
attempted O
in O
the O
study O
hereby O
presented O
. O

al O
. O


 O
We O
study O
the O
pretrained O
LSTM B-TaskName
language O
model O
made O
available O
by O
Gu O
- O
lordava O
et O
al O
. O

This O
model O
has O
an O
650 B-HyperparameterValue
- I-HyperparameterValue
dimensional I-HyperparameterValue
embedding B-HyperparameterName
layer I-HyperparameterName
, O
two O
650 B-HyperparameterValue
- I-HyperparameterValue
dimensional I-HyperparameterValue
hidden B-HyperparameterName
layers I-HyperparameterName
, O
and O
an O
output O
layer O
with O
vocabulary B-HyperparameterName
size I-HyperparameterName
50,000 B-HyperparameterValue
. O

The O
model O
was O
trained O
on O
Wikipedia B-DatasetName
data I-DatasetName
without O
focusing O
on O
number O
agreement O
, O
and O
still O
obtained O
perplexity O
close O
to O
state O
of O
the O
art O
in O
the O
experiments O
of O
Gulordava O
et O
al.2 O
Number O
- O
Agreement O
Tasks O
. O
We O
complement O
analysis O
of O
the O
naturalistic O
, O
corpus B-DatasetName
- O
derived O
number O
- O
agreement O
test O
set O
of O
Linzen O
. O

The O
situation O
is O
illustrated O
in O
Table O
2 O
, O
which O
shows O
that O
there O
are O
4 O
answers O
in O
which O
the O
subject O
of O
the O
sentence O
is O
a O
boy O
, O
and O
there O
are O
3 O
answers O
in O
which O
the O
subject O
is O
the O
guy O
. O


 O
2.2.2.2 O
. O
Number O
of O
answers O
and O
subject O


 O
The O
table O
below O
shows O
a O
summary O
of O
the O
answers O
that O
were O
given O
by O
the O
participants O
in O
response O
to O
the O
singular O
sentences O
. O


 O
Table O
2 O
: O
Number O
of O
answers O
according O
to O
the O
subject O
of O
the O
sentence O


 O
Number O
of O
answers O
according O
to O
the O
subject O
of O
the O
sentence O
Subject O
boy O
guy O
Total O
number O
of O
answers O
4 O
3 O
7 O


 O
The O
situation O
is O
illustrated O
in O
Table O
2 O
, O
which O
shows O
that O
there O
are O
4 O
answers O
in O
which O
the O
subject O
of O
the O
sentence O
is O
a O
boy O
, O
and O
there O
are O
3 O
answers O
in O
which O
the O
subject O
is O
the O
guy O
. O


 O
2.2.2.3 O
. O
Number O
of O
answers O
according O
to O
type O
of O
answer O


 O
Table O
3 O
shows O
a O
summary O
of O
the O
answers O
that O
were O
given O
by O
the O
participants O
in O
response O
to O
the O
singular O
sentences O
. O


 O
Table O
3 O
: O
Number O
of O
answers O
according O
to O
the O
type O
of O
answer O


 O
Number O
of O
answers O
according O
to O
the O
type O
of O
answer O
Type O
of O
answer O
Simple O
Adv O
Adv+Adv O
Adv+NounPP O
Adv+NounPP+Adv O
Total O
number O
of O
answers O
1 O
1 O
3 O
1 O
1 O
7 O

Each O
synthetic O
number O
- O
agreement O
task O
( O
e.g. O
NA B-TaskName
- O
task O
) O
instantiates O
a O
fixed O
syntactic O
structure O
with O
varied O
lexical O
material O
, O
in O
order O
to O
probe O
subject O
- O
verb O
number O
agreement O
in O
controlled O
and O
increasingly O
challenging O
setups O
. O
The O
different O
structures O
are O
illustrated O
in O
Table O
1 O
, O
where O
all O
forms O
are O
in O
the O
singular O
. O

For O
every O
task O
NA B-TaskName
, O
we O
generated O
both O
a O
singular O
and O
plural O
version O
of O
each O
sentence O
. O

For O
all O
tasks O
in O
the O
set O
NA B-TaskName
, O
each O
condition O
consisted O
of O
600 O
sentences O
. O
We O
probed O
the O
model O
's O
ability O
to O
implicitly O
parse O
syntax O
by O
testing O
it O
on O
various O
conditions O
. O

The O
final O
data O
set O
contains O
4,033 O
positions O
from O
1,303 O
sentences O
. O
To O
successfully O
perform O
the O
NA B-TaskName
- O
task O
, O
the O
LSTM B-TaskName
should O
: O
( O
1 O
) O
encode O
and O
store O
the O
grammatical O
number O
of O
the O
subject O
; O
and O
( O
2 O
) O
track O
the O
main O
subject O
- O
verb O
syntactic O
dependency O
. O

This O
section O
describes O
the O
' O
neural O
circuit O
' O
that O
encodes O
and O
processes O
information O
in O
the O
LSTM B-TaskName
. O

We O
first O
tested O
the O
performance O
of O
the O
LSTM B-TaskName
on O
the O
Linzen O
's O
data O
and O
on O
the O
NA B-TaskName
- O
tasks O
in O
Table O
1 O
. O

We O
computed O
the O
likelihood O
that O
the O
LSTM B-TaskName
assigns O
to O
the O
main O
verb O
of O
each O
sentence O
given O
the O
preceding O
context O
and O
compared O
it O
to O
the O
likelihood O
it O
assigns O
to O
the O
wrong O
verb O
inflection O
. O

Our O
results O
on O
the O
Linzen O
NA B-TaskName
- O
task O
confirm O
previous O
findings O
by O
Gulordava O
et O
al O
. O
( O
2018 O
) O
. O

The O
results O
of O
the O
other O
NA B-TaskName
tasks O
show O
that O
some O
tasks O
and O
conditions O
are O
more O
difficult O
than O
others O
. O

The O
task O
at O
hand O
is O
better O
than O
the O
one O
before O
it O
, O
which O
is O
better O
than O
the O
one O
before O
that O
. O

Second O
, O
as O
expected O
, O
network O
performance O
is O
reduced O
in O
conditions O
where O
the O
numbers O
do O
n't O
match O
( O
the O
namePP B-TaskName
, O
nounPP B-TaskName
, O
and O
nounPPAdv B-TaskName
conditions O
) O
. O

The O
results O
of O
the O
ablation O
experiments O
are O
shown O
in O
Table O
2 O
. O

The O
percentage O
of O
accuracy O
in O
all O
tasks O
NA B-TaskName
. O

For O
long O
- O
range O
dependencies O
, O
encoding O
a O
singular O
subject O
across O
an O
interfering O
noun O
is O
more O
difficult O
than O
a O
plural O
subject O
: O
for O
both O
nounPP B-TaskName
and O
nounPPAdv B-TaskName
, O
PS O
is O
easier O
than O
SP O
. O

If O
these O
units O
were O
removed O
, O
it O
would O
lead O
to O
a O
significant O
decrease O
in O
performance O
on O
the O
NA B-TaskName
tasks O
. O

We O
tested O
the O
network O
by O
fixing O
the O
activation O
of O
each O
unit O
to O
zero O
, O
one O
at O
a O
time O
, O
and O
tested O
it O
on O
the O
NA B-TaskName
- O
tasks O
. O

The O
columns O
in O
Table O
2 O
labeled O
776 O
and O
988 O
had O
the O
biggest O
impact O
on O
network O
performance O
. O
Without O
those O
columns O
, O
network O
performance O
would O
be O
reduced O
by O
more O
than O
10 B-MetricValue
% I-MetricValue
under O
various O
conditions O
. O
It O
's O
especially O
important O
to O
note O
that O
those O
columns O
were O
the O
only O
ones O
whose O
removal O
consistently O
brought O
network O
performance O
down O
to O
around O
the O
level O
of O
chance O
in O
the O
more O
difficult O
incongruent O
conditions O
of O
the O
namePP B-TaskName
, O
nounPP B-TaskName
, O
and O
nounPPAdv B-TaskName
tasks O
. O

Although O
the O
Linzen O
NA B-TaskName
- O
task O
contained O
mixed O
stimuli O
from O
many O
types O
of O
conditions O
, O
we O
found O
that O
the O
plural O
unit O
had O
a O
substantial O
effect O
on O
average O
network O
performance O
. O

This O
case O
did O
not O
show O
a O
similar O
effect O
with O
singular O
units O
, O
which O
highlights O
the O
importance O
of O
using O
carefully O
crafted O
stimuli O
, O
as O
in O
the O
nounPP B-TaskName
and O
nounPPAdv B-TaskName
tasks O
, O
to O
understand O
network O
dynamics O
. O

The O
task O
we O
focus O
on O
is O
the O
simplest O
task O
that O
includes O
a O
long O
- O
range O
dependency O
with O
an O
interfering O
noun O
, O
in O
both O
SP O
and O
PS O
conditions O
. O

Recall O
the O
standard O
memory O
update O
and O
output O
rules O
LSTM B-TaskName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

We O
do O
not O
make O
precise O
predictions O
for O
gate O
and O
cell O
activity O
at O
time O
points O
unrelated O
to O
solving O
the O
NA B-TaskName
- O
task O
, O
so O
we O
mask O
them O
with O
white O
. O

The O
update O
rule O
for O
the O
LSTM B-TaskName
cell O
has O
two O
terms O
( O
Eq O
. O
1 O
) O
. O
The O
first O
term O
, O
ft*Ct-1 O
, O
the O
forget O
gate O
controls O
whether O
to O
keep O
the O
previous O
cell O
content O
( O
ft=1 O
: O
perfect O
remembering O
) O
or O
forget O
it O
( O
ft=0 O
: O
complete O
forgetting O
) O
. O

The O
performance O
on O
the O
easier O
tasks O
was O
not O
impaired O
by O
the O
single O
- O
unit O
ablations O
. O

We O
trained O
a O
linear O
model O
to O
predict O
the O
grammatical O
number O
of O
the O
subject O
from O
network O
activity O
in O
response O
to O
the O
presentation O
of O
the O
subject O
, O
and O
tested O
its O
prediction O
on O
test O
sets O
from O
all O
time O
points O
( O
King O
and O
Dehaene O
, O
2014 O
) O
, O
in O
incongruent O
conditions O
only O
of O
the O
nounPP B-TaskName
task O
. O

We O
evaluated O
model O
performance O
using O
Area B-MetricName
under I-MetricName
of I-MetricName
Curve I-MetricName
( O
AUC B-MetricName
) O
. O

The O
results O
show O
that O
number O
information O
can O
be O
decoded O
efficiently O
from O
other O
units O
in O
the O
network O
, O
and O
that O
this O
information O
can O
be O
carried O
for O
several O
time O
steps O
up O
to O
the O
second O
determiner O
. O

However O
, O
the O
way O
in O
which O
these O
units O
encode O
number O
is O
sensitive O
to O
the O
last O
encountered O
noun O
, O
decreasing O
from O
AUC B-MetricName
. O

17a O
. O
The O
activity O
of O
syntax O
unit O
1150 O
while O
processing O
various O
syntactic O
structures O
is O
shown O
in O
Figure O
3 O
. O


 O
17b O
. O
The O
activity O
of O
syntax O
unit O
1150 O
while O
processing O
various O
syntactic O
structures O
is O
shown O
in O
Figure O
3 O
. O


 O
17c O
. O
The O
activity O
of O
syntax O
unit O
1150 O
while O
processing O
various O
syntactic O
structures O
is O
shown O
in O
Figure O
3 O
. O


 O
17d O
. O
The O
activity O
of O
syntax O
unit O
1150 O
while O
processing O
various O
syntactic O
structures O
is O
shown O
in O
Figure O
3 O
. O

The O
average O
value O
across O
all O
stimuli O
in O
an O
NA B-TaskName
- O
task O
is O
shown O
, O
with O
error O
bars O
representing O
the O
standard O
deviation O
. O

The O
task O
stimuli O
were O
specifically O
generated O
for O
this O
visualization O
. O

The O
units O
were O
only O
sensitive O
to O
the O
last O
encountered O
noun O
in O
regards O
to O
subject O
number O
and O
activity O
swaps O
. O

To O
validate O
the O
role O
of O
SR O
- O
number O
units O
in O
encoding O
number O
for O
easier O
NA B-TaskName
- O
tasks O
, O
we O
ablated O
both O
SR O
and O
LR O
number O
units O
( O
12 O
in O
total O
) O
or O
SR O
units O
only O
( O
10 O
in O
total O
) O
and O
evaluated O
network O
performance O
on O
these O
NA O
- O
tasks O
. O

Since O
the O
interpretation O
of O
the O
regression O
weights O
may O
be O
affected O
by O
possible O
correlations O
among O
the O
features O
, O
we O
also O
tested O
the O
causal O
effect O
of O
these O
units O
on O
NA B-TaskName
- O
task O
performance O
. O

Removing O
the O
syntax O
units O
together O
resulted O
in O
a O
significant O
decrease O
in O
performance O
for O
tasks O
that O
have O
an O
interfering O
noun O
. O

Linzen O
NA B-TaskName
- O
task O
: O
p O
= O
0:024 O
, O
nounPPAdv- O
. O


 O
This O
sentence O
is O
saying O
that O
Linzen O
NA B-TaskName
- O
task O
: O
p O
= O
0:024 O
is O
a O
noun O
phrase O
that O
modifies O
the O
adverb O
phrase O
" O
nounPPAdv- O
. O
" O

There O
was O
a O
significant O
difference O
between O
the O
two O
groups O
in O
nounPPAdv B-TaskName
, O
nounPP B-TaskName
, O
and O
nounPP B-TaskName
, O
with O
the O
group O
receiving O
the O
intervention O
having O
a O
lower O
p O
- O
value O
in O
all O
three O
cases O
. O

Figures O
3a O
and O
3b O
show O
the O
cell O
activity O
of O
this O
unit O
during O
the O
processing O
of O
stimuli O
from O
the O
2Adv O
and O
nounPP B-TaskName
tasks O
. O

We O
look O
at O
the O
connections O
between O
syntax O
unit O
1150 O
, O
which O
appears O
to O
be O
more O
closely O
involved O
in O
tracking O
subject O
- O
verb O
agreement O
, O
and O
the O
LR O
number O
units O
, O
as O
well O
as O
at O
the O
connections O
between O
the O
LR O
number O
units O
themselves O
. O

Figures O
4a O
and O
4b O
show O
the O
distribution O
of O
all O
the O
afferent O
recurrent O
weights O
to O
the O
input O
and O
forget O
gates O
of O
the O
LR O
units O
, O
scaled O
by O
the O
maximal O
activity O
of O
the O
pre O
- O
synaptic O
units O
during O
the O
nounPP B-TaskName
task O
. O
This O
scaling O
evaluates O
the O
effective O
input O
to O
the O
units O
and O
did O
not O
change O
the O
conclusions O
described O
below O
. O

We O
were O
the O
first O
to O
provide O
a O
detailed O
description O
of O
how O
an O
LSTM B-TaskName
language O
- O
model O
performs O
long O
- O
distance O
number O
agreement O
. O

In O
other O
words O
, O
training O
a O
model O
on O
raw O
text O
data O
using O
a O
language O
- O
model O
objective O
caused O
individual O
units O
within O
the O
model O
to O
learn O
very O
specific O
linguistic O
information O
. O

This O
research O
suggests O
that O
when O
training O
a O
language O
model O
, O
highly O
local O
encoding O
of O
linguistic O
features O
can O
emerge O
, O
as O
has O
been O
suggested O
by O
previous O
studies O
of O
artificial O
neural O
networks O
and O
neuroscience O
. O

The O
relationship O
we O
uncovered O
between O
syntax O
and O
number O
units O
suggests O
that O
agreement O
in O
an O
LSTM B-TaskName
language O
model O
can O
not O
be O
entirely O
explained O
away O
by O
superﬁcial O
heuristics O
, O
and O
the O
networks O
have O
, O
to O
some O
extent O
, O
learned O
to O
build O
and O
exploit O
structure O
- O
based O
syntactic O
representations O
, O
akin O
to O
those O
conjectured O
to O
support O
human O
- O
sentence O
processing O
. O

We O
hope O
that O
our O
study O
will O
inspire O
more O
analyses O
of O
the O
inner O
dynamics O
of O
LSTMs B-TaskName
and O
other O
sequence O
- O
processing O
networks O
, O
which O
would O
complement O
the O
currently O
popular O
" O
black O
- O
box O
probing O
" O
approach O
. O

RNN B-TaskName
simulations O
show O
that O
people O
can O
make O
grammaticality O
judgments O
on O
long O
- O
distance O
dependencies O
. O

The O
sentence O
is O
about O
a O
workshop O
that O
took O
place O
in O
Brussels O
, O
Belgium O
. O

At O
the O
2018 O
EMNLP B-TaskName
Workshop O
, O
it O
was O
noted O
that O
black- O
. O

This O
sentence O
is O
in O
all O
capital O
letters O
. O

The O
analysis O
and O
interpretation O
of O
neural O
networks O
for O
NLP B-TaskName
can O
be O
found O
on O
pages O
222 O
- O
231 O
. O

This O
sentence O
is O
from O
a O
paper O
discussing O
the O
analysis O
and O
interpretation O
of O
neural O
networks O
. O

LSTMs B-TaskName
performs O
better O
when O
modeling O
structure O
is O
taken O
into O
account O
when O
learning O
syntax O
- O
sensitive O
dependencies O
. O

Visualizing O
and O
understanding O
neural O
models O
in O
NLP B-TaskName
. O

To O
what O
extent O
does O
LSTMs B-TaskName
struggle O
with O
learning O
syntax O
- O
sensitive O
dependencies O
? O

What O
do O
language O
models O
that O
use O
the O
RNN B-TaskName
algorithm O
learn O
about O
dependencies O
between O
fillers O
and O
gaps O
? O

In O
the O
2018 O
EMNLP B-TaskName
Workshop O
, O
it O
was O
discussed O
that O
black- O
. O

The O
sentence O
can O
not O
be O
paraphrased O
because O
it O
is O
incomplete O
. O

Analyzing O
and O
interpreting O
neural O
networks O
for O
NLP B-TaskName
can O
be O
found O
on O
pages O
211 O
- O
221 O
. O

Unsupervised O
learning O
for O
arguments O
from O
consequences O
. O

Our O
experiments O
provide O
promising O
results O
that O
are O
comparable O
to O
, O
and O
in O
particular O
regards O
even O
outperform O
, O
BERT B-MethodName
. O

We O
will O
focus O
on O
arguments O
stance B-TaskName
detection I-TaskName
prob- I-TaskName
lem I-TaskName
in O
this O
paper O
. O

The O
task O
of O
deciding O
whether O
a O
text O
is O
in O
favor O
of O
, O
against O
, O
or O
unrelated O
to O
a O
given O
topic O
is O
called O
Stance B-TaskName
detection I-TaskName
or O
stance B-TaskName
classiﬁca- I-TaskName
tion I-TaskName
. O

The O
problem O
is O
related O
to O
opinion B-TaskName
mining I-TaskName
, O
but O
while O
opinion B-TaskName
mining I-TaskName
focuses O
on O
the O
sentiment O
polarity O
explicitly O
expressed O
by O
a O
text O
, O
stance B-TaskName
detection I-TaskName
aims O
to O
determine O
the O
position O
that O
the O
text O
holds O
with O
respect O
to O
a O
topic O
that O
is O
generally O
more O
abstract O
and O
might O
not O
be O
mentioned O
in O
the O
text O
. O

In O
stancedetection B-TaskName
, O
texts O
can O
transmit O
a O
negative O
sentiment O
or O
opinion O
about O
the O
targeted O
topic O
, O
but O
be O
in O
favor O
of O
the O
targeted O
topic O
. O

As O
shown O
in O
a O
recent O
survey O
by O
K O
¨uc O
¸¨uk O
and O
Can O
( O
2020 O
) O
, O
the O
problem O
of O
stance B-TaskName
detection I-TaskName
has O
been O
receiving O
more O
and O
more O
attention O
from O
the O
scientific O
community O
. O

Most O
approaches O
to O
this O
problem O
learn O
stance B-TaskName
classiﬁcation I-TaskName
models O
for O
each O
topic O
. O

We O
do O
n't O
think O
that O
a O
one O
- O
size O
- O
fits O
- O
all O
approach O
to O
stance B-TaskName
detection I-TaskName
is O
possible O
, O
but O
we O
have O
a O
different O
opinion O
. O

The O
task O
of O
stance B-TaskName
detec- I-TaskName
tion I-TaskName
is O
then O
to O
determine O
if O
the O
argument O
is O
against O
or O
in O
favor O
of O
A. O
Our O
solution O
to O
find O
the O
stance O
of O
such O
arguments O
revolves O
around O
extracting O
and O
analyzing O
cause O
- O
effect O
relations O
in O
order O
to O
infer O
if O
the O
consequences O
are O
good O
or O
bad O
. O

We O
compared O
our O
system O
's O
performance O
to O
a O
baseline O
model O
and O
a O
model O
that O
has O
been O
fine O
- O
tuned O
. O

Our O
results O
are O
comparable O
to O
those O
of O
BERT’s B-MethodName
, O
and O
in O
some O
cases O
, O
our O
results O
are O
even O
better O
. O
We O
do O
not O
need O
annotated O
training O
data O
, O
which O
is O
an O
advantage O
. O
Additionally O
, O
our O
approach O
can O
provide O
human O
- O
understandable O
explanations O
for O
the O
results O
, O
as O
well O
as O
cause O
- O
effect O
relations O
between O
concepts O
brought O
up O
in O
arguments O
. O

The O
crowdsourced B-DatasetName
dataset O
is O
described O
in O
Section O
4 O
, O
which O
is O
used O
to O
evaluate O
the O
approach O
in O
Section O
5 O
. O

Previous O
studies O
on O
Stance B-TaskName
detection I-TaskName
have O
focused O
on O
various O
types O
of O
formal O
texts O
, O
such O
as O
congressional O
debates O
( O
Thomas O
et O
al O
. O
, O
2006 O
) O
and O
company O
- O
internal O
discussions O
( O
Murakami O
and O
Raymond O
, O
2010 O
) O
. O

These O
approaches O
, O
which O
have O
been O
mostly O
triggered O
by O
a O
recent O
SemEval B-TaskName
shared I-TaskName
task3 O
( O
Mohammad O
et O
al O
. O
, O
2016 O
) O
, O
learn O
topic O
- O
specific O
models O
. O

al O
. O
, O
2015 O
) O


 O
Other O
approaches O
to O
stance B-TaskName
detection I-TaskName
use O
context O
clues O
from O
the O
post O
, O
such O
as O
its O
relationship O
to O
other O
posts O
in O
the O
debate O
, O
the O
network O
of O
authors O
, O
or O
the O
author O
’s O
identity O
. O

They O
also O
propose O
a O
topic- O
independent O
solution O
to O
stance B-TaskName
detection I-TaskName
for O
short O
claims O
without O
considering O
context O
, O
but O
they O
do O
not O
specifically O
address O
arguments O
from O
consequences O
. O

Our O
contribution O
can O
be O
summarized O
in O
three O
ways O
: O
( O
1 O
) O
we O
propose O
a O
fully O
unsupervised O
approach O
for O
stance B-TaskName
detection I-TaskName
, O
focusing O
on O
arguments O
that O
refer O
to O
consequences O
; O
( O
2 O
) O
we O
define O
rules O
over O
grammatical O
dependencies O
that O
exploit O
sentiment O
as O
well O
as O
effect O
words O
in O
order O
to O
determine O
good B-MetricValue
and O
bad B-MetricValue
consequences O
; O
( O
3 O
) O
we O
publish O
a O
new O
stance B-TaskName
detection I-TaskName
dataset O
that O
labels O
claims O
that O
refer O
to O
consequences O
, O
and O
which O
was O
crowdsourced B-DatasetName
on O
AMT O
. O

This O
sentence O
is O
discussing O
how O
statements O
can O
express O
different O
positions O
on O
a O
concept O
, O
which O
the O
author O
refers O
to O
as O
the O
target O
. O

The O
main O
idea O
behind O
our O
approach O
is O
that O
when O
the O
stance O
of O
a O
statement O
towards O
its O
target O
is O
favorable B-MetricValue
, O
the O
text O
either O
emphasizes O
the O
positive O
outcomes O
if O
the O
target O
is O
achieved O
( O
e.g. O
, O
Electing O
an O
EU O
president O
directly O
will O
increase O
accountability O
) O
, O
or O
it O
emphasizes O
the O
negative O
outcomes O
if O
the O
target O
is O
not O
achieved O
( O
e.g. O
, O
Sinking O
organic O
blooms O
can O
render O
the O
deep O
sea O
anoxic O
) O
. O

The O
effect B-HyperparameterName
triple I-HyperparameterName
is O
the O
core O
of O
our O
approach O
. O

T O
is O
a O
subset O
of O
P O
such O
that O
dir O
is O
a O
relation O
from O
T O
to O
e. O

We O
expect O
the O
sentiment B-MetricName
of O
an O
object O
to O
reﬂect O
whether O
it O
is O
generally O
regarded O
as O
a O
good B-MetricValue
thing O
( O
sent O
= O
+1 O
) O
or O
a O
bad B-MetricValue
thing O
( O
sent O
= O
 1 O
) O
. O
Our O
approach O
’s O
core O
idea O
is O
to O
distill O
such O
an O
effect O
triple O
from O
the O
claim O
and O
use O
it O
to O
infer O
the O
claim O
’s O
stance O
towards O
Tc O
. O

We O
can O
figure O
out O
if O
the O
object O
of O
the O
effect O
is O
either O
good B-MetricValue
or O
bad B-MetricValue
by O
using O
a O
combination O
of O
different O
sentiment O
lexicons O
, O
including O
the O
MPQA O
lexicon O
, O
the O
opinion O
lexicon O
of O
Hu O
and O
Liu O
, O
and O
the O
sentiment O
lexicon O
of O
Toledo O
- O
Ronen O
et O
al O
. O

We O
use O
Stanford O
Core O
NLP O
for O
POS B-TaskName
tagging I-TaskName
and O
lemmatizing B-TaskName
. O

We O
only O
aim O
to O
extract O
an O
effect B-HyperparameterName
triple I-HyperparameterName
from O
the O
claim O
, O
but O
the O
text O
expressing O
the O
topic O
makes O
this O
difficult O
. O

The O
stance B-TaskName
detection I-TaskName
process O
is O
easy O
to O
understand O
and O
can O
be O
used O
to O
create O
human O
- O
readable O
explanations O
for O
the O
results O
it O
returns O
. O

This O
is O
particularly O
relevant O
for O
helping O
users O
get O
more O
control O
over O
the O
process O
, O
especially O
given O
subsequent O
applications O
built O
on O
top O
of O
stance B-TaskName
detection I-TaskName
. O

We O
denote O
the O
process O
in O
which O
all O
the O
previous O
steps O
are O
fulfilled O
and O
an O
effect B-HyperparameterName
triple I-HyperparameterName
is O
extracted O
as O
TPO O
. O

However O
, O
for O
a O
variety O
of O
reasons O
that O
we O
analyze O
in O
Section O
5.4 O
, O
we O
might O
not O
be O
able O
to O
extract O
a O
complete O
effect B-HyperparameterName
triple I-HyperparameterName
. O

Lastly O
, O
if O
the O
above O
strategies O
fail O
to O
create O
an O
effect B-HyperparameterName
triple I-HyperparameterName
, O
we O
use O
a O
heuristic O
: O
if O
Twas O
is O
found O
, O
we O
set O
dir O
accordingly O
. O

We O
create O
a O
corpus O
by O
running O
an O
AMT O
crowd- O
sourcing O
study O
, O
where O
we O
annotate O
claims O
and O
topics O
extracted O
from O
Debatepedia B-DatasetName
. O

We O
also O
use O
the O
claim B-DatasetName
stance I-DatasetName
dataset O
published O
by O
Bar O
- O
Haim O
et O
al O
. O
( O
2017a O
) O
to O
check O
the O
performance O
of O
the O
systems O
on O
an O
independent O
dataset O
. O

We O
compare O
our O
system O
with O
the O
effect B-MethodName
lexicon I-MethodName
lexi- I-MethodName
con I-MethodName
system O
described O
in O
Section O
3.1 O
( O
ECF B-MethodName
) O
as O
well O
as O
with O
the O
+ B-MethodName
/ I-MethodName
- I-MethodName
EffectWordNet I-MethodName
( O
EWN B-MethodName
) O
. O

For O
comparison O
, O
we O
implement O
two O
other O
approaches O
. O
One O
approach O
is O
a O
system O
that O
simply O
sums O
up O
all O
the O
sentiment O
scores O
in O
the O
claim O
. O

We O
use O
BERT B-MethodName
, O
which O
was O
recently O
shown O
to O
outperform O
a O
series O
of O
alternative O
stance O
detection O
systems O
( O
Ghosh O
et O
al O
. O
, O
2019 O
) O
, O
as O
our O
state O
of O
the O
art O
. O

We O
fine O
- O
tune O
the O
BERT B-MethodName
using O
the O
large O
, O
uncased O
pre O
- O
trained O
weights O
. O

We O
set O
the O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
to O
5 B-HyperparameterValue
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
16 B-HyperparameterValue
in O
2020 O
. O

First O
, O
as O
expected O
, O
our O
system O
performs O
better O
on O
arguments O
related O
to O
consequences O
than O
on O
other O
arguments O
, O
with O
a O
difference O
of O
10pp B-MetricValue
between O
conseq O
and O
other O
. O

Our O
system O
outperforms O
the O
baseline O
on O
all O
datasets O
, O
but O
there O
are O
systems O
that O
outperform O
ours O
on O
some O
datasets O
. O

This O
is O
not O
surprising O
given O
that O
we O
use O
a O
pre O
- O
trained O
BERT B-MethodName
and O
then O
fine O
- O
tune O
it O
to O
our O
data O
. O

Our O
system O
with O
ECF B-MethodName
achieves O
better O
results O
than O
BERT B-MethodName
in O
terms O
of O
macro B-MetricName
F1 I-MetricName
score O
on O
the O
arguments O
that O
are O
not O
related O
to O
consequences O
( O
other O
) O
, O
and O
on O
the O
complete O
debate O
dataset O
. O

Although O
our O
system O
is O
better O
than O
BERT B-MethodName
at O
predicting O
the O
proclass O
in O
arguments O
from O
consequences O
, O
it O
is O
outperformed O
on O
the O
conclass O
when O
considering O
both O
stance O
classes O
and O
lexicon O
settings O
. O

In O
contrast O
, O
BERT B-MethodName
's O
performance O
varies O
drastically O
, O
with O
a O
difference O
of O
approximately O
17pp B-MetricName
in O
favor O
of O
the O
conclass O
. O

The O
high O
variability O
of O
BERT B-MethodName
is O
also O
indicated O
by O
the O
high O
standard O
deviation O
on O
the O
10 O
folds O
. O

We O
also O
computed O
the O
standard O
deviation O
of O
our O
system O
when O
run O
on O
the O
same O
10 O
folds O
for O
comparison O
, O
and O
the O
values O
lie O
between O
: O
_ O
_ O
_ O
_ O
on O
debate O
and O
_ O
_ O
_ O
_ O
on O
conseq O
. O

Our O
system O
is O
more O
consistent O
when O
using O
ECF B-MethodName
for O
the O
two O
effect O
lexicons O
than O
when O
using O
EWN B-MethodName
. O

Our O
analysis O
shows O
that O
the O
EWN B-MethodName
lexicon O
has O
a O
lot O
of O
coverage O
, O
but O
this O
comes O
at O
the O
expense O
of O
accuracy B-MetricName
. O

We O
will O
only O
refer O
to O
our O
system O
as O
ECF B-MethodName
from O
now O
on O
. O

BERT B-MethodName
does O
much O
better O
than O
our O
system O
, O
especially O
on O
the O
wiki O
data O
. O

Bar O
- O
Haim O
et O
al O
. O
's O
( O
2017a O
, O
b O
) O
reported O
accu- B-MetricName
racy I-MetricName
on O
wiki O
data O
when O
no O
context O
features O
were O
used O
. O
This O
is O
lower O
than O
BERT B-MethodName
's O
( O
. B-MetricValue
70 I-MetricValue
) O
but O
higher O
than O
ours O
( O
(. B-MetricName
65 I-MetricName
) O
for O
evaluating O
on O
the O
dedicated O
test O
set O
. O

We O
find O
a O
target O
in O
more O
than O
. B-MetricValue
75 I-MetricValue
of O
the O
data O
instances O
on O
all O
Debatepedia O
based O
datasets O
, O
and O
the O
results O
are O
slightly O
better O
when O
a O
target O
is O
found O
. O

We O
identify O
a O
potential O
consequence O
of O
the O
arguments O
for O
. B-MetricValue
6 I-MetricValue
in O
conseq O
. O

The O
total O
amount O
of O
instances O
solved O
by O
TPO O
and O
TP O
strategies O
on O
the O
conseq O
dataset O
is O
. B-MetricValue
44 I-MetricValue
, O
but O
it O
is O
much O
lower O
on O
other O
datasets O
( O
e.g. O
only O
. B-MetricValue
17 I-MetricValue
on O
the O
wiki O
) O
. O

The O
system O
needs O
to O
apply O
the O
Heuristic O
strategy O
to O
different O
parts O
of O
the O
dataset O
, O
depending O
on O
the O
data O
. O

There O
are O
multiple O
senses O
in O
the O
EWN B-MethodName
, O
but O
we O
always O
use O
the O
most O
probable O
effect O
. O

Aseel O
Addawood O
, O
Jodi O
Schneider O
, O
and O
Masooda O
Bashir O
argue O
that O
the O
encryption O
debate O
on O
Twitter O
is O
a O
prime O
example O
of O
how O
online O
debates O
generally O
unfold O
. O

Khalid O
Al O
- O
Khatib O
, O
Yufang O
Hou O
, O
Henning O
Wachsmuth O
, O
Charles O
Jochim O
, O
Francesca O
Bonin O
, O
and O
Benno O
Stein O
presented O
" O
End O
- O
to O
- O
end O
argumentation B-TaskName
knowledge I-TaskName
graph I-TaskName
construction I-TaskName
" O
at O
the O
Thirty O
- O
Fourth O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
( O
AAAI O
2020 O
) O
. O

Roy O
Bar O
- O
Haim O
, O
Indrajit O
Bhattacharya O
, O
Francesco O
Dinuzzo O
, O
Amrita O
Saha O
, O
and O
Noam O
Slonim O
. O
2017a O
. O
Stance B-TaskName
classiﬁcation I-TaskName
of O
context O
- O
dependent O
claims O
. O
In O
Proceedings O
of O
the O
15th O
Conference O
of O
the O
European O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Volume O
1 O
, O
Long O
Papers O
, O
pages O
251–261 O
, O
Valencia O
, O
Spain O
. O

Roy O
Bar O
- O
Haim O
, O
Lilach O
Edelstein O
, O
Charles O
Jochim O
, O
and O
Noam O
Slonim O
. O
2017b O
. O
Improving O
claim B-TaskName
stance I-TaskName
clas- I-TaskName
siﬁcation I-TaskName
with O
lexical O
knowledge O
expansion O
and O
context O
utilization O
. O
In O
Proceedings O
of O
the O
4th O
Workshop O
on O
Argument O
Mining O
. O
Association O
for O
Computational O
Linguistics O
. O

Yoonjung O
Choi O
and O
Janyce O
Wiebe O
. O
2014 O
. O
+ B-MethodName
/- I-MethodName
EffectWordNet I-MethodName
: O
Sense B-MethodName
- I-MethodName
level I-MethodName
lexicon I-MethodName
acquisition I-MethodName
for O
opinion O
inference O
. O
In O
Proceedings O
of O
the O
2014 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
1181 O
- O
1191 O
, O
Doha O
, O
Qatar O
. O
Association O
for O
Computational O
Linguistics O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
created O
Bert B-MethodName
: O
Pre B-MethodName
- I-MethodName
training I-MethodName
of I-MethodName
deep I-MethodName
bidirectional I-MethodName
transformers I-MethodName
to O
help O
with O
language O
understanding O
. O

Kuntal O
Dey O
, O
Ritvik O
Shrivastava O
, O
and O
Saroj O
Kaushik O
. O
2018 O
. O
Topical B-TaskName
stance I-TaskName
detection I-TaskName
for O
twitter O
: O
A O
two O
- O
phase O
lstm O
model O
using O
attention O
. O
In O
Advances O
in O
Information O
Retrieval O
, O
pages O
529–536 O
, O
Cham O
. O
Springer O
International O
Publishing O
. O

Jiachen O
Du O
, O
Ruifeng O
Xu O
, O
Yulan O
He O
, O
and O
Lin O
Gui O
. O
2017 O
. O
Stance B-TaskName
classiﬁcation I-TaskName
with O
target O
- O
specific O
neural O
attention O
. O
In O
Proceedings O
of O
the O
Twenty O
- O
Sixth O
International O
Joint O
Conference O
on O
Artificial O
Intelligence O
, O
IJCAI-17 O
, O
pages O
3988 O
- O
3994 O
. O

Adam O
Faulkner O
published O
a O
paper O
in O
2014 O
discussing O
a O
method O
of O
automated O
classiﬁcation B-TaskName
of I-TaskName
stance I-TaskName
in O
student O
essays O
that O
uses O
stance O
target O
information O
and O
the O
Wikipedia O
link O
- O
based O
measure O
. O

The O
sentence O
states O
the O
names O
of O
the O
authors O
of O
a O
study O
, O
the O
title O
of O
the O
study O
, O
and O
where O
it O
was O
published O
. O

The O
authors O
found O
that O
supervised O
Stance B-TaskName
Classiﬁcation I-TaskName
performed O
better O
than O
unsupervised O
Stance B-TaskName
Classiﬁcation I-TaskName
in O
online O
debates O
. O

Kazi O
Saidul O
Hasan O
and O
Vincent O
Ng O
( O
2013 O
) O
found O
that O
there O
are O
extra O
- O
linguistic O
constraints O
on O
stance B-TaskName
recognition I-TaskName
in O
ideological O
debates O
. O
Their O
study O
was O
published O
in O
the O
Proceedings O
of O
the O
51st O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
816 O
- O
821 O
. O

Anand O
Konjengbam O
, O
Subrata O
Ghosh O
, O
Nagendra O
Kumar O
, O
and O
Manish O
Singh O
use O
word O
embeddings O
to O
study O
Debate B-TaskName
stance I-TaskName
classiﬁca- I-TaskName
tion I-TaskName
in O
Big O
Data O
Analytics O
and O
Knowledge O
Discovery O
. O

Dilek O
Kücük O
and O
Fazli O
Can O
. O
2020 O
. O
Stance B-TaskName
detection I-TaskName
: O
A O
survey O
. O
ACM O
Comput O
. O
Surv O
. O
, O
53(1 O
) O
. O

The O
researchers O
Pavithra O
Rajendran O
, O
Danushka O
Bollegala O
, O
and O
Simon O
Parsons O
published O
a O
paper O
in O
2016 O
called O
" O
Contextual B-TaskName
stance I-TaskName
classiﬁcation I-TaskName
of I-TaskName
opinions I-TaskName
: O
A O
step O
towards O
enthymeme O
reconstruction O
in O
online O
reviews O
. O
" O

Benjamin O
Schiller O
, O
Johannes O
Daxenberger O
, O
and O
Iryna O
Gurevych O
. O
2020 O
. O
Stance B-TaskName
detection I-TaskName
benchmark O
: O
How O
robust O
is O
your O
stance B-TaskName
detection I-TaskName
? O

The O
sentence O
discusses O
a O
study O
done O
by O
Parinaz O
Sobhani O
, O
Saif O
Mohammad O
, O
and O
Svetlana O
Kir- O
itchenko O
in O
2016 O
on O
the O
Detecting B-TaskName
stance I-TaskName
in O
tweets O
and O
how O
it O
interacts O
with O
sentiment O
. O

Swapna O
Somasundaran O
and O
Janyce O
Wiebe O
( O
2009 O
) O
. O
" O
Rec- B-TaskName
ognizing I-TaskName
Stances O
in O
Online O
Debates O
" O
. O
In O
: O
Proceedings O
of O
the O
Joint O
Conference O
of O
the O
47th O
Annual O
Meeting O
of O
the O
ACL O
and O
the O
4th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
of O
the O
AFNLP O
: O
Volume O
1 O
- O
Volume O
1 O
, O
ACL'09 O
, O
pages O
226 O
- O
234 O
, O
Stroudsburg O
, O
PA O
, O
USA O
. O

Wiebe O
and O
Somasundaran O
( O
2010 O
) O
found O
that O
Rec- B-TaskName
ognizing I-TaskName
stances I-TaskName
was O
mentioned O
more O
often O
than O
other O
terms O
in O
on O
- O
line O
debates O
with O
an O
ideological O
slant O
. O

Dhanya O
Sridhar O
, O
Lise O
Getoor O
, O
and O
Marilyn O
Walker O
. O
2014 O
. O
Collective B-TaskName
stance I-TaskName
classiﬁcation I-TaskName
of O
posts O
in O
online O
debate O
forums O
. O
In O
Proceedings O
of O
the O
Joint O
Workshop O
on O
Social O
Dynamics O
and O
Personal O
Attributes O
in O
Social O
Media O
, O
pages O
109 O
- O
117 O
, O
Baltimore O
, O
Maryland O
. O
Association O
for O
Computational O
Linguistics O
. O

The O
sentence O
states O
that O
Qingying O
Sun O
, O
Zhongqing O
Wang O
, O
Qiaoming O
Zhu O
, O
and O
Guodong O
Zhou O
are O
the O
authors O
of O
a O
2018 O
paper O
titled O
" O
Stance B-TaskName
detection I-TaskName
with O
hierar- B-MethodName
chical I-MethodName
attention I-MethodName
network I-MethodName
. O
" O
The O
paper O
was O
presented O
at O
the O
27th O
International O
Conference O
on O
Computational O
Linguistics O
in O
Santa O
Fe O
, O
New O
Mexico O
, O
USA O
. O

The O
following O
sentence O
surveys O
opinion B-TaskName
mining I-TaskName
from O
stance O
to O
product O
aspect O
: O
Rui O
Wang O
, O
Deyu O
Zhou O
, O
Mingmin O
Jiang O
, O
Si O
Jiasheng O
, O
and O
Yang O
Yang O
. O
2019 O
. O
IEEE O
Access O
, O
PP:1 O
- O
1 O
. O

Theresa O
Wilson O
, O
Janyce O
Wiebe O
, O
and O
Paul O
Hoffmann O
. O
2005 O
. O
Recognizing B-TaskName
Contextual I-TaskName
Polarity I-TaskName
in O
Phrase O
- O
level O
Sentiment B-TaskName
Analysis I-TaskName
. O
In O
Proceedings O
of O
the O
Conference O
on O
Human O
Language O
Technology O
and O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
HLT O
’ O
05 O
, O
pages O
347–354 O
, O
Stroudsburg O
, O
PA O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Event O
- O
place O
: O
Vancouver O
, O
British O
Columbia O
, O
Canada O
. O

Neural O
networks O
that O
model O
rhyme O
and O
rhythm O
improve O
the O
performance O
of O
Lanqing O
Xue O
, O
Kaitao O
Song O
, O
Duocai O
Wu O
, O
Xu O
Tan O
, O
Nevin O
L. O
Zhang O
, O
Tao O
Qin O
, O
Wei O
- O
Qiang O
Zhang O
, O
and O
Tie O
- O
Yan O
Liu O
. O

. O


 O
The O
sentence O
is O
saying O
that O
a O
research O
project O
at O
Tsinghua O
University O
is O
looking O
into O
a O
way O
to O
produce O
lyrics O
and O
beats O
for O
singing O
that O
takes O
into O
account O
both O
rhyme O
and O
rhythm O
. O

Previous O
works O
for O
rap B-TaskName
generation I-TaskName
have O
focused O
on O
rhyming O
lyrics O
but O
have O
ignored O
rhythmic O
beats O
, O
which O
are O
important O
for O
rap O
performance O
. O

In O
this O
paper O
, O
we O
develop O
a O
Transformer O
- O
based O
rap O
generation O
system O
called O
DeepRapper B-MethodName
that O
can O
model O
both O
rhymes O
and O
rhythms O
. O

As O
far O
as O
we O
know O
, O
DeepRapper B-MethodName
is O
the O
first O
system O
to O
generate O
rap O
with O
both O
rhymes O
and O
rhythms O
. O

Both O
objective O
and O
subjective O
evaluations O
show O
that O
DeepRapper B-MethodName
creates O
high O
- O
quality O
raps O
with O
creative O
rhymes O
and O
rhythms O
. O

, O
2015 O
) O


 O
The O
rapid O
development O
of O
artificial O
intelligence O
has O
led O
to O
increased O
interest O
in O
automatic O
rap B-TaskName
lyrics I-TaskName
generation I-TaskName
from O
academia O
( O
Potash O
et O
. O
, O
2015 O
) O
. O

Previous O
works O
for O
rap B-TaskName
generation I-TaskName
have O
mainly O
focused O
on O
lyric O
generation O
, O
with O
some O
developing O
strategies O
for O
rhyme O
modeling O
. O

Although O
many O
works O
have O
studied O
rhyming O
modeling O
in O
other O
artistic O
genres O
( O
e.g. O
, O
poetry O
) O
( O
Li O
et O
al O
. O
, O
2020 O
; O
Van O
de O
Cruys O
, O
2020 O
; O
Liu O
et O
al O
. O
, O
2020 O
) O
, O
they O
are O
not O
suitable O
for O
rap B-TaskName
generation I-TaskName
due O
to O
the O
complex O
rhyme O
structure O
in O
rap O
. O

If O
a O
song O
has O
no O
rhythm O
, O
it O
can O
not O
be O
considered O
a O
full O
song O
. O

In O
this O
paper O
, O
we O
develop O
a O
Transformer O
- O
based O
rap O
generation O
system O
called O
DeepRapper B-MethodName
which O
can O
model O
both O
rhymes O
and O
rhythms O
. O

Our O
Deep- B-MethodName
Rapper I-MethodName
model O
is O
pre O
- O
trained O
in O
the O
pre O
- O
training O
stage O
using O
the O
above O
two O
datasets O
. O

The O
model O
is O
fine O
- O
tuned O
for O
the O
final O
rap B-TaskName
generation I-TaskName
. O

Both O
objective O
and O
subjective O
evaluations O
verify O
the O
advantages O
of O
DeepRapper B-MethodName
in O
generating O
rap O
lyrics O
with O
rhymes O
and O
rhythms O
. O

Our O
main O
contributions O
are O
as O
follows O
: O
modeling O
rhythms O
in O
rap B-TaskName
generation I-TaskName
, O
and O
developing O
. O

DeepRapper B-MethodName
is O
the O
first O
that O
we O
know O
of O
to O
explicitly O
model O
N O
- O
gram O
rhymes O
. O

As O
far O
as O
we O
know O
, O
DeepRapper B-MethodName
is O
the O
first O
system O
that O
models O
rhythms O
for O
rap B-TaskName
generation I-TaskName
. O

Since O
DeepRapper B-MethodName
generates O
rap O
lyrics O
with O
both O
rhyme O
and O
rhythm O
modeling O
, O
we O
will O
briefly O
introduce O
the O
related O
background O
: O
lyric O
generation O
, O
rhyme O
modeling O
, O
and O
rhythm O
modeling O
. O

. O
. O


 O
Lyric O
generation O
can O
broadly O
cover O
rap B-TaskName
lyric I-TaskName
generation I-TaskName
( O
Potash O
et O
al O
. O
, O
2015 O
; O
Nikolov O
et O
al O
. O
, O
2020 O
; O
... O

This O
paper O
introduces O
a O
novel O
language O
model O
for O
rap B-TaskName
generation I-TaskName
, O
which O
is O
designed O
to O
fit O
the O
characteristics O
of O
rap O
lyrics O
, O
unlike O
previous O
works O
that O
generate O
lyrics O
similar O
to O
natural O
language O
. O

Pre O
- O
trained O
language O
models O
have O
been O
successful O
in O
NLP O
applications O
, O
so O
we O
have O
incorporated O
pre O
- O
training O
into O
our O
model O
to O
improve O
it O
. O

Rhyme O
modeling O
is O
important O
in O
rap B-TaskName
generation I-TaskName
because O
it O
requires O
the O
last O
few O
tokens O
in O
consecutive O
sentences O
to O
rhyme O
. O

Although O
many O
works O
have O
explored O
rhyme O
modeling O
in O
other O
genres O
, O
most O
of O
them O
can O
not O
be O
directly O
used O
for O
rap B-TaskName
generation I-TaskName
. O

Therefore O
, O
N O
- O
gram O
rhyme O
modeling O
is O
introduced O
in O
DeepRapper B-MethodName
to O
handle O
the O
distinctive O
rhyme O
patterns O
in O
rap O
. O

To O
the O
best O
of O
our O
knowledge O
, O
none O
of O
the O
previous O
works O
have O
studied O
the O
rhythm O
modeling O
in O
rap B-TaskName
generation I-TaskName
. O

In O
this O
paper O
, O
we O
introduce O
a O
new O
beat O
modeling O
strategy O
for O
rhythm B-TaskName
generation I-TaskName
. O

Previous O
works O
on O
rap B-TaskName
generation I-TaskName
usually O
only O
used O
lyrics O
from O
rap O
datasets O
, O
without O
considering O
the O
rhythmic O
beat O
information O
. O

To O
create O
a O
model O
of O
rhythm O
in O
the O
song O
ABCDEFG O
, O
the O
dataset O
of O
rap O
lyrics O
should O
have O
beats O
that O
are O
aligned O
with O
the O
rhythm O
. O

We O
obtain O
a O
rap O
lyric O
dataset O
with O
aligned O
beats O
, O
which O
we O
have O
named O
D B-DatasetName
- I-DatasetName
RAP I-DatasetName
, O
and O
which O
satisfies O
the O
requirements O
of O
building O
a O
rap O
generation O
system O
with O
both O
rhyme O
and O
rhythm O
modeling O
. O

We O
divided O
the O
D B-DatasetName
- I-DatasetName
RAP B-DatasetName
dataset O
into O
the O
training B-HyperparameterName
and I-HyperparameterName
validation I-HyperparameterName
set I-HyperparameterName
with I-HyperparameterName
a I-HyperparameterName
ratio I-HyperparameterName
of O
4:1 B-HyperparameterValue
. O

Since O
rap O
is O
only O
one O
type O
of O
music O
, O
and O
rap O
songs O
typically O
have O
fewer O
beats O
compared O
to O
other O
types O
of O
music O
, O
we O
also O
mined O
two O
additional O
datasets O
to O
pre O
- O
train O
our O
model O
using O
the O
same O
pipeline O
: O
1 O
) O
non O
- O
rap O
songs O
with O
aligned O
beats O
( O
named O
D B-DatasetName
- I-DatasetName
SONG I-DatasetName
) O
; O
2 O
) O
pure O
lyrics O
without O
aligned O
beats O
( O
named O
D B-DatasetName
- I-DatasetName
LYRIC I-DatasetName
) O
. O

We O
use O
Transformer O
to O
build O
an O
autoregressive O
language O
model O
for O
rap B-TaskName
generation I-TaskName
, O
and O
introduce O
several O
new O
designs O
: O
1 O
) O
To O
better O
model O
rhymes O
, O
our O
model O
generates O
a O
sentence O
from O
right O
to O
left O
, O
since O
rhyming O
words O
are O
always O
at O
the O
end O
of O
the O
sentence O
; O
2 O
) O
As O
aforementioned O
, O
rhythms O
are O
critical O
for O
rap O
performance O
, O
so O
we O
insert O
a O
special O
token O
[ O
BEAT O
] O
for O
explicit O
beat O
modeling O
; O
3 O
) O
Unlike O
original O
Transformer O
with O
only O
word O
embedding O
and O
positional O
embedding O
, O
we O
add O
multiple O
additional O
embeddings O
to O
better O
model O
rhymes O
and O
rhythms O
. O

We O
model O
rhymes O
in O
DeepRapper B-MethodName
with O
three O
components O
: O
a O
reverse O
- O
order O
language O
model O
, O
a O
rhyme O
representation O
, O
and O
a O
rhyme O
constraint O
. O

balance B-HyperparameterName
the I-HyperparameterName
two I-HyperparameterName
terms I-HyperparameterName
is O
a O
hyper O
- O
parameter O
. O

The O
beat O
frequency O
distribution O
for O
our O
D B-DatasetName
- I-DatasetName
RAP I-DatasetName
dataset O
is O
shown O
in O
Figure O
5 O
. O

, O
2018 O
) O
, O
which O
takes O
as O
input O
a O
sequence O
of O
tokens O
x O
1 O
, O
. O
. O
. O
, O
x O
n O
, O
and O
outputs O
a O
sequence O
of O
tokens O
y O
1 O
, O
. O
. O
. O
, O
y O
n O
, O
where O
each O
token O
y O
i O
is O
a O
distribution O
over O
the O
vocabulary O
. O


 O
The O
DeepRapper B-MethodName
model O
is O
based O
on O
the O
autoregressive O
Transformer O
decoder O
( O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Radford O
et O
al O
. O
, O
2018 O
) O
, O
which O
takes O
a O
sequence O
of O
tokens O
x1 O
, O
... O
, O
xn O
as O
input O
and O
outputs O
a O
sequence O
of O
tokens O
y1 O
, O
... O
, O
yn O
, O
where O
each O
token O
yi O
is O
a O
distribution O
over O
the O
vocabulary O
. O

The O
hidden B-HyperparameterName
size I-HyperparameterName
, O
number B-HyperparameterName
of I-HyperparameterName
attention I-HyperparameterName
heads I-HyperparameterName
, O
and O
number B-HyperparameterName
of I-HyperparameterName
Transformer I-HyperparameterName
layers I-HyperparameterName
are O
set O
as O
768 B-HyperparameterValue
, O
12 B-HyperparameterValue
, O
and O
12 B-HyperparameterValue
. O

The O
0 O
in O
DeepRapper B-MethodName
is O
set O
as O
768 B-HyperparameterValue
. O

We O
first O
train O
our O
model O
on O
D B-DatasetName
- I-DatasetName
LYRIC I-DatasetName
and O
D B-DatasetName
- I-DatasetName
SONG I-DatasetName
for O
2 B-HyperparameterValue
mil- I-HyperparameterValue
lions I-HyperparameterValue
steps B-HyperparameterName
, O
and O
then O
fine O
- O
tune O
our O
model O
on O
D B-DatasetName
- I-DatasetName
RAP I-DatasetName
with O
3 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
as O
the O
size O
of O
D O
- O
RAP O
is O
smaller O
than O
our O
training O
corpus O
. O

We O
convert O
each O
song O
to O
a O
sequence O
with O
a O
length O
of O
1024 B-HyperparameterValue
tokens O
by O
cutting O
longer O
sequences O
or O
padding O
shorter O
sequences O
. O

Our O
model O
is O
trained O
using O
a O
data O
set O
of O
songs O
on O
4 O
NVIDIA O
TITAN O
V O
GPUs O
. O

We O
use O
Adam O
optimizer O
with O
a O
learning O
rate O
of O
0.001 O
. O

1= O
0.9 B-HyperparameterValue
, O
. O


 O
One O
equals O
ABCDEFG O
zero O
, O
period O
. O

The O
sum O
of O
2 O
and O
0 O
is O
ABCDEFG O
. O

10 6 B-HyperparameterValue
means O
" O
end O
of O
transmission O
. O
" O

We O
set O
the O
maxi- B-HyperparameterName
mum I-HyperparameterName
value I-HyperparameterName
of I-HyperparameterName
N I-HyperparameterName
- I-HyperparameterName
gram I-HyperparameterName
rhyme I-HyperparameterName
as O
3 B-HyperparameterValue
and O
the O
hyper O
- O
parameter O
. O

This O
equation O
can O
be O
read O
as O
0.95 B-HyperparameterValue
. O

We O
are O
choosing O
five O
different O
metrics O
to O
evaluate O
our O
model O
which O
are O
: O
1 O
) O
Perplexity B-MetricName
( O
PPL B-MetricName
) O
, O
a O
standard O
metric O
to O
evaluate O
the O
quality O
of O
a O
language O
model O
; O
2 O
) O
Rhyme B-MetricName
Accuracy I-MetricName
( O
RA B-MetricName
) O
, O
the O
ratio O
of O
sentences O
that O
have O
correctly O
predicted O
rhymes O
; O
3 O
) O
Rhyme B-MetricName
Den- I-MetricName
sity I-MetricName
( O
RD B-MetricName
) O
, O
the O
longest O
rhyme O
of O
a O
song O
, O
averaged O
over O
all O
songs O
, O
which O
is O
introduced O
by O
Malmi O
et O
al O
. O

We O
're O
averaging O
the O
maximum O
number O
of O
consecutive O
sentences O
with O
the O
same O
N O
- O
gram O
rhyme O
in O
a O
rap O
song O
over O
all O
songs O
to O
measure O
the O
quality O
of O
rhyming O
fluency O
. O
We O
study O
the O
accuracy O
of O
our O
model O
in O
beat O
prediction O
under O
the O
teacher O
- O
forcing O
mode O
. O

Table O
2 O
shows O
the O
objective O
and O
subjective O
results O
of O
DeepRapper B-MethodName
compared O
to O
two O
baselines O
: O
1 O
) O
Baseline O
: O
a O
standard O
autoregressive O
language O
model O
with O
the O
same O
model O
configuration O
as O
DeepRapper B-MethodName
but O
without O
our O
proposed O
rhyme O
and O
rhythm O
modeling O
; O
2 O
) O
Baseline O
+ O
PT O
, O
using O
pre O
- O
training O
on O
Baseline O
. O

DeepRap- B-MethodName
per I-MethodName
produces O
better O
results O
in O
all O
categories O
that O
involve O
human O
opinion O
, O
showing O
that O
it O
can O
create O
raps O
that O
are O
both O
high O
quality O
and O
pleasing O
to O
the O
ear O
. O

However O
, O
it O
still O
does O
n't O
perform O
as O
well O
as O
ABCDEFG O
. O

We O
conducted O
a O
series O
of O
ablation O
studies O
to O
further O
validate O
the O
necessity O
of O
each O
component O
in O
DeepRapper B-MethodName
, O
including O
removing O
rhyme O
modeling O
, O
rhythm O
modeling O
and O
pre O
- O
training O
. O

We O
have O
several O
observations O
: O

 O
1 O
) O
Removing O
rhyme O
modeling O
affects O
rhyme O
quality O
a O
lot O
as O
it O
results O
in O
a O
dramatic O
drop O
in O
rhyme O
accuracy O
and O
rhyme O
density O
; O

 O
2 O
) O
Removing O
each O
specific O
design O
in O
rhyme O
modeling O
( O
i.e. O
, O
RO O
: O
reverse O
order O
language O
model O
, O
VE O
: O
vowel O
embedding O
, O
IPE O
: O
intra O
- O
sentence O
position O
embedding O
, O
SE O
: O
sentence O
embedding O
) O
causes O
worse O
rhyme B-MetricName
accuracy I-MetricName
and O
rhyme B-MetricName
density I-MetricName
. O

Specifically O
, O
the O
analysis O
in O
Wu O
et O
al O
. O
shows O
that O
removing O
RO O
leads O
to O
a O
better O
PPL B-MetricName
since O
left O
- O
to O
- O
right O
order O
can O
be O
more O
easily O
modeled O
than O
right O
- O
to O
- O
left O
order O
. O

It O
seems O
that O
DeepRapper B-MethodName
can O
not O
produce O
any O
beat O
information O
without O
rhythm O
modeling O
; O
However O
, O
DeepRapper B-MethodName
obtains O
a O
higher O
rhyme B-MetricName
density I-MetricName
, O
though O
it O
affects O
perplexity B-MetricName
and O
rhyme B-MetricName
accu- I-MetricName
racy I-MetricName
a O
lot O
without O
pre O
- O
training O
. O

The O
sentence O
means O
that O
it O
is O
difficult O
to O
make O
broad O
statements O
about O
the O
subject O
matter O
because O
there O
is O
a O
lack O
of O
information O
. O

We O
verified O
this O
by O
counting O
the O
repetitive O
rate O
of O
rhyming O
words O
. O
We O
found O
that O
the O
rate O
of O
DeepRapper B-MethodName
is O
23:8 O
% O
while O
without O
pre O
- O
training O
is O
42:5 O
% O
. O
This O
is O
higher O
than O
using O
pre O
- O
training O
. O

The O
results O
above O
verify O
that O
each O
component O
in O
DeepRapper B-MethodName
is O
effective O
. O

We O
use O
Combo- B-MetricName
N I-MetricName
to O
measure O
the O
ability O
of O
each O
design O
in O
DeepRapper B-MethodName
to O
model O
N O
- O
gram O
rhyme O
in O
order O
to O
highlight O
the O
advantage O
of O
DeepRapper B-MethodName
in O
modeling O
N O
- O
gram O
rhyme O
. O

We O
randomly O
generate O
5,000 O
samples O
by O
DeepRapper B-MethodName
and O
DeepRapper B-MethodName
with O
beat O
frequency O
control O
to O
better O
measure O
beat O
quality O
. O

We O
propose O
the O
First B-MetricName
Order I-MetricName
Distribution I-MetricName
( O
FOD B-MetricName
) O
and O
the O
Second B-MetricName
Order I-MetricName
Distribution I-MetricName
( O
SOD B-MetricName
) O
and O
measure O
the O
distance O
( O
via O
Wasserstein O
) O
. O

There O
is O
a O
discrepancy O
between O
the O
generated O
samples O
and O
our O
DRAP B-DatasetName
dataset O
. O

Therefore O
, O
the O
FOD B-MetricName
is O
defined O
as O
the O
interval O
of O
the O
current O
beat O
. O

The O
SOD B-MetricName
is O
similarly O
defined O
as O
the O
distribution O
of O
the O
interval O
between O
the O
current O
[ O
BEAT O
] O
and O
the O
next O
[ O
BEAT O
] O
. O

The O
sentence O
is O
saying O
that O
DeepRapper B-MethodName
does O
a O
better O
job O
at O
beat O
modeling O
when O
beat O
frequency O
control O
is O
used O
, O
showing O
that O
beat O
frequency O
control O
is O
important O
for O
beat O
modeling O
. O

We O
include O
a O
sample O
case O
from O
our O
generated O
raps O
in O
Figure O
6 O
to O
demonstrate O
the O
good O
quality O
of O
the O
raps O
gen- O
erated O
by O
DeepRapper B-MethodName
. O

The O
example O
in O
Figure O
2 O
is O
generated O
by O
feeding O
the O
first O
sentence O
to O
DeepRapper B-MethodName
. O

In O
this O
paper O
, O
we O
develop O
a O
novel O
Transformer O
- O
based O
rap O
generation O
system O
, O
which O
leverages O
rhyme O
modeling O
, O
rhythm O
modeling O
and O
pre O
- O
training O
for O
improved O
performance O
. O

DeepRapper B-MethodName
is O
the O
first O
system O
that O
we O
know O
of O
that O
generates O
rap O
with O
both O
rhymes O
and O
rhythms O
. O

Both O
objective O
and O
subjective O
evaluations O
demonstrate O
that O
DeepRapper B-MethodName
generates O
high O
- O
quality O
raps O
with O
good O
rhymes O
and O
rhythms O
. O

Thanks O
to O
the O
design O
of O
DeepRapper B-MethodName
, O
we O
can O
further O
build O
another O
rap O
singing O
system O
that O
sings O
out O
raps O
according O
to O
rhymes O
and O
rhythms O
, O
which O
we O
will O
leave O
as O
future O
work O
. O

This O
is O
an O
unfinished O
project O
that O
requires O
more O
work O
. O

The O
proposed O
framework O
can O
be O
considered O
a O
new O
way O
of O
automatically O
creating O
art O
that O
takes O
into O
account O
ethical O
considerations O
. O

A O
computational O
approach O
to O
rap B-TaskName
lyrics I-TaskName
generation I-TaskName
is O
Dopelearning B-MethodName
. O

If O
rap B-TaskName
lyrics I-TaskName
generation I-TaskName
is O
true O
, O
then O
denoising O
autoencoders O
can O
be O
used O
to O
Rapformer B-MethodName
. O

An O
lstm O
can O
be O
used O
for O
automatic O
rap B-TaskName
lyric I-TaskName
generation I-TaskName
. O

, O
2005 O
) O
. O


 O
We O
provide O
a O
comparison O
of O
DeepRapper B-MethodName
and O
GhosterWriter B-MethodName
( O
Potash O
et O
. O
, O
2005 O
) O
. O

The O
results O
show O
that O
both O
DeepRapper B-MethodName
and O
base- O
lines O
outperform O
GhosterWriter B-MethodName
in O
terms O
of O
PPL B-MetricName
, O
rhyme B-MetricName
accuracy I-MetricName
, O
and O
rhyme B-MetricName
density I-MetricName
on O
all O
tasks O
. O

B O
samples O
with O
beat O
frequency O
control O
fast O
figure O
7 O
provides O
a O
rap O
generated O
by O
Deep- B-MethodName
Rapper I-MethodName
with O
fast O
beat O
frequency O
, O
which O
the O
frequency O
is O
4.3 O

中等节奏的8字型由ABCDEFG0生成，拥有中等频率，频率为2.6 O
。 O

Slow O
Figure O
9 O
is O
a O
rap O
generated O
by O
Deep- B-MethodName
Rapper I-MethodName
with O
a O
slow O
beat O
frequency O
of O
2.1 O
. O

AligNART B-MethodName
: O
Non B-MethodName
- I-MethodName
autoregressive I-MethodName
Neural I-MethodName
Machine I-MethodName
Translation I-MethodName
by I-MethodName
Jointly I-MethodName
Learning I-MethodName
to I-MethodName
Estimate I-MethodName
Alignment I-MethodName
and I-MethodName
Translate I-MethodName

Abstract O
Non B-MethodName
- I-MethodName
autoregressive I-MethodName
neural I-MethodName
machine I-MethodName
translation I-MethodName
and O
NART B-MethodName
models O
suffer O
from O
the O
multi O
- O
modality O
problem O
, O
which O
causes O
translation O
inconsistency O
, O
such O
as O
token O
repetition O
. O

In O
this O
paper O
, O
we O
introduce O
AligNART B-MethodName
, O
which O
uses O
full O
alignment O
information O
to O
explicitly O
reduce O
the O
modality O
of O
the O
target O
distribution O
. O

The O
task O
of O
machine B-TaskName
translation I-TaskName
is O
divided O
into O
two O
parts O
: O
( O
i O
) O
estimating O
the O
alignment O
and O
( O
ii O
) O
translating O
with O
aligned O
decoder O
inputs O
. O
This O
guide O
will O
help O
the O
decoder O
focus O
on O
simplified O
one O
- O
to O
- O
one O
translation O
. O

Our O
experiments O
show O
that O
the O
AligNART B-MethodName
outperforms O
previous O
non O
- O
iterative O
NART O
models O
that O
focus O
on O
explicit O
modality O
reduction O
on O
WMT14 B-DatasetName
En$De I-DatasetName
and O
WMT16 B-DatasetName
Ro I-DatasetName
! I-DatasetName
En I-DatasetName
. O

AligNART B-MethodName
produces O
results O
that O
are O
just O
as O
good O
as O
the O
best O
current O
models O
that O
use O
WMT14 B-DatasetName
En$De I-DatasetName
for O
temporal O
classification O
. O

AligNART B-MethodName
is O
effective O
at O
reducing O
token O
repetition O
even O
without O
sequence O
- O
level O
knowledge O
distillation O
. O

In O
the O
neural B-TaskName
machine I-TaskName
translation I-TaskName
( O
NMT B-TaskName
) O
domain O
, O
NART O
models O
( O
Gu O
et O
al O
. O
, O
2018 O
) O
have O
been O
proposed O
to O
alleviate O
the O
low O
translation O
speeds O
of O
ART O
models O
. O

Kasai O
et O
al O
. O
( O
2020a O
) O
, O
Lee O
et O
al O
. O
( O
2020 O
) O
, O
Guo O
et O
al O
. O
( O
2020 O
) O
, O
and O
Saharia O
et O
al O
. O
( O
2020 O
) O
all O
target O
to O
improve O
NART B-MethodName
without O
iteration O
, O
while O
other O
recent O
works O
( O
Qian O
et O
al O
. O
2021 O
; O
Gu O
and O
Kong O
, O
2021 O
) O
focus O
on O
improving O
NART B-MethodName
with O
iteration O
. O

Other O
inconsistency O
problems O
, O
such O
as O
token O
repetition O
or O
omission O
, O
also O
occur O
frequently O
for O
the O
same O
reason O
in O
non B-TaskName
- I-TaskName
iterative I-TaskName
NART I-TaskName
( O
Gu O
and O
Kong O
, O
2021 O
) O
. O

There O
are O
two O
main O
methods O
for O
addressing O
the O
multi O
- O
modality O
problem O
. O

In O
this O
work O
, O
we O
propose O
a O
non O
- O
iterative O
NART O
model O
, O
AligNART B-MethodName
, O
which O
mitigates O
the O
multi O
- O
modality O
problem O
by O
utilizing O
complete O
informa- O
tion O
in O
word O
alignments O
. O

The O
machine O
translation O
task O
is O
divided O
into O
( O
i O
) O
alignment O
estimation O
and O
( O
ii O
) O
non O
- O
autoregressive O
translation O
under O
the O
given O
alignments O
. O

In O
AligNART B-MethodName
, O
Aligner O
is O
a O
module O
that O
is O
used O
to O
generate O
aligned O
decoder O
inputs O
by O
estimating O
alignments O
. O

Our O
experimental O
results O
found O
that O
AligNART B-MethodName
did O
better O
than O
other O
non O
- O
iterative O
NART O
models O
of O
explicit O
modality O
reduction O
on O
WMT14 B-DatasetName
En I-DatasetName
$ I-DatasetName
De I-DatasetName
and O
WMT16 B-DatasetName
Ro!En I-DatasetName
. O

The O
AligNART B-MethodName
model O
performs O
just O
as O
well O
as O
the O
most O
recent O
state O
- O
of O
- O
the O
- O
art O
NART O
model O
on O
the O
WMT14 B-DatasetName
En$De I-DatasetName
task O
. O

Non O
- O
iterative O
NART O
models O
( O
Gu O
et O
al O
. O
, O
2018 O
; O
Sun O
et O
al O
. O
, O
2019 O
; O
Sun O
and O
Yang O
, O
2020 O
) O
assume O
conditional O
independence O
between O
the O
target O
tokens O
to O
improve O
translation O
speed O
: O
p(yjx O
) O
= O
p(Njx)NY O
n=1p(ynjx O
) O
; O
( O
2 O
) O
where O
ABCDEFG O
is O
the O
ABCDEFG O
to O
parallelize O
the O
decoding O
process O
. O

Given O
the O
word O
alignments O
between O
the O
source O
and O
target O
sentences O
, O
we O
can O
factorize O
the O
task O
into O
two O
parts O
: O
estimating O
the O
alignments O
, O
and O
translating O
with O
aligned O
decoder O
inputs O
. O

AligNART B-MethodName
is O
an O
encoder O
- O
decoder O
architecture O
with O
an O
alignment O
estimation O
module O
called O
Aligner O
, O
as O
shown O
in O
Figure O
1a O
. O

The O
aligned O
decoder O
inputs O
are O
created O
by O
the O
aligner O
, O
which O
uses O
the O
number B-HyperparameterName
of I-HyperparameterName
non I-HyperparameterName
- I-HyperparameterName
zero I-HyperparameterName
elements I-HyperparameterName
in O
the O
n B-HyperparameterName
- O
th O
row O
of O
A. O

Aligner O
is O
the O
key O
component O
of O
AligNART B-MethodName
. O
It O
models O
a O
conditional O
distribution O
of O
alignments O
Agiven O
the O
source O
sentence O
xduring O
training O
. O
During O
inference O
, O
it O
aligns O
encoder O
outputs O
using O
the O
estimated O
alignments O
, O
as O
depicted O
in O
Figure O
1b O
. O

In O
the O
permutation O
process O
, O
the O
elements O
corresponding O
to O
the O
same O
target O
token O
are O
placed O
adjacent O
to O
each O
other O
, O
with O
h' B-HyperparameterName
being O
reordered O
into O
d' B-HyperparameterName
. O

In O
the O
grouping O
process O
, O
each O
element O
in O
d' B-HyperparameterName
is O
clustered O
into O
Ngroups O
by O
predicting O
whether O
each O
element O
is O
mapped O
to O
the O
same O
target O
token O
as O
the O
previous O
element O
. O
rn O
= O
P O
mAn;m O
denotes O
the O
number B-HyperparameterName
of I-HyperparameterName
el- I-HyperparameterName
ements I-HyperparameterName
in I-HyperparameterName
the I-HyperparameterName
n I-HyperparameterName
- I-HyperparameterName
th I-HyperparameterName
group I-HyperparameterName
which O
is O
equivalent O
to O
rn O
in O
Equation O
4 O
. O

In O
the O
alignment O
matrix O
, O
The B-HyperparameterName
number I-HyperparameterName
of I-HyperparameterName
non- I-HyperparameterName
zero I-HyperparameterName
elements I-HyperparameterName
is O
defined O
as O
L B-HyperparameterName
= O
P O
mcm O
= O
P O
nrn O
. O

For O
the O
permutation O
loss O
, O
we O
minimize O
the O
difference O
between O
the O
prediction O
and O
the O
ground O
truth O
. O

Our O
final O
loss O
function O
is O
defined O
as O
the O
sum O
of O
the O
negative O
log O
- O
likelihood O
based O
translation O
loss O
LT O
and O
alignment O
loss O
LA O
. O
L O
= O
LT O
+ O
LA O
= O
LT O
+ O
LD O
+ O
LP O
+ O
LG O
. O
where O
we O
set O
= O
= O
= O
0.5 B-HyperparameterValue
for O
all O
the O
experiments O
. O

We O
obtain O
a O
permutation O
matrix O
^P O
that O
minimizes O
the O
KL B-MetricName
- I-MetricName
divergence I-MetricName
as O
follows O
: O
^P= B-HyperparameterName
arg O
min O
P( X O
iX O
jPi;jlogPpred O
i;j):(13 O
) O
We O
utilize O
the O
linear O
sum O
assignment O
problem O
solver O
provided O
by O
Jones O
et O
al O
. O
( O
2001 O
) O
to O
ﬁnd O
^P. B-HyperparameterName
. O

The O
architecture O
of O
AligNART B-MethodName
is O
as O
follows O
: O

We O
evaluate O
our O
method O
on O
two O
translation O
datasets O
: O
WMT14 B-DatasetName
English I-DatasetName
- I-DatasetName
German I-DatasetName
( I-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
) I-DatasetName
and O
WMT16 B-DatasetName
English I-DatasetName
- I-DatasetName
Romanian I-DatasetName
( I-DatasetName
En I-DatasetName
- I-DatasetName
Ro I-DatasetName
) I-DatasetName
. O

The O
WMT14 B-DatasetName
En- I-DatasetName
De I-DatasetName
and O
WMT16 B-DatasetName
En I-DatasetName
- I-DatasetName
Ro I-DatasetName
datasets O
contain O
4.5 O
million O
and O
610 O
thousand O
training O
pairs O
, O
respectively O
. O

We O
use O
the O
preprocessing O
pipelines O
provided O
by O
fairseq1(Ott O
et O
al O
. O
, O
2019 O
) O
for O
the O
WMT14 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
dataset O
. O

AligNART B-MethodName
is O
a O
deep O
- O
shallow O
Transformer O
architecture O
. O

We O
set O
( O
dmodel B-HyperparameterName
/ O
dhidden B-HyperparameterName
) O
to O
( O
512 B-HyperparameterValue
/ O
2048 B-HyperparameterValue
) O
and O
the O
( O
dropout B-HyperparameterName
rate I-HyperparameterName
) O
to O
( O
0.3 B-HyperparameterValue
) O
. O

The O
number B-HyperparameterName
of I-HyperparameterName
heads I-HyperparameterName
in O
multi O
- O
head O
attention O
modules O
is O
8 B-HyperparameterValue
except O
for O
the O
last O
attention O
module O
of O
the O
permutation O
predictor O
, O
which O
is O
1 B-HyperparameterValue
. O

We O
set O
the O
number O
of O
tokens O
to O
approximately O
64 B-HyperparameterValue
K I-HyperparameterValue
for O
all O
the O
models O
we O
implement O
. O

We O
train O
all O
these O
models O
for O
300 B-HyperparameterValue
K I-HyperparameterValue
/ O
50 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
on O
En- B-DatasetName
De I-DatasetName
/ O
En B-DatasetName
- I-DatasetName
Ro I-DatasetName
datasets O
, O
respectively O
. O

We O
average O
the O
highest O
validation O
scores O
from O
the O
latest O
checkpoints O
for O
AligNART B-MethodName
. O

We O
optimize O
by O
using O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
= O
( O
0.9 B-HyperparameterValue
; O
0.98 B-HyperparameterValue
) O
and O
= O
10-8 B-HyperparameterValue
. O

The O
scheduling O
for O
learning B-HyperparameterName
rate I-HyperparameterName
follows O
that O
of O
Vaswani O
et O
al O
. O
( O
2017 O
) O
, O
starting O
with O
10-7 B-HyperparameterValue
and O
warming O
up O
to O
5e-4 B-HyperparameterValue
in O
10 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
. O

We O
use O
the O
label O
smoothing O
technique O
with O
ls B-HyperparameterName
= O
0.1 B-HyperparameterValue
for O
the O
target O
token O
distribution O
and O
each O
row O
of O
permutation O
matrix O
. O

Table O
1 O
shows O
the O
BLEU B-MetricName
scores O
, O
translation B-MetricName
latency I-MetricName
, O
and O
speedup B-MetricName
on O
WMT14 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
and O
WMT16 B-DatasetName
En I-DatasetName
- I-DatasetName
Ro I-DatasetName
. O

In O
explicit O
modality O
reduction O
, O
we O
are O
able O
to O
achieve O
higher O
scores O
than O
Distortion O
and O
ReorderNAT B-MethodName
, O
who O
both O
utilize O
the O
same O
alignment O
tool O
. O
This O
is O
because O
we O
leverage O
the O
entire O
alignment O
information O
, O
rather O
than O
partial O
information O
, O
such O
as O
fertility O
or O
ordering O
. O

This O
model O
does O
a O
significantly O
better O
job O
than O
previous O
models O
at O
reducing O
explicit O
modality O
, O
with O
the O
exception O
of O
SNAT B-MethodName
on O
En B-DatasetName
! I-DatasetName
Ro I-DatasetName
. O

In O
implicit O
dependency O
modeling O
, O
AligNART B-MethodName
( I-MethodName
GZ I-MethodName
) I-MethodName
outperforms O
Imputer O
and O
shows O
performance O
comparable O
to O
that O
of O
the O
state O
- O
of O
- O
the O
- O
art O
CTC O
- O
based O
model O
on O
En$De B-DatasetName
by O
simply O
augmenting O
Aligner O
module O
to O
deep O
- O
shallow O
NAT O
. O

Table O
2 O
demonstrates O
the O
BLEU B-MetricName
scores O
with O
re O
- O
scoring O
decoding O
strategies O
for O
the O
non O
- O
iterative O
NART O
models O
. O

We O
set O
m0 B-HyperparameterName
to O
l0= B-HyperparameterName
4 B-HyperparameterValue
, O
a B-HyperparameterName
to O
4 B-HyperparameterValue
, O
and O
b B-HyperparameterName
to O
2 B-HyperparameterValue
for O
8 O
candidates O
. O

AligNART B-MethodName
does O
better O
than O
the O
other O
methods O
on O
En!De B-DatasetName
and O
Ro!En B-DatasetName
, O
and O
is O
comparable O
to O
GLAT B-MethodName
on O
De B-DatasetName
! I-DatasetName
En I-DatasetName
. O

Non O
- O
iterative O
NART O
for O
explicit O
modality O
reduction O
performs O
best O
on O
En$De B-DatasetName
and O
Ro!En B-DatasetName
. O

After O
KD O
, O
Alig O
- O
NART O
shows O
decreased O
accuracy O
on O
the O
raw O
dataset O
, O
but O
high O
prediction O
accuracy O
on O
the O
distillation O
set O
, O
resulting O
in O
increased O
BLEU B-MetricName
scores O
. O

Accuracy O
bottlenecks O
in O
permutation O
and O
duplication O
predictors O
for O
KD O
, O
AligNART B-MethodName
using O
fast O
align O
and O
GIZA++ O
are O
shown O
in O
Table O
3 O
. O

The O
well O
- O
aligned O
decoder O
inputs O
significantly O
reduce O
the O
modality O
of O
the O
target O
distribution O
, O
so O
AligNART B-MethodName
( I-MethodName
FA I-MethodName
) I-MethodName
does O
not O
have O
any O
inconsistency O
problems O
. O

We O
observe O
that O
AligNART B-MethodName
achieves O
BLEU B-MetricName
scores O
comparable O
to O
those O
of O
CTC B-MethodName
, O
even O
with O
only O
partial O
information O
about O
the O
ground O
truth O
word O
alignments O
. O

To O
evaluate O
the O
modality O
reduction O
effects O
of O
Alig- B-MethodName
NART I-MethodName
, O
we O
conducted O
experiments O
on O
two O
aspects O
: O
BLEU B-MetricName
score O
and O
token O
repetition O
ratio O
. O

The O
WMT14 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
scores O
can O
be O
found O
in O
Table O
6 O
. O

En!De B-DatasetName
, O
AligNART B-MethodName
using O
fast O
align O
without O
KD O
achieves O
higher O
BLEU B-MetricName
scores O
than O
previous O
mod- O
els O
without O
KD O
and O
deep O
- O
shallow O
NAT O
with O
KD O
. O

In O
Table O
7 O
, O
the O
token O
repetition O
ratio O
of O
AligNART O
is O
less O
than O
that O
of O
the O
CMLM O
- O
base O
( O
Ghazvininejad O
et O
al O
. O
, O
2019 O
) O
of O
5 O
iterations O
, O
AXE B-MethodName
, O
and O
GLAT B-MethodName
. O

The O
results O
from O
Table O
6 O
show O
that O
alignment O
information O
can O
help O
reduce O
token O
repetition O
, O
even O
when O
the O
BLEU B-MetricName
score O
is O
lower O
than O
the O
score O
for O
deep O
- O
shallow O
NAT O
with O
KD O
. O

Our O
method O
consistently O
improves O
the O
performance O
of O
AligNART B-MethodName
. O

We O
train O
AligNART B-MethodName
and O
deep O
- O
shallow O
NAT O
without O
a O
cross O
attention O
module O
to O
compare O
them O
. O

The O
NAT O
without O
the O
cross O
attention O
module O
has O
less O
of O
an O
impact O
on O
the O
BLEU B-MetricName
score O
than O
the O
deep O
- O
shallow O
NAT O
. O

The O
deep O
- O
shallow O
architecture O
has O
a O
significant O
impact O
on O
the O
BLEU B-MetricName
scores O
of O
Alig- B-MethodName
NART I-MethodName
, O
as O
shown O
in O
Table O
8 O
. O

We O
investigate O
the O
trade O
- O
off O
between O
the O
alignment B-HyperparameterName
score I-HyperparameterName
ﬁltering I-HyperparameterName
ratio I-HyperparameterName
and O
BLEU B-MetricName
score O
using O
AligNART B-MethodName
( I-MethodName
GZ I-MethodName
) I-MethodName
. O

We O
observe O
that O
filtering O
out O
5 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
samples O
improves O
the O
BLEU B-MetricName
score O
in O
both O
directions O
. O

Despite O
increasing O
the O
filtering O
ratio O
, O
the O
performance O
is O
not O
impacted O
thanks O
to O
the O
noise O
filtering O
capability O
. O

Gu O
et O
al O
. O
( O
2018 O
) O
and O
Zhou O
et O
al O
. O
( O
2020b O
) O
exploit O
partial O
information O
from O
the O
ground O
truth O
alignments O
, O
as O
does O
Ran O
et O
al O
. O
( O
2021 O
) O
. O

The O
GIZA++ O
software O
shows O
similar O
performance O
to O
the O
CTC O
- O
based O
implicit O
dependency O
modeling O
approach O
on O
WMT14 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
and O
is O
able O
to O
reduce O
the O
amount O
of O
data O
needed O
. O

The O
grouping O
predictor O
in O
AligNART B-MethodName
models O
mappings O
where O
one O
or O
more O
inputs O
map O
to O
one O
or O
more O
outputs O
. O

Adding O
an O
extra O
token O
to O
AligNART B-MethodName
( I-MethodName
FA I-MethodName
) I-MethodName
allows O
us O
to O
deal O
with O
the O
problem O
of O
spurious O
words O
, O
as O
described O
in O
Section O
C.2 O
. O

There O
are O
as O
many O
empty O
rows O
in O
the O
fast O
align O
architecture O
as O
there O
are O
in O
the O
WMT14 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
architecture O
. O

g B-HyperparameterName
is O
equal O
to O
the B-HyperparameterName
probability I-HyperparameterName
of I-HyperparameterName
an I-HyperparameterName
un I-HyperparameterName
- I-HyperparameterName
permuted I-HyperparameterName
case I-HyperparameterName
. O

We O
set O
the O
alignment B-HyperparameterName
score I-HyperparameterName
ﬁltering I-HyperparameterName
ratio I-HyperparameterName
to O
equal O
5 B-HyperparameterValue
% I-HyperparameterValue
. O

We O
conduct O
a O
case O
study O
on O
the O
WMT14 B-DatasetName
De I-DatasetName
! I-DatasetName
En I-DatasetName
validation O
set O
to O
analyze O
various O
alignments O
and O
their O
translations O
during O
re O
- O
scoring O
decoding O
. O

In O
this O
sample O
, O
we O
observe O
that O
Alig- B-MethodName
NART I-MethodName
: O

1 O
. O
Can O
capture O
non O
- O
diagonal O
alignments O

2 O
. O
Models O
multiple O
alignments O

3 O
. O
Translates O
corresponding O
to O
the O
given O
alignments O

Guanhua O
Chen1 O
, O
Shuming O
Ma2 O
, O
Yun O
Chen3 O
, O
Li O
Dong2 O
, O
Dongdong O
Zhang2 O
, O
Jia O
Pan1 O
, O
Wenping O
Wang4 O
, O
and O
Furu O
Wei2 O
contributed O
to O
the O
research O
of O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName
with O
Multilingual B-MethodName
Pretrained I-MethodName
Encoders I-MethodName
. O

The O
main O
focus O
of O
previous O
work O
has O
been O
on O
improving O
the O
cross O
- O
lingual O
transfer O
for O
tasks O
with O
a O
multilingual O
pretrained O
encoder O
( O
MPE O
) O
, O
or O
on O
improving O
the O
performance O
of O
supervised O
machine O
translation O
with O
BERT O
. O

It O
is O
not O
well O
understood O
whether O
the O
MPE O
can O
help O
improve O
the O
ability O
of O
the O
NMT B-TaskName
model O
to O
be O
used O
across O
different O
languages O
. O

In O
this O
paper O
, O
we O
focus O
on O
a O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
task O
in O
the O
NMT B-TaskName
domain O
. O

We O
propose O
SixT B-MethodName
, O
an O
effective O
model O
that O
is O
both O
simple O
and O
easy O
to O
use O
for O
this O
task O
. O

SixT B-MethodName
uses O
the O
MPE O
with O
a O
two O
- O
stage O
training O
schedule O
and O
improves O
further O
with O
a O
position O
disentangled O
encoder O
and O
a O
capacity O
- O
enhanced O
decoder O
. O

This O
method O
outperforms O
a O
pretrained O
multilingual O
encoder- O
decoder O
model O
with O
an O
average O
improvement O
of O
7.1 B-MetricValue
BLEU B-MetricName
on O
zero O
- O
shot O
any O
- O
to O
- O
English O
test O
sets O
across O
14 O
source O
languages O
. O

With O
much O
less O
training O
computation O
cost O
and O
training O
data O
, O
our O
model O
outperforms O
CRISS B-MethodName
and O
m2m-100 B-MethodName
on O
any O
- O
to O
- O
English O
test O
sets O
. O

Pretrained O
encoders O
that O
are O
multilingual O
( O
MPE O
) O
such O
as O
mBERT B-MethodName
( O
Wu O
and O
Dredze O
, O
2019 O
) O
, O
XLM B-MethodName
( O
Conneau O
and O
Lample O
, O
2019 O
) O
, O
and O
XLM B-MethodName
- I-MethodName
R I-MethodName
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
have O
shown O
to O
be O
remarkably O
effective O
for O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
on O
tasks O
including O
natural B-TaskName
language I-TaskName
understanding I-TaskName
( O
NLU B-TaskName
) O
, O
named B-TaskName
entity I-TaskName
recognition I-TaskName
( O
NER B-TaskName
) O
, O
ques- B-TaskName
tion I-TaskName
answering I-TaskName
( O
QA B-TaskName
) O
, O
and O
natural B-TaskName
language I-TaskName
infer- I-TaskName
ence I-TaskName
( O
NLI B-TaskName
) O
. O

The O
pretrained O
model O
is O
then O
ﬁne O
- O
tuned O
on O
a O
downstream O
task O
using O
labeled O
data O
in O
a O
single O
language O
and O
evaluated O
on O
the O
same O
task O
in O
other O
languages O
. O

Given O
that O
MPE O
has O
been O
successful O
in O
tasks O
like O
cross- B-TaskName
lingual I-TaskName
NLU I-TaskName
, O
it O
's O
worth O
researching O
how O
to O
use O
that O
knowledge O
to O
do O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
in O
the O
NMT B-TaskName
task O
by O
taking O
advantage O
of O
MPE B-MethodName
. O

However O
, O
replacing O
the O
monolingual O
pretrained O
encoder O
with O
MPE O
in O
previous O
studies O
does O
not O
work O
well O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
of O
NMT B-TaskName
, O
as O
seen O
in O
the O
baselines O
in O
Table O
2 O
. O

It O
is O
still O
unclear O
how O
to O
use O
existing O
multilingual O
pretrained O
encoders O
to O
conduct O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
for O
the O
NMT B-TaskName
model O
. O

The O
task O
of O
this O
paper O
is O
MPE O
, O
which O
is O
translating O
multiple O
unseen O
languages O
by O
leveraging O
an O
MPE O
, O
as O
seen O
in O
Figure O
1 O
. O

We O
are O
proposing O
a O
system O
( O
SixT B-MethodName
) O
which O
can O
directly O
translate O
languages O
unseen O
during O
supervised O
training O
. O

We O
start O
with O
the O
encoder O
and O
decoder O
embeddings O
of O
SixT B-MethodName
being O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
suggest O
a O
two O
- O
part O
training O
schedule O
that O
makes O
a O
trade O
- O
off O
between O
supervised O
performance O
and O
transferability O
. O

The O
SixT B-MethodName
model O
, O
which O
is O
trained O
with O
only O
one O
language O
pair O
, O
can O
be O
transferred O
to O
unseen O
languages O
and O
alleviates O
the O
effect O
of O
' O
catastrophic O
forgetting O
' O
. O

SixT B-MethodName
outperforms O
mBART B-MethodName
significantly O
, O
with O
an O
average O
improvement O
of O
7.1 B-MetricValue
BLEU B-MetricName
across O
14 O
source O
languages O
. O

The O
SixT B-MethodName
model O
outperforms O
CRISS B-MethodName
and O
m2m-100 B-MethodName
on O
any O
- O
to O
- O
English O
test O
sets O
, O
with O
less O
training O
computation O
cost O
and O
training O
data O
. O

The O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
NMT I-TaskName
transfer I-TaskName
task O
explores O
ways O
to O
improve O
the O
cross O
- O
lingual O
transferability O
of O
NMT O
models O
. O

In O
contrast O
to O
multilingual B-TaskName
NMT I-TaskName
( O
Johnson O
et O
al O
. O
, O
2017 O
) O
, O
unsupervised B-TaskName
NMT I-TaskName
( O
Lample O
et O
al O
. O
, O
2018 O
) O
or O
zero B-TaskName
- I-TaskName
resource I-TaskName
NMT I-TaskName
through O
pivoting O
( O
Chen O
et O
al O
. O
, O
2017 O
, O
2018 O
) O
, O
neither O
the O
parallel O
nor O
monolingual O
data O
in O
the O
language O
li O
zis O
directly O
accessible O
in O
the O
ZeXT B-TaskName
task O
. O

In O
this O
paper O
, O
we O
use O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
which O
is O
trained O
on O
100 O
languages O
, O
as O
the O
off O
- O
the O
- O
shelf O
MPE O
. O

The O
ZeXT B-TaskName
task O
is O
looking O
for O
ways O
to O
build O
a O
many O
- O
to O
- O
one O
NMT O
model O
efficiently O
, O
that O
can O
translate O
from O
100 O
languages O
supported O
by O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
using O
a O
parallel O
dataset O
of O
only O
one O
language O
pair O
. O

The O
ZeXT B-TaskName
task O
provides O
a O
new O
perspective O
for O
the O
evaluation O
of O
different O
MPEs O
, O
which O
can O
hopefully O
facilitate O
the O
research O
on O
MPEs O
. O

We O
observe O
that O
it O
is O
best O
to O
initialize O
the O
encoder O
embedding O
, O
the O
encoder O
layers O
and O
the O
decoder O
embedding O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
keep O
their O
parameters O
frozen O
, O
while O
randomly O
initializing O
the O
decoder O
layers O
( O
see O
Figure O
2 O
) O
. O

This O
inspires O
us O
to O
introduce O
a O
new O
model O
that O
can O
further O
improve O
on O
zero B-TaskName
- I-TaskName
shot I-TaskName
translations I-TaskName
. O

The O
encoder O
from O
XLM B-MethodName
- I-MethodName
R I-MethodName
has O
strong O
positional O
representations O
that O
correspond O
to O
the O
source O
sentence O
. O

The O
default O
decoder O
configuration O
for O
training O
an O
NMT O
on O
the O
Europarl B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
training O
dataset O
is O
Transformer O
base O
( O
Gu O
et O
al O
. O
, O
2018 O
; O
Currey O
et O
al O
. O
, O
2020 O
) O
. O

The O
model O
capacity O
of O
SixT B-MethodName
is O
smaller O
than O
the O
vanilla O
Transformer O
with O
the O
same O
size O
. O

We O
focus O
on O
the O
any O
- O
to O
- O
English O
translations O
for O
the O
ZeXT B-TaskName
task O
. O

The O
Europarl B-DatasetName
- I-DatasetName
v7 I-DatasetName
German I-DatasetName
and I-DatasetName
English I-DatasetName
is O
used O
for O
training O
. O

We O
use O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
base O
model O
as O
the O
MPE O
that O
is O
ready O
to O
use O
. O

We O
set O
the O
Transformer O
encoder O
to O
the O
same O
size O
as O
the O
XLM- B-MethodName
R I-MethodName
base O
model O
. O

We O
refer O
to O
the O
model O
configuration O
as O
SixT B-MethodName
and O
use O
this O
configuration O
for O
our O
NMT O
models O
throughout O
the O
paper O
unless O
otherwise O
noted O
. O

We O
use O
a O
smaller O
decoder O
, O
denoted O
as O
BaseDec B-MethodName
, O
for O
strategy O
( O
1)–(7 O
) O
where O
decoder O
layers O
are O
trained O
from O
scratch O
. O

This O
model O
configuration O
is O
denoted O
by O
the O
letters O
A O
through O
G O
, O
with O
0 O
representing O
a O
small O
size O
. O

We O
denote O
the O
decoder O
for O
the O
remaining O
strategies O
as O
BigDec B-MethodName
. O

The O
Adam O
optimizer O
with O
1= O
0.9 B-HyperparameterValue
and O
2= O
0.98 B-HyperparameterValue
is O
used O
for O
training O
and O
evaluation O
. O

We O
use O
label B-HyperparameterName
smoothing I-HyperparameterName
to O
represent O
the O
value O
0.1 B-HyperparameterValue
. O

At O
the O
first O
stage O
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
0.0005 B-HyperparameterValue
and O
warmup B-HyperparameterName
step I-HyperparameterName
is O
4000 B-HyperparameterValue
. O

For O
the O
second O
stage O
, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
as O
the O
0.0001 B-HyperparameterValue
and O
do O
not O
use O
warmup O
. O

All O
the O
settings O
are O
set O
to O
0.3 B-HyperparameterValue
. O

We O
use O
eight O
GPUs O
, O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
set O
as O
4096 B-HyperparameterValue
tokens O
per O
GPU O
. O

Maximum B-HyperparameterName
updates I-HyperparameterName
number I-HyperparameterName
is O
the O
first O
stage O
and O
200k B-HyperparameterValue
is O
the O
second O
stage O
. O

We O
use O
beam O
search O
and O
do O
not O
tune O
the O
length O
penalty O
. O

We O
evaluate O
the O
results O
with O
the O
sacrebleu B-MetricName
method O
. O

If O
no O
specific O
checkpoint O
is O
chosen O
, O
the O
best O
one O
will O
be O
selected O
based O
on O
the O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
performance O
on O
the O
validation O
set O
for O
all O
experiments O
. O

We O
compare O
our O
model O
to O
vanilla B-MethodName
Transformer I-MethodName
and O
five O
other O
methods O
that O
use O
a O
pretrained O
Transformer O
encoder O
for O
NMT O
tasks O
. O

The O
encoders O
in O
these O
methods O
are O
replaced O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
base O
for O
a O
fair O
comparison O
. O

The O
encoder O
has O
the O
same O
size O
as O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
base O
, O
while O
the O
decoder O
uses O
the O
size O
of O
the O
BaseDec B-MethodName
. O

The O
encoder O
is O
initialized O
with O
all O
parameters O
being O
trained O
. O

All O
parameters O
except O
those O
of O
the O
cross O
attention O
module O
are O
initialized O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
and I-MethodName
and O
then O
fine O
- O
tuned O
. O

Zhu O
et O
al O
. O
( O
2020 O
) O
found O
that O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
output O
improved O
the O
NMT O
model O
when O
used O
as O
the O
encoder O
input O
, O
and O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
model O
was O
kept O
fixed O
during O
training O
. O

The O
method O
Imamura O
and O
Sumita O
proposed O
in O
2019 O
initializes O
the O
encoder O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
only O
trains O
the O
decoder O
at O
the O
first O
step O
. O
Then O
, O
all O
components O
are O
trained O
at O
the O
second O
step O
. O

According O
to O
Zhu O
et O
al O
. O
( O
2020 O
) O
, O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
output O
is O
fused O
into O
the O
encoder O
and O
decoder O
separately O
using O
an O
attention O
mechanism O
. O

The O
encoder O
embedding O
is O
initialized O
from O
XLM B-MethodName
- I-MethodName
R I-MethodName
to I-MethodName
to O
facilitate O
transfer O
. O

The O
parameters O
of O
XLM B-MethodName
- I-MethodName
R I-MethodName
are I-MethodName
will O
not O
change O
during O
training O
. O

In O
general O
, O
we O
find O
that O
it O
is O
most O
effective O
to O
use O
a O
large O
decoder O
, O
initialize O
the O
decoder O
embedding O
and O
all O
encoder O
parameters O
with O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
and O
train O
the O
decoder O
layers O
from O
scratch O
( O
Strategy O
( O
10 O
) O
) O
. O

In O
order O
to O
verify O
the O
effect O
of O
a O
capacity O
- O
enhanced O
decoder O
in O
the O
ZeXT B-TaskName
task O
, O
we O
train O
two O
models O
, O
vanilla B-MethodName
Trans- I-MethodName
former I-MethodName
and O
BaseDec B-MethodName
, O
using O
the O
same O
size O
of O
Strategy O
( O
7 O
) O
and O
Strategy O
( O
10 O
) O
respectively O
, O
and O
using O
the O
same O
training O
corpus O
. O
The O
vanilla O
Transformer O
model O
with O
BaseDec B-MethodName
and O
BigDec B-MethodName
obtains O
a O
BLEU B-MetricName
score O
of O
23.5 B-MetricValue
and O
22.9 B-MetricValue
on O
the O
De B-DatasetName
- I-DatasetName
En I-DatasetName
test O
set O
, O
respectively O
. O

The O
big O
decoder O
improves O
the O
performance O
of O
SixT B-MethodName
, O
but O
does O
not O
improve O
the O
performance O
of O
vanilla B-MethodName
Trans- I-MethodName
former I-MethodName
. O

This O
demonstrates O
that O
BigDec B-MethodName
can O
improve O
the O
zero B-TaskName
- I-TaskName
shot I-TaskName
translation I-TaskName
performance O
of O
our O
model O
. O

Table O
2 O
compares O
the O
performance O
of O
the O
proposed O
SixT B-MethodName
with O
the O
baselines O
. O

SixT B-MethodName
outperforms O
the O
best O
baseline O
by O
5.4 B-HyperparameterValue
on O
average O
, O
showing O
that O
SixT B-MethodName
is O
able O
to O
learn O
to O
translate O
while O
preserving O
the O
cross O
- O
lingual O
transferability O
of O
XLM B-MethodName
- I-MethodName
R. I-MethodName
. O

SixT B-MethodName
outperforms O
all O
other O
language O
pairs O
in O
terms O
of O
transfer O
scores O
. O

In O
contrast O
, O
vanilla B-MethodName
Transformer I-MethodName
can O
not O
be O
transferred O
well O
to O
distant O
languages O
, O
and O
neither O
can O
other O
baselines O
. O

In O
addition O
to O
performing O
well O
on O
tasks O
for O
which O
it O
has O
not O
been O
trained O
, O
SixT B-MethodName
also O
achieves O
the O
best O
result O
on O
De O
- O
En O
test O
set O
. O

Previous O
work O
( O
Conneau O
et O
al O
. O
, O
2020 O
; O
Hu O
et O
al O
. O
, O
2020 O
) O
mainly O
uses O
XLM B-MethodName
- I-MethodName
R I-MethodName
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
trans- I-TaskName
fer O
on O
NLU O
tasks O
. O

The O
experiments O
demonstrate O
that O
XLM B-MethodName
- I-MethodName
R I-MethodName
can O
also O
be O
utilized O
for O
zero B-TaskName
- I-TaskName
shot I-TaskName
neu- I-TaskName
ral I-TaskName
machine I-TaskName
translation I-TaskName
if O
it O
is O
fine O
- O
tuned O
properly O
. O

We O
conduct O
an O
ablation O
study O
on O
the O
proposed O
SixT B-MethodName
, O
as O
shown O
in O
Table O
3 O
. O

In O
general O
, O
SixT B-MethodName
has O
the O
best O
zero B-TaskName
- I-TaskName
shot I-TaskName
translation I-TaskName
outcomes O
, O
which O
emphasizes O
the O
significance O
of O
all O
three O
elements O
. O

The O
results O
of O
1 O
- O
3 O
show O
that O
TwoStage B-MethodName
and O
BigDec B-MethodName
improve O
the O
zero- B-TaskName
shot I-TaskName
translation I-TaskName
performance O
by O
0.8 B-MetricValue
and O
0.4 B-MetricValue
on O
average O
over O
( O
1 O
) O
. O

However O
, O
combining O
them O
together O
significantly O
improves O
average O
BLEU B-MetricName
over O
( O
1 O
) O
. O

This O
means O
that O
TwoStage B-MethodName
and O
BigDec O
work O
well O
together O
and O
it O
is O
important O
to O
use O
both O
of O
them O
. O

The O
results O
of O
6!5 O
confirm O
our O
claim O
: O
without O
using O
BigDec O
, O
the O
performance O
of O
SixT B-MethodName
drops O
by O
1.8 B-MetricValue
average O
BLEU B-MetricName
. O

We O
observe O
that O
the O
supervised O
task O
improves O
with O
TwoStage B-MethodName
and O
BigDec B-MethodName
while O
it O
degrades O
with O
Resdrop B-MethodName
. O

Since O
Resdrop B-MethodName
helps O
to O
build O
a O
more O
language O
- O
agnostic O
encoder O
, O
this O
is O
expected O
. O

Although O
Res- B-MethodName
drop I-MethodName
has O
a O
negative O
effect O
on O
supervised O
performance O
, O
it O
has O
a O
positive O
effect O
on O
zero B-TaskName
- I-TaskName
shot I-TaskName
translation I-TaskName
. O

The O
performance O
of O
zero O
- O
shot O
translation O
can O
be O
improved O
by O
either O
enhancing O
the O
supervised O
performance O
( O
with O
TwoStage B-MethodName
and O
BigDec B-MethodName
) O
or O
the O
model O
transferability O
( O
with O
Resdrop B-MethodName
) O
. O

In O
this O
section O
, O
we O
compare O
SixT B-MethodName
with O
mBART B-MethodName
, O
CRISS B-MethodName
, O
and O
m2m-100 B-MethodName
on O
any B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
English I-TaskName
test O
sets O
. O

mBART B-MethodName
is O
a O
strong O
multilingual O
encoder O
- O
decoder O
that O
is O
explicitly O
designed O
for O
NMT B-TaskName
. O

We O
fine O
- O
tune O
all O
model O
parameters O
on O
the O
training O
set O
that O
follows O
their O
setting O
. O

The O
CRISS B-MethodName
and O
m2m-100 B-MethodName
models O
are O
the O
most O
advanced O
unsupervised O
and O
supervised O
multilingual O
neural O
machine O
translation O
models O
available O
, O
respectively O
. O

The O
CRISS B-MethodName
model O
is O
trained O
on O
the O
mBART B-MethodName
model O
and O
then O
iteratively O
improved O
on O
1.8 O
billion O
sentences O
from O
90 O
different O
language O
pairs O
. O

m2m-100 B-MethodName
is O
a O
machine O
translation O
system O
that O
has O
been O
trained O
with O
7.5 O
billion O
parallel O
sentences O
across O
2200 O
translation O
directions O
. O

The O
performance O
of O
CRISS B-MethodName
and O
m2m-100 B-MethodName
may O
degrade O
because O
they O
are O
many O
- O
to O
- O
many O
NMT O
models O
that O
compete O
with O
each O
other O
, O
while O
SixT B-MethodName
is O
a O
many B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
one I-TaskName
NMT I-TaskName
model O
. O

The O
ofﬁcial O
m2m-100 B-MethodName
model O
is O
available O
in O
three O
sizes O
: O
small O
( O
418 B-HyperparameterValue
M I-HyperparameterValue
parameters B-HyperparameterName
) O
, O
base O
( O
1.2B B-HyperparameterValue
parameters B-HyperparameterName
) O
and O
large O
( O
12B B-HyperparameterValue
parameters B-HyperparameterName
) O
. O

The O
results O
of O
the O
m2m-100 B-MethodName
( I-MethodName
small I-MethodName
) I-MethodName
model O
are O
being O
reported O
. O

We O
train O
a O
many O
- O
to O
- O
one O
model O
with O
WMT19 B-DatasetName
German- I-DatasetName
English I-DatasetName
training O
data O
, O
which O
only O
consists O
of O
41 O
million O
sentence O
pairs O
, O
to O
compare O
with O
these O
models O
. O

All O
you O
need O
is O
a O
pre O
- O
trained O
XLM B-MethodName
- I-MethodName
R I-MethodName
large I-MethodName
model I-MethodName
, O
and O
it O
does O
n't O
need O
to O
contain O
any O
data O
in O
other O
languages O
. O

After O
the O
self O
- O
attention O
sublayer O
of O
the O
23 B-HyperparameterValue
- I-HyperparameterValue
th I-HyperparameterValue
( O
penultimate O
) O
encoder O
layer O
, O
we O
remove O
the O
residual O
connection O
. O

The O
SixT B-MethodName
large I-MethodName
model O
is O
significantly O
better O
than O
the O
mBART B-MethodName
, O
CRISS B-MethodName
, O
and O
m2m-100 B-MethodName
models O
. O

On O
average O
, O
BLEU B-MetricName
is O
higher O
for O
all O
languages O
than O
7.1 B-MetricValue
, O
0.5 B-MetricValue
, O
and O
1.4 B-MetricValue
, O
respectively O
. O

The O
SixT B-MethodName
model O
is O
larger O
, O
but O
the O
SixT B-MethodName
results O
are O
impressive O
considering O
SixT B-MethodName
does O
n't O
use O
any O
monolingual O
or O
parallel O
texts O
except O
for O
German B-DatasetName
- I-DatasetName
English I-DatasetName
training O
data O
. O

The O
multilingual O
encoder O
outperforms O
mBART B-MethodName
, O
showing O
that O
with O
the O
right O
tuning O
strategy O
, O
it O
can O
produce O
better O
results O
on O
NMT O
tasks O
. O

The O
SixT B-MethodName
model O
translates O
well O
to O
distant O
resource O
- O
poor O
languages O
like O
Ne O
and O
Si O
, O
which O
indicates O
a O
promising O
approach O
to O
translate O
resource O
- O
poor O
languages O
. O

More O
data O
from O
different O
language O
pairs O
could O
potentially O
improve O
the O
performance O
of O
the O
SixT B-MethodName
. O

The O
SixT B-MethodName
models O
are O
trained O
on O
different O
supervised O
language O
pairs O
, O
including O
De O
- O
En O
, O
Es O
- O
En O
, O
Fi O
- O
En O
, O
Hi O
- O
En O
and O
Zh O
- O
En O
. O
These O
models O
are O
then O
applied O
to O
all O
test O
sets O
, O
as O
seen O
in O
Table O
5 O
. O

We O
find O
that O
the O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
performs O
better O
when O
the O
SixT B-MethodName
is O
trained O
on O
source O
languages O
that O
are O
from O
the O
same O
language O
family O
. O

The O
performance O
of O
Hi B-DatasetName
- I-DatasetName
En I-DatasetName
is O
the O
best O
transfer O
performance O
, O
with O
the O
exception O
of O
Ko B-DatasetName
- I-DatasetName
En I-DatasetName
. O

When O
trained O
on O
3.5 O
million O
Hi O
- O
En O
sentence O
pairs O
, O
SixT B-MethodName
obtains O
promising O
results O
on O
the O
Ne O
- O
En O
and O
Si O
- O
En O
translation O
, O
with O
a O
BLEU B-MetricName
score O
of O
16.7 B-MetricValue
and O
9.6 B-MetricValue
, O
respectively O
. O

As O
a O
comparison O
, O
the O
vanilla B-MethodName
Trans- I-MethodName
former I-MethodName
that O
is O
supervised O
with O
the O
FLoRes B-DatasetName
training O
set O
only O
receives O
14.5 B-MetricValue
and O
7.2 B-MetricValue
BLEU B-MetricName
score O
( O
Liu O
et O
al O
. O
, O
2020 O
) O
on O
the O
same O
test O
sets O
. O

A O
different O
way O
to O
translate O
resource O
- O
poor O
languages O
is O
to O
train O
SixT B-MethodName
on O
similar O
high O
- O
resource O
language O
pairs O
. O

We O
train O
vanilla B-MethodName
Transformer I-MethodName
configured O
as O
Transformer B-MethodName
big I-MethodName
without O
MPE O
initialization O
using O
the O
same O
training O
and O
validation O
sets O
for O
comparison O
. O

The O
poor O
performance O
of O
vanilla B-MethodName
Transformer I-MethodName
when O
zero O
- O
shot O
cross O
- O
lingualism O
is O
attempted O
. O

We O
compare O
the O
vanilla B-MethodName
Transformer I-MethodName
big I-MethodName
model O
and O
SixT B-MethodName
model O
to O
see O
if O
the O
SixT B-MethodName
model O
degrades O
performance O
on O
the O
supervised O
language O
pair O
in O
order O
to O
gain O
cross O
- O
lingual O
transfer O
ability O
. O

When O
there O
are O
more O
than O
20 O
million O
parallel O
sentences O
available O
, O
the O
performance O
of O
SixT B-MethodName
is O
lower O
than O
vanilla B-MethodName
Transformer I-MethodName
, O
but O
it O
performs O
better O
with O
fewer O
parallel O
sentences O
. O

The O
Hindi O
- O
to O
- O
English O
translation O
is O
an O
exception O
where O
the O
word O
" O
SixT B-MethodName
" O
has O
a O
lower O
BLEU B-MetricName
score O
. O

The O
SixT B-MethodName
model O
size O
is O
expected O
to O
increase O
when O
large O
amounts O
of O
bi O
- O
text O
data O
are O
given O
in O
order O
to O
fully O
digest O
the O
bi O
- O
text O
. O

If O
we O
replace O
SixT B-MethodName
with O
SixT B-MethodName
large I-MethodName
and O
train O
SixT B-MethodName
large I-MethodName
on O
WMT19 B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
, O
we O
get O
results O
on O
the O
De B-DatasetName
- I-DatasetName
En I-DatasetName
test O
set O
that O
are O
comparable O
to O
those O
obtained O
by O
vanilla B-MethodName
Transformer I-MethodName
( O
see O
Table O
4 O
) O
. O

We O
examine O
the O
relationship O
between O
the O
ability O
to O
transfer O
cross O
- O
lingually O
and O
the O
size O
of O
training O
data O
by O
comparing O
the O
zero O
- O
shot O
BLEU B-MetricName
scores O
of O
SixT O
models B-MethodName
trained O
on O
Eu- B-DatasetName
roparl I-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
and O
WMT19 B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
. O

The O
sentence O
shows O
that O
increasing O
the O
size O
of O
training O
data O
can O
improve O
the O
performance O
of O
zero- B-TaskName
shot I-TaskName
translation I-TaskName
. O

For O
example O
, O
if O
SixT B-MethodName
is O
trained O
with O
WMT19 B-DatasetName
, O
it O
will O
improve O
by O
3.4 B-MetricValue
average O
BLEU B-MetricName
compared O
to O
SixT B-MethodName
being O
trained O
with O
Europarl B-DatasetName
- I-DatasetName
v7 I-DatasetName
. O

We O
train O
two O
models O
, O
one O
on O
WMT16 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
and O
the O
other O
on O
WMT19 B-DatasetName
En I-DatasetName
- I-DatasetName
De I-DatasetName
, O
to O
build O
a O
many O
- O
to O
- O
one O
NMT O
model O
with O
another O
target O
language O
. O

When O
the O
target O
language O
is O
not O
English O
, O
SixT B-MethodName
can O
reasonably O
transfer O
scores O
to O
unseen O
source O
languages O
, O
as O
shown O
in O
Table O
8 O
. O

There O
is O
related O
work O
to O
this O
paper O
in O
the O
form O
of O
Zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
learning I-TaskName
. O


 O
There O
is O
similar O
research O
to O
what O
is O
presented O
in O
this O
paper O
in O
the O
form O
of O
Zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
learning I-TaskName
. O

Multilingual O
pretrained O
models O
have O
been O
successful O
for O
various O
NLP O
tasks O
, O
such O
as O
mBERT B-MethodName
( O
Wu O
and O
Dredze O
, O
2019 O
) O
, O
XLM B-MethodName
- I-MethodName
R I-MethodName
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
, O
mBART B-MethodName
( O
Liu O
et O
al O
. O
, O
2020 O
) O
, O
and O
mT5 B-MethodName
( O
Xue O
et O
al O
. O
, O
2021 O
) O
. O

For O
example O
, O
Liu O
et O
al O
. O
( O
2020 O
) O
propose O
an O
encoder O
- O
decoder O
- O
based O
Transformer O
that O
is O
explicitly O
designed O
for O
NMT O
and O
demonstrates O
that O
it O
can O
be O
fine O
- O
tuned O
for O
supervised O
and O
unsupervised O
tasks O
. O

We O
use O
MPE O
for O
zero- B-TaskName
shot I-TaskName
translation I-TaskName
instead O
of O
supervised O
translation O
, O
which O
is O
different O
from O
what O
they O
do O
. O

We O
focus O
on O
the O
task O
of O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross- I-TaskName
lingual I-TaskName
NMT I-TaskName
transfer I-TaskName
( O
ZeXT B-TaskName
) O
, O
which O
aims O
to O
use O
an O
MPE O
for O
machine O
translation O
while O
preserving O
its O
ability O
to O
cross O
- O
lingual O
transfer O
. O

In O
this O
task O
, O
only O
a O
multilingual O
pretrained O
encoder O
such O
as O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
one O
parallel O
dataset O
such O
as O
German O
- O
English O
are O
available O
. O

We O
suggest O
using O
SixT B-MethodName
for O
this O
task O
, O
which O
enables O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
for I-TaskName
NMT I-TaskName
by O
taking O
advantage O
of O
the O
la- O
belled O
data O
and O
improving O
the O
transferability O
of O
XLM B-MethodName
- I-MethodName
R. I-MethodName
. O

Extensive O
experiments O
have O
shown O
that O
SixT O
is O
an O
effective O
model O
, O
outperforming O
the O
pretrained O
encoder O
- O
decoder O
- O
based O
model O
mBART B-MethodName
that O
was O
specifically O
designed O
for O
NMT O
. O

It O
outperforms O
CRISS B-MethodName
and O
m2m-100 B-MethodName
, O
two O
strong O
multilingual O
NMT O
models O
, O
on O
15 O
any O
- O
to- O
tasks O
. O

The O
following O
sentence O
is O
referencing O
Roee O
Aharoni O
, O
Melvin O
Johnson O
, O
and O
Orhan O
Firat O
's O
2019 O
paper O
Massively B-TaskName
multilingual I-TaskName
neural I-TaskName
machine I-TaskName
translation I-TaskName
, O
which O
can O
be O
found O
in O
the O
Proceedings O
of O
NAACL O
on O
pages O
3874 O
- O
3884 O
. O

Yun O
Chen O
, O
Yang O
Liu O
, O
Yong O
Cheng O
, O
and O
Victor O
OK O
Li O
. O
2017 O
. O
A O
teacher O
- O
student O
framework O
for O
zero- B-TaskName
resource I-TaskName
neural I-TaskName
machine I-TaskName
translation I-TaskName
. O
In O
Proceedings O
of O
ACL O
, O
pages O
1925–1935 O
. O

The O
sentence O
is O
about O
a O
study O
done O
by O
Yun O
Chen O
, O
Yang O
Liu O
, O
and O
Victor O
OK O
Li O
in O
2018 O
. O
The O
study O
is O
titled O
" O
Zero- B-TaskName
resource I-TaskName
neural I-TaskName
machine I-TaskName
translation I-TaskName
with O
multi O
- O
agent O
communication O
game O
. O
" O
The O
study O
was O
presented O
at O
the O
Thirty O
- O
Second O
AAAI O
Conference O
on O
Artificial O
Intelligence O
. O

All O
texts O
are O
tokenized O
with O
the O
same O
XLM O
- O
R O
sentencepiece O
( O
Kudo O
, O
2018 O
) O
model O
to O
be O
compatible O
with O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
model O
. O

The O
< O
bos O
> O
token O
is O
added O
to O
the O
beginning O
of O
each O
source O
sentence O
while O
the O
< O
eos O
> O
token O
is O
appended O
to O
the O
end O
when O
the O
NMT O
model O
initializes O
the O
encoder O
with O
XLM B-MethodName
- I-MethodName
R. I-MethodName
. O

The O
source B-HyperparameterName
sentence I-HyperparameterName
length I-HyperparameterName
is O
restricted O
to O
512 B-HyperparameterValue
tokens I-HyperparameterValue
. O

The O
encoder O
of O
the O
SixT B-MethodName
model O
is O
the O
same O
size O
as O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
model O
. O

The O
number B-HyperparameterName
of I-HyperparameterName
attention I-HyperparameterName
heads I-HyperparameterName
is O
set O
to O
16 B-HyperparameterValue
for O
the O
decoder O
of O
the O
SixT B-MethodName
large I-MethodName
model O
, O
so O
that O
the O
dimension B-HyperparameterName
of I-HyperparameterName
hidden I-HyperparameterName
states I-HyperparameterName
can O
be O
divided O
by O
the O
number B-HyperparameterName
of I-HyperparameterName
attention I-HyperparameterName
heads I-HyperparameterName
. O

The O
source O
vocabulary O
and O
target O
vocabulary O
both O
use O
the O
same O
250k O
vocabulary O
of O
XLM B-MethodName
- I-MethodName
R I-MethodName
. O

In O
Table O
4 O
, O
we O
compare O
SixT B-MethodName
large I-MethodName
with O
CRISS B-MethodName
, O
m2m-100 B-MethodName
, O
and O
mBART B-MethodName
. O

We O
use O
the O
official O
model O
checkpoints O
of O
mBART15 B-MethodName
, O
(611 B-HyperparameterValue
M I-HyperparameterValue
, O
parame- B-HyperparameterName
ters I-HyperparameterName
, O
CRISS16 B-MethodName
, O
(680 B-HyperparameterValue
M I-HyperparameterValue
, O
parameters B-HyperparameterName
, O
and O
m2m-100 B-MethodName
. O

The O
mediators O
play O
a O
role O
in O
determining O
the O
order O
of O
processing O
for O
BERT B-MethodName
. O

We O
find O
that O
it O
is O
possible O
to O
get O
196 O
different O
rankings O
between O
the O
tasks O
when O
manipulating O
the O
distribution O
of O
context O
lengths O
in O
the O
probing O
dataset O
. O

, O
2019 O
) O


 O
Transformer O
- O
based O
pretrained O
embeddings O
are O
the O
basis O
for O
many O
tasks O
within O
NLP B-TaskName
, O
which O
underscores O
the O
importance O
in O
interpreting O
their O
behavior O
( O
Belinkov O
et O
al O
. O
, O
2020 O
) O
. O
This O
is O
especially O
true O
for O
the O
behavior O
of O
BERT B-MethodName
( O
Devlin O
et O
. O
, O
2019 O
) O
. O

probing O
. O


 O
In O
this O
work O
, O
we O
analyze O
the O
common O
approach O
of O
probing O
( O
2 O
) O
, O
used O
to O
localize O
where O
" O
knowledge O
" O
is O
located O
. O
The O
code O
is O
available O
at O
https://github.com/lovodkin93/ABCDEFG0-context-distance O
. O

We O
contest O
T19 O
's O
interpretation O
of O
the O
results O
, O
namely O
that O
the O
processing O
carried O
out O
by O
BERT B-MethodName
parallels O
the O
classical O
NLP B-TaskName
pipeline O
, O
despite O
reaffirming O
T19 O
's O
experimental O
findings O
. O

According O
to O
T19 O
, O
the O
lower O
layers O
are O
responsible O
for O
performing O
lexical O
tasks O
, O
while O
more O
semantic O
tasks O
are O
carried O
out O
by O
the O
higher O
layers O
. O

This O
sentence O
is O
saying O
that O
the O
amount O
equals O
the O
span O
length O
in O
tasks O
that O
involve O
a O
single O
span O
, O
and O
that O
it O
also O
equals O
the O
dependency O
length O
in O
tasks O
that O
involve O
two O
spans O
. O

We O
decided O
to O
consider O
context O
length O
as O
a O
mediator O
because O
previous O
work O
has O
shown O
that O
long O
- O
distance O
dependencies O
pose O
a O
difficulty O
in O
various O
NLP B-TaskName
tasks O
. O

Additionally O
, O
our O
findings O
demonstrate O
that O
when O
context O
length O
is O
not O
considered O
, O
one O
task O
( O
e.g. O
, O
dependency B-TaskName
parsing I-TaskName
) O
may O
appear O
to O
be O
processed O
at O
a O
higher O
level O
than O
another O
( O
e.g. O
, O
NER B-TaskName
) O
, O
when O
its O
expected O
level O
( O
as O
seen O
in O
$ O
2 O
) O
is O
actually O
lower O
for O
all O
ranges O
of O
context O
lengths O
( O
3.1.1 O
) O
. O

We O
follow O
T19 O
's O
lead O
and O
probe O
BERT B-MethodName
with O
Named O
Entity O
Recognition O
( O
NER B-TaskName
) O
, O
a O
task O
that O
classifies O
Non O
- O
terminals O
- O
Non O
- O
term O
. O
, O
Semantic O
Role O
Labeling O
( O
SRL B-TaskName
) O
, O
ABCDEFG3(ABCDEFG4 O
. O
) O
, O
Semantic O
Proto O
- O
Roles O
( O
SPR B-TaskName
; O
Reisinger O
et O
al O
. O
, O
2015 O
) O
, O
Relation O
Classification O
( O
RC B-TaskName
) O
, O
and O
the O
Stanford O
ABCDEFG7(ABCDEFG8 O
. O
; O
de O
Marneffe O
et O
al O
. O
, O
2006 O
) O
. O

The O
expected O
layer O
metric O
( O
referred O
to O
as O
Elayer B-MetricName
) O
of O
T19 O
assesses O
which O
layer O
in O
BERT B-MethodName
is O
most O
needed O
for O
prediction O
. O
A O
probing B-MetricName
classiﬁer I-MetricName
P I-MetricName
is O
trained O
on O
the O
lowest O
layers O
. O

Then O
, O
a O
differential O
score O
( O
l O
) O
is O
computed O
, O
which O
indicates O
the O
performance O
gain O
when O
taking O
into O
account O
one O
additional O
layer O
: O
( O
l O
) O
= O
Score(P(l O
) O
) O
- O
Score(P(l-1 O
) O
) O
. O
Once O
all O
the O
( O
l)s O
are O
computed O
, O
we O
may O
compute O
ABCDEFG O
: O
ABCDEFG O
= O
∑l O
( O
l)∑l O
( O
l O
) O
. O
Therefore O
, O
unlike O
standard O
edge O
probing O
, O
which O
is O
performed O
on O
each O
layer O
individually O
, O
computing O
ABCDEFG O
takes O
into O
account O
all O
layers O
up O
to O
a O
given O
l. O

In O
other O
words O
, O
for O
tasks O
with O
two O
spans O
, O
the O
context O
length O
is O
the O
distance O
between O
the O
end O
of O
the O
first O
span O
and O
the O
beginning O
of O
the O
second O
span O
. O
For O
tasks O
with O
just O
one O
span O
, O
the O
context O
length O
is O
the O
distance O
between O
the O
end O
of O
the O
span O
and O
the O
beginning O
of O
the O
context O
. O

We O
examine O
the O
effect O
of O
context O
length O
on O
Elayer B-MetricName
by O
modeling O
it O
as O
a O
mediating O
factor O
, O
which O
partly O
explains O
the O
relationship O
between O
two O
other O
variables O
( O
in O
this O
work O
, O
a O
task O
and O
its O
Elayer B-MetricName
) O
. O

There O
are O
two O
possible O
ways O
to O
look O
at O
how O
context O
length O
affects O
task O
Elayer B-MetricName
. O

The O
relationship O
we O
see O
between O
the O
task O
, O
the O
context O
length O
, O
and O
Elayer B-MetricName
is O
that O
they O
are O
all O
connected O
. O

Interestingly O
, O
we O
encounter O
a O
special O
edge O
case O
in O
3.1.1 O
, O
where O
the O
aggregated O
average O
( O
i.e. O
Elayer B-MetricName
) O
of O
one O
task O
is O
higher O
than O
another O
. O

We O
hypothesize O
that O
the O
context O
length O
is O
a O
mediating O
factor O
in O
the O
task O
. O

We O
use O
the O
SPR1 B-DatasetName
dataset O
to O
probe O
for O
the O
SPR B-TaskName
task O
, O
the O
SemEval O
2010 O
Task O
8 O
for O
the O
Dep B-TaskName
task O
, O
and O
the O
RC B-TaskName
dataset O
for O
the O
other O
tasks O
. O

We O
want O
to O
check O
if O
the O
context O
length O
really O
affects O
Elayer B-MetricName
and O
if O
the O
task O
is O
the O
only O
thing O
that O
contributes O
to O
this O
. O

We O
compile O
a O
dataset O
for O
a O
task O
containing O
only O
examples O
with O
context O
lengths O
shorter O
than O
a O
threshold O
, O
and O
use O
it O
to O
compute O
Elayer B-MetricName
. O

The O
Elayer B-MetricName
task O
increases O
when O
the O
threshold O
is O
increased O
, O
most O
notably O
the O
Co B-TaskName
- I-TaskName
ref I-TaskName
, O
SRL B-TaskName
, O
Dep B-TaskName
, O
and O
RC B-TaskName
tasks O
. O

We O
compute O
Elayer B-MetricName
for O
similar O
length O
subsets O
. O

The O
following O
sentence O
shows O
Figure O
2 O
as O
a O
function O
of O
a O
threshold O
on O
the O
context O
length O
. O

The O
y O
- O
axis O
value O
Elayer B-MetricName
is O
only O
computed O
based O
on O
examples O
with O
a O
context O
length O
of O
thr O
or O
less O
. O

Dep B-TaskName
, O
NER B-TaskName
, O
and O
their O
Elayer B-MetricName
cover O
the O
range O
of O
context O
lengths O
. O

We O
then O
consider O
a O
case O
where O
all O
the O
context O
lengths O
of O
Dep B-TaskName
are O
of O
length O
9 O
+ O
, O
while O
those O
of O
NER B-TaskName
are O
in O
the O
range O
of O
3 O
- O
5 O
( O
Figure O
3 O
) O
. O

We O
see O
that O
when O
controlling O
for O
context O
length O
, O
Dep B-TaskName
is O
computed O
in O
a O
lower O
layer O
than O
NER B-TaskName
, O
regardless O
of O
the O
range O
. O

The O
outcome O
may O
be O
completely O
different O
depending O
on O
the O
distribution O
of O
context O
lengths O
in O
the O
probing B-DatasetName
dataset I-DatasetName
. O

, O
2002 O
) O
. O


 O
These O
results O
suggest O
that O
the O
results O
of O
T19 O
do O
not O
necessarily O
mean O
that O
BERT B-MethodName
is O
performing O
a O
pipeline O
of O
computations O
( O
as O
is O
commonly O
believed O
, O
for O
example O
T19 O
and O
Blevins O
et O
al O
. O
, O
2002 O
) O
. O

It O
is O
important O
to O
consider O
mediating O
factors O
when O
interpreting O
Elayer B-MetricName
. O

Figure O
3 O
: O
Elayer B-MetricName
is O
made O
up O
of O
NER B-TaskName
and O
Dep B-TaskName
. O

The O
context O
length O
ranges O
for O
NER B-TaskName
are O
l2[3;5 O
] O
and O
for O
Dep B-TaskName
are O
_ O
_ O
_ O
_ O
. O

For O
some O
context O
length O
distribution O
, O
the O
order O
may O
be O
reversed O
, O
such O
that O
Dep B-TaskName
's O
Elayer B-MetricName
is O
bigger O
than O
NER B-TaskName
's O
. O

The O
meaning O
of O
the O
sentence O
changes O
depending O
on O
the O
distribution O
of O
context O
lengths O
in O
the O
probing B-DatasetName
dataset I-DatasetName
. O

We O
use O
mediation O
analysis O
, O
specifically O
focusing O
on O
the O
Natural O
Direct O
Effect O
( O
NDE O
) O
, O
which O
is O
the O
difference O
between O
two O
observed O
dependent O
variables O
( O
in O
our O
case O
Elayer B-MetricName
) O
when O
fixing O
the O
mediator O
. O

In O
our O
case O
, O
the O
NDE O
is O
the O
difference O
between O
the O
Elayer B-MetricName
of O
two O
tasks O
, O
while O
keeping O
the O
context O
length O
distribution O
the O
same O
for O
both O
. O

We O
propose O
a O
more O
detailed O
and O
accurate O
method O
to O
compare O
the O
expected O
layers O
, O
which O
does O
not O
rely O
on O
a O
specific O
length O
distribution O
, O
after O
observing O
that O
the O
distribution O
of O
context O
length O
in O
the O
probing B-DatasetName
dataset I-DatasetName
may O
affect O
the O
relative O
order O
of O
the O
expected O
layers O
. O

We O
plot O
the O
controlled O
effect O
, O
Elayer B-MetricName
, O
for O
each O
range O
separately O
. O

Our O
results O
in O
Figure O
5 O
show O
the O
range O
of O
possible O
expected O
layers O
for O
a O
task O
that O
may O
result O
from O
taking O
any O
context O
length O
distribution O
in O
Figure O
4 O
. O

There O
are O
four O
groups O
: O
NER B-TaskName
and O
RC B-TaskName
in O
the O
left O
group O
, O
SPR B-TaskName
and O
RC B-TaskName
in O
the O
middle O
group O
, O
and O
ABCDEFG4 O
and O
ABCDEFG5 O
in O
the O
right O
group O
. O

The O
employed O
context O
length O
distributions O
for O
NDE O
calculations O
are O
respectively O
Co B-TaskName
- I-TaskName
ref I-TaskName
, O
NER B-TaskName
, O
and O
SPR B-TaskName
. O

The O
figure O
below O
demonstrates O
the O
potential O
range O
of O
behaviors O
between O
different O
task O
pairs O
, O
from O
those O
with O
a O
notable O
difference O
in O
expected O
layers O
, O
like O
SRL B-TaskName
and O
Co B-TaskName
- I-TaskName
ref I-TaskName
, O
to O
those O
whose O
ordering O
of O
expected O
layers O
might O
be O
reversed O
, O
as O
is O
the O
case O
with O
SPR B-TaskName
and O
RC B-TaskName
. O

We O
get O
196 O
possible O
rankings O
of O
the O
seven O
tasks O
according O
to O
their O
Elayer B-MetricName
by O
taking O
into O
account O
every O
possible O
combination O
of O
context O
length O
distribution O
for O
each O
of O
the O
tasks O
. O

ABCDEFG O
is O
less O
than O
0 O
. O

. O
. O
< O
ABCDEFG9 O
< O
ABCDEFG10 O


 O
SRL B-TaskName
is O
less O
than O
RC B-TaskName
, O
which O
is O
less O
than O
. O
. O
. O
, O
which O
is O
less O
than O
ABCDEFG9 O
, O
which O
is O
less O
than O
ABCDEFG10 O
. O

The O
number O
Co B-TaskName
- I-TaskName
ref I-TaskName
is O
greater O
than O
NER B-TaskName
. O

This O
sentence O
has O
no O
meaning O
. O

To O
recap O
, O
the O
difference O
in O
Elayer B-MetricName
between O
some O
tasks O
may O
change O
considerably O
and O
their O
order O
may O
reverse O
, O
depending O
on O
the O
context O
length O
. O

We O
showed O
that O
the O
length O
of O
the O
context O
has O
a O
significant O
impact O
on O
the O
task O
's O
Elayer B-MetricName
. O

Our O
work O
suggests O
that O
other O
factors O
should O
be O
taken O
into O
account O
when O
basing O
analysis O
on O
the O
Elayer B-MetricName
. O

To O
generate O
typed O
dependency B-TaskName
parses I-TaskName
from O
phrase O
structure O
parses O
, O

The O
deep O
bidirectional O
transformers O
for O
language O
understanding O
are O
pretrained O
. O

Visualizing O
and O
understanding O
neural O
models O
in O
NLP B-TaskName
is O
important O
. O

BERT B-MethodName
rediscovers O
the O
pipeline O
that O
was O
originally O
NLP B-TaskName
. O

A O
dependency B-TaskName
parser I-TaskName
can O
be O
used O
to O
improve O
subject O
- O
object O
- O
verb O
languages O
. O

Figure O
7 O
: O
Elayer B-MetricName
of O
SRL B-TaskName
and O
Non O
- O
term O
. O


 O
This O
sentence O
is O
showing O
a O
figure O
that O
has O
two O
parts O
, O
Elayer B-MetricName
and O
SRL B-TaskName
, O
with O
the O
latter O
being O
non O
- O
terminal O
. O

For O
context O
length O
ranges O
4 O
and O
below O
, O
blue O
and O
yellow O
pairs O
are O
Elayer B-MetricName
, O
and O
all O
instances O
of O
SRL B-TaskName
are O
of O
context O
length O
l2[0;2 O
] O
. O

For O
some O
context O
length O
distributions O
, O
the O
Elayer B-MetricName
of O
SRL B-TaskName
may O
be O
smaller O
than O
that O
of O
Non O
- O
term O
. O

We O
compare O
the O
unmediated O
difference O
between O
task O
pairs O
with O
the O
pairs O
' O
NDE O
. O

For O
each O
task O
- O
pair O
in O
figure O
6 O
, O
we O
calculate O
the O
maximum O
difference O
between O
them O
. O

Namely O
, O
for O
each O
such O
pair O
, O
we O
juxtapose O
the O
difference O
between O
the O
maximal O
possible O
Elayer B-MetricName
of O
the O
first O
task O
and O
the O
minimal O
Elayer B-MetricName
of O
the O
second O
one O
with O
the O
opposite O
case O
( O
the O
difference O
between O
the O
minimal O
possible O
Elayer B-MetricName
of O
the O
first O
task O
and O
the O
maximal O
Elayer B-MetricName
of O
the O
second O
one O
) O
. O

The O
difference O
between O
unmediated O
Elayer B-MetricName
and O
NDE O
is O
shown O
in O
Figure O
9 O
for O
every O
task O
- O
pair O
. O

The O
employed O
context O
length O
distributions O
for O
the O
NDE O
calculations O
are O
, O
from O
left O
to O
right O
, O
NER B-TaskName
, O
SRL B-TaskName
, O
Dep B-TaskName
, O
Non O
- O
term O
. O
, O
SRL B-TaskName
, O
Co B-TaskName
- I-TaskName
ref I-TaskName
. O
, O
Dep B-TaskName
. O
, O
Non O
- O
term O
. O
, O
SRL B-TaskName
, O
Non O
- O
term O
. O
, O
SPR B-TaskName
, O
SRL B-TaskName
, O
SPR B-TaskName
, O
SPR B-TaskName
, O
Non O
- O
term O
. O
, O
SRL B-TaskName
, O
RC B-TaskName
, O
NER B-TaskName
, O
Non O
- O
term O
. O
, O
Dep B-TaskName
. O
and O
SRL B-TaskName
. O

Zhiyuan O
Ma O
, O
Jianjun O
Li O
, O
Guohui O
Li O
, O
and O
Yongjing O
Cheng O
from O
Huazhong O
University O
of O
Science O
and O
Technology O
and O
National O
University O
of O
Defense O
Technology O
in O
China O
respectively O
created O
a O
framework O
that O
uses O
a O
uniﬁed O
transformer O
to O
improve O
the O
semantic O
representation O
for O
Multimodal B-MethodName
Task I-MethodName
- I-MethodName
Oriented I-MethodName
Dialog I-MethodName
Systems I-MethodName
. O

We O
propose O
UniTranSeR B-MethodName
, O
a O
Uni O
ﬁed O
Transformer O
Semantic O
Representation O
frame- O
work O
with O
feature O
alignment O
and O
intention O
rea- O
to O
address O
these O
issues O
. O

The O
results O
of O
the O
experiment O
show O
that O
UniTranSeR B-MethodName
is O
much O
better O
than O
current O
state O
- O
of O
- O
the O
- O
art O
approaches O
on O
the O
representative O
MMD B-DatasetName
dataset O
. O

2018 O
saw O
the O
release O
of O
multimodal B-DatasetName
dialog I-DatasetName
dataset I-DatasetName
in O
the O
online O
retail O
domain O
. O

Since O
multimodal O
cues O
can O
improve O
and O
complement O
each O
other O
, O
it O
would O
be O
beneficial O
to O
learn O
the O
inter O
- O
modal O
features O
and O
improve O
natural O
language O
understanding O
. O

After O
acquiring O
intra O
- O
modal O
representations O
, O
without O
learning O
fine O
- O
grained O
alignment O
between O
different O
modalities O
before O
fusion O
, O
is O
not O
favorable O
to O
query O
knowledge O
for O
accurate O
multimodal B-TaskName
re- I-TaskName
sponse I-TaskName
generation I-TaskName
. O

We O
propose O
a O
framework O
called O
Uniﬁed B-MethodName
Transformer I-MethodName
Semantic I-MethodName
Repre- I-MethodName
sentation I-MethodName
that O
uses O
feature O
alignment O
and O
intention O
reasoning O
to O
address O
the O
aforementioned O
limitations O
. O

We O
carry O
out O
experiments O
on O
MMD B-DatasetName
, O
which O
is O
one O
of O
the O
most O
important O
benchmark O
datasets O
for O
multimodal B-TaskName
dialog I-TaskName
generation I-TaskName
. O

We O
use O
the O
evaluation O
script O
of O
dialog B-TaskName
generation I-TaskName
and O
demonstrate O
that O
UniTranSeR B-MethodName
outperforms O
the O
current O
state O
- O
of O
- O
the O
- O
art O
baselines O
. O

The O
efficacy O
of O
each O
component O
in O
improving O
the O
performance O
of O
dialog B-TaskName
generation I-TaskName
is O
shown O
in O
the O
ablation O
study O
, O
and O
our O
model O
is O
revealed O
to O
be O
effective O
in O
performing O
fine O
- O
grained O
token O
- O
level O
feature O
alignment O
for O
multimodal B-TaskName
dialog I-TaskName
generation I-TaskName
in O
a O
further O
case O
study O
. O

The O
MMD B-DatasetName
dataset O
was O
created O
to O
encourage O
research O
in O
the O
vertical O
retail O
domain O
, O
and O
multimodal B-MethodName
hierarchical I-MethodName
encoder- I-MethodName
decoder I-MethodName
model I-MethodName
was O
proposed O
as O
a O
baseline O
. O

Based O
on O
the O
data O
from O
ABCDEFG O
, O
Liao O
et O
al O
. O
came O
to O
the O
conclusion O
that O
... O

The O
sentence O
means O
that O
the O
writer O
took O
the O
style O
tips O
and O
put O
them O
into O
their O
own O
writing O
. O

The O
user B-MethodName
attention I-MethodName
- I-MethodName
guided I-MethodName
multimodal I-MethodName
dialog I-MethodName
system I-MethodName
( O
UMD B-MethodName
) O
was O
designed O
by O
taking O
into O
consideration O
the O
hierarchical O
product O
taxonomy O
and O
user O
’s O
attention O
to O
products O
. O

An O
ordi- B-MethodName
nal I-MethodName
and I-MethodName
attribute I-MethodName
aware I-MethodName
multimodal I-MethodName
dialog I-MethodName
system I-MethodName
was O
introduced O
in O
2019 O
by O
using O
a O
novel O
position O
and O
attribute O
aware O
attention O
mechanism O
. O

The O
sentence O
proposes O
a O
method O
which O
can O
use O
different O
forms O
of O
domain O
knowledge O
to O
generate O
different O
responses O
. O

The O
sentence O
is O
discussing O
how O
a O
certain O
method O
advanced O
by O
capturing O
context O
- O
aware O
dependencies O
of O
semantic O
elements O
for O
a O
certain O
purpose O
. O

Our O
proposed O
UniTranSeR B-MethodName
can O
project O
all O
multimodal O
features O
into O
a O
unified O
semantic O
space O
, O
allowing O
for O
fine O
- O
grained O
feature O
alignment O
and O
intention O
reasoning O
, O
leading O
to O
more O
accurate O
responses O
. O

The O
proposed O
methodology O
is O
mainly O
comprised O
of O
three O
parts O
: O
the O
Unified O
- O
modal O
Transformer O
Semantic O
( O
UTS O
) O
encoder O
, O
the O
Feature O
Alignment O
and O
Intention O
Reasoning O
( O
FAIR O
) O
layer O
, O
and O
the O
Hi O
- O
erarchical O
Transformer O
Response O
( O
HTR O
) O
decoder O
, O
as O
shown O
in O
Figure O
2 O
. O

The O
multimodal B-TaskName
dialog I-TaskName
generation I-TaskName
task O
is O
defined O
as O
generating O
the O
most O
likely O
response O
sequence O
Y= O
fy1;y2;;yngand O
selecting O
top- O
kmost O
matched O
images O
, O
giving O
multimodal O
context O
utterances O
U= O
fu1;u2;:::;ujUjgand O
. O

Our O
HTR O
decoder O
is O
similar O
to O
MAGIC B-MethodName
( O
Nie O
et O
al O
. O
, O
2019 O
) O
, O
and O
is O
designed O
to O
decode O
three O
types O
of O
responses O
: O
general O
responses O
that O
refer O
to O
highly O
frequent O
responses O
( O
e.g. O
courtesy O
greetings O
) O
in O
the O
conversation O
, O
such O
as O
“ O
How O
can O
I O
help O
you O
? O
” O
; O
intention O
- O
aware O
responses O
that O
refer O
to O
task O
- O
oriented O
utterances O
, O
such O
as O
“ O
Found O
some O
similar O
black O
leather O
- O
jackets O
for O
you O
” O
; O
and O
multimodal O
responses O
that O
refer O
to O
the O
intention O
- O
aware O
responses O
with O
image O
output O
. O

We O
equip O
the O
product O
knowledge O
base O
for O
each O
utterance O
through O
searching O
a O
fashion O
item O
table O
provided O
by O
MMD B-DatasetName
in O
order O
to O
integrate O
informative O
features O
from O
external O
knowledge1 O
into O
the O
task O
- O
oriented O
dialog O
. O

We O
built O
the O
MPM O
loss O
as O
follows O
: O
LMPM( O
) O
= O
E(w;v;k O
) O
. O

17 O
types O
are O
labeled O
in O
the O
MMD B-DatasetName
dataset O
. O
Each O
user O
’s O
utterance O
is O
labeled O
with O
a O
specific O
intention O
type O
. O

After O
MAGIC B-MethodName
, O
we O
tailor O
the O
response O
to O
fit O
each O
intention O
, O
as O
shown O
in O
Table O
1 O
. O

The O
512 B-HyperparameterValue
is O
the O
same O
as O
the O
embedding B-HyperparameterName
size I-HyperparameterName
in O
our O
Transformer O
encoder O
. O

It O
is O
easy O
to O
see O
that O
the O
above O
one O
- O
hop O
knowledge O
query O
can O
be O
extended O
to O
a O
multi O
- O
hop O
query O
by O
iteratively O
performing O
attention O
- O
based O
key O
- O
value O
reasoning O
, O
which O
would O
ultimately O
enable O
image O
recommendations O
. O

We O
use O
the O
UniTranSeR B-MethodName
to O
evaluate O
the O
performance O
on O
the O
widely O
- O
used O
benchmark O
dataset O
MMD B-DatasetName
. O

The O
MMD B-DatasetName
dataset O
consists O
of O
over O
150k O
conversations O
between O
users O
and O
chatbots O
in O
the O
retail O
domain O
. O
Each O
conversation O
in O
the O
dataset O
describes O
a O
complete O
online O
shopping O
process O
. O

MMD B-DatasetName
can O
be O
partitioned O
. O

We O
use O
Bleu- B-MetricName
n I-MetricName
, O
Nist B-MetricName
, O
and O
Recall@ B-MetricName
kto I-MetricName
to O
evaluate O
our O
model O
over O
two O
basic O
tasks O
separately O
, O
i.e. O
, O
text O
task O
and O
image O
task O
, O
following O
several O
previous O
works O
( O
Nie O
et O
al O
. O
, O
2019 O
; O
He O
et O
al O
. O
, O
2020 O
; O
Zhang O
et O
al O
. O
, O
2021 O
) O
. O

Since O
20.07 O
% O
of O
the O
target O
responses O
in O
MMD B-DatasetName
are O
shorter O
than O
4 O
words O
, O
such O
as O
" O
Hello O
! O
" O
, O
this O
indicates O
that O
the O
majority O
of O
responses O
are O
on O
- O
topic O
and O
relevant O
. O

To O
calculate O
Bleu- B-MetricName
n I-MetricName
, O
use O
the O
formula O
. O

The O
values O
of O
n B-HyperparameterName
change O
from O
1 B-HyperparameterValue
to O
4 B-HyperparameterValue
. O

For O
the O
image O
task O
, O
we O
adopt O
Recall@ B-MetricName
kto I-MetricName
to O
evaluate O
the O
k B-HyperparameterName
from O
1 B-MetricValue
to O
3 B-MetricValue
. O

This O
is O
the O
first O
work O
to O
integrate O
visual O
features O
into O
a O
hierarchical O
encoder O
- O
decoder O
model O
for O
their O
constructed O
dataset O
. O

The O
memory O
augmented O
neural O
model O
•KMD B-MethodName
( O
Liao O
et O
al O
. O
, O
2018 O
) O
incorporates O
style O
tips O
and O
uses O
deep O
reinforcement O
learning O
to O
improve O
performance O
. O

Cui O
et O
al O
. O
( O
2019)7 O
proposes O
a O
user O
attention O
- O
guided O
multimodal O
dialog O
system O
by O
considering O
the O
hierarchical O
product O
taxonomy O
and O
the O
user O
's O
attention O
to O
products O
. O

Chauhan O
et O
al O
. O
( O
2019 O
) O
proposes O
a O
novel O
attention O
mechanism O
that O
is O
aware O
of O
both O
ordinality O
and O
attributes O
for O
multimodal B-TaskName
dialog I-TaskName
generation I-TaskName
. O

Nie O
et O
al O
. O
( O
2019)8 O
use O
adaptive O
decoders O
with O
intention O
understanding O
to O
generate O
three O
types O
of O
responses O
. O

•MATE B-MethodName
( O
He O
et O
al O
. O
, O
2020 O
) O
uses O
a O
multi O
- O
modal O
element O
- O
level O
encoder O
to O
integrate O
dialog O
context O
and O
knowledge O
, O
and O
employs O
a O
two O
- O
stage O
decoder O
that O
is O
aware O
of O
this O
knowledge O
to O
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
. O

In O
our O
trainings O
, O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
set O
to O
64 B-HyperparameterValue
, O
learning B-HyperparameterName
rate I-HyperparameterName
is O
set O
to O
1e 4 B-HyperparameterValue
, O
and O
. O

The O
maximum O
number O
of O
training B-HyperparameterName
epoches I-HyperparameterName
is O
set O
to O
1e4 B-HyperparameterValue
. O

After O
KMD B-MethodName
, O
UMD B-MethodName
, O
and O
MAGIC B-MethodName
, O
we O
evaluated O
the O
model O
's O
performance O
from O
two O
aspects O
: O
text O
response O
and O
image O
response O
. O

Our O
model O
UniTranSeR B-MethodName
outperforms O
other O
models O
on O
both O
tasks O
, O
as O
shown O
in O
Table O
3 O
. O

Our O
model O
generates O
responses O
that O
are O
closer O
to O
the O
golden O
responses O
than O
other O
baselines O
, O
specifically O
in O
the O
text O
task O
, O
where O
UniTranSeR B-MethodName
exhibits O
the O
highest O
Bleu B-MetricName
- I-MetricName
n I-MetricName
with O
varying O
n B-HyperparameterName
from O
1 B-HyperparameterValue
to O
4 B-HyperparameterValue
. O

Our O
model O
outperforms O
MATE B-MethodName
by O
26.3% B-MetricValue
in O
Bleu-4 B-MetricName
score O
, O
which O
verifies O
the O
effectiveness O
of O
our O
model O
in O
learning O
cross O
- O
modal O
feature O
alignment O
and O
conduct O
intention O
reasoning O
to O
generate O
more O
accurate O
and O
informative O
responses O
. O

We O
randomly O
selected O
200 O
dialogs O
from O
the O
MMD B-DatasetName
datasets O
and O
used O
different O
models O
to O
generate O
responses O
, O
including O
UMD B-MethodName
, O
OAM B-MethodName
, O
MAGIC B-MethodName
, O
and O
MATE B-MethodName
. O

We O
hired O
human O
experts O
to O
score O
the O
responses O
and O
golden O
responses O
in O
blind O
review O
on O
a O
scale O
from O
1to5 O
to O
simulate O
a O
real O
- O
life O
multimodal O
task O
- O
oriented O
conversation O
scenario O
. O

UniTranSeR B-MethodName
outperforms O
the O
other O
four O
models O
on O
all O
metrics O
, O
which O
is O
consistent O
with O
the O
results O
of O
automatic O
evaluation O
. O

We O
focus O
on O
five O
crucial O
components O
and O
set O
them O
ac- O
cordingly O
: O
1 O
) O
w/o O
UTS O
Encoder O
denotes O
that O
we O
use O
a O
BiGRU O
to O
replace O
the O
uni- O
fied O
- O
modal O
Transformer O
encoder O
for O
multimodal O
encoding O
; O
2 O
) O
w/o O
HTR O
De- O
coder O
denotes O
that O
we O
use O
a O
Uni O
- O
directional O
GRU O
to O
replace O
the O
hierarchical O
Transformer O
decoder O
for O
response B-TaskName
generation I-TaskName
; O
3 O
) O
w/o O
ITM O
denotes O
that O
we O
remove O
theLITMloss O
to O
make O
the O
parameters O
not O
updated O
; O
4 O
) O
w/o O
WPA O
denotes O
that O
we O
remove O
the O
LWPA O
loss O
and O
just O
regard O
the O
sentence- O
level O
rep- O
resentationf([CLS O
] O
) O
as O
query O
vector O
Qto O
query O
knowledge O
; O
5 O
) O
w/o O
IR O
Module O
denotes O
that O
we O
re- O
move O
the O
IC O
and O
KQ O
components O
and O
just O
adopt O
the O
context O
vector O
f([CLS O
] O
) O
to O
generate O
responses10 O
; O
From O
Table O
5 O
, O
we O
can O
observe O
that O
removing O
each O
component O
will O
result O
in O
a O
performance O
degrada- O
tion O
. O

Without O
the O
IR O
module O
, O
there O
is O
a O
decrease O
in O
the O
Bleu-4 B-MetricName
score O
and O
a O
decrease O
in O
the O
Nist B-MetricName
score O
. O

In O
addition O
, O
without O
WPA O
, O
ITM O
, O
and O
UTS O
Encoder O
, O
28.54 B-MetricValue
% I-MetricValue
, O
20.48 B-MetricValue
% I-MetricValue
, O
and O
14.37 B-MetricValue
% I-MetricValue
score O
decreases O
, O
which O
further O
validates O
the O
effectiveness O
of O
cross O
- O
modal O
feature O
alignment O
and O
unified O
- O
modal O
semantic O
encoding O
. O

In O
this O
paper O
, O
we O
propose O
a O
framework O
with O
feature O
alignment O
and O
intention O
reasoning O
, O
referred O
to O
as O
Uni- B-MethodName
TranSeR. I-MethodName
. O

Our O
UniTranSeR B-MethodName
model O
is O
both O
effective O
and O
superior O
in O
performance O
compared O
to O
other O
models O
, O
as O
demonstrated O
through O
experiments O
on O
the O
representative O
MMD B-DatasetName
dataset O
. O

A O
multimodal O
dialogue O
system O
that O
is O
aware O
of O
ordinals O
and O
attributes O
. O

Open O
domain O
dialogue B-TaskName
genera- I-TaskName
tion I-TaskName
to O
see O
latent O
images O
. O

ABCDEFGO O
, O
. O

, O


 O
The O
letters O
ABCDEFG O
followed O
by O
the O
number O
0 O
and O
a O
period O
. O

D B-HyperparameterName
are O
hyperparameters O
that O
are O
initialized O
equally O
, O
i.e. O
, O
0.33 B-HyperparameterValue
, O
0.33 B-HyperparameterValue
, O
and O
0.33 B-HyperparameterValue
. O

We O
then O
use O
the O
verification O
set O
to O
fine O
- O
tune O
the O
weights O
of O
0.30 B-HyperparameterValue
, O
0.35 B-HyperparameterValue
, O
and O
0.35 B-HyperparameterValue
. O

A O
detailed O
statistics O
of O
the O
MMD B-DatasetName
dataset O
is O
presented O
in O
Table O
8 O
. O

In O
order O
to O
improve O
our O
understanding O
of O
the O
limitations O
of O
our O
model O
, O
we O
analyze O
the O
errors O
made O
by O
UniTranSeR. B-MethodName
. O

We O
select O
100 O
responses O
generated O
by O
Uni- B-MethodName
TranSeR I-MethodName
at O
random O
that O
have O
low O
human O
evaluation O
scores O
in O
the O
test O
set O
of O
MMD B-DatasetName
. O
