DeepRapper B-MethodName

: O

Neural O

Rap B-TaskName

Generation I-TaskName

with O

Rhyme O

and O

Rhythm O

Modeling O

Lanqing O

Xue1 O

, O

Kaitao O

Song2 O

, O

Duocai O

Wu3 O

, O

Xu O

Tan4 O

, O

Nevin O

L. O

Zhang1 O

, O

Tao O

Qin4 O

, O

Wei O

- O

Qiang O

Zhang5 O

, O

Tie O

- O

Yan O

Liu4 O

1The O

Hong O

Kong O

University O

of O

Science O

and O

Technology O

2Nanjing O

University O

of O

Science O

and O

Technology O

3Fudan O

University4Microsoft O



Research O

Asia5Tsinghua O

University O

flxueaa O

, O

lzhang O

g@cse.ust.hk O

kt.song@njust.edu.cn O

dcwu18@fudan.edu.cn O

fxuta O

, O

taoqin O

, O

tyliu O

g@microsoft.com O

wqzhang@tsinghua.edu.cn O

Abstract O

Rap B-TaskName

generation I-TaskName

, O

which O

aims O

to O

produce O

lyrics O

and O

corresponding O

singing O

beats O

, O

needs O

to O

model O

both O

rhymes O

and O

rhythms O

. O



Previous O

works O

for O

rap B-TaskName

generation I-TaskName

focused O

on O

rhyming O

lyrics O

but O

ignored O

rhythmic O

beats O

, O

which O

are O

important O

for O

rap O

performance O

. O



In O

this O

paper O

, O

we O

develop O

DeepRapper B-MethodName

, O

a O

Transformer O

- O

based O

rap O

generation O

system O

that O

can O

model O

both O

rhymes O

and O

rhythms O

. O



Since O

there O

is O

no O

avail- O

able O

rap O

dataset O

with O

rhythmic O

beats O

, O

we O

de- O

velop O



a O

data O

mining O

pipeline O

to O

collect O

a O

large- O

scale O

rap O

dataset O

, O

which O

includes O

a O

large O

num- O

ber O

of O

rap O

songs O

with O

aligned O

lyrics O

and O

rhyth- O

mic O

beats O

. O



Second O

, O

we O

design O

a O

Transformer- O

based O

autoregressive O

language O

model O

which O

carefully O

models O

rhymes O

and O

rhythms O

. O



Specif- O



ically O

, O

we O

generate O

lyrics O

in O

the O

reverse O

or- O

der O

with O

rhyme O

representation O

and O

constraint O

for O

rhyme O

enhancement O

and O

insert O

a O

beat O

sym- O

bol O

into O

lyrics O

for O

rhythm O

/ O

beat O

modeling O

. O



To O

our O

knowledge O

, O

DeepRapper B-MethodName

is O

the O

ﬁrst O

sys- O

tem O

to O

generate O

rap O

with O

both O

rhymes O

and O

rhythms O

. O



Both O

objective O

and O

subjective O

evalu- O

ations O

demonstrate O

that O

DeepRapper B-MethodName

generates O

creative O

and O

high O

- O

quality O

raps O

with O

rhymes O

and O

rhythms O

. O



1 O

Introduction O

Rap O

is O

a O

musical O

form O

originating O

from O

America O

in O

1970s O

, O

and O

has O

quickly O

developed O

as O

one O

of O

the O

mainstream O

music O

genres O

in O

the O

world O

( O

Keyes O

, O

2004 O

) O

. O



With O

the O

rapid O

development O

of O

artiﬁcial O

intelligence O

, O

automatic O
rap B-TaskName

lyrics I-TaskName

generation I-TaskName

has O

drawn O

attention O

from O

academia O

( O

Potash O

et O



al O

. O

, O

2015 O

; O

Malmi O

et O

al O

. O

, O

2016 O

; O

Liang O

et O

al O

. O

, O

2018 O

; O

Nikolov O

et O

al O

. O

, O

2020 O

) O



. O



Generally O

speaking O

, O

rap O

lyrics O

need O

to O

be O

semantically O

meaningful O

and O

fashionable O

to O

con- O

vey O

interesting O

stories O

or O

express O

feelings O

. O



Different O

from O

natural O

language O

or O

other O

artistic O

genres O

( O

e.g. O

, O

 O

Corresponding O

author O

: O

Xu O

Tan O

, O

xuta@microsoft.comlyrics O

or O

poetry O

) O

, O

rap O

has O

distinctive O

characteris- O

tics O

: O

1 O

) O

it O

usually O

contains O

complex O

rhyme O

patterns O

among O

several O

consecutive O

sentences O

, O

which O

are O

the O

key O

to O

form O

a O

good O

ﬂow O

; O

2 O

) O

it O

needs O

to O

align O

with O

the O

singing O

beat O

since O

rap O

lyrics O

are O

usually O

rapped O

according O

to O

some O

rhythmic O

accompani- O

ments O

. O



Therefore O

, O

how O

to O

generate O

rap O

lyrics O

with O

good O

rhymes O

and O

rhythms O

is O

a O

troublesome O

prob- O

lem O

. O



Previous O

works O

( O

Potash O

et O

al O

. O

, O

2015 O

; O

Malmi O

et O

al O

. O

, O

2016 O

; O

Liang O

et O

al O

. O

, O

2018 O

; O

Nikolov O

et O

al O

. O

, O

2020 O

) O

for O

rap B-TaskName

generation I-TaskName

mainly O

focused O

on O

lyric B-TaskName

generation I-TaskName

and O

some O

of O

them O

developed O

strategies O

for O

rhyme O

modeling O

. O



Potash O

et O



al O

. O



( O

2015 O

) O

directly O

added O

a O

“ O

< O

endLine O

> O

” O

token O

at O

the O

end O

of O

verse O

lines O

and O

ex- O

pected O

to O

learn O

rhyme O

patterns O

implicitly O

. O



Nikolov O

et O

al O

. O



( O

2020 O

) O

applied O

a O

two O

- O

step O

strategy O

, O

which O

ﬁrst O

generates O

rap O

lyrics O

and O

then O

adds O

rhyme O

tokens O

to O

the O

end O

of O

generated O

lyrics O

. O



However O

, O

these O

meth- O

ods O

can O

not O

guarantee O

the O

rhyme O

patterns O

for O

every O

lyric O

line O

and O

only O

care O

the O

rhyme O

on O

the O

last O

token O

. O



Although O

many O

works O

have O

studied O

rhyming O

mod- O

eling O

in O

other O

artistic O

genres O

( O

e.g. O

, O

poetry O

) O

( O

Li O

et O

al O

. O

, O

2020 O

; O

Van O

de O

Cruys O

, O

2020 O

; O

Liu O

et O

al O

. O

, O

2020 O

) O

, O

they O

are O

not O

suitable O

for O

rap B-TaskName

generation I-TaskName

due O

to O

the O

com- O

plex O

rhyme O

structure O

in O

rap O

. O



For O

example O

, O

poetry O

needs O

to O

rhyme O

with O

only O

the O

last O

word O

in O

each O

sen- O

tence O

, O

while O

rap O

rhymes O

with O

multiple O

consecutive O

tokens O

at O

the O

end O

of O

each O

sentence O

. O



No O

previous O

works O

have O

studied O

rhythm O

model- O



ing O

( O

i.e. O

, O

beats O

in O

rap O

) O

, O

to O

our O

knowledge O

. O



One O

of O

the O

main O

reasons O

is O

the O

lack O

of O

rap O

datasets O

with O

beat O

- O

lyric O

alignment O

. O



Consequently O

, O

the O

generation O

of O

lyrics O

without O

rhythmic O

beats O

can O

not O

be O

regarded O

as O

a O

full O

rap B-TaskName

generation I-TaskName

. O



In O

this O

paper O

, O

we O

develop O

DeepRapper B-MethodName

, O

a O

Trans- O

former O

( O

Vaswani O

et O

al O

. O

, O

2017 O

) O

based O

rap O

generation O

system O

which O

can O

model O

both O

rhymes O

and O

rhythms O

. O



To O

build O

the O

system O

, O

since O

there O

is O

no O

available O

rap O

datasets O

with O

aligned O

rhythmic O

beats O

, O

we O

design O

a O



70data O

mining O

pipeline O

and O

collect O

a O

large O

- O

scale O

rap O

dataset O

for O

rhythm O

modeling O

. O



Speciﬁcally O

, O

we O

ﬁrst O

crawl O

many O

rap O

songs O

, O

each O

song O

with O

both O

rap O

lyrics O

and O

audios O

, O

from O

the O

Web O

. O



For O

each O

crawled O

rap O

song O

, O

we O

perform O

a O

series O

of O

data O

preprocessing O

steps O

to O

extract O

rhythmic O

beats O

as O

well O

as O

beat O

- O

lyric O

alignment O

. O



To O

better O

model O

rhyme O

, O

we O

generate O

the O

words O

in O

a O

rap O

sentence O

from O

right O

to O

left O

in O

an O

au- O

toregressive O

manner O

. O



Doing O

so O

we O

can O

easily O

iden- O

tify O

the O

last O

few O

words O

of O

a O

sentence O

( O

now O

become O

the O

ﬁrst O

words O

of O

the O

reverse O

sentence O

) O

to O

rhyme O

with O

. O



Additionally O

, O

we O

incorporate O

several O

rhyme- O

related O

representations O

into O

our O

language O

model O

to O

further O

improve O

the O

rhyming O

quality O

, O

and O

encourage O

theN O

- O

gram O

rhyme O

in O

generated O

rap O

lyrics O

through O

rhyme O

constraint O

during O

inference O

. O



We O

use O

a O

spe- O

cial O

token O



[ O

BEAT O

] O

to O

represent O

the O

rhythmic O

beat O

and O

insert O

it O

into O

lyrics O

right O

before O

the O

correspond- O

ing O

word O

. O



In O

this O

way O

, O

we O

can O

model O

the O

beat O

in O

the O

lyric O

sequence O

both O

in O

training O

and O

generation O

. O



Inspired O

by O

the O

success O

of O

pre O

- O

trained O

language O

models O

( O

Devlin O

et O

al O

. O

, O

2019 O

; O

Radford O

et O

al O

. O

, O

2018 O

; O

Yang O

et O

al O

. O

, O

2019 O

; O

Song O

et O

al O

. O

, O

2019 O

; O

Liu O

et O

al O

. O

, O

2019 O

) O

, O

we O

incorporate O

pre O

- O

training O

into O

our O

sys- O

tem O

. O



To O

obtain O

large O

- O

scale O

data O

for O

pre O

- O

training O

, O

we O

also O

use O

our O

data O

mining O

pipeline O

to O

collect O

another O

two O

datasets O

: O

1 O

) O

non O

- O

rap O

songs O

with O

aligned O

beats O

, O

which O

can O

be O

larger O

than O

rap O

dataset O

since O

non O

- O

rap O

songs O

are O

more O

general O

than O

rap O

songs O

; O

2 O

) O

pure O

lyrics O

, O

which O

can O

be O

even O

larger O

than O

non O

- O

rap O

songs O

. O



In O

the O

pre O

- O

training O

stage O

, O

we O

pre O

- O

train O

our O

Deep- B-MethodName

Rapper I-MethodName

model O

based O

on O

the O

above O

two O

datasets O

. O



Then O

we O

ﬁne O

- O

tune O

our O

pre O

- O

trained O

model O

on O

the O

rap O

songs O

with O

aligned O

beats O

. O



The O

ﬁne O

- O

tuned O

model O

is O

used O

for O

ﬁnal O

rap B-TaskName

generation I-TaskName

. O



Both O

objective O

and O

subjective O

evaluations O

verify O

the O

advantages O

of O

DeepRapper B-MethodName

in O

generating O

rap O

lyrics O

with O

rhymes O

and O

rhythms O

. O



Our O

main O

contributions O

can O

be O

summarized O

as O

follows O

: O

•To O

model O

rhythms O

in O

rap B-TaskName

generation I-TaskName

, O

we O

de- O

velop O



a O

data O

mining O

pipeline O

to O

create O

rap O

datasets O

with O

aligned O

rhythmic O

beats O

. O



•To O

better O

model O

rhymes O

, O

we O

design O

an O

autore- O

gressive O

language O

model O

to O

generate O

rap O

lyrics O

from O

right O

to O

left O

with O

rhyme O

constraint O

. O



As O

far O

as O

we O

know O

, O

DeepRapper B-MethodName

is O

the O

ﬁrst O

to O

explicitly O

model O

N O

- O

gram O

rhymes O

. O



•We O

elaborately O

insert O

the O

beat O

token O

inside O

lyrics O

to O

model O

the O

rhythmic O

beats O

. O



To O

ourknowledge O

, O

DeepRapper B-MethodName

is O

the O

ﬁrst O

system O

that O

models O

rhythms O

for O

rap B-TaskName

generation I-TaskName

. O



2 O

Background O

Since O

DeepRapper B-MethodName

generates O

rap O

lyrics O

with O

both O

rhyme O

and O

rhythm O

modeling O

, O

in O

this O

section O

, O

we O

brieﬂy O

introduce O

the O

related O

background O

: O

lyric B-TaskName

gen- I-TaskName

eration I-TaskName

, O

rhyme O

modeling O

and O

rhythm O

modeling O

. O



Lyric B-TaskName

Generation I-TaskName

Broadly O

speaking O

, O

lyric O

gen- O

eration O

can O

cover O

rap B-TaskName

lyric I-TaskName

generation I-TaskName

( O

Potash O

et O

al O

. O

, O

2015 O

; O

Nikolov O

et O

al O

. O

, O

2020 O

; O



Liang O

et O

al O

. O

, O

2018 O

) O

, O

song B-TaskName

lyric I-TaskName

generation I-TaskName

( O

Watanabe O

et O

al O

. O

, O

2018 O

; O

Lu O

et O

al O

. O

, O

2019 O

; O

Chen O

and O

Lerch O

, O

2020 O

; O

Sheng O

et O

al O

. O

, O

2020 O

) O

, O

general O

poetry B-TaskName

generation I-TaskName

( O

Zhang O

and O

Lapata O

, O

2014 O

; O

Lau O

et O

al O

. O

, O

2018 O

; O

Li O

et O

al O

. O

, O

2020 O

; O

Liu O

et O

al O

. O

, O

2020 O

) O

and O

etc O



. O



Different O

from O

previous O

works O

that O

leverage O

language O

model O

to O

generate O

lyrics O

similar O

to O

natural O

language O

, O

in O

this O

paper O

, O

we O

introduce O

a O

novel O

lan- O

guage O

model O

for O

rap B-TaskName

generation I-TaskName

, O

with O

well O

- O

designed O

rhyme O

and O

rhythm O

modeling O

to O

ﬁt O

the O

characteris- O

tics O

of O

rap O

lyrics O

. O



Additionally O

, O

inspired O

by O

the O

successes O

of O

pre O

- O

trained O

language O

models O

( O

Devlin O

et O

al O

. O

, O

2019 O

; O

Yang O

et O

al O

. O

, O

2019 O

; O

Liu O

et O

al O

. O

, O

2019 O

; O

Radford O

et O

al O

. O

, O

2019 O

; O

Song O

et O

al O

. O

, O

2019 O

) O

in O

NLP O

applications O

, O

we O

also O

incorporate O

pre O

- O

training O

into O

our O

model O

to O

further O

improve O

the O

quality O

of O

rap B-TaskName

generation I-TaskName

. O



Rhyme O

Modeling O

Rhyme O

modeling O

plays O

an O

im- O

portant O

role O

in O

rap B-TaskName

generation I-TaskName

, O

which O

requires O

the O

last O

few O

tokens O

in O

consecutive O

sentences O

to O

have O

the O

same O

rhyme O

pattern O

. O



Existing O

rap O

genera- O

tion O

systems O

either O

directly O

add O

a O

special O

token O

“ O

< O

endLine O

> O

” O

at O

the O

end O

of O

rap O

lyric O

to O

encour- O

age O

the O

model O

to O

learn O

rhyme O

structure O

( O

Potash O

et O



al O

. O

, O

2015 O

) O

, O

or O

introduce O

a O

two O

- O

step O

strategy O

for O

rhyme O

modeling O

that O

ﬁrst O

generates O

rap O

lyrics O

and O

then O

adds O

rhyme O

tokens O

after O

the O

generated O

lyrics O

( O

Nikolov O

et O

al O

. O

, O

2020 O

) O

. O



However O

, O

these O

works O

only O

focused O

on O

unigram O

rhyme O

while O

rap O

appre- O

ciates O

more O

for O

n O

- O

gram O

rhyme O

. O



Although O

a O

lot O

of O

works O

have O

explored O

rhyme O

modeling O

in O

other O

gen- O

res O

, O

most O

of O

them O

can O

not O

be O

directly O

used O

for O

rap B-TaskName

generation I-TaskName

. O



For O

example O

, O

poetry B-TaskName

generation I-TaskName

( O

Lau O

et O

al O

. O

, O

2018 O

; O

Zhipeng O

et O



al O

. O

, O

2019 O

; O

Liao O

et O

al O

. O

, O

2019 O

; O

Li O

et O

al O

. O

, O

2020 O

) O

usually O

used O

pre O

- O

deﬁned O

format O

to O

control O

the O

rhyme O

pattern O

since O

poetry O

usually O

has O

ﬁxed O

number O

of O

words O

and O

only O

cares O

the O

rhyme O

pattern O

for O

the O

last O

word O

. O



However O

, O

rap O

lyrics O

have O

diverse O

rhyme O

structures O

across O

multiple O

consecu- O

tive O

sentences O

and O

most O

importantly O

multiple O

con- O

secutive O

words O

. O



Therefore O

, O

we O

introduce O

N O

- O

gram O

rhyme O

modeling O

in O

DeepRapper B-MethodName

to O

handle O

the O

dis- O

tinctive O

rhyme O

patterns O

in O

rap O

. O



Besides O

, O

we O

also O

train O

our O

language O

model O

in O

a O

reverse O

order O

( O

i.e. O

, O

right O

to O

left O

) O

, O

similar O

to O

previous O

works O

( O

Van O

de O

Cruys O

, O

2020 O

) O

, O

to O

better O

model O

rhymes O

since O

they O

always O

occur O

at O

the O

end O

of O

sentence O

. O



Rhythm O

Modeling O

Rhythm O

modeling O

is O

usually O

used O

in O

music B-TaskName

generation I-TaskName

( O

Zhu O

et O

al O

. O

, O

2018 O

; O

Huang O

and O

Yang O

, O

2020 O

; O



Ren O

et O

al O

. O

, O

2020 O

) O

which O

generates O

the O

duration O

of O

notes O

along O

with O

the O

note O

pitch O

to O

form O

rhythmic O

beats O

in O

melody O

and O

accompaniment O

generation O

. O



Different O

from O

music B-TaskName

generation I-TaskName

, O

rap O

cares O

more O

about O

rhythmic O

beats O

instead O

of O

note O

pitches O

( O

i.e. O

melody O

) O

. O



In O

this O

way O

, O

the O

generated O

rap O

lyrics O

need O

to O

align O

with O

the O

corresponding O

rhythmic O

beats O

in O

order O

to O

be O

rapped O

, O

otherwise O

it O

can O

not O

be O

regarded O

as O

a O

complete O

rap O

. O



However O

, O

to O

the O

best O

of O

our O

knowledge O

, O

none O

of O

previous O

works O

have O

studied O

the O

rhythm O

modeling O

in O

rap B-TaskName

generation I-TaskName

. O



In O

this O

paper O

, O

we O

introduce O

a O

novel O

beat O

modeling O

strategy O

in O

DeepRapper B-MethodName

for O

rhythm B-TaskName

generation I-TaskName

. O



3 O

Rap O

Dataset O

Mining O

Previous O

works O

( O

Potash O

et O

al O

. O

, O

2015 O

; O

Liang O

et O

al O

. O

, O

2018 O

; O

Nikolov O

et O

al O

. O

, O

2020 O

) O

for O

rap B-TaskName

generation I-TaskName

usu- O

ally O

used O

rap O

datasets O

with O

only O

lyrics O

, O

without O

con- O

sidering O

the O

rhythmic O

beat O

information O

. O



To O

model O

rhythm O

in O

rap B-TaskName

generation I-TaskName

, O

the O

rap O

dataset O

should O

contain O

lyrics O

with O

aligned O

rhythmic O

beats O

. O



How- O

ever O

, O

beat O

alignments O

are O

quite O

difﬁcult O

to O

obtain O

, O

since O

their O

annotations O

require O

musicians O

with O

pro- O

fessional O

knowledge O

to O

identify O

stressing O

syllable O

in O

rap O

songs O

. O



To O

handle O

this O

problem O

, O

we O

design O

a O

data O

mining O

pipeline O

to O

automatically O

extract O

beat- O

lyric O

alignments O

. O



In O

this O

section O

, O

we O

introduce O

the O

details O

of O

the O

data O

mining O

pipeline O

and O

our O

mined O

dataset O

based O

on O

this O

pipeline O

. O



3.1 O

Data O

Mining O

Pipeline O

Figure O

1 O

overviews O

our O

data O

mining O

pipeline O

, O

which O

consists O

of O

5 O

steps O

: O

data O

crawling O

, O

vocal O

and O

accom- O

paniment O

separation O

, O

vocal O

and O

lyric O

alignment O

, O

beat O

detection O

, O

and O

lyric O

and O

beat O

alignment O

. O



Data O

Crawling O

To O

mine O

a O

large O

- O

scale O

rap O

dataset O

, O

we O

ﬁrst O

crawl O

a O

large O

amount O

of O

rap O

songs O

with O

both O

lyrics O

and O

singing O

audios O

from O

the O

Web O

. O



To O

ensure O

the O

lyric O

and O

audio O

can O

be O

aligned O

in O

the O

sentence O

level O

which O

is O

beneﬁcial O

for O

our O

later O

word O

- O

level O

beat O

alignment O

, O

we O

also O

crawl O

the O

start O

and O

end O

time O

of O

each O

lyric O

sentence O

corresponding O

to O

the O

audio O

. O



Vocal O

and O

Accompaniment O

Separation O

For O

each O

rap O

song O

, O

we O

utilize O

Spleeter O

( O

Hennequin O

et O

al O

. O

, O

2020)1 O

, O

a O

public O

music O

separation O

tool O

, O

to O

separate O

the O

vocal O

( O

containing O

rap O

singing O

) O

and O

accompani- O

ment O

( O

containing O

rhythmic O

beats O

) O

from O

the O

crawled O

rap O

audio O

. O



Vocal O

and O

Lyric O

Alignment O



We O

split O

the O

sepa- O

rated O

vocals O

into O

the O

sentence O

level O

according O

to O

the O

crawled O

start O

and O

end O

time O

of O

each O

lyric O

sen- O

tence O

, O

and O

thus O

we O

can O

get O

the O

vocal O

- O

lyric O

align- O

ments O

in O

the O

sentence O

level O

. O



We O

convert O

lyrics O

into O

phonemes O

via O

Phonemizer2and O

utilize O

Montreal O

Forced O

Aligner3to O

obtain O

vocal O

- O

lyric O

alignments O

in O

the O

phoneme O

level O

. O



Based O

on O

these O

phoneme O

- O

level O

vocal O

- O

lyric O

alignments O

, O

we O

obtain O

the O

correspond- O

ing O

timestamp O

of O

each O

word O

in O

the O

singing O

audio O

. O



Beat O

Detection O



To O

obtain O

the O

alignments O

be- O

tween O

lyrics O

and O

beats O

, O

we O

need O

to O

know O

the O

times- O

tamp O

of O

each O

beat O

. O



Therefore O

, O

we O

use O

a O

beat O

track O

detection O

tool O

, O

Librosa O

( O

McFee O

et O

al O

. O

, O

2020)4 O

, O

to O

track O

the O

timestamp O

of O

each O

beat O

from O

the O

separated O

accompaniment O

that O

obtained O

from O

the O

second O

step O

. O



Lyric O

and O

Beat O

Alignment O

After O

we O

obtain O

the O

timestamp O

of O

each O

word O

and O

each O

beat O

, O

we O

can O

align O

them O

together O

according O

to O

their O

timestamps O

. O



How- O

ever O

, O

since O

a O

rapper O

may O

not O

sing O

a O

word O

exactly O

following O

the O

beat O

, O

directly O

using O

the O

timestamp O

to O

exactly O

match O

the O

word O

and O

beat O

is O

inappropriate O

. O



Therefore O

, O

we O

propose O

an O

approximate O

method O

to O

align O

them O

. O



Denote O

the O

word O

sequence O

of O

a O

lyric O

1https://github.com/deezer/spleeter O

2https://github.com/bootphon/phonemizer O

3https://github.com/MontrealCorpusTools/Montreal- O

Forced O

- O

Aligner O

4https://github.com/librosa/librosa O



72sentence O

as O

W O

= O

fw1;w2;;wjWjg O

, O

and O

its O

beat O

sequence O

as O

B O

= O

fb1;b2;;bjBjg O

, O

wherewiand O

bjrepresenti O

- O

th O

word O

and O

j O

- O

th O

beat O

. O



We O

use O

Twi O

andTbjto O

represent O

the O

timestamps O

of O

wiandbj O

respectively O

. O



For O

each O

beat O

bj O

, O

we O

ﬁrst O

ﬁlter O

out O

a O

word O

set O

~W O



= O

fw O

: O





Tbj Tw O





r=2;w2Wg O

, O

whererrepresents O

the O

average O

duration O

of O

each O

word O

in O

the O

song O

( O

i.e. O

, O

the O

total O

duration O

divides O

the O

number O

of O

words O

) O

. O



Next O

, O

word O

wiis O

aligned O

with O

beatbjif O

it O

satisﬁes O

the O

following O

condition O

: O

wi= O

min O

wjTbj Twj;w2 O

~ O

W O

: O

( O

1 O

) O



3.2 O

Mined O

Datasets O

Using O

the O

above O

data O

mining O

pipeline O



, O

we O

obtain O

a O

rap O

lyric O

dataset O

with O

aligned O

beats O

( O

named O

as O

D B-DatasetName

- I-DatasetName

RAP I-DatasetName

, O

where O

D O

represents O

“ O

dataset O

” O

) O

, O

which O

sat- O

isﬁes O

the O

requirements O

of O

building O

a O

rap O

genera- O

tion O

system O

with O

both O

rhyme O

and O

rhythm O

model- O

ing O

. O



We O

split O

the O

D B-DatasetName

- I-DatasetName

RAP I-DatasetName
dataset O

into O

the O

training O

and O

validation O

set O

with O

a O

ratio O

of O

4:1 O

. O



Since O

rap O

is O

only O

one O

of O

music O

genres O

and O

the O

number O

of O

rap O

songs O

is O

usually O

smaller O

compared O

with O

more O

general O

songs O

, O

we O

also O

mine O

another O

two O

datasets O

to O

pre O

- O

train O

our O

DeepRapper B-MethodName

model O

with O

the O

same O

mining O

pipeline O

: O

1 O

) O

non O

- O

rap O

songs O

with O

aligned O

beats O

( O

named O

as O

D B-DatasetName

- I-DatasetName

SONG I-DatasetName

) O

; O

2 O

) O

pure O

lyrics O

without O

aligned O

beats O

( O

named O

as O

D B-DatasetName

- I-DatasetName

LYRIC I-DatasetName

) O

. O



We O

summarize O

the O

statistics O

of O

the O

three O

datasets O

in O

Table O

1 O

and O

show O

a O

rap O

song O

with O

aligned O

beats O

from O

D O

- O

Rap O

in O

Figure O

2 O

. O

4 O

Rap O

Generation O

Model O



In O

this O

section O

, O

we O

introduce O

the O

architecture O

of O

our O

rap O

generation O

model O

, O

and O

the O

details O

of O

its O

rhyme O

modeling O

and O

rhythm O

modeling O

. O



4.1 O

Model O

Overview O

Figure O

3 O

illustrates O

the O

detailed O

architecture O

of O

our O

rap O

generation O

model O

. O



We O

use O

Trans- O

former O

( O

Vaswani O

et O

al O

. O

, O

2017 O

) O

to O

build O

an O

autoregres- O

sive O

language O

model O

( O

Radford O

et O

al O

. O

, O

2018 O

, O

2019 O

) O

for O

rap B-TaskName

generation I-TaskName

, O

and O

introduce O

several O

new O

de- O

signs O

: O

1 O

) O

To O

better O

model O

rhymes O

, O

our O

model O

gen- O

erates O

a O

sentence O

from O

right O

to O

left O

, O

since O

rhyming O

words O

are O

always O

at O

the O

end O

of O

the O

sentence O

; O

2 O

) O

As O

aforementioned O

, O

rhythms O

are O

critical O

for O

rap O

per- O

formance O

, O

so O

we O

insert O

a O

special O

token O

[ O

BEAT O

] O

for O

explicit O

beat O

modeling O

; O

3 O

) O

Unlike O

original O

Trans- O

former O

with O

only O

word O

embedding O

and O

positional O

embedding O

, O

we O

add O

multiple O

additional O

embed- O

dings O

to O

better O

model O

rhymes O

and O

rhythms O

. O



Next O

, O

we O

introduce O

our O

rhyme O

modeling O

in O

subsection O

4.2 O

and O

rhythm O

modeling O

in O

subsection O

4.3 O

. O



4.2 O

Rhyme O

Modeling O

Rhymes O

are O

the O

key O

to O

form O

a O

good O

rap O

ﬂow O

. O



In O

DeepRapper B-MethodName

, O

we O

model O

rhymes O

with O

three O

compo- O

nents O

: O

1 O

) O

reverse O

- O

order O

language O

model O

; O

2 O

) O

rhyme O

representation O

; O

and O

3 O

) O

rhyme O

constraint O

. O



4.2.1 O

Reverse O

- O

Order O

Language O

Model O



Rhyming O

words O

usually O

occur O

at O

the O

end O

of O

each O

lyric O

sentence O

. O



If O

using O

a O

standard O

autoregressive O

language O

model O

and O

generating O

tokens O

from O

left O

to O

right O

, O

we O

need O

to O

identify O

whether O

the O

current O

gen- O

eration O

step O

is O

the O

end O

of O

a O

sentence O

, O

which O

decides O

whether O

to O

generate O

rhyming O

words O

to O

be O

consis- O

tent O

with O

that O

in O

previous O

sentences O

. O



Therefore O

, O

to O

better O

model O

rhymes O

, O

we O

use O

a O

reverse O

- O

order O

lan- O

guage O

model O

to O

generate O

sentences O

from O

right O

to O

left O

, O

as O

shown O

in O

Figure O

3 O

. O



Doing O

so O

we O

can O

easily O

identify O

the O

last O

few O

words O

of O

a O

sentence O

( O

now O

be- O

come O

the O

ﬁrst O

few O

words O

of O

the O

reverse O

sentence O

) O

to O

control O

their O

rhymes O

. O



Note O

that O

we O

only O

reverse O

words O

inside O

a O

sentence O

, O

and O

still O

generate O

different O

sentences O

in O

the O

original O

order O

. O



Figure O

4 O

compares O

the O

sentences O

in O

left O

- O

to O

- O

right O

order O

and O

right O

- O

to O

- O

left O

order O

, O

from O

which O

we O

can O

see O

that O

rhyming O

words O

of O

each O

sentence O

share O

the O

same O

relative O

positions O

( O

offset O

to O

the O

ﬁrst O

token O

) O

in O

the O

reverse O

order O

, O

and O

are O

easy O

to O

model O

and O

control O

. O



4.2.2 O

Rhyme O

Representation O

Rhyming O

words O

have O

two O

important O

features O

: O

1 O

) O

its O

vowel O

that O

used O

for O

rhyming O

and O

2 O

) O

its O

relative O

posi- O

tion O

in O

a O

sentence O

to O

decide O

the O

correspondence O

be- O



tween O



the O

rhyming O

words O

in O

consecutive O

sentences O

( O

e.g. O

, O

in O

the O

reverse O

order O

setting O

, O

the O

ﬁrst O

/ O

second O

word O

of O

the O

current O

sentence O

should O

be O

rhymed O

with O

the O

ﬁrst O

/ O

second O

word O

in O

the O

previous O

sentence O

) O

. O



We O

use O

the O

vowel O

in O

the O

Pinyin5of O

Chinese O

char- O

acters O

to O

represent O

their O

rhymes O

. O



To O

this O

end O

, O

we O

build O

a O

vowel O

dictionary O

F()to O

identify O

the O

vowel O

of O

each O

word O

. O



As O

shown O

in O

Figure O

3 O

, O

we O

add O

an O

ad- O

ditional O

vowel O

embedding O

Fand O

an O

intra O

- O

sentence O

relative O

positional O

embedding O

Rto O

enhance O

rhyme O

representation O

for O

each O

token O

. O



Besides O

, O

to O

better O

identify O

different O

sentences O

, O

we O

introduce O

a O

sen- O

tence O

embedding O

Sto O

differentiate O

different O

sen- O

tences O

. O



4.2.3 O

Rhyme O

Constraint O



In O

addition O

to O

reverse O

- O

order O

language O

model O

and O

rhyme O

representation O

, O

we O

also O

introduce O

rhyme O

con- O

straint O

to O

improve O

the O

quality O

of O

rhyme O

generation O

in O

inference O

. O



As O

shown O

in O

Figure O

4 O

, O

sentences O

in O

rap O

lyrics O

not O

only O

rhyme O

with O

the O

last O

token O

, O

but O

also O

with O

multiple O

consecutive O

tokens O

at O

the O

end O

. O



We O

call O

this O

phenomenon O

as O

N O

- O

gram O

rhymes O

, O

which O

mean O

the O

current O

sentence O

and O

the O

previous O

sentence O

keep O

the O

same O

rhyme O

for O

the O

last O

Nconsecutive O

tokens O

. O



To O

our O

knowledge O

, O

no O

previous O

work O

has O

inves- O

tigatedN O

- O

gram O

rhymes O

( O

N O

> O

1 O

) O

, O

although O

it O

is O

important O

to O

improve O

rap O

quality O

. O



Our O

proposed O

rhyme O

constraint O

enables O

our O

model O

to O

adjust O

the O

probability O

of O

next O

predicted O

token O

to O

further O

en- O



courageN O

- O

gram O

rhyme O

generation O

. O



The O

constraint O

is O

introduced O

as O

follows O

. O



To O

generate O

the O

i O

- O

th O

wordwiin O

the O

standard O

inference O

procedure O

, O

we O

usually O

choose O

the O

pre- O

dicted O

token O

with O

the O

maximum O

probability O

, O

i.e. O

, O

wi= O

arg O

max O

p(wjw O

< O

i; O

) O

, O

wherew O

< O

idenotes O

the O

words O

before O

position O

iin O

the O

reverse O

sentence O

andis O

the O

model O

. O



When O

the O

words O

before O

posi- O



74tioniof O

the O

current O

and O

previous O

sentence O

have O

the O

same O

rhyme O

pattern O

, O

we O

will O

use O

an O

adjusted O

probability O

distribution O

~p(wjw O

< O

i;)to O

encourage O

thei O

- O

th O

generated O

word O

to O

be O

rhymed O

according O

to O

thei O

- O

th O

word O

in O

the O

previous O

sentence O

, O

so O

as O

to O

formN O

- O

gram O

rhymes O

. O



The O

adjusted O

probability O

distribution O

~p(wjw O

< O

i;)is O

: O

~p(wjw O

< O

i; O

) O



= O



p(wjw O

< O

i; O

) O

+ O

( O

1  O



) O

(w O

) O

( O

2 O

) O

where(w)is O

a O

vowel O

check O

function O

and O



is O

a O

hyper O
- O

parameter O

to O

balance B-HyperparameterName

the I-HyperparameterName

two I-HyperparameterName

terms I-HyperparameterName

. O



Here O

, O

(w)is O

1 O

if O

the O

predicted O

whas O

the O

same O

vowel O

with O

thei O

- O

th O

token O

in O

the O

previous O

sentence O

, O

other- O

wise O

0 O

. O



In O

other O

words O

, O

when O

predicting O

i O

- O

th O

token O

( O

iN O

) O

, O

we O

encourage O

our O

model O

to O

pay O

more O

attention O

for O

these O

words O

with O

same O

vowel O

with O

the O

i O

- O

th O

token O

in O

the O

previous O

sentence O

. O



In O

this O

way O

, O

the O

model O

tends O

to O

generate O

N O

- O

gram O

rhymes O

with O

large O

N. O

4.3 O

Rhythm O

Modeling O

Generating O

lyrics O

with O

aligned O

beats O

is O

necessary O

since O

rap O

lyrics O

need O

to O

be O

rapped O

with O

rhythmic O

beats O

. O



Therefore O

, O

we O

model O

and O

generate O

rhythmic O

beats O

along O

with O

the O

lyrics O

with O

a O

speciﬁc O

symbol O

: O

we O

regard O

beat O

as O

a O

special O

token O



[ O

BEAT O

] O

and O

insert O

it O

into O

lyric O

sequences O

for O

model O

training O

. O



As O

shown O

in O

Figure O

3 O

, O

we O

insert O



[ O

BEAT O

] O

before O

its O

aligned O

words O

like O

the O

following O

examples O

: O

“ O

我[BEAT O

] O

抬 O

头[BEAT O

] O

仰望。天空[BEAT O

] O

的苍[BEAT O

] O

茫 O

。 O

” O

. O



Rap O

usually O

contains O

different O

beat O

frequencies O

, O

i.e. O

, O

the O

ratios O

between O

the O

total O

number O

of O

words O

and O

the O

total O

number O

of O

beats O

in O

a O

rap O

song O

. O



To O

explicitly O

model O

and O

generate O

rap O

with O

different O

beat O

frequencies O

, O

we O

use O

three O

tokens O

[ O

S],[M O

] O

, O

and O

[ O

F]to O

represent O

the O

slow O

, O

medium O

and O

fast O

beat O

frequencies O

and O

add O

the O

corresponding O

tokens O

at O

the O

start O

of O

a O

rap O

song O

for O

training O

and O

inference O

. O



In O

our O

D B-DatasetName

- I-DatasetName

RAP I-DatasetName

dataset O

, O

the O

distribution O

of O

beat O

frequency O

is O

displayed O

in O

Figure O

5 O

. O



According O

to O

the O

distribution O

, O

we O

assign O

[ O

S],[M O

] O

, O

and O

[ O

F]to O

songs O

with O

beat O

frequency O

less O

than O

3 O

, O

equal O

to O

3 O

, O

and O

greater O

than O

3 O

respectively O

. O



5 O

Experimental O

Setup O

5.1 O

Model O

, O

Data O

, O

and O

Training O

Conﬁguration O

Our O

DeepRapper B-MethodName

model O

is O

built O

on O

the O

autoregres- O

sive O

Transformer O

decoder O

( O

Vaswani O

et O

al O

. O

, O

2017 O

; O

Radford O

et O



al O

. O

, O

2018 O

, O

2019 O

) O

, O

where O

the O

hidden B-HyperparameterName

size I-HyperparameterName

, O

the O

number B-HyperparameterName

of I-HyperparameterName

attention I-HyperparameterName

heads I-HyperparameterName

and O

the O

number B-HyperparameterName

of I-HyperparameterName

Transformer I-HyperparameterName

layers I-HyperparameterName

are O

set O

as O

768 B-HyperparameterValue

, O

12 B-HyperparameterValue

, O

12 B-HyperparameterValue

. O



The O

dimension B-HyperparameterName

of I-HyperparameterName

all I-HyperparameterName

different I-HyperparameterName

kinds I-HyperparameterName

of I-HyperparameterName

embedding I-HyperparameterName

in O

DeepRapper B-MethodName

is O

set O

as O

768 B-HyperparameterValue

. O



Considering O

there O

is O

no O

existing O

pre O

- O

trained O

language O

model O

in O

reverse O

order O

, O

we O

do O

not O

utilize O

any O

pre O

- O

trained O

language O

models O

for O

initialization O

. O



Instead O

, O

we O

ﬁrst O

pre O

- O

train O

our O

model O

on O

D B-DatasetName

- I-DatasetName

LYRIC I-DatasetName

and O

D B-DatasetName

- I-DatasetName

SONG I-DatasetName

for O

2 B-HyperparameterValue

mil- I-HyperparameterValue

lions I-HyperparameterValue

steps B-HyperparameterName

, O

and O

then O

ﬁne O

- O

tune O

our O

model O

on O

D B-DatasetName

- I-DatasetName

RAP I-DatasetName

with O

3 B-HyperparameterValue

K I-HyperparameterValue

steps B-HyperparameterName

as O

the O

size O

of O

D O

- O

RAP O

is O

smaller O

than O

our O

pre O

- O

training O

corpus O

. O



We O

convert O

each O

song O

to O

a O

sequence O

with O

a O

length O

of O

1024 B-HyperparameterValue

tokens O

by O

cutting O

longer O

sequence O

or O

padding O

shorter O

sequence O

. O



Our O

model O

is O

trained O

with O

a O

batch B-HyperparameterName

size I-HyperparameterName

of O

8 B-HyperparameterValue

songs O

on O

4 O

NVIDIA O

TITAN O

V O

GPUs O

. O



We O

use O

Adam O

opti- O

mizer O

with O

a O

learning B-HyperparameterName

rate I-HyperparameterName

of O

0:00015 B-HyperparameterValue

, O



1= O

0.9 B-HyperparameterValue

, O



2= O

0.999 B-HyperparameterValue

, O

and= O



10 6 B-HyperparameterValue

. O



We O

set O

the O

maxi- B-HyperparameterName

mum I-HyperparameterName

value I-HyperparameterName

of I-HyperparameterName

N I-HyperparameterName

- I-HyperparameterName

gram I-HyperparameterName

rhyme I-HyperparameterName

as O

3 B-HyperparameterValue

and O

the O

hyper- O

parameter O



in O

Equation O

2 O

as O

0.95 B-HyperparameterValue

. O



Samples O

are O

generated O

conditioned O

on O

a O

given O

sentence O

in O

refer- O

ence O

. O



5.2 O

Evaluation O

Metrics O

In O

this O

subsection O

, O

we O

introduce O

the O

objective O

and O

subjective O

metrics O

to O

evaluate O

the O

quality O

of O

the O

generated O

raps O

. O



Objective O

Evaluation O

We O

evaluate O

the O

gener- O

ated O

raps O

in O

terms O

of O

the O

quality O

of O

language O

, O

rhyme O

and O

rhythm O

. O



We O

choose O

ﬁve O

metrics O

to O

evaluate O

our O

model O

: O

1 O

) O

Perplexity B-MetricName

( O

PPL B-MetricName

) O

, O

a O

standard O

met- O

ric O

to O

evaluate O

the O

quality O

of O

a O

language O

model O

; O

2 O

) O

Rhyme B-MetricName

Accuracy I-MetricName

( O

RA B-MetricName

) O

, O

the O

ratio O

of O

sentences O

that O

have O

correctly O

predicted O

rhymes O

; O

3 O

) O

Rhyme B-MetricName

Den- I-MetricName

sity I-MetricName

( O
RD B-MetricName
) O

, O

the O

longest O

rhyme O

of O

a O

song O

, O

averaged O

over O

all O

songs O

, O

which O

is O

introduced O

by O

Malmi O

et O

al O

. O



( O

2016 O

) O

to O

measure O

the O

quality O

of O

rhyming O

ﬂuency O

; O

4)Combo- B-MetricName

N I-MetricName

, O

the O

maximum O

number O

of O

consecu- O

tive O

sentences O

with O

the O

same O

N O

- O

gram O

rhyme O

in O

a O

rap O

song O

, O

averaged O

over O

all O

songs O

, O

where O

we O

study O

N B-HyperparameterName

= O
1 B-HyperparameterValue

, O
2 B-HyperparameterValue

, O
3 B-HyperparameterValue

; O

5)Beat B-MetricName

Accuracy I-MetricName

( O

BA B-MetricName

) O

, O

the O

accuracy O

of O

our O

model O

in O

beat O

prediction O

, O

under O

the O

teacher- O

forcing O

mode O

. O



Subjective O

Evaluation O

Similar O

to O

previous O

works O

( O

Zhang O

and O

Lapata O

, O

2014 O

; O

Nikolov O

et O

al O

. O

, O

2020 O

) O

in O

artistic O

creation O



, O

we O

also O

use O

human O

evalu- O

ation O

to O

accurately O

evaluate O

the O

quality O

of O

the O

gen- O

erated O

raps O

. O



We O

invite O

10 O

participants O

with O

profes- O

sional O

knowledge O

in O

music O

as O

human O

annotators O

to O

evaluate O

100 O

sampled O

raps O

. O



Each O

annotator O

is O

required O

to O

score O

from O

1 O

( O

Poor O

) O

to O

5 O

( O

Perfect O

) O

on O

the O

following O

perspectives O

: O

1 O

) O

the O

clearness O

of O

the O

theme O

of O

the O

rap O

lyrics O

; O

2 O

) O

the O

ﬂuency O

of O

the O

rap O

lyrics O

; O

3 O

) O

the O

quality O

of O

the O

rhyme O

; O

4 O

) O

the O

diversity O

of O

the O

rhyme O

. O



The O

averaged O

score O

of O

all O

annotators O

on O

all O

sampled O

raps O

is O

used O

as O

the O

evaluation O

score O

for O

each O

perspective O

. O



6 O

Experimental O

Results O

Results O

Table O

2 O

shows O

the O

objective O

and O

subjec- O

tive O

results O

of O

DeepRapper B-MethodName

compared O

with O

two O

base- O

lines O

: O

1 O

) O

Baseline O

: O

a O

standard O

autoregressive O

lan- O

guage O

model O

with O

the O

same O

model O

conﬁguration O

with O

DeepRapper B-MethodName

but O

without O

our O

proposed O

rhyme O

and O

rhythm O

modeling O

; O

2 O

) O

Baseline O

+ O

PT O

, O

using O

pre- O

training O

on O

Baseline O

. O



We O

have O

several O

observations O

from O

Table O

2 O

: O

1 O

) O

DeepRapper O

achieves O

better O

per- O

plexity O

, O

rhyme O

accuracy O

and O

rhyme O

density O

than O

the O

two O

baselines O

, O

which O

demonstrates O

the O

advantages O

of O

our O

method O

in O

generating O

high O

- O

quality O

rap O

lyrics O

with O

accurate O

and O

diverse O

rhymes O

. O



2 O

) O

DeepRap- B-MethodName

per I-MethodName

achieves O

better O

scores O

in O

all O

subjective O

metrics O

, O

demonstrating O

that O

DeepRapper B-MethodName

can O

generate O

high- O

quality O

and O

rhyming O

raps O

that O

accord O

with O

human O

taste O

. O



3 O

) O

Pre O

- O

training O

improves O

the O

performance O

of O

baseline O

in O

both O

objective O

and O

subjective O

met- O

rics O

, O

which O

indicates O

the O

importance O

of O

pre O

- O

training O

. O



However O

, O

its O

performance O

is O

still O

worse O

than O

Deep- B-MethodName

Rapper I-MethodName

. O



Ablation O

Studies O

To O

further O

validate O

the O

neces- O

sity O

of O

each O

component O

in O

DeepRapper B-MethodName

, O

we O

con- O

duct O

a O

series O

of O

ablation O

studies O

, O

including O

remov O

- O

ing O

rhyme O

modeling O

, O

rhythm O

modeling O

and O

pre- O

training O

, O

respectively O

. O



The O

results O

are O

reported O

in O

Table O

3 O

. O



We O

have O

several O

observations O

: O

1 O

) O

Remov- O

ing O

rhyme O

modeling O

affects O

rhyme O

quality O

a O

lot O

as O

it O

results O

in O

a O

dramatic O

drop O

in O

rhyme O

accuracy O

and O

rhyme O

density O

; O

2 O

) O

Removing O

each O

speciﬁc O

de- O

sign O

in O

rhyme O

modeling O

( O

i.e. O

, O

RO O

: O

reverse O

order O

language O

model O

, O

VE O

: O

vowel O

embedding O

, O

IPE O

: O

intra- O

sentence O

position O

embedding O

, O

SE O

: O

sentence O

em- O

bedding O

) O

causes O

worse O

rhyme B-MetricName

accuracy I-MetricName

and O

rhyme B-MetricName

density I-MetricName

. O



Speciﬁcally O

, O

while O

removing O

RO O

leads O

to O

a O

better O

PPL B-MetricName

since O

left O

- O

to O

- O

right O

order O

can O

be O

more O

easily O

modeled O

than O

right O

- O

to O

- O

left O

order O

according O

to O

the O

analysis O

in O

Wu O

et O

al O

. O



( O

2018 O

) O

, O

it O

causes O

large O

accuracy O

drop O

in O

rhyme O

quality O

. O



3 O

) O

Apparently O

, O

DeepRapper B-MethodName

without O

rhythm O

modeling O

can O

not O

pro- O

duce O

any O

beat O

information O

; O

4 O

) O

DeepRapper B-MethodName

without O

pre O

- O

training O

affects O

the O

perplexity B-MetricName

and O

rhyme B-MetricName

accu- I-MetricName

racy I-MetricName

a O

lot O

, O

however O

, O

obtains O

a O

higher O

rhyme B-MetricName

density I-MetricName

. O



The O

reason O

is O

that O

without O

pre O

- O

training O

, O

DeepRap- O

per O

tends O

to O

copy O

previous O

rhyme O

tokens O

due O

to O

the O



76lack O

of O

generalization O

( O

larger O

PPL B-MetricName

) O

. O



To O

verify O

this O

, O

we O

count O

the O

repetitive O

rate O

of O

rhyming O

words O

and O

found O

that O

the O

rate O

of O

DeepRapper B-MethodName

is O

23:8%while O

without O

pre O

- O

training O

is O

42:5 O

% O

, O

which O

is O

higher O

than O

using O

pre O

- O

training O

. O



The O

above O

results O

verify O

the O

effectiveness O

of O

each O

component O

in O

DeepRapper B-MethodName

. O



N O

- O

gram O

Rhyme O

To O

highlight O

the O

advantage O

of O

DeepRapper B-MethodName

in O

modeling O

N O

- O

gram O

rhyme O

, O

we O

use O

Combo- B-MetricName

N I-MetricName

to O

measure O

the O

ability O

of O

each O

design O

in O

DeepRapper B-MethodName

to O

model O

N O

- O

gram O

rhyme O

. O



The O

re- O

sults O

are O

reported O

in O

Table O

4 O

. O



We O

can O

ﬁnd O

that O

1 O

) O

The O

model O

without O

rhyme O

modeling O

can O

hardly O

generate O

good O

rhyme O

, O

regardless O

of O

the O

value O

of O

NinN O

- O

gram O

; O

2 O

) O

Removing O

rhyme O

constraint O

also O

weakens O

the O

capacity O

of O

generating O

N O

- O

gram O

rhyme O

. O



These O

results O

further O

demonstrate O

the O

importance O

of O

our O

rhyme O

modeling O

and O

rhyme O

constraint O

in O

generating O

multiple O

consecutive O

rhymes O

. O



Beat O

Frequency O



To O

better O

measure O

the O

beat O

qual- O

ity O

, O

we O

randomly O

generate O

about O

5,000 O

samples O

by O

DeepRapper B-MethodName

and O

DeepRapper B-MethodName

with O

beat O

frequency O

control O

. O



We O

propose O

the O

First B-MetricName

Order I-MetricName

Distribution I-MetricName

( O

FOD B-MetricName

) O

and O

the O

Second B-MetricName

Order I-MetricName

Distribution I-MetricName

( O

SOD B-MetricName

) O

and O

measure O

the O

distance O

( O

via O

Wasserstein O



Dis- O

tance O

( O

Vallender O

, O

1974 O

) O

) O

of O

these O

distributions O

be- O



tween O

the O

generated O

samples O

and O

our O

DRAP B-DatasetName

dataset O

. O



We O

deﬁne O

the O

interval O

of O

the O

current O

[ O

BEAT O

] O

as O

the O

number O

of O

words O

between O

the O

current O

[ O

BEAT O

] O

and O

the O

next O

[ O

BEAT O

] O

. O



Therefore O

, O

the O

FOD B-MetricName

is O

deﬁned O

as O

the O

distribution O

of O

the O

interval O

of O

the O

current O

[ O

BEAT O

] O

. O



Similarly O

, O

the O

SOD B-MetricName

is O

deﬁned O

the O

dis- O

tribution O

of O

the O

difference O

between O

the O

interval O

of O

the O

current O

[ O

BEAT O

] O

and O

the O

next O

[ O

BEAT O

] O

. O



The O

results O

of O

the O

distance O

are O

normalized O

into O

[ O

0;1 O

] O

and O

are O

reported O

in O

Table O

5 O

. O



It O

can O

be O

seen O

that O

DeepRapper B-MethodName

with O

beat O

frequency O

control O

achieves O

better O

performance O

in O

beat O

modeling O

, O

which O

indi- O

cates O

the O

importance O

of O

beat O

frequency O

control O

in O

beat O

modeling O

. O



Case O

Analyses O

on O

Generated O

Raps O

We O

list O

a O

sample O

case O

from O

our O

generated O

raps O

in O

Figure O

6 O

to O

demonstrate O

the O

good O

quality O

of O

the O

raps O

gen- O

erated O

by O

DeepRapper B-MethodName

. O



The O

sample O

is O

generated O

by O

feeding O

the O

ﬁrst O

sentence O

of O

the O

example O

in O

Fig- O

ure O

2 O

to O

DeepRapper B-MethodName

. O



As O

we O

can O

see O

, O

the O

gen- O

erated O

sample O

exhibits O

good O

theme O

, O

ﬂuency O

and O

rhyme O

. O



The O

sample O

is O

a O

rap O

with O

a O

number O

of O

1- O

gram O

, O

2 O

- O

gram O

, O

3 O

- O

gram O

, O

and O

even O

4 O

- O

gram O

rhyme O

. O



The O

generated O

lyrics O

depicts O

the O

fond O

memories O

of O

childhood O

and O

the O

beautiful O

visions O

for O

the O

fu- O

tures O

. O



We O

also O

provide O

a O

group O

of O

samples O

gener- O

ated O

with O

beat O

frequency O

control O

. O



To O

save O

space O

, O

we O

put O

them O

and O

the O

translation O

of O

all O

the O

sam- O

ples O

to O

Appendix O

. O



More O

samples O

are O

provided O

in O

https://deeprapper.github.io O

. O



7 O

Conclusions O

In O

this O

paper O

, O

we O

develop O

DeepRapper B-MethodName

, O

a O

novel O

Transformer O

- O

based O

rap O
generation O
system O

, O

which O

leverages O

rhyme O

modeling O

, O

rhythm O

modeling O

and O

pre O

- O

training O

for O

rap B-TaskName

generation I-TaskName

. O



Considering O

there O

is O

no O

available O

rap O

dataset O

with O

aligned O

rhythmic O

beats O

for O

rhythm O

modeling O

, O

we O

propose O

a O

data O

min- O

ing O

pipeline O

to O

mine O

a O

rap O

dataset O

with O

beat O

- O

lyric O

alignments O

. O



We O

leverage O

right O

- O

to O

- O

left O

generation O

, O

rhyme O

representation O

and O

rhyme O

constraint O

to O

bet- O

ter O

model O

rhyme O

and O

encourage O

N O

- O

gram O

rhyme O

, O

and O

explicitly O

model O

beat O

information O

by O

insert O

beat O

token O

beside O

the O

corresponding O

word O

in O

the O

lyric O

sequence O

. O



To O

our O

knowledge O

, O

DeepRapper B-MethodName

is O

the O

ﬁrst O

system O

to O

generate O

rap O

with O

both O

rhymes O

and O

rhythms O

. O



Both O

objective O

and O

subjective O

eval- O

uations O

demonstrate O

that O

DeepRapper B-MethodName

generates O

high O

- O

quality O

raps O

with O

good O

rhymes O

and O

rhythms O

. O



Thanks O

to O

the O

design O

of O

DeepRapper B-MethodName

, O

we O

can O

fur- O

ther O

build O

another O

rap O

singing O

system O

to O

sing O

out O

the O

raps O

according O

to O

the O

rhymes O

and O

rhythms O

, O

which O

we O

leave O

as O

future O

work O

. O



We O

also O

leave O

Multilingual O

DeepRapper B-MethodName

as O

future O

work O

. O



Acknowledgements O

We O

would O

like O

to O

acknowledge O

the O

anonymous O

re- O

viewers O

for O

their O

insightful O

comments O

. O



Research O

on O

this O

paper O

was O

supported O

by O

Hong O

Kong O

Research O

Grants O

Council O

under O

grant O

16204920 O

. O



Ethical O

Considerations O

The O

proposed O

framework O

can O

be O

considered O

a O

novel O

language O

model O

for O

rap B-TaskName

generation I-TaskName

in O

automatic O

artistic O

creation O

. O



Speciﬁcally O

, O

the O

proposed O

frame- O

work O

has O

been O

conﬁgured O

with O

novel O

rhyme O

mod- O

eling O

as O

rhyme O

is O

quite O

important O

in O

music O

genres O

. O



Therefore O

, O

our O

proposed O

framework O

is O

also O

bene- O

ﬁcial O

for O

generating O

other O

music O

genres O

. O



On O

the O

other O

hand O

, O

although O

we O

collect O

large O

- O

scale O

lyric O

data O

for O

pre O

- O

training O

, O

it O

still O

can O

not O

fully O

utilize O

the O

potential O

of O

pre O

- O

training O

. O



In O

the O

future O

, O

we O

expect O

to O

employ O

more O

large O

- O

scale O

data O

in O

the O

open O

domain O

plus O

the O

music O

domain O

for O

pre O

- O

training O

to O

improve O

the O

capacity O

of O

the O

language O

model O

. O



In O

addition O

, O

our O

training O

datasets O

may O

have O

biases O

, O

which O

may O

bring O

some O

potential O

risks O

of O

model O

bias O

. O



Hence O

, O

we O

encourage O

future O

works O

to O

study O

how O

to O

apply O

other O

techniques O

in O

mitigating O

similar O

problems O

in O

our O

framework O

. O



References O

Yihao O

Chen O

and O

Alexander O

Lerch O

. O



2020 O

. O



Melody- O

conditioned O

lyrics B-TaskName

generation I-TaskName

with O

seqgans O

. O



In O

arXiv O

. O



Tim O

Van O

de O

Cruys O

. O



2020 O

. O



Automatic O

poetry B-TaskName

generation I-TaskName

from O

prosaic O

text O

. O



In O

Proceedings O

of O

the O

58th O

An- O

nual O

Meeting O

of O

the O

Association O

for O

Computational O

Linguistics O

, O

pages O

2471–2480 O

. O



Jacob O

Devlin O

, O

Ming O

- O

Wei O

Chang O

, O

Kenton O

Lee O

, O

and O

Kristina O

Toutanova O

. O

2019 O

. O



BERT O

: O

Pre O

- O

training O

of O

deep O

bidirectional O

transformers O

for O

language B-TaskName

under- I-TaskName

standing I-TaskName

. O



In O

NAACL O

, O

pages O

4171–4186 O

. O



Romain O

Hennequin O

, O

Anis O

Khlif O

, O

Felix O

V O

oituret O

, O

and O

Manuel O

Moussallam O

. O



2020 O

. O



Spleeter O

: O

a O

fast O

and O

efﬁcient O

music O

source O

separation O

tool O

with O

pre- O

trained O

models O

. O



Journal O

of O

Open O

Source O

Software O

, O

5(50):2154 O

. O



Deezer O

Research O

. O



Yu O

- O

Siang O

Huang O

and O

Yi O

- O

Hsuan O

Yang O

. O



2020 O

. O



Pop O

music O

transformer O

: O

Beat O

- O

based O

modeling O

and O

generation O

of O

expressive O

pop O

piano O

compositions O

. O



In O

Proceedings O

of O

the O

28th O

ACM O

International O

Conference O

on O

Multi- O

media O

, O

page O

1180–1188 O

. O



Cheryl O

Lynette O

Keyes O

. O



2004 O

. O



Rap O

music O

and O

street O

con- O

sciousness O

, O

volume O

501 O

. O



University O

of O

Illinois O

Press O

. O



Jey O

Han O

Lau O

, O

Trevor O

Cohn O

, O

Timothy O

Baldwin O

, O

Julian O

Brooke O

, O

and O

Adam O

Hammond O

. O



2018 O

. O



Deep B-MethodName

- I-MethodName

speare I-MethodName

: O

A O

joint O

neural O

model O

of O

poetic O

language O

, O

meter O

and O

rhyme O

. O



In O

Proceedings O

of O

the O

56th O

Annual O

Meet- O

ing O

of O

the O

Association O

for O

Computational O

Linguistics O

( O

Volume O

1 O

: O

Long O

Papers O

) O

, O

pages O

1948–1958 O

. O



Piji O

Li O

, O

Haisong O

Zhang O

, O

Xiaojiang O

Liu O

, O

and O

Shuming O

Shi O

. O

2020 O

. O



Rigid O

formats O

controlled O

text B-TaskName

generation I-TaskName

. O



InACL O

, O

pages O

742–751 O

. O



Hongru O

Liang O

, O

Qian O

Li O

, O

Haozheng O

Wang O

, O

Hang O

Li O

, O

Jin- O

Mao O

Wei O

, O

and O

Zhenglu O

Yang O

. O



2018 O

. O



Attae O

- O

rl2 O

: O

At- O

tention O

based O

autoencoder O

for O

rap B-TaskName

lyrics I-TaskName

representa- I-TaskName

tion I-TaskName

learning I-TaskName

. O



In O

Companion O

Proceedings O

of O

the O

The O

Web O

Conference O

2018 O

, O

pages O

7–8 O

. O



Yi O

Liao O

, O

Yasheng O

Wang O

, O

Qun O

Liu O

, O

and O

Xin O

Jiang O

. O



2019 O

. O



Gpt O

- O

based O

generation O

for O

classical O

chinese O

poetry O

. O



arXiv O

preprint O

arXiv:1907.00151 O

. O



Yinhan O

Liu O

, O

Myle O

Ott O

, O

Naman O

Goyal O

, O

Jingfei O

Du O

, O

Man- O

dar O

Joshi O

, O

Danqi O

Chen O

, O

Omer O

Levy O

, O

Mike O

Lewis O

, O

Luke O

Zettlemoyer O

, O

and O

Veselin O

Stoyanov O

. O



2019 O

. O



Roberta O

: O

A O

robustly O

optimized O

BERT O

pretraining O

ap- O

proach O

. O



CoRR O

, O

abs/1907.11692 O

. O



78Yusen O

Liu O

, O

Dayiheng O

Liu O

, O

and O

Jiancheng O

Lv O

. O

2020 O

. O



Deep B-MethodName

poetry I-MethodName

: O

A O

chinese O

classical O

poetry O

generation O

system O

. O



In O

Proceedings O

of O

the O

AAAI O

Conference O

on O

Artiﬁcial O

Intelligence O

, O

volume O

34 O

, O

pages O

13626 O

– O

13627 O

. O



Xu O

Lu O

, O

Jie O

Wang O

, O

Bojin O

Zhuang O

, O

Shaojun O

Wang O

, O

and O

Jing O

Xiao O

. O

2019 O

. O



A O

syllable O

- O

structured O

, O

contextually- O

based O

conditionally O

generation O

of O

chinese O

lyrics O

. O



In O

PRICAI O

, O

volume O

11672 O

, O

pages O

257–265 O

. O



Eric O

Malmi O

, O

Pyry O

Takala O

, O

Hannu O

Toivonen O

, O

Tapani O

Raiko O

, O

and O

Aristides O

Gionis O

. O

2016 O

. O



Dopelearning B-MethodName

: O

A O

computational O

approach O

to O

rap B-TaskName

lyrics I-TaskName

generation I-TaskName

. O



InProceedings O

of O

the O

22nd O

ACM O

SIGKDD O

Inter- O

national O

Conference O

on O

Knowledge O

Discovery O

and O

Data O

Mining O

, O

pages O

195–204 O

. O



Brian O

McFee O

, O

Vincent O

Lostanlen O

, O

Alexandros O

Met- O

sai O

, O

Matt O

McVicar O

, O

Stefan O

Balke O

, O

Carl O

Thom O

´ O

e O

, O

Colin O

Raffel O

, O

Frank O

Zalkow O

, O

Ayoub O

Malek O

, O

Dana O

, O

Kyungyun O

Lee O

, O

Oriol O

Nieto O

, O

Jack O

Mason O

, O

Dan O

El- O

lis O

, O

Eric O

Battenberg O

, O

Scott O

Seyfarth O

, O

Ryuichi O

Ya- O

mamoto O

, O

Keunwoo O

Choi O

, O

viktorandreevichmorozov O

, O

Josh O

Moore O

, O

Rachel O

Bittner O

, O

Shunsuke O

Hidaka O

, O

Ziyao O

Wei O

, O

nullmightybofo O

, O

Dar O

´ O

ıo O

Here O

˜n´u O

, O

Fabian- O

Robert O

St O

¨oter O

, O

Pius O

Friesch O

, O

Adam O

Weiss O

, O

Matt O

V O

oll- O

rath O

, O

and O

Taewoon O

Kim O

. O

2020 O

. O



librosa O

/ O

librosa O

: O

0.8.0 O

. O



Nikola O

I. O

Nikolov O

, O

Eric O

Malmi O

, O

Curtis O

Northcutt O

, O

and O

Loreto O

Parisi O

. O



2020 O

. O



Rapformer B-MethodName

: O

Conditional O

rap B-TaskName

lyrics I-TaskName

generation I-TaskName

with O

denoising O

autoencoders O

. O



In O

Proceedings O

of O

the O

13th O

International O

Conference O

on O

Natural O

Language O

Generation O

, O

pages O

360–373 O

. O



Peter O

Potash O

, O

Alexey O

Romanov O

, O

and O

Anna O

Rumshisky O

. O

2015 O

. O



Ghostwriter B-MethodName

: O

Using O

an O

lstm O

for O

automatic O

rap B-TaskName

lyric I-TaskName

generation I-TaskName

. O



In O

Proceedings O

of O

the O

2015 O

Con- O

ference O

on O

Empirical O

Methods O

in O

Natural O

Language O

Processing O

, O

pages O

1919–1924 O

. O



Alec O

Radford O

, O

Karthik O

Narasimhan O

, O

Tim O

Salimans O

, O

and O

Ilya O

Sutskever O

. O

2018 O

. O



Improving O

language O

under- O

standing O

by O

generative O

pre O

- O

training O

. O



Alec O

Radford O

, O

Jeff O

Wu O

, O

Rewon O

Child O

, O

David O

Luan O

, O

Dario O

Amodei O

, O

and O

Ilya O

Sutskever O

. O

2019 O

. O



Language O

models O

are O

unsupervised O

multitask O

learners O

. O



Yi O

Ren O

, O

Jinzheng O

He O

, O

Xu O

Tan O

, O

Tao O

Qin O

, O

Zhou O

Zhao O

, O

and O

Tie O

- O

Yan O

Liu O

. O

2020 O

. O



Popmag B-MethodName

: O



Pop B-MethodName

music I-MethodName

ac- I-MethodName

companiment I-MethodName

generation I-MethodName

. O



In O

Proceedings O

of O

the O

28th O

ACM O

International O

Conference O

on O

Multimedia O

, O

pages O

1198–1206 O

. O



Zhonghao O

Sheng O

, O

Kaitao O

Song O

, O

Xu O

Tan O

, O

Yi O

Ren O

, O

Wei O

Ye O

, O

Shikun O

Zhang O

, O

and O

Tao O

Qin O

. O

2020 O

. O



Songmass B-MethodName

: O

Automatic O

song B-TaskName

writing I-TaskName

with O

pre O

- O

training O

and O

align- O

ment O

constraint O

. O



arXiv O

preprint O

arXiv:2012.05168 O

. O



Kaitao O

Song O

, O

Xu O

Tan O

, O

Tao O

Qin O

, O

Jianfeng O

Lu O

, O

and O

Tie- O

Yan O

Liu O

. O

2019 O

. O



Mass B-MethodName

: O



Masked B-MethodName

sequence I-MethodName

to I-MethodName

se- I-MethodName

quence I-MethodName

pre I-MethodName

- I-MethodName

training I-MethodName

for O

language B-TaskName

generation I-TaskName

. O



In O

In- O

ternational O

Conference O

on O

Machine O

Learning O

, O

pages O

5926–5936.SS O

Vallender O

. O



1974 O

. O



Calculation O

of O

the O

wasserstein O

dis- O



tance O

between O

probability O

distributions O

on O

the O

line O

. O



Theory O

of O

Probability O

& O

Its O

Applications O

, O

18(4):784 O

– O

786 O

. O



Ashish O

Vaswani O

, O

Noam O

Shazeer O

, O

Niki O

Parmar O

, O

Jakob O

Uszkoreit O

, O

Llion O

Jones O

, O

Aidan O

N O

Gomez O

, O

Ł O

ukasz O

Kaiser O

, O

and O

Illia O

Polosukhin O

. O

2017 O

. O



Attention O

is O

all O

you O

need O

. O



In O

NIPS O

, O

pages O

5998–6008 O

. O



Kento O

Watanabe O

, O

Yuichiroh O

Matsubayashi O

, O

Satoru O

Fukayama O

, O

Masataka O

Goto O

, O

Kentaro O

Inui O

, O

and O

To- O

moyasu O

Nakano O

. O



2018 O

. O



A O

melody O

- O

conditioned O

lyrics O

language O

model O

. O



In O

NAACL O

, O

pages O

163–172 O

. O



Lijun O

Wu O

, O

Xu O

Tan O

, O

Di O

He O

, O

Fei O

Tian O

, O

Tao O

Qin O

, O

Jianhuang O

Lai O

, O

and O

Tie O

- O

Yan O

Liu O

. O



2018 O

. O



Beyond O

error O

propaga- O

tion O

in O

neural B-TaskName

machine I-TaskName

translation I-TaskName

: O

Characteristics O

of O

language O

also O

matter O

. O



In O

Proceedings O

of O

the O

2018 O

Conference O

on O

Empirical O

Methods O

in O

Natural O

Lan- O

guage O

Processing O

, O

pages O

3602–3611 O

. O



Zhilin O

Yang O

, O

Zihang O

Dai O

, O

Yiming O

Yang O

, O

Jaime O

Car- O

bonell O

, O

Russ O

R O

Salakhutdinov O

, O

and O

Quoc O

V O

Le O

. O



2019 O

. O



Xlnet B-MethodName

: O



Generalized O

autoregressive O

pretraining O

for O

language B-TaskName

understanding I-TaskName

. O



In O

Advances O

in O

neural O

in- O

formation O

processing O

systems O

, O

pages O

5753–5763 O

. O



Xingxing O

Zhang O

and O

Mirella O

Lapata O

. O



2014 O

. O



Chinese B-TaskName

poetry I-TaskName

generation I-TaskName

with O

recurrent O

neural O

networks O

. O



In O

Proceedings O

of O

the O

2014 O

Conference O

on O

Empirical O

Methods O

in O

Natural O

Language O

Processing O

( O

EMNLP O

) O

, O

pages O

670–680 O

. O



Guo O

Zhipeng O

, O

Xiaoyuan O

Yi O

, O

Maosong O

Sun O

, O

Wenhao O

Li O

, O

Cheng O

Yang O

, O

Jiannan O

Liang O

, O

Huimin O

Chen O

, O

Yuhui O

Zhang O

, O

and O

Ruoyu O

Li O

. O

2019 O

. O



Jiuge B-MethodName

: O



A O

human- O

machine O

collaborative O

chinese O

classical O

poetry O

gen- O

eration O

system O

. O



In O

Proceedings O

of O

the O

57th O

Annual O

Meeting O

of O

the O

Association O

for O

Computational O

Lin- O

guistics O

: O

System O

Demonstrations O

, O

pages O

25–30 O

. O



Hongyuan O

Zhu O

, O

Qi O

Liu O

, O

Nicholas O

Jing O

Yuan O

, O

Chuan O

Qin O

, O

Jiawei O

Li O

, O

Kun O

Zhang O

, O

Guang O

Zhou O

, O

Furu O

Wei O

, O

Yuanchun O

Xu O

, O

and O

Enhong O

Chen O

. O



2018 O

. O



Xiaoice B-MethodName

band I-MethodName

: O

A O

melody O

and O

arrangement O

generation O

frame- O

work O

for O

pop O

music O

. O



In O

Proceedings O

of O

the O

24th O

ACM O

SIGKDD O

International O

Conference O

on O

Knowl- O

edge O

Discovery O

and O

Data O

Mining O

, O

page O

2837–2846 O

. O



A O

Comparison O

with O

GhostWriter B-MethodName

We O

provide O

a O

comparison O

between O

DeepRapper B-MethodName

and O

GhosterWriter B-MethodName

( O

Potash O

et O



al O

. O

, O

2015 O

) O

in O

Table O

6 O

. O



The O

results O

show O

that O

both O

DeepRapper B-MethodName

and O

base- O

lines O

outperform O

GhosterWriter B-MethodName

in O

terms O

of O

PPL B-MetricName

, O

rhyme B-MetricName

accuracy I-MetricName

, O

and O

rhyme B-MetricName

density I-MetricName

on O

rap B-TaskName

genera- I-TaskName

tion I-TaskName

tasks O

. O



B O

Samples O

with O

Beat O

Frequency O

Control O

Fast O

Figure O

7 O

provides O

a O

rap O

generated O

by O

Deep- B-MethodName

Rapper I-MethodName

with O

fast O

beat O

frequency O

, O

which O

the O

fre- O

quency O

is O

4.3 O

. O



The O

rap O

express O

ones O

beat O

wished O

to O

his O

/ O

her O

lover O

. O



The O

following O

is O

the O

translation O

of O

texts O

in O

Figure O

7 O

. O

我长大的地方像一个简朴的寨 O

The O

place O

where O

I O

grew O

up O

is O

like O

a O

simple O

village O

遥望远方镜子里的海 O

Looking O

into O

the O

distance O

, O

the O

sea O

is O

in O

the O

mirror O

看见理想很实际的说一句拜拜 O

See O

my O

dream O

and O

say O

goodbye O

这世界在我眼里和千里之外 O



The O

world O

is O

sometimes O

in O

my O

eyes O

and O

sometimes O

thousands O

of O

miles O

away O

穿过河流沙漠和人海 O

Across O

rivers O

, O

deserts O

and O

crowds O

一山万水渡过岁月冲洗我的爱 O



A O

mountain O

and O

a O

million O

rivers O

wash O

my O

love O

through O

the O

years O

和那亲爱的你一起去踩一路的尘埃 O

Step O

on O

the O

dust O

all O

the O

way O

with O

dear O

you O

我一生中最重要的你我壹一都在 O



The O

most O

important O

you O

in O

my O

life O

, O

I O

’ll O

always O

by O

your O

side O

点点轻地落在你那冰封的大门外 O

Little O

by O

little O

, O

it O

falls O

outside O

your O

frozen O

gate O

我在默默的祝福你 O

I O

am O

blessing O

you O

silently O

我在无数个夜里默默地祝福你 O



I O

have O

secretly O

blessed O

you O

for O

countless O

nights O

远远地保护着你我的爱 O

Protecting O

you O

and O

my O

love O

from O

a O

distance O

Medium O

Figure O

8 O

provides O

a O

rap O

generated O

by O

DeepRapper B-MethodName

with O

medium O

beat O

frequency O

, O

which O

the O

frequency O

is O

2.6 O

. O



The O

rap O

praises O

the O

times O

we O

live O

in O

. O



The O

following O

is O

the O

translation O

of O

texts O

in O

Figure O

8 O

. O



我长大的地方像一个简朴的寨 O

The O

place O

where O

I O

grew O

up O

is O

like O

a O

simple O

village O

简朴的看着简朴的海 O



Simply O

looking O

at O

the O

simple O

sea O

爸爸拿着一个简朴的麦 O

Dad O

holding O

a O

simple O

wheat O

有人真实的努力就有人背负着爱 O



Someone O

takes O

effort O

, O

somebody O

is O

carrying O

love O

那生活的美好让人人们热爱 O

The O

beauty O

of O

life O

makes O

people O

love O

这世界的美好纯粹是意外 O



The O

beauty O

of O

this O

world O

is O

pure O

accident O

而我长大的地方是个简朴的寨 O

And O

the O

place O

where O

I O

grew O

up O

is O

a O

simple O

village O

让我们在这里开心的喝彩 O



Let O

’s O

cheer O

happily O

here O

伟大母亲怀抱着爱 O



Great O

mother O

embrace O

love O

看着幸福的人们敞开淳朴的怀 O

Watching O

happy O

people O

open O

their O

simple O

arms O

我们最美好的这个快乐海 O

We O

are O

in O

the O

most O

beautiful O

happy O

sea O

唱出我们的时代 O



Sing O

our O

time O



Slow O

Figure O

9 O

provides O

a O

rap O

generated O

by O

Deep- B-MethodName

Rapper I-MethodName

with O

slow O

beat O

frequency O

, O

where O

the O

fre- O

quency O

is O

2.1 O

. O



The O

rap O

express O

ones O

relief O

from O

life O

. O



The O

following O

is O

the O

translation O

of O

texts O

in O

Figure O

9 O

. O



我长大的地方像一个简朴的寨 O

The O

place O

where O

I O

grew O

up O

is O

like O

a O

simple O

village O

快到有一天看见了父母的爱 O

Almost O

one O

day O

I O

saw O

the O

love O

of O

my O

parents O

我的时间你实在不用去考虑自己多坏 O



You O

do O

n’t O

have O

to O

think O

about O

how O

bad O

you O

are O

in O

my O

time O

当我脚步在外从没过的这么可爱 O



I O

’ve O

never O

been O

so O

cute O

when O

I O

’m O

out O

我只是一次旅行 O

I O

’m O

just O

a O

trip O

to O

your O

life O

你现在的校服我也想换 O

I O

want O

to O

change O

your O

current O

school O

uniform O

我曾经追你 O

I O

used O

to O

chase O

you O

你的运气也不摔 O



Your O

luck O

wo O

n’t O

fall O

毕竟上次 O

After O

all O

last O

time你爱的姑娘你也想看 O

You O

want O

to O

see O

the O

girl O

you O

love O

她们和你一定要分离 O



They O

must O

be O

separated O

from O

you O

你就这样子一笑而去 O



You O

just O

leave O

with O

a O

smile O



C O

Translation O

of O

Chinese O

Examples O

in O

the O

Paper O

Words O

in O

red O

are O

rhymes O

. O



Translation O

of O

Chinese O

in O

Figure O

2 O

我长大的地方像一个简朴的寨 O

The O

place O

where O

I O

grew O

up O

is O

like O

a O

simple O

village O

简朴的人吃着最简朴的菜 O

Simple O

people O

eat O

the O

simplest O

dishes O

简朴的话包含着简朴的爱 O

Simple O

words O

contain O

simple O

love O

简朴的道理传给一代又一代 O

Simple O

principles O

are O

passed O

on O

from O

generation O

to O

generation O



81难以忘记的画面不需相机 O



Unforgettable O

picture O

do O

not O

need O

camera O

to O

capture O

难以再闻到的是巷子里的香气 O



What O

is O

hard O

to O

smell O

is O

the O

aroma O

in O

the O

alley O

常常想起外婆家的躺椅 O

I O

often O

think O

of O

grandma O

’s O

recliner O

最珍贵的椰奶往往藏在床底 O

The O

most O

precious O

coconut O

milk O

is O

often O

hidden O

under O

the O

bed O



先填饱肚子再想那些背不下的书 O

Fill O

your O

stomach O

ﬁrst O

, O

then O

think O

about O

the O

books O

that O

you O

ca O

n’t O

remember O

外婆做的火腿肠比外面炸的酥 O

Grandma O

’s O

ham O

sausage O

is O

crispier O

than O

fried O

outside O



油烟的香味弥漫不那么大的屋 O

The O

smell O

of O

lampblack O

pervades O

the O

not O

so O

big O

house O

外婆的故事总会让大人笑着哭 O



Grandma O

’s O

stories O

always O

make O

adults O

laugh O

and O

cry O

Translation O

of O

Chinese O

in O

Figure O

3 O

我抬头仰望。天空的苍茫 O

。 O



I O

looked O

up O

. O



The O

sky O

is O

vast O

. O



Translation O

of O

Chinese O

in O

Figure O

4 O

是这座城市的气象 O

It O

is O

the O

weather O

of O

this O

city O

让你感受生命的力量 O

makes O

you O

feel O

the O

power O

of O

living O

Translation O

of O

Chinese O

in O

Figure O

6 O

我长大的地方像一个简朴的寨 O

The O

place O

where O

I O

grew O

up O

is O

like O

a O

simple O

village O

公里也许大的远方简直是个小小的寨 O



A O

small O

far O

away O

village O

偶尔穿件毛衣那样子很可爱 O

It O

is O

cute O

to O

wear O

a O

sweater O

occasionally O

远方可单纯的姑娘还是单纯的孩 O

Is O

it O

a O

simple O

girl O

or O

a O

simple O

child O

far O

away O

是放不下的故事大声的喝彩 O

Cheers O

loudly O

for O

the O

unforgettable O

story O

像快乐的小孩莫名的敞着怀 O

Happy O

kids O

like O

happy O

kids几百公里我们相约到未来 O

Through O

hundreds O

of O

kilometers O

, O

we O

meet O

in O

the O

future O

在那无尽的沙漠和海 O

In O

the O

endless O

desert O

and O

sea O

看着温暖花开 O



Watching O

the O

warm O

ﬂowers O

bloom O

花一样的在 O



Like O

ﬂowers O

be O

there O

写动人的天籁 O

Write O

moving O

sounds O

of O

nature O

跟着自由自在 O



Feeling O

the O

freedom O

消沉在那片海 O



Sometimes O

depressed O

in O

the O

sea O

不懂儿时的他们不懂什么是爱 O

I O

do O

n’t O

understand O

their O

childish O

. O



I O

do O

n’t O

know O

what O

love O

is O

到现在你看来 O



Till O

now O

you O

see O

最真的迷彩 O

It O

is O

The O

most O

true O

fantasy O


