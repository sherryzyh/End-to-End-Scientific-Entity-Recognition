Language O
Model O
Augmented O
Monotonic O
Attention O
for O
Simultaneous O
Translation I-TaskName
Sathish O
Indurthi§∗Mohd O
Abbas O
Zaidi‡ O
Beomseok O
Lee‡†Nikhil O
Kumar O
Lakumarapu‡†Sangha O
Kim‡ O
‡Samsung O
Research I-MethodName
, O
South B-DatasetName
Korea§Zoom I-DatasetName
AI I-MethodName
Lab I-MethodName
, O
Singapore O
sathishreddy.indurthi@zoom.us O
, O
{ O
abbas.zaidi O
, O
bsgunn.lee O
, O
n07.kumar O
, O
sangha01.kim}@samsung.com O

Abstract O
The O
state O
- O
of O
- O
the O
- O
art O
adaptive O
policies O
for O
simul- B-TaskName
taneous I-TaskName
neural I-TaskName
machine I-TaskName
translation I-TaskName
( O
SNMT B-TaskName
) O
use O
monotonic O
attention O
to O
perform O
read O
/ O
write O
decisions O
based O
on O
the O
partial O
source O
and O
target O
sequences O
. O

The O
lack O
of O
sufficient O
information O
might O
cause O
the O
monotonic O
attention O
to O
take O
poor O
read O
/ O
write O
decisions O
, O
which O
in O
turn O
neg- O
atively O
affects O
the O
performance O
of O
the O
SNMT B-MethodName
model O
. O

On O
the O
other O
hand O
, O
human O
translators O
make O
better O
read O
/ O
write O
decisions O
since O
they O
can O
anticipate O
the O
immediate O
future O
words O
using O
linguistic O
information O
and O
domain O
knowledge O
. O

In O
this O
work O
, O
we O
propose O
a O
framework O
to O
aid O
monotonic O
attention O
with O
an O
external O
language O
model O
to O
improve O
its O
decisions O
. O

Experiments O
on O
MuST-C B-DatasetName
English I-DatasetName
- I-DatasetName
German I-DatasetName
and O
English B-DatasetName
- I-DatasetName
French I-DatasetName
speech I-TaskName
- O
to I-TaskName
- O
text I-TaskName
translation I-TaskName
tasks O
show O
the O
future O
information O
from O
language O
model O
improves O
the O
state O
- O
of O
- O
the O
- O
art O
monotonic O
multi O
- O
head O
attention O
model O
further O
. O

1 O
Introduction O
A O
typical O
application O
of O
simultaneous B-TaskName
neural I-TaskName
ma- I-TaskName
chine I-TaskName
translation I-TaskName
( O
SNMT B-TaskName
) O
is O
conversational B-TaskName
speech I-TaskName
or O
live B-TaskName
video I-TaskName
caption I-TaskName
translation I-TaskName
. O

In O
order O
to O
achieve O
live O
translation O
, O
an O
SNMT B-MethodName
model O
alternates O
be- O
tween O
performing O
read O
from O
source O
sequence O
and O
write O
to O
target O
sequence O
. O

For O
a O
model O
to O
decide O
whether O
to O
read O
orwrite O
at O
certain O
moment O
, O
either O
a O
fixed O
or O
an O
adaptive O
read O
/ O
write O
policy O
can O
be O
used O
. O

Earlier O
approaches O
in O
simultaneous O
translation O
such O
as O
Ma O
et O
al O
. O
( O
2019a O
) O
and O
Dalvi O
et O
al O
. O

( O
2018 O
) O
employ O
a O
fixed O
policy O
that O
alternate O
between O
read O
andwrite O
after O
the O
waiting O
period O
of O
ktokens O
. O

To O
alleviate O
possible O
long O
delay O
of O
fixed O
polices O
, O
re- O
cent O
works O
such O
as O
monotonic O
infinite O
lookback O
attention O
( O
MILk B-MethodName
) O
( O
Arivazhagan O
et O
al O
. O
, O
2019 O
) O
, O
and O
monotonic B-TaskName
multihead I-TaskName
attention I-TaskName
( O
MMA B-TaskName
) O
( O
Ma O
et O
al O
. O
, O
2019c O
) O
developed O
flexible O
policies O
using O
monotonic O
attention O
( O
Raffel O
et O
al O
. O
, O
2017 O
) O
. O

While O
these O
monotonic O
attention O
anticipates O
tar- B-TaskName
get O
words O
using O
only O
available O
prefix O
source O
and O
target O
sequence O
, O
human O
translators O
anticipate O
the O
target O
words O
using O
their O
language O
expertise O
( O
linguis- O
tic O
anticipation O
) O
as O
well O
as O
contextual O
information O
( O
extra O
- O
linguistic O
anticipation O
) O
( O
Vandepitte O
, O
2001 O
) O
. O

Inspired O
by O
human O
translation O
experts O
, O
we O
aim O
to O
augment O
monotonic O
attention O
with O
future O
informa- O
tion I-TaskName
using O
language O
models O
( O
LM B-TaskName
) O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Conneau O
et O
al O
. O
, O
2019 O
) O
. O

Integrating O
the O
external O
information O
effectively O
into O
text B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
text I-TaskName
machine I-TaskName
translation I-TaskName
( O
MT B-TaskName
) O
systems O
has O
been O
explored O
by O
several O
works O
( O
Khandelwal O
et O
al O
. O
, O
2020 O
; O
Gulcehre O
et O
al O
. O
, O
2015 O
, O
2017 O
; O
Stahlberg O
et O
al O
. O
, O
2018 O
) O
. O

Also O
, O
integrating O
future O
information O
implicitly O
into O
SNMT B-TaskName
system O
during O
training O
is O
ex- O
plored O
in O
Wu O
et O
al O
. O

( O
2020 O
) O
by O
simultaneously O
train- O
ing O
different O
wait-K O
SNMT O
systems O
. O

However O
, O
no O
previous O
works O
make O
use O
of O
explicit O
future O
informa- O
tion O
both O
during O
training O
and O
inference O
. O

To O
utilize O
explicit O
future O
information O
, O
we O
explored O
to O
inte- O
grate O
future O
information O
from O
LM B-TaskName
directly O
into O
the O
output O
layer O
of O
the O
MMA B-MethodName
model O
. O

However O
, O
it O
did O
not O
provide O
any O
improvements O
( O
refer O
to O
Appendix O
A O
) O
, O
thus O
motivating O
us O
to O
explore O
a O
tighter O
integra- O
tion O
of O
the O
LM B-TaskName
information O
into O
SNMT B-TaskName
model O
. O

In O
this O
work O
, O
we O
explicitly O
use O
plausible O
future O
information O
from O
LM B-TaskName
during O
training O
by O
transform- O
ing O
the O
monotonic O
attention O
mechanism O
. O

As O
shown O
in O
Figure O
1 O
, O
at O
each O
step O
, O
the O
LM B-TaskName
takes O
the O
prefix O
target O
( O
and O
source O
, O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
LM B-TaskName
) O
sequence O
and O
predicts O
the O
probable O
future O
information O
. O

We O
hypothesize O
that O
aiding O
the O
monotonic O
attention O
with O
this O
future O
information O
can O
improve O
MMA B-MethodName
model O
’s O
read O
/ O
write O
policy O
, O
eventually O
leading O
to O
better O
translation O
with O
less O
delay O
. O

Several O
experi- O
ments O
on O
MuST-C B-MethodName
( O
Di O
Gangi O
et O
al O
. O
, O
2019 O
) O

English- I-DatasetName

German B-DatasetName
and O
English B-DatasetName
- I-DatasetName
French I-DatasetName
speech O
- O
to O
- O
text O
translation O
tasks O
with O
our O
proposed O
approach O
show O
clear O
improvements O
of O
latency O
- O
quality O
trade O
- O
offs O
over O
the O
state O
- O
of O
- O
the O
- O
art O
MMA B-MethodName
models O
. O

2 O
Monotonic O
Attention O
with O
Future B-MethodName
Information O
Model I-MethodName
2.1 O
Monotonic O
Attention O
In O
simultaneous I-TaskName
machine I-TaskName
translation I-TaskName
( O
SNMT B-TaskName
) O
mod- O
els O
, O
the O
probability O
of O
predicting O
the O
target O
token O
yi∈ydepends O
on O
the O
partial O
source O
and O
target O
sequences O
( O
x≤j∈x O
, O
y O
< O
i∈y O
) O
. O

In O
sequence O
- O
to- O
sequence O
based O
SNMT B-MethodName
model O
, O
each O
target O
token O
yi O
is O
generated O
as O
follows O
: O

hj B-HyperparameterName
= O
E(x≤j B-HyperparameterName
) O
( O
1 O
) O
si B-HyperparameterName
= O
D(y B-HyperparameterName
< O
i O
, O
ci O
= O
A(si−1 O
, O
h≤j O
) O
) O
( O
2 O
) O
yi B-HyperparameterName
= O
Output O
( O
si O
) O
( O
3 O
) O
where O
E(.)andD(.)are O
the O
encoder O
and O
decoder O
layers O
, O
and O
ciis O
a O
context O
vector O
. O

In O
monotonic B-TaskName
attention I-TaskName
based I-TaskName
SNMT B-TaskName
, O
the O
context O
vector O
is O
com- O
puted O
as O
follows O
: O
ei O
, O
j O
= O
MonotonicEnergy O
( O
si−1 O
, O
hj)(4 O
) O
pi O
, O
j O
= O
Sigmoid O
( O
ei O
, O
j O
) O
( O
5 O
) O
zi O
, O
j∼Bernoulli O
( O
pi O
, O
j O
) O
( O

When O
generating O
a O
target O
token O
yi O
, O
the O
decoder O
chooses O
whether O
to O
read O
/ O
write O
based O
on O
Bernoulli O
selection O
probability O
pi O
, O
j. O
When O
zi O
, O
j= O
1 O
( O
write O
) O
, O
model O
sets O
ti O
= O
j O
, O
ci O
= O
hjand O
generates O
the O
target O
token O
yi O
. O

Forzi O
, O
j= B-HyperparameterName
0(read B-HyperparameterValue
) O
, O
it O
sets O
ti B-HyperparameterName
= O
j+ B-HyperparameterName
1and B-HyperparameterValue
repeats O
Eq O
. O
4 B-HyperparameterValue
to O
6 B-HyperparameterValue
. O

Here O
tirefers O
to O
the O
index O
of O
the O
encoder O
when O
decoder O
needs O
to O
produce O
the O
ith O
target O
token O
. O

Instead O
of O
hard O
alignment O
of O
ci B-HyperparameterName
= O
hj B-HyperparameterName
, O
Raffel O
et O
al O
. O

( O
2017 O
) O
compute O
an O
expected O
alignment O
in O
a O
recurrent O
manner O
and O
propose O
a O
closed O
- O
form O
parallel O
solution O
. O

Arivazhagan O
et O

al O
. O

( O
2019 O
) O
adopt O
monotonic O
attention O
into O
SNMT B-TaskName
and O
later O
, O
Ma O
et O
al O
. O

( O
2019c O
) O
extend O
it O
to O
MMA O
to O
integrate O
it O
into O
the O
Transformer O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O

2.2 O
Monotonic O
Attention O
with O
Future O
Information O

The O
monotonic O
attention O
described O
in O
Section O
2.1 O
performs O
anticipation O
based O
only O
on O
the O
currently O
available O
source O
and O
target O
information O
. O

To O
aug- O
ment O
this O
anticipation O
process O
using O
future O
informa- O
tion O
extracted O
using O
LMs B-TaskName
, O
we O
propose O
the O
following O
modifications O
to O
the O
monotonic O
attention O
. O

Future O
Representation O
Layer O
: O

At O
every O
de- O
coding O
step O
i O
, O
the O
previous O
target O
token O
yi−1is O
equipped O
with O
a O
plausible O
future O
token O
ˆyias O
shown O
in O
the O
Figure O
2 O
. O

Since O
the O
token O
ˆyicomes O
from O
an O
LM B-TaskName
possibly O
with O
a O
different O
tokenizer O
and O
vo- O
cabulary O
set O
, O
applying O
the O
model O
’s O
tokenizer O
and O
vocabulary O
might O
split O
the O
token O
ˆyifurther O
into O
mul- O
tiple O
sub O
- O
tokens O
{ O
ˆy1 O
i,ˆy2 O
i,···,ˆym O
i O
} O
. O

To O
get O
a O
single O
future O
token O
representation O
˜si∈ O
Rdfrom O
all O
the O
sub O
- O
tokens O
, O
we O
apply O
a O
sub O
- O
token O
summary O
layer O
: O

˜si= O
Γ({ˆy1 O
i,ˆy2 O
i,···,ˆym O
i O
} O
) O
( O
7 O
) O
TheΓrepresents B-MethodName
a O
general O
sequence O
representation O
layer O
such O
as O
a O
Transformer O
encoder O
layer O
or O
a O
sim- O
ple O
normalized O
sum O
of O
sub O
- O
token O
representations O
. O

We O
enrich O
˜siat B-HyperparameterName
every O
layer I-HyperparameterName
lof O
the O
decoder O
block O
by O
applying O
a O
residual O
feed O
- O
forward O
network O
. O

˜sl O
i O
= O
FFN O
( O
˜yl−1 O
i O
) O
( O
8) O
Monotonic B-MethodName
Energy I-MethodName
Layer I-MethodName
with O
Future O
Informa- O
tion O
: O
Despite O
the O
fact O
that O
we O
can O
add O
the O
plau- O
sible O
future O
information O
to O
the O
output O
layer O
( O
Ap- O
pendix O
A O
) O
or O
append O
it O
to O
the O
target O
token O
represen- O

tation O
yi−1 O
, O
the O
MMA B-TaskName
read O
/ O
write O
decisions O
happen O
in O
Eq O
. O

4 B-HyperparameterValue
. O

Therefore O
, O
we O
integrate O
˜siinto O
the O
Eq O
. O
4 O
instead O
. O

The O
integration O
is O
carried O
out O
by O
modifying O
Eq O
. O
4 O
- O
Eq O
. O

5 B-HyperparameterValue
. O

We O
compute O
the O
monotonic O
energy O
for O
future O
information O
using O
the O
enriched O
future O
token O
representation O
˜siavailable O
at O
each O
layer O
: O
˜ei O
, O
j O
= O
MonotonicEnergy O
( O
˜si O
, O
hj O
) O
( O
9 O
) O
We O
integrate O
the O
future O
monotonic O
energy O
function O
into O
Eq O
. O
5 O
as O
follows O
: O
˜pi O
, O
j O
= O
Sigmoid O
( O
ei O
, O
j+ O
˜ei O
, O
j O
) O
( O
10 O
) O
After O
computing O
˜pi O
, O
j O
, O
we O
compute O
cisimilar O
to O
MMA B-MethodName
model O
. O

This O
way O
of O
integration O
of O
future O
information O
allows O
the O
model O
to O
condition O
the O
LM B-TaskName
output O
us- O
age O
on O
the O
input O
sequence O
. O

The O
model O
can O
control O
the O
relative O
weightage O
given O
to O
the O
LM B-TaskName
output O
by O
varying O
the O
˜ei O
, O
j. O
In O
case O
of O
insufficient O
source O
in- O
formation O
in O
the O
low O
latency O
regime O
, O
we O
expect O
the O
model O
’s O
decision O
policy O
to O
rely O
more O
on O
˜ei O
, O
j. O
Inference O
: O

During O
inference O
, O
the O
start O
token O
does O
not O
contain O
any O
plausible O
information O
. O

After O
pre- O
dicting O
the O
first O
target O
token O
, O
for O
every O
subsequent O
prediction O
of O
target O
token O
yi O
, O
we O
invoke O
the O
LM B-TaskName
to O
predict O
the O
next O
plausible O
future O
token O
and O
integrate O
this O
new O
information O
into O
Eq O
. O

10 O
. O
3 O
Experiments O
and O
Results O
3.1 O
Experimental O
Settings O
Datasets O
and O
Metrics O
: O
We O
conduct O
our O
experi- O
ments O
on O
the O
MuST-C B-DatasetName
English(En)-German(De I-DatasetName
) I-DatasetName
and O
English(En)-French(Fr B-DatasetName
) I-DatasetName
speech O
- O
to O
- O
text O
( O
ST B-TaskName
) O
translation O
task O
. O

The O
speech O
sequence O
is O
repre- O
sented O
using O
80 O
- O
dimensional O
log O
- O
mel O
filter O
bank O
features O
. O

The O
target O
sequence O
is O
represented O
as O
sub- O
words O
using O
a O
SentencePiece B-MethodName
( O
Kudo O
and O
Richard- O
son O
, O
2018 O
) O
model O
with O
a O
unigram O
vocabulary O
of O
size O
10,000 B-HyperparameterValue
. O

We O
evaluate O
the O
performance O
of O
the O
models O
on O
both O
the O
latency B-HyperparameterName
and O
quality B-HyperparameterName
aspects O
. O

We O
use O
Average B-MetricName
Lagging I-MetricName
( I-MetricName
AL I-MetricName
) I-MetricName
as O
our O
latency O
metric O
and O
case O
- O
sensitive O
detokenized O
SacreBLEU O
( O
Post O
, O
2018 O
) O
to O
measure O
the O
translation O
quality O
, O
similar O
to O
( O
Ma O
et O
al O
. O
, O
2020 O
) O
. O

The O
best O
models O
are O
chosen O
based O
on O
the O
dev O
set O
results O
and O
reported O
results O
are O
from O
the O
MuST-C B-DatasetName
test O
( O
tst-COMMON O
) O
sets O
. O

Language O
Models O
We O
use O
two O
language O
mod- O
els O
to O
train O
our O
proposed O
modified O
MMA B-MethodName
model O
. O

Firstly O
, O
we O
use O
the O
pretrained O
XLM B-MethodName
- I-MethodName
RoBERTa I-MethodName
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
model O
from O
Huggingface O
Transformers1model I-MethodName
repository O
. O

Since O
the O
LM B-TaskName
out- O
put O
can O
be O
very O
open O
- O
ended O
and O
might O
not O
directly O
suit O
/ O
cater O
to O
our O
task O
and O
dataset O
, O
we O
finetune O
the O
head O
of O
the O
model O
using O
the O
MuST-C B-DatasetName
target O
text O
data O
for O
each O
task O
. O

We O
also O
train O
a O
smaller O
language O
model O
( O
SLM B-TaskName
) O
, O
which O
contains O
6 O
Transformer O
decoder O
layers O
, O
512 B-HyperparameterValue
hidden B-HyperparameterName
- O
states I-HyperparameterName
and O
24 B-HyperparameterValue
M I-HyperparameterValue
parameters B-HyperparameterName
. O

We O
use O
the O
MuST-C B-DatasetName
data O
along O
with O
additional O
data O
augmen- O
tation O
to O
reduce O
overfitting O
. O

The O
SLM B-TaskName
helps O
to O
remove O
the O
issues O
related O
to O
vocabulary O
mismatch O
as O
discussed O
in O
the O
Section O
2.2 O
. O

Implementation O
Details O
: O
Our O
base O
model O
is O
adopted O
from O
Ma O
et O
al O
. O

( O
2020 O
) O
. O

We O
use O
a O
pre- O
decision O
ratio I-HyperparameterName
of O
7 B-HyperparameterValue
, O
which O
means O
that O
the O
simultane- O
ousread O
/ O
write O
decisions O
are O
made O
after O
every O
seven O
encoder O
states O
. O

We O
use O
λ B-HyperparameterName
or O
λlatency B-MetricName
to O
refer O
to O
the O
hyperparameter O
corresponding O
to O
the O
weighted O
average O
( O
λavg O
) O
in O
MMA B-MethodName
. O

The O
values O
of I-HyperparameterName
this O
hyperpa- O
rameter I-MetricName
λ B-HyperparameterName
are O
chosen O
from O
the O
set O
{ O
0.01,0.05,0.1 B-HyperparameterValue
} O
. O

TheΓlayer O
in O
Eq O
. O

7 O
computes O
the O
normalized O
sum O
of O
the O
sub O
- O
token O
representations O
. O

For O
SLM B-TaskName
, O
it O
sim- O
ply O
finds O
the O
embedding O
since O
it O
shares O
the O
same O
vocabulary O
set O
. O

All O
the O
models O
are O
trained O
on O
a O
NVIDIA B-DatasetName
v100 I-DatasetName
GPU O
with O
update_freq B-HyperparameterName
set O
to O
8 B-HyperparameterValue
. O

Simultaneous O
Translation O
Models O
: O
Even O
though O
future O
information O
can O
be O
integrated O
explicitly O
into O
the O
fixed O
policy O
approaches O
such O
as O
Wait O
- I-MetricName
K I-MetricName
( O
Ma O
et O
al O
. O
, O
2019b O
) O
, O
we O
choose O
monotonic O
attention O
as O
our O
baseline O
due O
to O
its O
superior O
performance O
( O
Arivazhagan O
et O
al O
. O
, O
2019 O
; O
Ma O
et O
al O
. O
, O
2019c O
) O
. O

We O
train O
a O
baseline O
based O
on O
Ma O
et O
al O
. O

( O
2020 O
) O
work O
, O
called O
as O
MMA B-MethodName
model O
. O

The O
MMA B-MethodName
model O
encoder O
and O
decoder O
embedding O
dimensions O
are O
set O
to O
392 B-HyperparameterValue
, O
whereas O
our O
proposed O
model O
’s O
encoder O
and O
decoder O
embeddings O
are O
set O
to O
256 B-HyperparameterValue
to O
have O
similar O
parameters O
( O
≈39 O
M I-HyperparameterValue
) O
for O
a O
fair O
comparison O
. O

We O
train O
two O
models O
using O
the O
modified O
MMA B-MethodName
based I-MethodName
on I-MethodName
two I-MethodName
LMs I-MethodName
( O
XLM B-MethodName
, I-MethodName
SLM I-MethodName
) O
, O
referred O
as O
MMA B-MethodName
- I-MethodName
XLM I-MethodName
and O
MMA B-MethodName
- I-MethodName
SLM I-MethodName
. O

3.2 O
Results O
We O
first O
analyze O
how O
the O
LM B-TaskName
predictions O
are O
being O
utilized O
by O
the O
our O
model O
. O

In O
order O
to O
measure O
the O
relative O
weight O
given O
to O
model O
’s O
internal O
states O
ver- O

sus O
the O
predictions O
from O
the O
LM B-TaskName
, O
we O
compare O
the O
norm B-HyperparameterName
of O
the O
monotonic I-HyperparameterName
energies I-HyperparameterName
corresponding O
to O
the O
LM B-TaskName
predictions O
epred(Eq O
. O

9 O
) O
and O
the O
previous O
output O
tokens O
eoutput O
( O
Eq O
. O
4 O
) O
. O

Let O
us O
define O
LM B-TaskName
prediction O
weight O
as O
follows O
: O
LMpw=/parenleftbigg∥epred∥ B-TaskName
∥eoutput∥/parenrightbigg O
( O
11 O
) O
In O
Figure O
3 O
, O
we O
plot O
the O
variation O
of O
LMpw B-MetricName
( O
averaged O
) O
vs. O
λ B-HyperparameterName
. O

We O
use O
two O
additional O
values O
of O
λ B-HyperparameterName
∈ O
{ O
0.005 B-HyperparameterValue
0.001 I-HyperparameterValue
}to O
obtain O
this O
plot O
. O

We O
can O
observe O
that O
as O
the O
latency B-HyperparameterName
requirements O
become O
more O
and O
more O
strict O
, O
the O
model O
starts O
to O
give O
more O
weightage O
to O
the O
predictions O
coming O
from O
the O
LM B-TaskName
. O

This O
shows O
that O
the O
model O
learns O
to O
utilize O
the O
in- O
formation O
coming O
from O
LM B-TaskName
predictions O
based O
on O
latency B-HyperparameterName
requirements O
. O

Next O
, O
we O
discuss O
the O
performance O
improvements O
obtained O
from O
our O
proposed O
approach O
. O

By O
vary- O
ing O
the O
λ B-HyperparameterName
, O
we O
train O
separate O
models O
for O
different O
latency B-HyperparameterName
regimes O
. O

Moreover O
, O
the O
quality O
and O
latency O
for O
a O
particular O
model O
can O
also O
be O
varied O
by O
control- O
ling O
the O
speech O
segment O
size O
during O
the O
inference O
. O

Speech O
segment O
size O
or O
step B-HyperparameterName
size I-HyperparameterName
refers O
to O
the O
du- B-HyperparameterName
ration I-HyperparameterName
of O
speech O
( O
in O
ms O
) O
processed O
corresponding O
to O
each O
read O
decision O
. O

We O
vary O
these O
hyperparame- O
ters O
for O
all O
the O
three O
models O
, O
namely O
MMA B-MethodName
, O
MMA- B-MethodName
XLM I-MethodName
and O
MMA B-MethodName
- I-MethodName
SLM I-MethodName
. O

The O
BLEU B-MetricName
- I-MetricName
AL I-MetricName
curves O
for O
all O
the O
models O
have O
been O
provided O
in O
Figure O
4 O
and O
BLEU B-MetricName
- I-MetricName
AL I-MetricName
num- I-MetricName
bers O
for O
all O
models O
are O
included O
in O
Appendix O
F O
for O
reference O
. O

We O
vary O
the O
step B-HyperparameterName
sizes I-HyperparameterName
in O
intervals O
of O
80ms B-HyperparameterValue
from O
120ms B-HyperparameterValue
to O
520ms B-HyperparameterValue
in O
order O
to O
get O
performances O
corresponding O
to O
different O
latency B-HyperparameterName
regimes O
. O

We O
can O
observe O
that O
the O
LM B-TaskName
- O
based O
mod- O
els O
using O
both O
XLM B-MethodName
and O
SLM B-MethodName
provide O
a O
significant O
performance O
improvement O
over O
the O
baseline O
MMA B-MethodName
model O
. O

We O
observe O
improvements O
in O
the O
range O
of O
1 B-HyperparameterValue
- I-HyperparameterValue
2 B-HyperparameterValue
BLEU B-MetricName
scores O
consistently O
across O
all O
the O
latency B-HyperparameterName
regimes O
( O
λ B-HyperparameterName
= O
0.1 B-HyperparameterValue
0.05 B-HyperparameterValue
0.01 B-HyperparameterValue
) O
. O

The O
MMA B-MethodName
using O
SLM B-TaskName
language O
model O
performs O
slightly O
better O
than O
MMA B-MethodName
using O
XLM B-MethodName
language O
model O
. O

This O
is O
due O
to O
SLM B-TaskName
’s O
higher O
accuracy O
on O
the O
next O
token O
predic- O
tion O
task O
as O
compared O
to O
XLM B-MethodName
, O
30.15% B-MetricValue
vs. O
21.5% B-MetricValue
for O
German O
& O
31.65% B-MetricValue
vs. O
18.45% B-MetricValue
for O
French O
. O

The O
high O
accuracy O
of O
SLM B-TaskName
is O
attributed O
to O
its O
training O
on O
in O
- O
domain O
data O
. O

4 O
Conclusion O
In O
this O
work O
, O
we O
provide O
a O
generic O
framework O
to O
integrate O
the O
linguistic O
and O
extra O
- O
linguistic O
infor- O
mation O
into O
simultaneous O
models O
. O

We O
rely O
on O
lan- O
guage O
models O
to O
extract O
this O
plausible O
future O
in- O
formation O
and O
propose O
a O
new O
monotonic O
attention O
mechanism O
to O
infuse O
this O
information O
. O

Several O
ex- O
periments O
on O
speech B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
text I-TaskName
translation I-TaskName
tasks O
show O
the O
effectiveness O
of O
proposed O
approach O
on O
obtain- O
ing O
superior O
quality O
- O
latency O
trade O
- O
offs O
, O
compared O
to O
the O
state O
- O
of O
- O
the O
- O
art O
monotonic O
multihead O
attention O
. O

References O
Naveen B-MetricName
Arivazhagan O
, O
Colin O
Cherry O
, O
Wolfgang O
Macherey O
, O
Chung O
- O
Cheng O
Chiu O
, O
Semih B-MetricName
Yavuz O
, O
Ruom- O
ing O
Pang O
, O
Wei O
Li O
, O
and O
Colin O
Raffel O
. O

2019 O
. O

Monotonic O
infinite O
lookback O
attention O
for O
simultaneous O
machine I-TaskName
translation I-TaskName
. O

In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
, O
pages O
1313–1323 O
, O
Florence O
, O
Italy O
. O
Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
. O

Alexis B-MetricName
Conneau O
, O
Kartikay B-MetricName
Khandelwal O
, O
Naman B-MetricName
Goyal O
, O
Vishrav B-MetricName
Chaudhary O
, O
Guillaume B-MetricName
Wenzek O
, O
Francisco O
Guzmán O
, O
Edouard B-MetricName
Grave O
, O
Myle B-MetricName
Ott O
, O
Luke B-MetricName
Zettle- O
moyer O
, O
and O
Veselin B-MetricName
Stoyanov O
. O

2019 O
. O

Unsupervised O
cross B-TaskName
- I-TaskName
lingual I-TaskName
representation I-TaskName
learning O
at O
scale O
. O

arXiv O
preprint O
arXiv:1911.02116 O
. O

Fahim B-MetricName
Dalvi O
, O
Nadir B-MetricName
Durrani O
, O
Hassan B-MetricName
Sajjad O
, O
and O
Stephan O
V O
ogel O
. O

2018 O
. O

Incremental O
decoding O
and O
training O
methods O
for O
simultaneous B-TaskName
translation I-TaskName
in O
neural B-TaskName
machine I-TaskName
translation I-TaskName
. O

In O
Proceedings O
of O
the O
2018 O
Con- O
ference O
of O
the O
North B-TaskName
American I-DatasetName
Chapter O
of O
the O
Asso- O
ciation O
for O
Computational O
Linguistics I-MethodName
: O
Human O
Lan- I-MethodName
guage I-MethodName
Technologies I-MethodName
, O
Volume O
2 O
( O
Short O
Papers O
) O
, O
pages O
493–499 O
, O
New B-TaskName
Orleans I-DatasetName
, O
Louisiana O
. O
Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
. O

Jacob B-MetricName
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton B-MetricName
Lee O
, O
and O
Kristina B-MetricName
Toutanova O
. O
2019 O
. O

BERT B-MethodName
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
under- O
standing O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North B-TaskName
American I-DatasetName
Chapter O
of O
the O
Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
: O
Human O
Language O
Tech- O
nologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
, O
Minneapolis O
, O
Minnesota O
. O

Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
. O

Mattia B-MetricName
A. O
Di O
Gangi O
, O
Roldano B-MetricName
Cattoni O
, O
Luisa B-MetricName
Bentivogli O
, O
Matteo O
Negri O
, O
and O
Marco O
Turchi O
. O

2019 O
. O

MuST-C B-MethodName
: O
a O
Multilingual B-MethodName
Speech I-MethodName
Translation I-MethodName
Corpus I-MethodName
. O

In O
Proceed- O
ings O
of O
the O
2019 O
Conference O
of O
the O
North B-TaskName
American I-DatasetName
Chapter O
of O
the O
Association B-MethodName
for I-MethodName
Computational I-MethodName
Lin- I-MethodName
guistics I-MethodName
: O
Human B-MethodName
Language I-MethodName
Technologies I-MethodName
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
2012–2017 O
, O
Min- O
neapolis O
, O
Minnesota O
. O

Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
. O

Caglar B-MetricName
Gulcehre O
, O
Orhan B-MetricName
Firat O
, O
Kelvin B-MetricName
Xu O
, O
Kyunghyun B-MetricName
Cho O
, O
Loic B-MetricName
Barrault O
, O
Huei O
- O
Chi O
Lin O
, O
Fethi B-MetricName
Bougares O
, O
Holger B-MetricName
Schwenk O
, O
and O
Yoshua B-MetricName
Bengio O
. O
2015 O
. O

On O
using O
monolingual O
corpora O
in O
neural B-TaskName
machine I-TaskName
trans- I-TaskName
lation I-TaskName
. O

Caglar B-MetricName
Gulcehre O
, O
Orhan B-MetricName
Firat O
, O
Kelvin B-MetricName
Xu O
, O
Kyunghyun B-MetricName
Cho O
, O
and O
Yoshua B-MetricName
Bengio O
. O
2017 O
. O

On O
integrating O
a O
lan- O
guage O
model O
into O
neural B-TaskName
machine I-TaskName
translation I-TaskName
. O

Com- B-TaskName
puter I-TaskName
Speech I-TaskName
and O
Language I-TaskName
, O
45:137–148 O
. O

Urvashi B-MetricName
Khandelwal O
, O
Angela O
Fan O
, O
Dan B-MetricName
Jurafsky O
, O
Luke O
Zettlemoyer O
, O
and O
Mike O
Lewis O
. O

2020 O
. O

Nearest B-TaskName
neighbor I-TaskName
machine I-TaskName
translation I-TaskName
. O

arXiv O
preprint O
arXiv:2010.00710 O
. O

Sosuke B-MetricName
Kobayashi O
. O

2018 O
. O

Contextual O
augmentation O
: O
Data O
augmentation O
by O
words O
with O
paradigmatic O
re- O
lations O
. O

In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North B-TaskName
American I-DatasetName
Chapter O
of O
the O
Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
: O
Human O
Language O
Tech- I-MethodName
nologies I-MethodName
, O
Volume O
2 O
( O
Short O
Papers O
) O
, O
pages O
452–457 O
, O
New B-TaskName
Orleans I-DatasetName
, O
Louisiana O
. O

Association B-MethodName
for I-MethodName
Computa- I-MethodName
tional I-MethodName
Linguistics I-MethodName
. O

Taku B-MetricName
Kudo O
and O
John B-MetricName
Richardson O
. O

2018 O
. O

Sentencepiece O
: O

A O
simple O
and O
language O
independent O
subword O
tok- O
enizer O
and O
detokenizer O
for O
neural B-TaskName
text I-TaskName
processing I-TaskName
. O

arXiv O
preprint O
arXiv:1808.06226 O
. O

Mingbo B-MetricName
Ma O
, O
Liang O
Huang O
, O
Hao O
Xiong O
, O
Renjie O
Zheng O
, O
Kaibo B-MetricName
Liu O
, O
Baigong O
Zheng O
, O
Chuanqiang O
Zhang O
, O
Zhongjun O
He O
, O
Hairong B-MetricName
Liu O
, O
Xing O
Li O
, O
Hua O
Wu O
, O
and O
Haifeng O
Wang O
. O
2019a O
. O

Stacl B-MetricName
: O

Simultaneous O
trans- O
lation O
with O
implicit O
anticipation O
and O
controllable O
la- O
tency O
using O
prefix O
- O
to O
- O
prefix O
framework O
. O

Mingbo B-MetricName
Ma O
, O
Liang O
Huang O
, O
Hao O
Xiong O
, O
Renjie O
Zheng O
, O
Kaibo B-MetricName
Liu O
, O
Baigong B-MetricName
Zheng O
, O
Chuanqiang B-MetricName
Zhang O
, O
Zhongjun B-MetricName
He O
, O
Hairong B-MetricName
Liu O
, O
Xing O
Li O
, O
Hua B-MetricName
Wu O
, O
and O
Haifeng B-MetricName
Wang O
. O

2019b O
. O

STACL O
: O

Simultaneous O
trans- O
lation O
with O
implicit O
anticipation O
and O
controllable O
la- O
tency O
using O
prefix O
- O
to O
- O
prefix O
framework O
. O

In O
Proceed- O
ings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
, O
pages O
3025–3036 O
, O
Flo- O
rence O
, O
Italy O
. O

Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguis- I-MethodName
tics I-MethodName
. O

Xutai O
Ma O
, O
Juan O
Pino O
, O
James O
Cross O
, O
Liezl O
Puzon O
, O
and O
Jiatao O
Gu O
. O
2019c O
. O

Monotonic O
multihead O
attention O
. O

Xutai B-MetricName
Ma O
, O
Juan O
Pino O
, O
and O
Philipp O
Koehn O
. O
2020 O
. O

Simulmt O
to O
simulst O
: O
Adapting O
simultaneous O
text I-TaskName
translation I-TaskName
to O
end O
- O
to O
- O
end O
simultaneous O
speech O
trans- O
lation O
. O

arXiv O
preprint O
arXiv:2011.02048 O
. O

Matt B-MetricName
Post O
. O
2018 O
. O

A O
call O
for O
clarity O
in O
reporting O
bleu B-MetricName
scores O
. O

arXiv O
preprint O
arXiv:1804.08771 O
. O

Colin O
Raffel O
, O
Minh O
- O
Thang O
Luong O
, O
Peter O
J. O
Liu O
, O
Ron O
J. O
Weiss O
, O
and O
Douglas O
Eck O
. O
2017 O
. O

Online O
and O
linear- O
time O
attention O
by O
enforcing O
monotonic O
alignments O
. O

InProceedings O
of O
the O
34th O
International I-DatasetName
Conference I-DatasetName
on I-DatasetName
Machine I-MethodName
Learning I-TaskName
, O
volume O
70 O
of O
Proceedings O
of I-MethodName
Machine I-MethodName
Learning I-MethodName
Research I-MethodName
, O
pages O
2837–2846 O
. O
PMLR B-MethodName
. O

Felix O
Stahlberg O
, O
James B-MetricName
Cross O
, O
and O
Veselin B-MetricName
Stoyanov O
. O

2018 O
. O

Simple O
fusion O
: O
Return O
of O
the O
language O
model O
. O

InProceedings O
of O
the O
Third O
Conference O
on O
Machine B-TaskName
Translation I-TaskName
: O
Research O
Papers O
, O
pages O
204–211 O
, O
Brus- O
sels O
, O
Belgium O
. O

Association B-MethodName
for I-MethodName
Computational I-MethodName
Lin- I-MethodName
guistics.42 I-MethodName

Sonia B-MetricName
Vandepitte B-DatasetName
. O

2001 O
. O

Anticipation O
in O
conference B-TaskName
interpreting O
: O
a O
cognitive O
process O
. O

Alicante B-MethodName
Journal I-MethodName
of I-MethodName
English I-MethodName
Studies I-MethodName
/ O
Revista B-MethodName
Alicantina I-MethodName
de I-MethodName
Estudios I-MethodName
Ingleses I-DatasetName
, O
0(14):323–335 O
. O

Ashish B-MetricName
Vaswani O
, O
Noam B-MetricName
Shazeer O
, O
Niki B-MetricName
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion B-MetricName
Jones O
, O
Aidan O
N O
Gomez O
, O
Ł O
ukasz O
Kaiser O
, O
and O
Illia B-MetricName
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
Advances O
in O
Neural B-TaskName
Information I-TaskName
Pro- I-TaskName

cessing B-MethodName
Systems O
30 O
, O
pages O
5998–6008 O
. O

Curran O
Asso- O
ciates O
, O

Inc. B-MethodName

Xueqing O
Wu O
, O
Yingce O
Xia O
, O
Lijun B-MetricName
Wu O
, O
Shufang O
Xie O
, O
Weiqing O
Liu O
, O
Jiang O
Bian O
, O
Tao O
Qin O
, O
and O
Tie O
- O
Yan O
Liu O
. O
2020 O
. O

Learn O
to O
use O
future O
information O
in O
simultane- O
ous O
translation O
. O

A O
LM B-TaskName
at O
MMA O
Output O
Layer O
We O
explored O
a O
naive O
approach O
of O
integrating O
LM B-TaskName
information O
into O
the O
MMA B-MethodName
. O

In O
this O
approach O
, O
we O
in- O
tegrate O
the O
future O
information O
obtained O
from O
the O
LM B-TaskName
directly O
into O
the O
output O
layer O
of O
the O
MMA B-MethodName
model O
. O

We O
refer O
to O
this O
experiment O
as O
‘ O
LM B-TaskName
Rescor- I-MethodName
ing(LMR I-MethodName
) O
’ O
, O
and O
the O
corresponding O
model O
is O
called O
MMA B-MethodName
- I-MethodName
LMR I-MethodName
. O

As O
observed O
in O
Figure O
5 O
, O
MMA B-MethodName
- I-MethodName
LMR I-MethodName
has O
infe- O
rior O
performance O
compared O
to O
the O
MMA B-MethodName
model O
. O

Since O
the O
LM B-TaskName
information O
integration O
is O
only O
done O
at O
the O
output O
layer O
of O
the O
model O
, O
the O
MMA B-MethodName
model O
can O
not O
easily O
discard O
the O
incorrect O
information O
from O
LM B-TaskName
. O

This O
motivates O
us O
to O
tightly O
integrate O
the O
LM B-TaskName
information O
into O
the O
simultaneous O
model O
. O

B O
Language O
Models O
As O
mentioned O
earlier O
, O
we O
train O
two O
different O
lan- O
guage O
models O
( O
LMs B-TaskName
) O
and O
use O
them O
to O
improve O
the O
anticipation O
in O
monotonic O
attention O
based O
Simulta- O
neous O
models O
. O

B.1 O
XLM B-MethodName
- I-MethodName
RoBERTa I-MethodName
- I-MethodName
R I-MethodName
) I-MethodName
XLM I-MethodName
- I-MethodName
R I-MethodName
Large I-MethodName
model2was O
trained O
on O
the O
100 O
lan- I-HyperparameterValue
guages O
Common O
Crawl O
corpora O
total O
size O
of O
2.5 B-HyperparameterValue
TB O
with O
550 B-HyperparameterValue
M I-HyperparameterValue
parameters O
from O
24 B-HyperparameterValue
layers O
, O
1024 B-HyperparameterValue
hid- O
den O
states O
, O
4096 O
feed O
- O
forward O
hidden O
- O
states O
, O
and O
16 B-HyperparameterValue
heads O
. O

Total O
number I-HyperparameterName
of I-HyperparameterName
parameters I-HyperparameterName
is O
558M. B-HyperparameterValue
We O
finetune O
the O
head O
of O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
LM I-MethodName
model O
using O
the O
Masked B-MethodName
Language I-MethodName
Modeling I-MethodName
objective O
which O
accounts O
for O
0.23 B-MetricValue
% I-MetricValue
of O
the O
total O
model O
parameters O
, O
i.e. O
, O
1.3 B-HyperparameterValue
M I-HyperparameterValue
parameters B-HyperparameterName
. O

B.2 O
Smaller O
Language O
Model O
Since O
the O
LM B-TaskName
predictions O
are O
computed O
serially O
during O
inference O
, O
the O
time O
taken O
to O
compute O
the O
token O
serves O
as O
a O
bottleneck O
to O
the O
latency O
re- O
quirements O
. O

To O
reduce O
the O
LM B-TaskName
computation O
time O
, O
we O
train O
a O
smaller O
Language O
Model I-MethodName
( O
SLM B-MethodName
) O
from O
scratch O
using O
the O
Causal O
Language O
Modeling O
ob- O
jective O
. O

SLM B-TaskName
is O
composed O
of O
6 O
Transformer O
decoder O
blocks O
, O
512 B-HyperparameterValue
hidden O
- O
states O
, O
2048 B-HyperparameterValue
feed O
- O
forward O
hidden O
- O
states O
& O
8 B-HyperparameterValue
attention O
heads O
. O

It O
alleviates O
the O
need O
for O
the O
sub O
- O
token O
summary O
layer O
since O
it O
shares O
the O
vocabulary O
and O
tokenization O
with O
the O
MMA B-MethodName
models O
. O

The O
train O
examples O
are O
at O
the O
sen- O
tence O
level O
, O
rather O
than O
forming O
a O
block O
out O
of O
multi- O
ple O
sentences(which O
is O
the O
usual O
case O
for O
Language O
Models I-MethodName
) O
. O

Since O
the O
target O
texts O
contain O
lesser O
than O
250k B-HyperparameterValue
examples O
, O
we O
use O
additional O
data O
augmentation O
techniques O
to O
upsample O
the O
target O
data O
. O

We O
also O
use O
additional O
data O
to O
avoid O
overfitting O
on O
the O
MuST-C B-DatasetName
target O
text O
. O

Details O
have O
been O
provided O
in O
B.2.1 B-DatasetName
. O

B.2.1 O
Data O
Augmentation O
Up O
- O
Sampling O
: O
To O
boost O
the O
LM B-TaskName
performance O
and O
mitigate O
overfitting O
, O
we O
use O
contextual O
data O
augmentation O
( O
Kobayashi O
, O
2018 O
) O
to O
upsample O
the O
MuST-C B-DatasetName
target O
text O
data O
by O
substituting O
and O
insert- O
ing O
words O
based O
on O
LM B-TaskName
predictions O
. O

We O
use O
the O
NLPAUG3package B-MethodName
to O
get O
similar O
words O
based O
on O
contextual O
embeddings O
. O

From O
the O
Hugging O
Face O
Repository O
, O
we O
use O
two O
different O
pretrained O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
models O
for O
German O
bert O
- O
base- O
german O
- O
dbmdz O
- O
cased O
& O
bert O
- O
base O
- O
german O
- O
dbmdz- O
uncased O
andbert O
- O
base O
- O
fr O
- O
cased O
for O
French O
. O

We O
upsample O
German B-DatasetName
to O
1.13 B-HyperparameterValue
M I-HyperparameterValue
examples O
and O
French B-DatasetName
to O
1.38 B-HyperparameterValue
M I-HyperparameterValue
examples O
. O

Additional O
Data O
: O
We O
also O
use O
additional O
data O
to O
avoid O
overfitting O
. O

For O
German O
we O
use O
the O
Newscrawl B-DatasetName
( O
WMT19 B-DatasetName
) O
data O
which O
includes O
58 O
M O
examples O
. O

For O
French O
, O
we O
use O
Common B-MetricName
Crawl I-MetricName
and O
Europarl B-DatasetName
to O
augment O
4 O
M I-HyperparameterValue
extra O
training O
examples O
. O

We O
observe O
that O
both O
upsampling O
and O
data O
aug- O
mentation O
help O
us O
to O
reduce O
the O
overfitting O
on O
the O
MuST-C B-DatasetName
dev O
set O
. O

B.3 O
Token O
Prediction O
For O
each O
output O
token O
, O
the O
LM B-TaskName
prediction O
is O
ob- O
tained O
by O
feeding O
the O
prefix O
upto O
that O
token O
to O
the O
LM B-TaskName
model O
. O

These O
predictions O
are O
pre O
- O
computed O
for O
training O
and O
validation O
sets O
. O

This O
ensures O
par- O
allelization O
and O
avoids O
the O
overhead O
to O
run O
the O
LM B-TaskName
simultaneously O
during O
the O
training O
process O
. O

During O
inference O
, O
the O
LM B-TaskName
model O
is O
called O
every O
time O
a O
new O
output O
token O
is O
written O
. O

C B-HyperparameterName
Dataset O

The O
MuST-C B-DatasetName
dataset O
comprises O
of O
English B-DatasetName
TED I-DatasetName
talks O
, O
the O
translations O
and O
transcriptions O
have O
been O
aligned O
with O
the O
speech O
at O
sentence O
level O
. O

Dataset O
statistics O
have O
been O
provided O
in O
the O
Table O
1 O
. O

D O
Effect O
of O
LM B-TaskName
Size O
on O
Latency O
- O
Quality O
We O
train O
several O
SLM B-TaskName
models O
with O
varying O
sizes O
in O
our O
experiments O
and O
choose O
the O
best O
model O
based O
on O
the O
top-1 O
accuracy O
. O

As O
we O
increase O
the O
number O
of I-HyperparameterName
layers I-HyperparameterName
in O
the O
LM B-TaskName
model O
from O
2 B-HyperparameterValue
to O
4 B-HyperparameterValue
to O
6 B-HyperparameterValue
layers O
, O
the O
SLM B-TaskName
and O
the O
proposed O
MMA B-TaskName
with O
future O
infor- O
mation O
models O
have O
shown O
performance O
improve- O
ments O
. O

However O
, O
increasing O
the O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
greater O
than O
6 B-HyperparameterValue
does O
not O
yield O
any O
performance O
im- O
provements O
. O

We O
also O
notice O
this O
degradation O
of O
performance O
with O
the O
XLM B-MethodName
model O
while O
varying O
the O
number O
of I-HyperparameterName
hidden I-HyperparameterName
layers I-HyperparameterName
in O
the O
LM B-TaskName
head O
. O

E B-TaskName
Training I-DatasetName
Details O

We O
follow O
the O
training O
process O
similar O
to O
Ma O
et O
al O
. O

( O
2020 O
) O
training B-HyperparameterName
process O
. O

We O
train O
an O
English B-MethodName
ASR I-MethodName
model O
using O
the O
source O
speech O
data O
. O

Next O
, O
we O
train O
a O
simultaneous O
model O
without O
the O
latency B-HyperparameterName
loss O
( O
setting O
λlatency B-MetricName
= O
0 B-HyperparameterValue
) O
after O
initializing O
the O
encoder O
from O
the O
English B-MethodName
ASR I-MethodName
model O
. O

After O
this O
step O
, O
we O
finetune O
the O
simultaneous O
model O
for O
different O
λs B-HyperparameterName
. O

This O
training O
process O
is O
repeated O
for O
all O
the O
reported O
models O
and O
for O
each O
task O
. O

The O
details O
regarding O
the O
hyperparameters O
for O
the O
model O
have O
been O
provided O
in O
Table O
2 O
. O

F B-DatasetName
BLEU I-DatasetName
- I-DatasetName
AL I-DatasetName
Numbers O
As O
mentioned O
in O
the O
results O
section O
of O
the O
main O
pa- O
per O
, O
we O
vary O
the O
latency B-HyperparameterName
weight I-HyperparameterName
hyperparameter O
( O
λ B-HyperparameterName
) O
to O
train O
different O
models O
to O
obtain O
different O
latency O
regimes O
. O

We O
also O
vary O
the O
step B-HyperparameterName
- I-HyperparameterName
size I-HyperparameterName
/ O
speech O
seg- I-HyperparameterName
ment I-HyperparameterName
size I-HyperparameterName
during O
inference O
. O

In O
total O
, O
we O
obtain O
18 B-HyperparameterValue
different O
data O
points O
corresponding O
to O
each O
model O
. O

In O
Table O
3 O
, O
we O
compare O
the O
results O
obtained O
using O
MMA B-MethodName
, O
MMA B-MethodName
- I-MethodName
XLM I-MethodName
and O
MMA B-MethodName
- I-MethodName
SLM I-MethodName
under O
similar O
hyperparameter O
settings O
. O

It O
will O
help O
the O
reader O
to O
quantify O
the O
benefits O
obtained O
from O
our O
proposed O
approach O
. O

BERT B-MethodName
: O
Pre O
- O
training O
of O
Deep O
Bidirectional O
Transformers O
for O
Language O
Understanding I-TaskName
Jacob O
Devlin O
Ming O
- O
Wei O
Chang O
Kenton O
Lee O
Kristina O
Toutanova O
Google O
AI O
Language O
fjacobdevlin O
, O
mingweichang O
, O
kentonl O
, O
kristout O
g@google.com O

Abstract O
We O
introduce O
a O
new O
language O
representa- O
tion O
model O
called O
BERT B-MethodName
, O
which O
stands O
for O
Bidirectional B-MethodName
Encoder I-MethodName
Representations O
from O
Transformers B-MethodName
. O

Unlike O
recent O
language O
repre- O
sentation O
models O
( O
Peters O
et O
al O
. O
, O
2018a O
; O
Rad- O
ford O
et O
al O
. O
, O
2018 O
) O
, O
BERT B-MethodName
is O
designed O
to O
pre- O
train O
deep O
bidirectional O
representations O
from O
unlabeled O
text O
by O
jointly O
conditioning O
on O
both O
left O
and O
right O
context O
in O
all O
layers O
. O

As O
a O
re- O
sult O
, O
the O
pre O
- O
trained O
BERT B-MethodName
model O
can O
be O
ﬁne- O
tuned O
with O
just O
one O
additional O
output O
layer O
to O
create O
state O
- O
of O
- O
the O
- O
art O
models O
for O
a O
wide O
range O
of O
tasks O
, O
such O
as O
question O
answering O
and O
language O
inference O
, O
without O
substantial O
task- O
speciﬁc O
architecture O
modiﬁcations O
. O

BERT B-MethodName
is O
conceptually O
simple O
and O
empirically O
powerful O
. O

It O
obtains O
new O
state O
- O
of O
- O
the O
- O
art O
re- O
sults O
on O
eleven O
natural O
language O
processing O
tasks O
, O
including O
pushing O
the O
GLUE B-DatasetName
score O
to O
80.5% B-MetricValue
( O
7.7% B-MetricValue
point O
absolute O
improvement O
) O
, O
MultiNLI B-MethodName
accuracy O
to O
86.7% B-MetricValue
( O
4.6% B-MetricValue
absolute O
improvement O
) O
, O
SQuAD B-MethodName
v1.1 O
question O
answering O
Test O
F1 O
to O
93.2 B-MetricValue
( O
1.5 B-MetricValue
point O
absolute O
im- O
provement O
) O
and O
SQuAD B-MethodName
v2.0 O
Test O
F1 O
to O
83.1 B-MetricValue
( O
5.1 B-MetricValue
point O
absolute O
improvement O
) O
. O

1 O
Introduction O
Language O
model O
pre O
- O
training O
has O
been O
shown O
to O
be O
effective O
for O
improving O
many O
natural O
language O
processing O
tasks O
( O
Dai O
and O
Le O
, O
2015 O
; O
Peters O
et O
al O
. O
, O
2018a O
; O
Radford O
et O
al O
. O
, O
2018 O
; O
Howard O
and O
Ruder O
, O
2018 O
) O
. O

These O
include O
sentence O
- O
level O
tasks O
such O
as O
natural B-TaskName
language I-TaskName
inference I-TaskName
( O
Bowman O
et O
al O
. O
, O
2015 O
; O
Williams O
et O
al O
. O
, O
2018 O
) O
and O
paraphrasing O
( O
Dolan O
and O
Brockett O
, O
2005 O
) O
, O
which O
aim O
to O
predict O
the O
re- O
lationships O
between O
sentences O
by O
analyzing O
them O
holistically O
, O
as O
well O
as O
token O
- O
level O
tasks O
such O
as O
named B-TaskName
entity I-TaskName
recognition I-TaskName
and O
question B-TaskName
answering I-TaskName
, O
where O
models O
are O
required O
to O
produce O
ﬁne O
- O
grained O
output O
at O
the O
token O
level O
( O
Tjong O
Kim O
Sang O
and O
De O
Meulder O
, O
2003 O
; O

Rajpurkar O
et O
al O
. O
, O
2016). O

There O
are O
two O
existing O
strategies O
for O
apply- O
ing O
pre O
- O
trained O
language O
representations O
to O
down- O
stream O
tasks O
: O
feature O
- O
based O
andﬁne O
- O
tuning O
. O

The O
feature O
- O
based O
approach O
, O
such O
as O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018a O
) O
, O
uses O
task O
- O
speciﬁc O
architectures O
that O
include O
the O
pre O
- O
trained O
representations O
as O
addi- O
tional O
features O
. O

The O
ﬁne O
- O
tuning O
approach O
, O
such O
as O
the O
Generative B-MethodName
Pre I-MethodName
- I-MethodName
trained I-MethodName
Transformer I-MethodName
( O
OpenAI B-MethodName
GPT I-MethodName
) O
( O
Radford O
et O
al O
. O
, O
2018 O
) O
, O
introduces O
minimal O
task O
- O
speciﬁc O
parameters O
, O
and O
is O
trained O
on O
the O
downstream O
tasks O
by O
simply O
ﬁne O
- O
tuning O
allpre- O
trained O
parameters O
. O

The O
two O
approaches O
share O
the O
same O
objective O
function O
during O
pre O
- O
training O
, O
where O
they O
use O
unidirectional O
language O
models O
to O
learn O
general O
language O
representations O
. O

We O
argue O
that O
current O
techniques O
restrict O
the O
power O
of O
the O
pre O
- O
trained O
representations O
, O
espe- O
cially O
for O
the O
ﬁne O
- O
tuning O
approaches O
. O

The O
ma- O
jor O
limitation O
is O
that O
standard O
language O
models O
are O
unidirectional O
, O
and O
this O
limits O
the O
choice O
of O
archi- O
tectures O
that O
can O
be O
used O
during O
pre O
- O
training O
. O

For O
example O
, O
in O
OpenAI B-MethodName
GPT I-MethodName
, O
the O
authors O
use O
a O
left O
- O
to- O
right O
architecture O
, O
where O
every O
token O
can O
only O
at- O
tend O
to O
previous O
tokens O
in O
the O
self O
- O
attention O
layers O
of O
the O
Transformer O
( O
Vaswani O
et O

al O
. O
, O
2017 O
) O
. O

Such O
re- O
strictions O
are O
sub O
- O
optimal O
for O
sentence O
- O
level O
tasks O
, O
and O
could O
be O
very O
harmful O
when O
applying O
ﬁne- O
tuning O
based O
approaches O
to O
token O
- O
level O
tasks O
such O
as O
question B-TaskName
answering O
, O
where O
it O
is O
crucial O
to O
incor- O
porate O
context O
from O
both O
directions O
. O

In O
this O
paper O
, O
we O
improve O
the O
ﬁne O
- O
tuning O
based O
approaches O
by O
proposing O
BERT B-MethodName
: O
Bidirectional O
Encoder I-MethodName
Representations I-MethodName
from O
Transformers B-MethodName
. O

BERT B-MethodName
alleviates O
the O
previously O
mentioned O
unidi- O
rectionality O
constraint O
by O
using O
a O
“ O
masked O
lan- O
guage O
model I-MethodName
” O
( O
MLM B-MethodName
) O
pre O
- O
training O
objective O
, O
in- O
spired O
by O
the O
Cloze B-MetricName
task O
( O
Taylor O
, O
1953 O
) O
. O

The O
masked O
language O
model O
randomly O
masks O
some O
of O
the O
tokens O
from O
the O
input O
, O
and O
the O
objective O
is O
to O
predict O
the O
original O
vocabulary O
i O
d O
of O
the O
masked O
word O
based O
only O
on O
its O
context O
. O

Unlike O
left O
- O
to- O
right O
language O
model O
pre O
- O
training O
, O
the O
MLM B-MethodName
ob- I-MethodName
jective I-MethodName
enables O
the O
representation O
to O
fuse O
the O
left O
and O
the O
right O
context O
, O
which O
allows O
us O
to O
pre- O
train O
a O
deep O
bidirectional O
Transformer O
. O

In O
addi- O
tion O
to O
the O
masked O
language O
model O
, O
we O
also O
use O
a O
“ O
next O
sentence O
prediction O
” O
task O
that O
jointly O
pre- O
trains O
text O
- O
pair O
representations O
. O

The O
contributions O
of O
our O
paper O
are O
as O
follows O
: O
• O
We O
demonstrate O
the O
importance O
of O
bidirectional O
pre O
- O
training O
for O
language O
representations O
. O

Un- O
like O
Radford O
et O
al O
. O

( O
2018 O
) O
, O
which O
uses O
unidirec- O
tional O
language O
models O
for O
pre O
- O
training O
, O
BERT B-MethodName
uses O
masked O
language O
models O
to O
enable O
pre- O
trained O
deep O
bidirectional O
representations O
. O

This O
is O
also O
in O
contrast O
to O
Peters O
et O
al O
. O
( O
2018a O
) O
, O
which O
uses O
a O
shallow O
concatenation O
of O
independently O
trained O
left O
- O
to O
- O
right O
and O
right O
- O
to O
- O
left O
LMs B-TaskName
. O

• O

We O
show O
that O
pre O
- O
trained O
representations O
reduce O
the O
need O
for O
many O
heavily O
- O
engineered O
task- O
speciﬁc O
architectures O
. O

BERT B-MethodName
is O
the O
ﬁrst O
ﬁne- O
tuning O
based O
representation O
model O
that O
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
a O
large O
suite O
of O
sentence O
- O
level O
andtoken O
- O
level O
tasks O
, O
outper- O
forming O
many O
task O
- O
speciﬁc O
architectures O
. O

• B-MethodName
BERT B-MethodName
advances O
the O
state O
of O
the O
art O
for O
eleven O
NLP B-TaskName
tasks O
. O

The O
code O
and O
pre O
- O
trained O
mod- O
els O
are O
available O
at O
https://github.com/ O
google O
- O
research O
/ O
bert O
. O

2 O
Related O
Work O
There O
is O
a O
long O
history O
of O
pre O
- O
training O
general O
lan- O
guage O
representations O
, O
and O
we O
brieﬂy O
review O
the O
most O
widely O
- O
used O
approaches O
in O
this O
section O
. O

2.1 O
Unsupervised O
Feature O
- O
based O
Approaches O
Learning O
widely O
applicable O
representations O
of O
words O
has O
been O
an O
active O
area O
of O
research O
for O
decades O
, O
including O
non O
- O
neural O
( O
Brown O
et O
al O
. O
, O
1992 O
; O
Ando O
and O
Zhang O
, O
2005 O
; O
Blitzer O
et O
al O
. O
, O
2006 O
) O
and O
neural O
( O
Mikolov O
et O
al O
. O
, O
2013 O
; O
Pennington O
et O
al O
. O
, O
2014 O
) O
methods O
. O

Pre O
- O
trained O
word O
embeddings O
are O
an O
integral O
part O
of O
modern O
NLP B-TaskName
systems O
, O
of- O
fering O
signiﬁcant O
improvements O
over O
embeddings O
learned O
from O
scratch O
( O
Turian O
et O

al O
. O
, O
2010 O
) O
. O

To O
pre- O
train O
word O
embedding O
vectors O
, O
left O
- O
to O
- O
right O
lan- O
guage O
modeling O
objectives O
have O
been O
used O
( O
Mnih O
and O
Hinton O
, O
2009 O
) O
, O
as O
well O
as O
objectives O
to O
dis- O
criminate O
correct O
from O
incorrect O
words O
in O
left O
and O
right O
context O
( O
Mikolov O
et O
al O
. O
, O
2013).These O
approaches O
have O
been O
generalized O
to O
coarser O
granularities O
, O
such O
as O
sentence O
embed- O
dings O
( O
Kiros O
et O
al O
. O
, O
2015 O
; O
Logeswaran O
and O
Lee O
, O
2018 O
) O
or O
paragraph O
embeddings O
( O
Le O
and O
Mikolov O
, O
2014 O
) O
. O

To O
train O
sentence O
representations O
, O
prior O
work O
has O
used O
objectives O
to O
rank O
candidate O
next O
sentences O
( O
Jernite O
et O
al O
. O
, O
2017 O
; O
Logeswaran O
and O
Lee O
, O
2018 O
) O
, O
left O
- O
to O
- O
right O
generation O
of O
next O
sen- O
tence O
words O
given O
a O
representation O
of O
the O
previous O
sentence O
( O
Kiros O
et O
al O
. O
, O
2015 O
) O
, O
or O
denoising O
auto- O
encoder O
derived O
objectives O
( O
Hill O
et O
al O
. O
, O
2016 O
) O
. O

ELMo B-MethodName
and O
its O
predecessor O
( O
Peters O
et O
al O
. O
, O
2017 O
, O
2018a O
) O
generalize O
traditional O
word O
embedding I-TaskName
re- I-TaskName
search O
along O
a O
different O
dimension O
. O

They O
extract O
context O
- O
sensitive O
features O
from O
a O
left O
- O
to O
- O
right O
and O
a O
right O
- O
to O
- O
left O
language O
model O
. O

The O
contextual O
rep- O
resentation O
of O
each O
token O
is O
the O
concatenation O
of O
the O
left O
- O
to O
- O
right O
and O
right O
- O
to O
- O
left O
representations O
. O

When O
integrating O
contextual O
word O
embeddings O
with O
existing O
task O
- O
speciﬁc O
architectures O
, O
ELMo B-MethodName
advances O
the O
state O
of O
the O
art O
for O
several O
major O
NLP B-TaskName
benchmarks O
( O
Peters O
et O
al O
. O
, O
2018a O
) O
including O
ques- B-TaskName
tion I-TaskName
answering I-TaskName
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
, O
sentiment B-TaskName
analysis I-TaskName
( O
Socher O
et O
al O
. O
, O
2013 O
) O
, O
and O
named B-TaskName
entity I-TaskName
recognition I-TaskName
( O
Tjong O
Kim O
Sang O
and O
De O
Meulder O
, O
2003 O
) O
. O

Melamud O
et O

al O
. O

( O
2016 O
) O
proposed O
learning O
contextual O
representations O
through O
a O
task O
to O
pre- O
dict O
a O
single O
word O
from O
both O
left O
and O
right O
context O
using O
LSTMs B-TaskName
. O

Similar O
to O
ELMo B-MethodName
, O
their O
model O
is O
feature O
- O
based O
and O
not O
deeply O
bidirectional O
. O

Fedus O
et O

al O
. O

( O
2018 O
) O
shows O
that O
the O
cloze B-TaskName
task O
can O
be O
used O
to O
improve O
the O
robustness O
of O
text B-TaskName
generation I-TaskName
mod- O
els O
. O

2.2 O
Unsupervised O
Fine O
- O
tuning O
Approaches O
As O
with O
the O
feature O
- O
based O
approaches O
, O
the O
ﬁrst O
works O
in O
this O
direction O
only O
pre O
- O
trained O
word O
em- O
bedding O
parameters O
from O
unlabeled O
text O
( O
Col- O
lobert O
and O
Weston O
, O
2008 O
) O
. O

More O
recently O
, O
sentence O
or O
document O
encoders O
which O
produce O
contextual O
token O
representations O
have O
been O
pre O
- O
trained O
from O
unlabeled O
text O
and O
ﬁne O
- O
tuned O
for O
a O
supervised O
downstream O
task O
( O
Dai O
and O
Le O
, O
2015 O
; O
Howard O
and O
Ruder O
, O
2018 O
; O
Radford O
et O
al O
. O
, O
2018 O
) O
. O

The O
advantage O
of O
these O
approaches O
is O
that O
few O
parameters O
need O
to O
be O
learned O
from O
scratch O
. O

At O
least O
partly O
due O
to O
this O
advantage O
, O
OpenAI B-MethodName
GPT I-MethodName
( O
Radford O
et O
al O
. O
, O
2018 O
) O
achieved O
pre- O
viously O
state O
- O
of O
- O
the O
- O
art O
results O
on O
many O
sentence- O
level O
tasks O
from O
the O
GLUE B-DatasetName
benchmark O
( O
Wang O
et O
al O
. O
, O
2018a O
) O
. O

Left O
- O
to O
- O
right O
language O
model- O
ing O
and O
auto O
- O
encoder O
objectives O
have O
been O
used O
for O
pre O
- O
training O
such O
models O
( O
Howard O
and O
Ruder O
, O
2018 O
; O
Radford O
et O

al O
. O
, O
2018 O
; O
Dai O
and O
Le O
, O
2015 O
) O

2.3 O
Transfer O
Learning O
from O
Supervised O
Data O
There O
has O
also O
been O
work O
showing O
effective O
trans- O
fer O
from O
supervised O
tasks O
with O
large O
datasets O
, O
such O
as O
natural B-TaskName
language I-TaskName
inference I-TaskName
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
and O
machine B-TaskName
translation I-TaskName
( O
McCann O
et O
al O
. O
, O
2017 O
) O
. O

Computer O
vision O
research O
has O
also O
demon- O
strated O
the O
importance O
of O
transfer O
learning O
from O
large O
pre O
- O
trained O
models O
, O
where O
an O
effective O
recipe O
is O
to O
ﬁne O
- O
tune O
models O
pre O
- O
trained O
with O
Ima- B-MethodName
geNet I-MethodName
( O
Deng O
et O
al O
. O
, O
2009 O
; O
Yosinski O
et O
al O
. O
, O
2014 O
) O
. O

3 O
BERT B-MethodName
We O
introduce O
BERT B-MethodName
and O
its O
detailed O
implementa- O
tion O
in O
this O
section O
. O

There O
are O
two O
steps O
in O
our O
framework O
: O
pre O
- O
training O
and O
ﬁne O
- O
tuning O
. O

Dur- O
ing O
pre O
- O
training O
, O
the O
model O
is O
trained O
on O
unlabeled O
data O
over O
different O
pre O
- O
training O
tasks O
. O

For O
ﬁne- O
tuning O
, O
the O
BERT B-MethodName
model O
is O
ﬁrst O
initialized O
with O
the O
pre O
- O
trained O
parameters O
, O
and O
all O
of O
the O
param- O
eters O
are O
ﬁne O
- O
tuned O
using O
labeled O
data O
from O
the O
downstream O
tasks O
. O

Each O
downstream O
task O
has O
sep- O
arate O
ﬁne O
- O
tuned O
models O
, O
even O
though O
they O
are O
ini- O
tialized O
with O
the O
same O
pre O
- O
trained O
parameters O
. O

The O
question O
- O
answering O
example O
in O
Figure O
1 O
will O
serve O
as O
a O
running O
example O
for O
this O
section O
. O

A O
distinctive O
feature O
of O
BERT B-MethodName
is O
its O
uniﬁed O
ar- O
chitecture O
across O
different O
tasks O
. O

There O
is O
mini O
- O
mal O
difference O
between O
the O
pre O
- O
trained O
architec- O
ture O
and O
the O
ﬁnal O
downstream O
architecture O
. O

Model B-MethodName
Architecture I-MethodName
BERT B-MethodName
’s O
model O
architec- O
ture O
is O
a O
multi O
- O
layer O
bidirectional O
Transformer O
en- O
coder O
based O
on O
the O
original O
implementation O
de- O
scribed O
in O
Vaswani O

et O

al O
. O
( O
2017 O
) O
and O
released O
in O
thetensor2tensor O
library.1Because O
the O
use O
of O
Transformers O
has O
become O
common O
and O
our O
im- O
plementation O
is O
almost O
identical O
to O
the O
original O
, O
we O
will O
omit O
an O
exhaustive O
background O
descrip- O
tion O
of O
the O
model O
architecture O
and O
refer O
readers O
to O
Vaswani O
et O

al O
. O
( O
2017 O
) O
as O
well O
as O
excellent O
guides O
such O
as O
“ O
The O
Annotated I-MethodName
Transformer I-MethodName
. O
”2 O

In O
this O
work O
, O
we O
denote O
the O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
( O
i.e. O
, O
Transformer O
blocks I-HyperparameterName
) O
as O
L B-HyperparameterName
, O
the O
hidden B-HyperparameterName
size I-HyperparameterName
as O
H B-HyperparameterName
, O
and O
the O
number B-HyperparameterName
of I-HyperparameterName
self I-HyperparameterName
- I-HyperparameterName
attention I-HyperparameterName
heads I-HyperparameterName
as O
A B-HyperparameterName
. O

We O
primarily O
report O
results O
on O
two O
model O
sizes O
: O
BERT B-MethodName
BASE I-MethodName
( O
L B-HyperparameterName
= O
12 B-HyperparameterValue
, O
H B-HyperparameterName
= O
768 B-HyperparameterValue
, O
A B-HyperparameterName
= O
12 B-HyperparameterValue
, O
Total O
Param- O
eters=110 O
M I-HyperparameterValue
) O
and O
BERT B-MethodName
LARGE I-MethodName
( O
L B-HyperparameterName
= O
24 B-HyperparameterValue
, O
H B-HyperparameterName
= O
1024 B-HyperparameterValue
, O
A B-HyperparameterName
= O
16 B-HyperparameterValue
, O
Total B-HyperparameterName
Parameters=340 I-HyperparameterName
M I-HyperparameterValue
) O
. O

BERT B-MethodName
BASE I-MethodName
was O
chosen O
to O
have O
the O
same O
model O
size O
as O
OpenAI B-MethodName
GPT I-MethodName
for O
comparison O
purposes O
. O

Critically O
, O
however O
, O
the O
BERT B-MethodName
Transformer I-MethodName
uses O
bidirectional O
self O
- O
attention O
, O
while O
the O
GPT B-MethodName
Trans- I-MethodName
former I-MethodName
uses O
constrained O
self O
- O
attention O
where O
every O
token O
can O
only O
attend O
to O
context O
to O
its O
left O
. O

Input O
/ O
Output O
Representations O
To O
make O
BERT B-MethodName
handle O
a O
variety O
of O
down O
- O
stream O
tasks O
, O
our O
input O
representation O
is O
able O
to O
unambiguously O
represent O
both O
a O
single O
sentence O
and O
a O
pair O
of O
sentences O
( O
e.g. O
,hQuestion O
, O
Answeri B-TaskName
) O
in O
one O
token O
sequence O
. O

Throughout O
this O
work O
, O
a O
“ O
sentence O
” O
can O
be O
an O
arbi- O
trary O
span O
of O
contiguous O
text O
, O
rather O
than O
an O
actual O
linguistic O
sentence O
. O

A O
“ O
sequence O
” O
refers O
to O
the O
in- O
put O
token O
sequence O
to O
BERT B-MethodName
, O
which O
may O
be O
a O
sin- O
gle O
sentence O
or O
two O
sentences O
packed O
together O
. O

We O
use O
WordPiece B-MethodName
embeddings O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
with O
a O
30,000 B-HyperparameterValue
token O
vocabulary O
. O

The O
ﬁrst O
token O
of O
every O
sequence O
is O
always O
a O
special O
clas- O
siﬁcation O
token O

( O
[ B-MethodName
CLS B-MethodName
] O
) O
. O

The O
ﬁnal O
hidden O
state O
corresponding O
to O
this O
token O
is O
used O
as O
the O
ag- O
gregate O
sequence O
representation O
for O
classiﬁcation B-TaskName
tasks O
. O

Sentence O
pairs O
are O
packed O
together O
into O
a O
single O
sequence O
. O

We O
differentiate O
the O
sentences O
in O
two O
ways O
. O

First O
, O
we O
separate O
them O
with O
a O
special O
token O
( O
[ O
SEP O
] O
) O
. O

Second O
, O
we O
add O
a O
learned O
embed- O
ding O
to O
every O
token O
indicating O
whether O
it O
belongs O
to O
sentence O
Aor O
sentence O
B. O
As O
shown O
in O
Figure O
1 O
, O
we O
denote O
input O
embedding O
as O
E O
, O
the O
ﬁnal O
hidden O
vector O
of O
the O
special O
[ O
CLS O
] O
token O
asC2RH O
, O
and O
the O
ﬁnal O
hidden O
vector O
for O
the O
ithinput O
token O
asTi2RH O
. O

For O
a O
given O
token O
, O
its O
input O
representation O
is O
constructed O
by O
summing O
the O
corresponding O
token O
, O
segment O
, O
and O
position O
embeddings O
. O

A O
visualiza- O
tion O
of O
this O
construction O
can O
be O
seen O
in O
Figure O
2 O
. O

3.1 O
Pre O
- O
training O
BERT B-MethodName
Unlike O
Peters O
et O
al O
. O
( O
2018a O
) O
and O
Radford O
et O

al O
. O

( O
2018 O
) O
, O
we O
do O
not O
use O
traditional O
left O
- O
to O
- O
right O
or O
right O
- O
to O
- O
left O
language O
models O
to O
pre O
- O
train O
BERT B-MethodName
. O

Instead O
, O
we O
pre O
- O
train O
BERT B-MethodName
using O
two O
unsuper- O
vised O
tasks O
, O
described O
in O
this O
section O
. O

This O
step O
is O
presented O
in O
the O
left O
part O
of O
Figure O
1 O
. O

Task O
# O
1 O
: O
Masked B-TaskName
LM B-TaskName

Intuitively O
, O
it O
is O
reason- O
able O
to O
believe O
that O
a O
deep O
bidirectional O
model O
is O
strictly O
more O
powerful O
than O
either O
a O
left O
- O
to O
- O
right O
model O
or O
the O
shallow O
concatenation O
of O
a O
left O
- O
to- O
right O
and O
a O
right O
- O
to O
- O
left O
model O
. O

Unfortunately O
, O
standard O
conditional O
language O
models O
can O
only O
be O
trained O
left O
- O
to O
- O
right O
orright O
- O
to O
- O
left O
, O
since O
bidirec- O
tional I-TaskName
conditioning O
would O
allow O
each O
word O
to O
in- O

directly O
“ O
see O
itself O
” O
, O
and O
the O
model O
could O
trivially O
predict O
the O
target O
word O
in O
a O
multi O
- O
layered O
context O
. O

former O
is O
often O
referred O
to O
as O
a O
“ O
Transformer I-MethodName
encoder O
” O
while O
the O
left O
- O
context O
- O
only O
version O
is O
referred O
to O
as O
a O
“ O
Transformer I-MethodName
decoder O
” O
since O
it O
can O
be O
used O
for O
text B-TaskName
generation I-TaskName
. O

In O
order O
to O
train O
a O
deep O
bidirectional O
representa- O
tion O
, O
we O
simply O
mask O
some O
percentage O
of O
the O
input O
tokens O
at O
random O
, O
and O
then O
predict O
those O
masked O
tokens O
. O

We O
refer O
to O
this O
procedure O
as O
a O
“ O
masked O
LM O
” O
( O
MLM B-MethodName
) O
, O
although O
it O
is O
often O
referred O
to O
as O
a O
Cloze B-MetricName
task O
in O
the O
literature O
( O
Taylor O
, O
1953 O
) O
. O

In O
this O
case O
, O
the O
ﬁnal O
hidden O
vectors O
corresponding O
to O
the O
mask O
tokens O
are O
fed O
into O
an O
output O
softmax O
over O
the O
vocabulary O
, O
as O
in O
a O
standard O
LM B-TaskName
. O

In O
all O
of O
our O
experiments O
, O
we O
mask O
15% B-HyperparameterValue
of O
all O
WordPiece B-MethodName
to- O
kens O
in O
each O
sequence O
at O
random O
. O

In O
contrast O
to O
denoising O
auto O
- O
encoders O
( O
Vincent O
et O
al O
. O
, O
2008 O
) O
, O
we O
only O
predict O
the O
masked O
words O
rather O
than O
recon- O
structing O
the O
entire O
input O
. O

Although O
this O
allows O
us O
to O
obtain O
a O
bidirec- O
tional O
pre O
- O
trained O
model O
, O
a O
downside O
is O
that O
we O
are O
creating O
a O
mismatch O
between O
pre O
- O
training O
and O
ﬁne O
- O
tuning O
, O
since O
the O
[ O
MASK B-MetricName
] O
token O
does O
not O
ap- O
pear O
during O
ﬁne O
- O
tuning O
. O

To O
mitigate O
this O
, O
we O
do O
not O
always O
replace O
“ O
masked O
” O
words O
with O
the O
ac- O

tual[MASK O
] O
token O
. O

The O
training O
data O
generator O
chooses O
15% B-HyperparameterValue
of O
the O
token O
positions O
at O
random O
for O
prediction O
. O

If O
the O
i O
- O
th O
token O
is O
chosen O
, O
we O
replace O
thei O
- O
th O
token O
with O
( O
1 O
) O
the O
[ O
MASK B-MetricName
] O
token O
80% O
of O
the O
time O
( O
2 O
) O
a O
random O
token O
10% O
of O
the O
time O
( O
3 O
) O
the O
unchanged O
i O
- O
th O
token O
10% B-HyperparameterValue
of O
the O
time O
. O

Then O
, O
Tiwill O
be O
used O
to O
predict O
the O
original O
token O
with O
cross O
entropy O
loss O
. O

We O
compare O
variations O
of O
this O
procedure O
in O
Appendix O
C.2 O
. O

Task O
# O
2 O
: O
Next O
Sentence O
Prediction O
( O
NSP B-TaskName
) O
Many O
important O
downstream O
tasks O
such O
as O
Ques- B-TaskName
tion I-TaskName
Answering I-TaskName
( O
QA B-TaskName
) O
and O
Natural B-MethodName
Language I-MethodName
Infer- I-MethodName
ence I-MethodName
( O
NLI B-TaskName
) O
are O
based O
on O
understanding O
the O
rela- O
tionship O
between O
two O
sentences O
, O
which O
is O
not O
di- O
rectly O
captured O
by O
language O
modeling O
. O

In O
order O
to O
train O
a O
model O
that O
understands O
sentence O
rela- O
tionships O
, O
we O
pre O
- O
train O
for O
a O
binarized O
next O
sen- O
tence O
prediction O
task O
that O
can O
be O
trivially O
gener- O
ated O
from O
any O
monolingual O
corpus O
. O

Speciﬁcally O
, O
when O
choosing O
the O
sentences O
AandBfor O
each O
pre- O
training O
example O
, O
50% O
of O
the O
time O
Bis O
the O
actual O
next O
sentence O
that O
follows O
A(labeled O
as O
IsNext B-MethodName
) O
, O
and O
50% O
of O
the O
time O
it O
is O
a O
random O
sentence O
from O
the O
corpus O
( O
labeled O
as O
NotNext B-MethodName
) O
. O

As O
we O
show O
in O
Figure O
1 O
, O
Cis O
used O
for O
next O
sentence O
predic- O
tion I-TaskName
( O
NSP).5Despite B-TaskName
its O
simplicity O
, O
we O
demon- O
strate O
in O
Section O
5.1 O
that O
pre O
- O
training O
towards O
this O
task O
is O
very O
beneﬁcial O
to O
both O
QA B-TaskName
and O
NLI B-TaskName
. O

The O
NSP B-TaskName
task O
is O
closely O
related O
to O
representation- B-TaskName
learning O
objectives O
used O
in O
Jernite O
et O
al O
. O

( O
2017 O
) O
and O
Logeswaran O
and O
Lee O
( O
2018 O
) O
. O

However O
, O
in O
prior O
work O
, O
only O
sentence O
embeddings O
are O
transferred O
to O
down O
- O
stream O
tasks O
, O
where O
BERT B-MethodName
transfers O
all O
pa- O
rameters O
to O
initialize O
end O
- O
task O
model O
parameters O
. O

Pre O
- O
training O
data O
The O
pre O
- O
training O
procedure O
largely O
follows O
the O
existing O
literature O
on O
language O
model O
pre O
- O
training O
. O

For O
the O
pre O
- O
training O
corpus O
we O
use O
the O
BooksCorpus B-MethodName
( O
800 B-HyperparameterValue
M I-HyperparameterValue
words O
) O

( O
Zhu O
et O
al O
. O
, O
2015 O
) O
and O
English B-DatasetName
Wikipedia I-DatasetName
( O
2,500 O
M I-HyperparameterValue
words O
) O
. O

For O
Wikipedia B-DatasetName
we O
extract O
only O
the O
text O
passages O
and O
ignore O
lists O
, O
tables O
, O
and O
headers O
. O

It O
is O
criti- O
cal O
to O
use O
a O
document O
- O
level O
corpus O
rather O
than O
a O
shufﬂed O
sentence O
- O
level O
corpus O
such O
as O
the O
Billion B-DatasetName
Word I-DatasetName
Benchmark I-MetricName
( O
Chelba O
et O
al O
. O
, O
2013 O
) O
in O
order O
to O
extract O
long O
contiguous O
sequences O
. O

3.2 O
Fine O
- O
tuning O
BERT B-MethodName
Fine O
- O
tuning O
is O
straightforward O
since O
the O
self- O
attention O
mechanism O
in O
the O
Transformer O
al- O
lows O
BERT B-MethodName
to O
model O
many O
downstream O
tasks O
— O
whether O
they O
involve O
single O
text O
or O
text O
pairs O
— O
by O
swapping O
out O
the O
appropriate O
inputs O
and O
outputs O
. O

For O
applications O
involving O
text O
pairs O
, O
a O
common O
pattern O
is O
to O
independently O
encode O
text O
pairs O
be- O

fore O
applying O
bidirectional O
cross O
attention O
, O
such O
as O
Parikh O
et O
al O
. O

( O
2016 O
) O
; O

Seo O
et O
al O
. O
( O
2017 O
) O
. O

BERT B-MethodName
instead O
uses O
the O
self O
- O
attention O
mechanism O
to O
unify O
these O
two O
stages O
, O
as O
encoding O
a O
concatenated O
text O
pair O
with O
self O
- O
attention O
effectively O
includes O
bidi- O
rectional O
cross O
attention O
between O
two O
sentences O
. O

For O
each O
task O
, O
we O
simply O
plug O
in O
the O
task- O
speciﬁc O
inputs O
and O
outputs O
into O
BERT B-MethodName
and O
ﬁne- O
tune O
all O
the O
parameters O
end O
- O
to O
- O
end O
. O

At O
the O
in- O
put O
, O
sentence O
Aand O
sentence O
Bfrom O
pre O
- O
training O
are O
analogous O
to O
( O
1 O
) O
sentence O
pairs O
in O
paraphras- O
ing O
, O
( O
2 O
) O
hypothesis O
- O
premise O
pairs O
in O
entailment O
, O
( O
3 O
) O
question O
- O
passage O
pairs O
in O
question O
answering O
, O
and(4 O
) O
a O
degenerate O
text- O
? O

pair O
in O
text O
classiﬁcation O
or O
sequence O
tagging O
. O

At O
the O
output O
, O
the O
token O
rep- O
resentations O
are O
fed O
into O
an O
output O
layer O
for O
token- O
level O
tasks O
, O
such O
as O
sequence B-TaskName
tagging I-TaskName
or O
question O
answering O
, O
and O
the O
[ O
CLS B-MethodName
] O
representation O
is O
fed O
into O
an O
output O
layer O
for O
classiﬁcation B-TaskName
, O
such O
as O
en- B-TaskName
tailment I-TaskName
or O
sentiment B-TaskName
analysis I-TaskName
. O

Compared O
to O
pre O
- O
training O
, O
ﬁne O
- O
tuning O
is O
rela- O
tively O
inexpensive O
. O

All O
of O
the O
results O
in O
the O
pa- O
per O
can O
be O
replicated O
in O
at O
most O
1 O
hour O
on O
a O
sin- O
gle O
Cloud O
TPU O
, O
or O
a O
few O
hours O
on O
a O
GPU O
, O
starting O
from O
the O
exact O
same O
pre O
- O
trained O
model.7We O
de- O
scribe O
the O
task O
- O
speciﬁc O
details O
in O
the O
correspond- O
ing O
subsections O
of O
Section O
4 O
. O

More O
details O
can O
be O
found O
in O
Appendix O
A.5 O
. O

4 O
Experiments O
In O
this O
section O
, O
we O
present O
BERT B-MethodName
ﬁne O
- O
tuning O
re- O
sults O
on O
11 O
NLP B-TaskName
tasks O
. O

4.1 O
GLUE O
The O
General B-TaskName
Language I-TaskName
Understanding I-TaskName
Evaluation I-MetricName
( O
GLUE B-TaskName
) O
benchmark O
( O
Wang O
et O
al O
. O
, O
2018a O
) O
is O
a O
col- O
lection O
of O
diverse O
natural O
language O
understanding O
tasks O
. O

Detailed O
descriptions O
of O
GLUE B-DatasetName
datasets O
are O
included O
in O
Appendix O
B.1 O
. O

To O
ﬁne O
- O
tune O
on O
GLUE B-DatasetName
, O
we O
represent O
the O
input O
sequence O
( O
for O
single O
sentence O
or O
sentence O
pairs O
) O
as O
described O
in O
Section O
3 O
, O
and O
use O
the O
ﬁnal O
hid- O
den O
vectorC2RHcorresponding O
to O
the O
ﬁrst O
input O
token O
( O
[ B-MethodName
CLS B-MethodName
] O
) O
as O
the O
aggregate O
representa- O
tion O
. O

The O
only O
new O
parameters O
introduced O
during O
ﬁne O
- O
tuning O
are O
classiﬁcation O
layer I-HyperparameterName
weights O
W O
RKH I-DatasetName
, O
where O
K B-HyperparameterName
is O
the O
number B-HyperparameterName
of I-HyperparameterName
labels I-HyperparameterName
. O

We O
com- O
pute O
a O
standard O
classiﬁcation O
loss O
with O
CandW B-MethodName
, O
i.e. O
,log(softmax O
( O
CWT B-TaskName
) O
) O
. O

We O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
and O
ﬁne O
- O
tune O
for O
3 B-HyperparameterValue
epochs O
over O
the O
data O
for O
all O
GLUE B-TaskName
tasks O
. O

For O
each O
task O
, O
we O
selected O
the O
best O
ﬁne O
- O
tuning I-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
( O
among O
5e-5 B-HyperparameterValue
, O
4e-5 B-HyperparameterValue
, O
3e-5 B-HyperparameterValue
, O
and O
2e-5 B-HyperparameterValue
) O
on O
the O
Dev O
set O
. O

Additionally O
, O
for O
BERT B-MethodName
LARGE I-MethodName
we O
found O
that O
ﬁne- O
tuning O
was O
sometimes O
unstable O
on O
small O
datasets O
, O
so O
we O
ran O
several O
random O
restarts O
and O
selected O
the O
best O
model O
on O
the O
Dev O
set O
. O

With O
random O
restarts O
, O
we O
use O
the O
same O
pre O
- O
trained O
checkpoint O
but O
per- O
form O
different O
ﬁne O
- O
tuning O
data O
shufﬂing O
and O

clas- O
siﬁer I-TaskName
layer I-TaskName
initialization.9 O
Results O
are O
presented O
in O
Table O
1 O
. O

Both O
BERT B-MethodName
BASE I-MethodName
and O
BERT B-MethodName
LARGE I-MethodName
outperform O
all O
sys- O
tems O
on O
all O
tasks O
by O
a O
substantial O
margin O
, O
obtaining O
4.5% B-MetricValue
and O
7.0% B-MetricValue
respective O
average O
accuracy O
im- O
provement O
over O
the O
prior O
state O
of O
the O
art O
. O

Note O
that O
BERT B-MethodName
BASE I-MethodName
and O
OpenAI B-MethodName
GPT I-MethodName
are O
nearly O
identical O
in O
terms O
of O
model O
architecture O
apart O
from O
the O
at- O
tention O
masking O
. O

For O
the O
largest O
and O
most O
widely O
reported O
GLUE O
task O
, O
MNLI B-MethodName
, O
BERT B-MethodName
obtains O
a O
4.6% B-MetricValue
absolute O
accuracy B-MetricName
improvement O
. O

On O
the O
ofﬁcial O
GLUE O
leaderboard10 O
, O
BERT B-MethodName
LARGE I-MethodName
obtains O
a O
score O
of O
80.5 B-MetricValue
, O
compared O
to O
OpenAI B-MethodName
GPT I-MethodName
, O
which O
obtains O
72.8 B-MetricValue
as O
of O
the O
date O
of O
writing O
. O

We O
ﬁnd O
that O
BERT B-MethodName
LARGE I-MethodName
signiﬁcantly O
outper- O
forms O
BERT B-MethodName
BASE I-MethodName
across O
all O
tasks O
, O
especially O
those O
with O
very O
little O
training O
data O
. O

The O
effect O
of O
model O
size O
is O
explored O
more O
thoroughly O
in O
Section O
5.2 O
. O

4.2 O
SQuAD B-DatasetName
v1.1 O
The O
Stanford B-MethodName
Question I-MethodName
Answering I-MethodName
Dataset I-MethodName
( O
SQuAD B-DatasetName
v1.1 O
) O
is O
a O
collection O
of O
100k O
crowd- O
sourced O
question O
/ O
answer O
pairs O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
. O

Given O
a O
question O
and O
a O
passage O
from O
Wikipedia B-DatasetName
containing O
the O
answer O
, O
the O
task O
is O
to O
predict O
the O
answer O
text O
span O
in O
the O
passage O
. O

As O
shown O
in O
Figure O
1 O
, O
in O
the O
question O
answer- O
ing O
task O
, O
we O
represent O
the O
input O
question O
and O
pas- O
sage O
as O
a O
single O
packed O
sequence O
, O
with O
the O
ques- O
tion O
using O
the O
Aembedding O
and O
the O
passage O
using O
theBembedding O
. O

We O
only O
introduce O
a O
start O
vec- O
torS2RHand O
an O
end O
vector O
E2RHduring O
ﬁne O
- O
tuning O
. O

The O
probability O
of O
word O
ibeing O
the O
start O
of O
the O
answer O
span I-HyperparameterName
is O
computed O
as O
a O
dot O
prod- O
uct O
between O
TiandSfollowed O
by O
a O
softmax O
over O
all O
of O
the O
words O
in O
the O
paragraph O
: O
Pi O
= O
eSTiP O
jeSTj O
. O

The O
analogous O
formula O
is O
used O
for O
the O
end O
of O
the O
answer O
span O
. O

The O
score O
of O
a O
candidate O
span O
from O
positionito O
positionjis O
deﬁned O
as O
STi+ETj B-TaskName
, O
and O
the O
maximum O
scoring O
span O
where O
jiis O
used O
as O
a O
prediction O
. O

The O
training B-HyperparameterName
objective O
is O
the O
sum O
of O
the O
log B-HyperparameterName
- I-HyperparameterName
likelihoods I-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
correct O
start I-HyperparameterName
and I-HyperparameterName
end I-HyperparameterName
positions I-HyperparameterName
. O

We O
ﬁne O
- O
tune O
for O
3 B-HyperparameterValue
epochs I-HyperparameterName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5e-5 B-HyperparameterValue
and O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
. O

Table O
2 O
shows O
top O
leaderboard O
entries O
as O
well O
as O
results O
from O
top O
published O
systems O
( O
Seo O
et O
al O
. O
, O
2017 O
; O
Clark O
and O
Gardner O
, O
2018 O
; O
Peters O
et O
al O
. O
, O
2018a O
; O
Hu O
et O
al O
. O
, O
2018 O
) O
. O

The O
top O
results O
from O
the O
SQuAD B-MethodName
leaderboard O
do O
not O
have O
up O
- O
to O
- O
date O
public O
system O
descriptions O
available,11and O
are O
allowed O
to O
use O
any O
public O
data O
when O
training O
their O
systems O
. O

We O
therefore O
use O
modest O
data O
augmentation O
in O
our O
system O
by O
ﬁrst O
ﬁne O
- O
tuning O
on O
TriviaQA B-MethodName
( O
Joshi O
et O
al O
. O
, O
2017 O
) O
befor O
ﬁne O
- O
tuning O
on O
SQuAD B-DatasetName
. O

Our O
best O
performing O
system O
outperforms O
the O
top O
leaderboard O
system O
by O
+1.5 B-MetricValue
F1 B-MetricName
in O
ensembling O
and O
+1.3 B-MetricValue
F1 B-MetricName
as O
a O
single O
system O
. O

In O
fact O
, O
our O
single O
BERT B-MethodName
model O
outperforms O
the O
top O
ensemble O
sys- O
tem O
in O
terms O
of O
F1 B-MetricName
score O
. O

Without O
TriviaQA B-MethodName
ﬁne- O
tuning O
data O
, O
we O
only O
lose O
0.1 B-MetricValue
- I-MetricValue
0.4 I-MetricValue
F1 I-MetricValue
, O
still O
outper- O
forming O
all O
existing O
systems O
by O
a O
wide O
margin O
. O

4.3 O
SQuAD B-MethodName
v2.0 O

The O
SQuAD B-DatasetName
2.0 I-DatasetName
task O
extends O
the O
SQuAD B-DatasetName
1.1 I-DatasetName
problem O
deﬁnition O
by O
allowing O
for O
the O
possibility O
that O
no O
short O
answer O
exists O
in O
the O
provided O
para- O
graph O
, O
making O
the O
problem O
more O
realistic O
. O

We O
use O
a O
simple O
approach O
to O
extend O
the O
SQuAD B-MethodName
v1.1 O
BERT B-MethodName
model O
for O
this O
task O
. O

We O
treat O
ques- O
tions O
that O
do O
not O
have O
an O
answer O
as O
having O
an O
an- O
swer O
span O
with O
start O
and O
end O
at O
the O
[ O
CLS O
] O
to- O
ken O
. O

The O
probability O
space I-HyperparameterName
for O
the O
start O
and O
end I-HyperparameterName
answer O
span I-HyperparameterName
positions O
is O
extended O
to O
include O
the O
position O
of O
the O
[ O
CLS B-MetricName
] O
token O
. O

For O
prediction O
, O
we O
compare O
the O
score O
of I-HyperparameterName
the O
no O
- O
answer O
span I-HyperparameterName
: O
snull= O
SC+ECto B-MetricName
the O
score O
of O
the O
best O
non O
- O
null O
span O
^si;j O
= O
maxjiSTi+ETj O
. O

We O
predict O
a O
non O
- O
null O
answer O
when O
^si;j B-HyperparameterName
> O
s O
null+ O

, O
where O
the O
thresh- O
old O

is O
selected O
on O
the O
dev O
set O
to O
maximize O
F1 B-MethodName
. O

We O
did O
not O
use O
TriviaQA B-DatasetName
data O
for O
this O
model O
. O

We O
ﬁne O
- O
tuned O
for O
2 B-HyperparameterValue
epochs I-HyperparameterName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5e-5 B-HyperparameterValue
and O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
48 B-HyperparameterValue
. O

The O
results O
compared O
to O
prior O
leaderboard O
en- O

tries O
and O
top O
published O
work O
( O
Sun O
et O
al O
. O
, O
2018 O
; O
Wang O
et O
al O
. O
, O
2018b O
) O
are O
shown O
in O
Table O
3 O
, O
exclud- O
ing O
systems O
that O
use O
BERT B-MethodName
as O
one O
of O
their O
com- O
ponents O
. O

We O
observe O
a O
+5.1 B-MetricValue
F1 B-MethodName
improvement O
over O
the O
previous O
best O
system O
. O

4.4 O
SWAG B-MethodName
The O
Situations B-MethodName
With I-MethodName
Adversarial I-MethodName
Generations I-MethodName
( O
SWAG B-MethodName
) O
dataset O
contains O
113k O
sentence O
- O
pair O
com- O
pletion O
examples O
that O
evaluate O
grounded O
common- O
sense O
inference O
( O
Zellers O
et O
al O
. O
, O
2018 O
) O
. O

Given O
a O
sen- O
tence O
, O
the O
task O
is O
to O
choose O
the O
most O
plausible O
con- O
tinuation O
among O
four O
choices O
. O

When O
ﬁne O
- O
tuning O
on O
the O
SWAG B-DatasetName
dataset O
, O
we O
construct O
four O
input O
sequences O
, O
each O
containing O
the O
concatenation O
of O
the O
given O
sentence O
( O
sentence O
A O
) O
and O
a O
possible O
continuation O
( O
sentence O
B O
) O
. O

The O
only O
task O
- O
speciﬁc O
parameters O
introduced O
is O
a O
vec- O
tor O
whose O
dot O
product O
with O
the O
[ O
CLS O
] O
token O
rep- O

resentation O
Cdenotes B-MetricName
a O
score O
for O
each O
choice O
which O
is O
normalized O
with O
a O
softmax B-HyperparameterName
layer I-HyperparameterName
. O

We O
ﬁne O
- O
tune O
the O
model O
for O
3 O
epochs O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
2e-5 B-HyperparameterValue
and O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
16 B-HyperparameterValue
. O

Re- O
sults O
are O
presented O
in O
Table O
4 O
. O

BERT B-MethodName
LARGE I-MethodName
out- O
performs O
the O
authors O
’ O
baseline O
ESIM+ELMo B-MethodName
sys- O
tem O
by O
+27.1% B-MetricValue
and O
OpenAI B-MethodName
GPT I-MethodName
by O
8.3% B-MetricValue
. O

5 O
Ablation O
Studies O

In O
this O
section O
, O
we O
perform O
ablation O
experiments O
over O
a O
number O
of O
facets O
of O
BERT B-MethodName
in O
order O
to O
better O
understand O
their O
relative O
importance O
. O

Additional O
ablation O
studies O
can O
be O
found O
in O
Appendix O
C O
. O

5.1 O
Effect O
of O
Pre O
- O
training O
Tasks O
We O
demonstrate O
the O
importance O
of O
the O
deep O
bidi- O
rectionality O
of O
BERT B-MethodName
by O
evaluating O
two O
pre- O
training O
objectives O
using O
exactly O
the O
same O
pre- O
training O
data O
, O
ﬁne O
- O
tuning O
scheme O
, O
and O
hyperpa- O
rameters O
as O
BERT B-MethodName
BASE O
: O
No O
NSP O
: O
A O
bidirectional O
model O
which O
is O
trained O
using O
the O
“ O
masked O
LM O
” O
( O
MLM O
) O
but O
without O
the O
“ O
next O
sentence O
prediction O
” O
( O
NSP B-TaskName
) O
task O
. O

LTR B-MethodName
& I-MethodName
No I-MethodName
NSP I-MethodName
: O
A O
left O
- O
context O
- O
only O
model O
which O
is O
trained O
using O
a O
standard O
Left O
- O
to O
- O
Right O
( O
LTR B-TaskName
) O
LM O
, O
rather O
than O
an O
MLM O
. O

The O
left O
- O
only O
constraint O
was O
also O
applied O
at O
ﬁne O
- O
tuning O
, O
because O
removing O
it O
introduced O
a O
pre O
- O
train/ﬁne O
- O
tune O
mismatch O
that O
degraded O
downstream O
performance O
. O

Additionally O
, O
this O
model O
was O
pre O
- O
trained O
without O
the O
NSP B-TaskName
task O
. O

This O
is O
directly O
comparable O
to O
OpenAI B-MethodName
GPT I-MethodName
, O
but O
using O
our O
larger O
training O
dataset O
, O
our O
input O
repre- O
sentation O
, O
and O
our O
ﬁne O
- O
tuning O
scheme O
. O

We O
ﬁrst O
examine O
the O
impact O
brought O
by O
the O
NSP B-TaskName
task O
. O

In O
Table O
5 O
, O
we O
show O
that O
removing O
NSP B-TaskName
hurts O
performance O
signiﬁcantly O
on O
QNLI B-DatasetName
, O
MNLI B-DatasetName
, O
and O
SQuAD B-DatasetName
1.1 O
. O

Next O
, O
we O
evaluate O
the O
impact O
of O
training O
bidirectional O
representations O
by O
com- O
paring O
“ O
No O
NSP O
” O
to O
“ O
LTR B-MethodName
& O
No I-MethodName
NSP O
” O
. O

The O
LTR B-MethodName
model O
performs O
worse O
than O
the O
MLM B-MethodName
model O
on O
all O
tasks O
, O
with O
large O
drops O
on O
MRPC B-MethodName
and O
SQuAD B-MethodName
. O

For O
SQuAD B-MethodName
it O
is O
intuitively O
clear O
that O
a O
LTR B-MethodName
model O
will O
perform O
poorly O
at O
token O
predictions O
, O
since O
the O
token O
- O
level O
hidden O
states O
have O
no O
right- O
side O
context O
. O

In O
order O
to O
make O
a O
good O
faith O
at- O
tempt O
at O
strengthening O
the O
LTR B-MethodName
system O
, O
we O
added O
a O
randomly O
initialized O
BiLSTM B-MethodName
on O
top O
. O

This O
does O
signiﬁcantly O
improve O
results O
on O
SQuAD B-MethodName
, O
but O
theresults O
are O
still O
far O
worse O
than O
those O
of O
the O
pre- O
trained O
bidirectional O
models O
. O

The O
BiLSTM B-MethodName
hurts O
performance O
on O
the O
GLUE B-TaskName
tasks O
. O

We O
recognize O
that O
it O
would O
also O
be O
possible O
to O
train O
separate O
LTR B-MethodName
and O
RTL B-MethodName
models O
and O
represent O
each O
token O
as O
the O
concatenation O
of O
the O
two O
mod- O
els O
, O
as O
ELMo B-MethodName
does O
. O

However O
: O
( O
a O
) O
this O
is O
twice O
as O
expensive O
as O
a O
single O
bidirectional O
model O
; O
( O
b O
) O
this O
is O
non O
- O
intuitive O
for O
tasks O
like O
QA B-TaskName
, O
since O
the O
RTL B-MethodName
model O
would O
not O
be O
able O
to O
condition O
the O
answer O
on O
the O
question O
; O
( O
c O
) O
this O
it O
is O
strictly O
less O
powerful O
than O
a O
deep O
bidirectional O
model O
, O
since O
it O
can O
use O
both O
left O
and O
right O
context O
at O
every O
layer O
. O

5.2 O
Effect O
of O
Model O
Size O
In O
this O
section O
, O
we O
explore O
the O
effect O
of O
model O
size O
on O
ﬁne O
- O
tuning O
task O
accuracy O
. O

We O
trained O
a O
number O
of O
BERT B-MethodName
models O
with O
a O
differing O
number O
of O
layers I-HyperparameterName
, O
hidden O
units O
, O
and O
attention O
heads O
, O
while O
otherwise O
using O
the O
same O
hyperparameters O
and O
training O
pro- O
cedure O
as O
described O
previously O
. O

Results O
on O
selected O
GLUE B-TaskName
tasks O
are O
shown O
in O
Table O
6 O
. O

In O
this O
table O
, O
we O
report O
the O
average O
Dev B-MetricName
Set I-MetricName
accuracy O
from O
5 B-HyperparameterValue
random O
restarts O
of O
ﬁne O
- O
tuning O
. O

We O
can O
see O
that O
larger O
models O
lead O
to O
a O
strict O
ac- O
curacy O
improvement O
across O
all O
four O
datasets O
, O
even O
for O
MRPC B-MethodName
which O
only O
has O
3,600 O
labeled O
train- O
ing O
examples O
, O
and O
is O
substantially O
different O
from O
the O
pre O
- O
training O
tasks O
. O

It O
is O
also O
perhaps O
surpris- O
ing O
that O
we O
are O
able O
to O
achieve O
such O
signiﬁcant O
improvements O
on O
top O
of O
models O
which O
are O
al- O
ready O
quite O
large O
relative O
to O
the O
existing O
literature O
. O

For O
example O
, O
the O
largest O
Transformer O
explored O
in O
Vaswani O
et O
al O
. O

( O
2017 O
) O
is O
( O
L B-HyperparameterName
= O
6 B-HyperparameterValue
, O
H B-HyperparameterName
= O
1024 B-HyperparameterValue
, O
A B-HyperparameterName
= O
16 B-HyperparameterValue
) O
with O
100M B-HyperparameterValue
parameters O
for O
the O
encoder O
, O
and O
the O
largest O
Transformer O
we O
have O
found O
in O
the O
literature O
is O
( O
L B-HyperparameterName
= O
64 B-HyperparameterValue
, O
H B-HyperparameterName
= O
512 B-HyperparameterValue
, O
A B-HyperparameterName
= O
2 B-HyperparameterValue
) O
with O
235M B-HyperparameterValue
parameters O
( O
Al O
- O
Rfou O
et O
al O
. O
, O
2018 O
) O
. O

By O
contrast O
, O
BERT B-MethodName
BASE I-MethodName
contains O
110M B-HyperparameterValue
parameters O
and O
BERT B-MethodName
LARGE I-MethodName
con- O
tains O
340M B-HyperparameterValue
parameters O
. O

It O
has O
long O
been O
known O
that O
increasing O
the O
model O
size O
will O
lead O
to O
continual O
improvements O
on O
large O
- O
scale O
tasks O
such O
as O
machine B-TaskName
translation I-TaskName
and O
language O
modeling O
, O
which O
is O
demonstrated O
by O
the O
LM B-TaskName
perplexity O
of O
held O
- O
out O
training O
data O
shown O
in O
Table O
6 O
. O

However O
, O
we O
believe O
that O
this O
is O
the O
ﬁrst O
work O
to O
demonstrate O
convinc- O
ingly O
that O
scaling O
to O
extreme O
model O
sizes O
also O
leads O
to O
large O
improvements O
on O
very O
small O
scale O
tasks O
, O
provided O
that O
the O
model O
has O
been O
sufﬁ- O
ciently O
pre O
- O
trained O
. O

Peters O
et O
al O
. O
( O
2018b O
) O
presented O
mixed O
results O
on O
the O
downstream O
task O
impact O
of O
increasing O
the O
pre O
- O
trained O
bi B-TaskName
- O
LM O
size O
from O
two B-HyperparameterValue
to O
four B-HyperparameterValue
layers O
and O
Melamud O
et O
al O
. O

( O
2016 O
) O

men- O
tioned O
in O
passing O
that O
increasing O
hidden O
dimen- B-HyperparameterName
sion I-HyperparameterName
size I-HyperparameterName
from O
200 B-HyperparameterValue
to O
600 B-HyperparameterValue
helped O
, O
but O
increasing O
further O
to O
1,000 B-HyperparameterValue
did O
not O
bring O
further O
improve- O

ments O
. O

Both O
of O
these O
prior O
works O
used O
a O
feature- O
based O
approach O
— O
we O
hypothesize O
that O
when O
the O
model O
is O
ﬁne O
- O
tuned O
directly O
on O
the O
downstream O
tasks O
and O
uses O
only O
a O
very O
small O
number O
of O
ran- O
domly O
initialized O
additional O
parameters O
, O
the O
task- O
speciﬁc O
models O
can O
beneﬁt O
from O
the O
larger O
, O
more O
expressive O
pre O
- O
trained O
representations O
even O
when O
downstream O
task O
data O
is O
very O
small O
. O

5.3 O
Feature O
- O
based O
Approach O
with O
BERT B-MethodName
All O
of O
the O
BERT B-MethodName
results O
presented O
so O
far O
have O
used O
the O
ﬁne O
- O
tuning O
approach O
, O
where O
a O
simple O
classiﬁ- O
cation O
layer O
is O
added O
to O
the O
pre O
- O
trained O
model O
, O
and O
all O
parameters O
are O
jointly O
ﬁne O
- O
tuned O
on O
a O
down- O
stream O
task O
. O

However O
, O
the O
feature O
- O
based O
approach O
, O
where O
ﬁxed O
features O
are O
extracted O
from O
the O
pre- O
trained O
model O
, O
has O
certain O
advantages O
. O

First O
, O
not O
all O
tasks O
can O
be O
easily O
represented O
by O
a O
Trans- O
former O
encoder O
architecture O
, O
and O
therefore O
require O
a O
task O
- O
speciﬁc O
model O
architecture O
to O
be O
added O
. O

Second O
, O
there O
are O
major O
computational O
beneﬁts O
to O
pre O
- O
compute O
an O
expensive O
representation O
of O
the O
training O
data O
once O
and O
then O
run O
many O
experiments O
with O
cheaper O
models O
on O
top O
of O
this O
representation O
. O

In O
this O
section O
, O
we O
compare O
the O
two O
approaches O
by O
applying O
BERT B-MethodName
to O
the O
CoNLL-2003 B-DatasetName
Named O
Entity I-MethodName
Recognition I-TaskName
( O
NER B-TaskName
) O
task O
( O
Tjong O
Kim O
Sang O
and O
De O
Meulder O
, O
2003 O
) O
. O

In O
the O
input O
to O
BERT B-MethodName
, O
we O
use O
a O
case O
- I-MethodName
preserving O
WordPiece I-MethodName
model O
, O
and O
we O
include O
the O
maximal O
document O
context O
provided O
by O
the O
data O
. O

Following O
standard O
practice O
, O
we O
for- O
mulate O
this O
as O
a O
tagging O
task O
but O
do O
not O
use O
a O
CRF O
layer O
in O
the O
output O
. O

We O
use O
the O
representation O
of O
the O
ﬁrst O
sub O
- O
token O
as O
the O
input O
to O
the O
token O
- O
level O
classiﬁer O
over O
the O
NER B-TaskName
label O
set O
. O

To O
ablate O
the O
ﬁne O
- O
tuning O
approach O
, O
we O
apply O
the O
feature O
- O
based O
approach O
by O
extracting O
the O
activa- O
tions O
from O
one O
or O
more O
layers O
without O
ﬁne O
- O
tuning O
any O
parameters O
of O
BERT B-MethodName
. O

These O
contextual O
em- O
beddings O
are O
used O
as O
input O
to O
a O
randomly O
initial- O
ized O
two O
- O
layer O
768 O
- O
dimensional O
BiLSTM B-MethodName
before O
the O
classiﬁcation O
layer O
. O

Results O
are O
presented O
in O
Table O
7 O
. O

BERT B-MethodName
LARGE I-MethodName
performs O
competitively O
with O
state O
- O
of O
- O
the O
- O
art O
meth- O
ods O
. O

The O
best O
performing O
method O
concatenates O
the O
token O
representations O
from O
the O
top O
four O
hidden O
lay- O
ers O
of O
the O
pre O
- O
trained O
Transformer O
, O
which O
is O
only O
0.3 B-MetricValue
F1 I-MetricValue
behind O
ﬁne O
- O
tuning O
the O
entire O
model O
. O

This O
demonstrates O
that O
BERT B-MethodName
is O
effective O
for O
both O
ﬁne- O
tuning O
and O
feature O
- O
based O
approaches O
. O

6 O
Conclusion O
Recent O
empirical O
improvements O
due O
to O
transfer O
learning O
with O
language O
models O
have O
demonstrated O
that O
rich O
, O
unsupervised O
pre O
- O
training O
is O
an O
integral O
part O
of O
many O
language O
understanding O
systems O
. O

In O
particular O
, O
these O
results O
enable O
even O
low O
- O
resource O
tasks O
to O
beneﬁt O
from O
deep O
unidirectional O
architec- O
tures O
. O

Our O
major O
contribution O
is O
further O
general- O
izing O
these O
ﬁndings O
to O
deep O
bidirectional O
architec- O
tures O
, O
allowing O
the O
same O
pre O
- O
trained O
model O
to O
suc- O
cessfully O
tackle O
a O
broad O
set O
of O
NLP B-TaskName
tasks O
. O

References O
Alan O
Akbik O
, O
Duncan O
Blythe O
, O
and O
Roland B-MetricName
V O
ollgraf O
. O

2018 O
. O

Contextual O
string O
embeddings O
for O
sequence O
labeling O
. O

In O
Proceedings O
of O
the O
27th O
International I-DatasetName
Conference O
on O
Computational O
Linguistics I-TaskName
, O
pages O
1638–1649 O
. O

Rami B-MetricName
Al O
- O
Rfou O
, O
Dokook B-MetricName
Choe O
, O
Noah B-MetricName
Constant O
, O
Mandy B-MetricName
Guo O
, O
and O
Llion B-MetricName
Jones O
. O

2018 O
. O

Character O
- O
level O
lan- O
guage O
modeling O
with O
deeper O
self O
- O
attention O
. O

arXiv O
preprint O
arXiv:1808.04444 O
. O

Rie B-MetricName
Kubota O
Ando O
and O
Tong O
Zhang O
. O

2005 O
. O

A O
framework O
for O
learning O
predictive O
structures O
from O
multiple O
tasks O
and O
unlabeled O
data O
. O

Journal B-MethodName
of I-MethodName
Machine I-MethodName
Learning I-MethodName
Research I-MethodName
, O
6(Nov):1817–1853 O
. O

Luisa B-MetricName
Bentivogli O
, O
Bernardo B-MetricName
Magnini O
, O
Ido B-MetricName
Dagan O
, O
Hoa B-MetricName
Trang O
Dang O
, O
and O
Danilo B-MetricName
Giampiccolo O
. O

2009 O
. O

The O
ﬁfth O
PASCAL B-MethodName
recognizing O
textual O
entailment O
challenge O
. O

In O
TAC B-MethodName
. O

NIST B-MethodName
. O

John B-MetricName
Blitzer O
, O
Ryan O
McDonald O
, O
and O
Fernando O
Pereira O
. O
2006 O
. O

Domain B-TaskName
adaptation I-TaskName
with O
structural O
correspon- O
dence O
learning O
. O

In O
Proceedings O
of O
the O
2006 O
confer- O
ence O
on O
empirical O
methods O
in O
natural B-TaskName
language I-TaskName
pro- I-TaskName

cessing O
, O
pages O
120–128 O
. O

Association B-MethodName
for I-MethodName
Computa- I-MethodName
tional I-MethodName
Linguistics I-MethodName
. O

Samuel O
R. O
Bowman O
, O
Gabor B-MetricName
Angeli O
, O
Christopher O
Potts O
, O
and O
Christopher O
D. O
Manning O
. O

2015 O
. O

A O
large O
anno- O
tated O
corpus O
for O
learning O
natural B-TaskName
language I-TaskName
inference I-TaskName
. O

InEMNLP B-MethodName
. O

Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguis- I-MethodName
tics I-MethodName
. O

Peter O
F O
Brown O
, O
Peter O
V O
Desouza O
, O
Robert O
L O
Mercer O
, O
Vincent O
J O
Della O
Pietra O
, O
and O
Jenifer O
C O
Lai O
. O

1992 O
. O

Class O
- O
based O
n O
- O
gram O
models O
of O
natural O
language O
. O

Computational B-TaskName
linguistics I-TaskName
, O
18(4):467–479 O
. O

Daniel B-MetricName
Cer O
, O
Mona B-MetricName
Diab O
, O
Eneko B-MetricName
Agirre O
, O
Inigo B-MetricName
Lopez- B-DatasetName
Gazpio O
, O
and O
Lucia O
Specia O
. O

2017 O
. O

Semeval-2017 B-TaskName
task O
1 O
: O
Semantic B-TaskName
textual I-TaskName
similarity I-TaskName
multilingual O
and O
crosslingual B-TaskName
focused O
evaluation O
. O

In O
Proceedings O
of O
the O
11th O
International O
Workshop O
on O
Semantic B-TaskName
Evaluation I-TaskName
( O
SemEval-2017 B-TaskName
) O
, O
pages O
1–14 O
, O
Vancou- O
ver O
, O
Canada O
. O

Association B-MethodName
for I-MethodName
Computational I-MethodName
Lin- I-MethodName
guistics I-MethodName
. O

Ciprian B-MetricName
Chelba O
, O
Tomas B-MetricName
Mikolov O
, O
Mike O
Schuster O
, O
Qi O
Ge O
, O
Thorsten B-MetricName
Brants O
, O
Phillipp B-MetricName
Koehn O
, O
and O
Tony O
Robin- O
son O
. O
2013 O
. O

One O
billion O
word O
benchmark O
for O
measur- O
ing O
progress O
in O
statistical O
language O
modeling O
. O

arXiv O
preprint O
arXiv:1312.3005 O
. O

Z. O
Chen O
, O
H. O
Zhang O
, O
X. O
Zhang O
, O
and O
L. O
Zhao O
. O

2018 O
. O

Quora O
question O
pairs O
. O

Christopher B-MetricName
Clark O
and O
Matt B-MetricName
Gardner O
. O

2018 O
. O

Simple O
and O
effective O
multi O
- O
paragraph O
reading O
comprehen- O
sion O
. O

In O
ACL.Kevin B-TaskName
Clark O
, O
Minh O
- O
Thang O
Luong O
, O
Christopher O
D O
Man- O
ning O
, O
and O
Quoc O
Le O
. O
2018 O
. O

Semi O
- O
supervised O
se- B-TaskName
quence I-TaskName
modeling I-TaskName
with O
cross B-TaskName
- I-TaskName
view I-TaskName
training O
. O

In O
Pro- O
ceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Meth- O
ods O
in O
Natural O
Language I-MethodName
Processing I-MethodName
, O
pages O
1914 O
– O
1925 O
. O

Ronan B-MetricName
Collobert O
and O
Jason B-MetricName
Weston O
. O

2008 O
. O

A O
uniﬁed O
architecture O
for O
natural B-TaskName
language I-TaskName
processing I-TaskName
: O

Deep O
neural O
networks O
with O
multitask B-TaskName
learning O
. O

In O
Pro- O
ceedings O
of O
the O
25th O
international O
conference O
on O
Machine B-TaskName
learning I-TaskName
, O
pages O
160–167 O
. O
ACM B-MethodName
. O

Alexis B-MetricName
Conneau O
, O
Douwe B-MetricName
Kiela O
, O
Holger B-MetricName
Schwenk O
, O
Lo B-MetricName
¨ıc O
Barrault O
, O
and O
Antoine B-MetricName
Bordes O
. O

2017 O
. O

Supervised O
learning O
of O
universal O
sentence O
representations O
from O
natural O
language O
inference O
data O
. O

In O
Proceedings O
of O
the O
2017 O
Conference O
on O
Empirical O
Methods O
in O
Nat- B-TaskName
ural I-TaskName
Language I-TaskName
Processing I-TaskName
, O
pages O
670–680 O
, O
Copen- O
hagen O
, O
Denmark O
. O

Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
. O

Andrew O
M O
Dai O
and O
Quoc O
V O
Le O
. O
2015 O
. O

Semi O
- O
supervised O
sequence O
learning O
. O

In O
Advances O
in O
neural O
informa- O
tion I-TaskName
processing O
systems O
, O
pages O
3079–3087 O
. O

J. O
Deng O
, O
W. O
Dong O
, O
R. O
Socher O
, O
L.-J. O
Li O
, O
K. O
Li O
, O
and O
L. O
Fei- O
Fei O
. O
2009 O
. O

ImageNet B-MethodName
: O

A O
Large B-MethodName
- I-MethodName
Scale I-MethodName
Hierarchical I-MethodName
Image I-MethodName
Database I-MethodName
. O

In O
CVPR09 B-DatasetName
. O

William O
B O
Dolan O
and O
Chris B-MetricName
Brockett O
. O

2005 O
. O

Automati- O
cally O
constructing O
a O
corpus O
of O
sentential O
paraphrases O
. O

InProceedings O
of O
the O
Third O
International O
Workshop O
on O
Paraphrasing B-TaskName
( O
IWP2005 B-DatasetName
) O
. O

William O
Fedus O
, O
Ian O
Goodfellow O
, O
and O
Andrew O
M O
Dai O
. O
2018 O
. O

Maskgan B-MethodName
: O
Better O
text B-TaskName
generation I-TaskName
via O
ﬁlling O
in O
the.arXiv O
preprint O
arXiv:1801.07736 O
. O

Dan B-MetricName
Hendrycks B-DatasetName
and O
Kevin B-MetricName
Gimpel O
. O

2016 O
. O

Bridging O
nonlinearities O
and O
stochastic O
regularizers O
with O
gaus- O
sian O
error O
linear O
units O
. O

CoRR B-TaskName
, O
abs/1606.08415 O
. O

Felix B-MetricName
Hill O
, O
Kyunghyun B-MetricName
Cho O
, O
and O
Anna B-MetricName
Korhonen O
. O

2016 O
. O

Learning O
distributed O
representations O
of O
sentences O
from O
unlabelled O
data O
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
of O
the O
North B-TaskName
American I-DatasetName
Chapter O
of O
the O
Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
: O
Human B-MethodName
Language I-MethodName
Technologies I-MethodName
. O

Association B-MethodName
for I-MethodName
Computa- I-MethodName
tional I-MethodName
Linguistics I-MethodName
. O

Jeremy B-MetricName
Howard O
and O
Sebastian B-MetricName
Ruder O
. O

2018 O
. O

Universal O
language O
model O
ﬁne O
- O
tuning O
for O
text O
classiﬁcation I-TaskName
. O

In O
ACL B-TaskName
. O

Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
. O

Minghao B-MetricName
Hu O
, O
Yuxing O
Peng O
, O
Zhen O
Huang O
, O
Xipeng O
Qiu O
, O
Furu B-MetricName
Wei O
, O
and O
Ming O
Zhou O
. O

2018 O
. O

Reinforced O
mnemonic O
reader O
for O
machine B-TaskName
reading I-TaskName
comprehen- O
sion O
. O

In O
IJCAI B-MethodName
. O

Yacine B-MetricName
Jernite O
, O
Samuel O
R. O
Bowman O
, O
and O
David O
Son- O
tag O
. O

2017 O
. O

Discourse O
- O
based O
objectives O
for O
fast O
un- O
supervised O
sentence O
representation O
learning O
. O

CoRR B-TaskName
, O
abs/1705.00557 O
. O

4181Mandar O
Joshi O
, O
Eunsol B-MetricName
Choi O
, O
Daniel O
S O
Weld O
, O
and O
Luke O
Zettlemoyer O
. O
2017 O
. O

Triviaqa B-MethodName
: O

A O
large O
scale O
distantly O
supervised O
challenge O
dataset O
for O
reading O
comprehen- O
sion O
. O

In O
ACL B-TaskName
. O

Ryan B-MetricName
Kiros O
, O
Yukun B-MetricName
Zhu O
, O
Ruslan B-MetricName
R O
Salakhutdinov O
, O
Richard O
Zemel O
, O
Raquel B-MetricName
Urtasun O
, O
Antonio O
Torralba O
, O
and O
Sanja B-MetricName
Fidler O
. O

2015 O
. O

Skip O
- O
thought O
vectors O
. O

In O
Advances O
in O
neural O
information O
processing O
systems O
, O
pages O
3294–3302 O
. O

Quoc O
Le O
and O
Tomas B-MetricName
Mikolov O
. O

2014 O
. O

Distributed O
rep- O
resentations O
of O
sentences O
and O
documents O
. O

In O
Inter- B-DatasetName
national I-DatasetName
Conference I-DatasetName
on I-DatasetName
Machine I-DatasetName
Learning I-DatasetName
, O
pages O
1188–1196 O
. O

Hector O
J O
Levesque O
, O
Ernest O
Davis O
, O
and O
Leora B-MetricName
Morgen- O
stern O
. O

2011 O
. O

The O
winograd O
schema O
challenge O
. O

In O
Aaai O
spring O
symposium O
: O
Logical O
formalizations O
of O
commonsense O
reasoning O
, O
volume O
46 O
, O
page O
47 O
. O

Lajanugen B-MetricName
Logeswaran O
and O
Honglak B-MetricName
Lee O
. O
2018 O
. O

An O
efﬁcient O
framework O
for O
learning O
sentence O
represen- O

tations O
. O

In O
International B-DatasetName
Conference I-DatasetName
on O
Learning O
Representations I-TaskName
. O

Bryan O
McCann O
, O
James O
Bradbury O
, O
Caiming B-MetricName
Xiong O
, O
and O
Richard O
Socher O
. O

2017 O
. O

Learned O
in O
translation O
: O
Con- O
textualized O
word O
vectors O
. O

In O
NIPS B-MethodName
. O

Oren B-MetricName
Melamud O
, O
Jacob B-MetricName
Goldberger O
, O
and O
Ido B-MetricName
Dagan B-DatasetName
. O

2016 O
. O

context2vec B-TaskName
: O
Learning O
generic O
context O
em- O
bedding O
with O
bidirectional O
LSTM B-TaskName
. O

In O
CoNLL B-TaskName
. O

Tomas B-MetricName
Mikolov O
, O
Ilya B-MetricName
Sutskever O
, O
Kai B-MetricName
Chen O
, O
Greg O
S O
Cor- O
rado O
, O
and O
Jeff O
Dean O
. O

2013 O
. O

Distributed O
representa- O
tions O
of O
words O
and O
phrases O
and O
their O
compositional- O
ity O
. O

In O
Advances O
in O
Neural B-TaskName
Information I-TaskName
Processing I-TaskName
Systems I-MethodName
26 O
, O
pages O
3111–3119 O
. O

Curran B-MethodName
Associates I-MethodName
, I-MethodName
Inc. I-MethodName
Andriy B-MetricName
Mnih O
and O
Geoffrey O
E O
Hinton O
. O

2009 O
. O

A O
scal- B-MethodName
able I-MethodName
hierarchical I-MethodName
distributed I-MethodName
language O
model O
. O

In O
D. O
Koller O
, O
D. O
Schuurmans O
, O
Y O
. O

Bengio O
, O
and O
L. O
Bot- O
tou O
, O
editors O
, O
Advances B-MethodName
in I-MethodName
Neural I-MethodName
Information I-MethodName
Pro- I-MethodName

cessing B-MethodName
Systems O
21 O
, O
pages O
1081–1088 O
. O

Curran B-MethodName
As- I-MethodName
sociates I-MethodName
, I-MethodName
Inc. I-MethodName

Ankur B-MetricName
P O
Parikh O
, O
Oscar O
T O
¨ackstr O
¨om O
, O
Dipanjan B-MetricName
Das O
, O
and O
Jakob B-MetricName
Uszkoreit O
. O

2016 O
. O

A O
decomposable O
attention O
model O
for O
natural B-TaskName
language I-TaskName
inference I-TaskName
. O

In O
EMNLP B-TaskName
. O

Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christo- O
pher O
D. O
Manning O
. O

2014 O
. O

Glove O
: O
Global O
vectors O
for O
word O
representation O
. O

In O
Empirical O
Methods O
in I-MethodName
Nat- I-MethodName
ural I-MethodName
Language I-MethodName
Processing I-MethodName
( O
EMNLP B-MethodName
) O
, O
pages O
1532 O
– O
1543 O
. O

Matthew O
Peters O
, O
Waleed B-MetricName
Ammar O
, O
Chandra O
Bhagavat- O
ula O
, O
and O
Russell O
Power O
. O
2017 O
. O

Semi O
- O
supervised O
se- B-TaskName
quence I-TaskName
tagging I-TaskName
with O
bidirectional O
language O
models O
. O

InACL B-MethodName
. O

Matthew B-MetricName
Peters O
, O
Mark O
Neumann O
, O
Mohit B-MetricName
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton B-MetricName
Lee O
, O
and O
Luke B-MetricName
Zettlemoyer O
. O

2018a O
. O

Deep O
contextualized O
word O
rep- O
resentations O
. O

In O
NAACL B-MethodName
.Matthew O

Peters O
, O
Mark O
Neumann O
, O
Luke O
Zettlemoyer O
, O
and O
Wen O
- O
tau O
Yih O
. O
2018b O
. O

Dissecting O
contextual O
word O
embeddings O
: O
Architecture O
and O
representation O
. O

InProceedings O
of O
the O
2018 O
Conference O
on O
Empiri- O
cal I-MetricName
Methods O
in O
Natural B-TaskName
Language I-TaskName
Processing I-TaskName
, O
pages O
1499–1509 O
. O

Alec B-MetricName
Radford O
, O
Karthik B-MetricName
Narasimhan O
, O
Tim B-MetricName
Salimans O
, O
and O
Ilya B-MetricName
Sutskever O
. O
2018 O
. O

Improving O
language O
under- O
standing O
with O
unsupervised O
learning O
. O

Technical O
re- O
port O
, O
OpenAI B-MethodName
. O

Pranav B-MetricName
Rajpurkar O
, O
Jian B-MetricName
Zhang O
, O
Konstantin O
Lopyrev O
, O
and O
Percy B-MetricName
Liang O
. O

2016 O
. O

Squad O
: O
100,000 B-HyperparameterValue
+ I-HyperparameterValue
questions O
for O
machine O
comprehension O
of O
text O
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Nat- B-TaskName
ural I-TaskName
Language I-TaskName
Processing I-TaskName
, O
pages O
2383–2392 O
. O

Minjoon B-MetricName
Seo O
, O
Aniruddha B-MetricName
Kembhavi O
, O
Ali O
Farhadi O
, O
and O
Hannaneh B-MetricName
Hajishirzi O
. O
2017 O
. O

Bidirectional O
attention O
ﬂow O
for O
machine B-TaskName
comprehension I-TaskName
. O

In O
ICLR B-MethodName
. O

Richard O
Socher O
, O
Alex O
Perelygin O
, O
Jean O
Wu O
, O
Jason O
Chuang O
, O
Christopher O
D O
Manning O
, O
Andrew O
Ng O
, O
and O
Christopher O
Potts O
. O
2013 O
. O

Recursive O
deep O
models O
for O
semantic O
compositionality O
over O
a O
sentiment O
tree- O
bank O
. O

In O
Proceedings O
of O
the O
2013 O
conference O
on O
empirical O
methods O
in O
natural B-TaskName
language I-TaskName
processing I-TaskName
, O
pages O
1631–1642 O
. O

Fu B-MetricName
Sun O
, O
Linyang B-MetricName
Li O
, O
Xipeng O
Qiu O
, O
and O
Yang O
Liu O
. O

2018 O
. O

U B-MethodName
- O
net O
: O
Machine B-TaskName
reading I-TaskName
comprehension I-TaskName
with O
unanswerable O
questions O
. O

arXiv O
preprint O
arXiv:1810.06638 O
. O

Wilson O
L O
Taylor O
. O

1953 O
. O

Cloze B-MetricName
procedure O
: O
A O
new O
tool O
for O
measuring O
readability O
. O

Journalism B-MethodName
Bulletin I-MethodName
, O
30(4):415–433 O
. O

Erik O
F O
Tjong O
Kim O
Sang O
and O
Fien B-MetricName
De O
Meulder O
. O

2003 O
. O

Introduction O
to O
the O
conll-2003 O
shared O
task O
: O
Language B-TaskName
- I-TaskName
independent I-TaskName
named I-TaskName
entity I-TaskName
recognition I-TaskName
. O

In O
CoNLL B-TaskName
. O

Joseph B-MetricName
Turian O
, O
Lev B-MetricName
Ratinov O
, O
and O
Yoshua B-MetricName
Bengio O
. O

2010 O
. O

Word O
representations O
: O
A O
simple O
and O
general O
method O
for O
semi B-TaskName
- I-TaskName
supervised I-TaskName
learning O
. O

In O
Proceedings O
of O
the O
48th O
Annual O
Meeting O
of O
the O
Association B-MethodName
for I-MethodName
Compu- I-MethodName
tational I-MethodName
Linguistics I-MethodName
, O
ACL B-DatasetName
’ I-MethodName
10 O
, O
pages O
384–394 O
. O

Ashish B-MetricName
Vaswani O
, O
Noam B-MetricName
Shazeer O
, O
Niki B-MetricName
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion B-MetricName
Jones O
, O
Aidan O
N O
Gomez O
, O
Lukasz B-MetricName
Kaiser O
, O
and O
Illia B-MetricName
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
Advances O
in O
Neural B-TaskName
Information I-TaskName
Pro- I-TaskName
cessing I-TaskName
Systems I-MethodName
, O
pages O
6000–6010 O
. O

Pascal B-MetricName
Vincent O
, O
Hugo O
Larochelle O
, O
Yoshua B-MetricName
Bengio O
, O
and O
Pierre B-MetricName
- O
Antoine O
Manzagol O
. O
2008 O
. O

Extracting O
and O
composing O
robust O
features O
with O
denoising O
autoen- O
coders O
. O

In O
Proceedings O
of O
the O
25th O
international O
conference O
on O
Machine B-TaskName
learning I-TaskName
, O
pages O
1096–1103 O
. O
ACM B-MethodName
. O

Alex B-MetricName
Wang O
, O
Amanpreet B-MetricName
Singh O
, O
Julian B-MetricName
Michael O
, O
Fe- B-MethodName
lix O
Hill O
, O
Omer B-MetricName
Levy O
, O
and O
Samuel O
Bowman O
. O

2018a O
. O

Glue O
: O
A O
multi O
- O
task O
benchmark O
and O
analysis O
platform O

4182for O
natural B-TaskName
language I-TaskName
understanding I-TaskName
. O

In O
Proceedings O
of O
the O
2018 O
EMNLP B-TaskName
Workshop O
BlackboxNLP B-TaskName
: O
An- O
alyzing O
and O
Interpreting O
Neural O
Networks O
for O
NLP B-TaskName
, O
pages O
353–355 O
. O

Wei O
Wang O
, O
Ming O
Yan O
, O
and O
Chen O
Wu O
. O
2018b O
. O

Multi- B-TaskName

granularity O
hierarchical O
attention O
fusion O
networks O
for O
reading B-TaskName
comprehension I-TaskName
and O
question B-TaskName
answering I-TaskName
. O

InProceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
As- O
sociation O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
. O

Association B-MethodName
for I-MethodName
Computational I-MethodName
Lin- I-MethodName
guistics I-MethodName
. O

Alex B-MetricName
Warstadt O
, O
Amanpreet B-MetricName
Singh O
, O
and O
Samuel B-MetricName
R O
Bow- O
man O
. O

2018 O
. O

Neural B-TaskName
network I-TaskName
acceptability O
judg- O
ments O
. O

arXiv O
preprint O
arXiv:1805.12471 O
. O

Adina B-MetricName
Williams O
, O
Nikita B-MetricName
Nangia O
, O
and O
Samuel B-MetricName
R O
Bow- O
man O
. O

2018 O
. O

A O
broad O
- O
coverage O
challenge O
corpus B-MetricName
for O
sentence O
understanding O
through O
inference O
. O

In O
NAACL B-MethodName
. O

Yonghui O
Wu O
, O
Mike O
Schuster O
, O
Zhifeng O
Chen O
, O
Quoc O
V O
Le O
, O
Mohammad O
Norouzi O
, O
Wolfgang O
Macherey O
, O
Maxim O
Krikun O
, O
Yuan O
Cao O
, O
Qin O
Gao O
, O
Klaus O
Macherey O
, O
et O
al O
. O
2016 O
. O

Google B-MethodName
’s O
neural B-TaskName
ma- O
chine O
translation O
system O
: O
Bridging O
the O
gap O
between O
human B-TaskName
and O
machine O
translation O
. O

arXiv O
preprint O
arXiv:1609.08144 O
. O

Jason B-MetricName
Yosinski O
, O
Jeff B-MetricName
Clune O
, O
Yoshua B-MetricName
Bengio O
, O
and O
Hod B-MetricName
Lipson O
. O

2014 O
. O

How O
transferable O
are O
features O
in O
deep O
neural O
networks O
? O

In O
Advances O
in O
neural O
information O
processing O
systems O
, O
pages O
3320–3328 O
. O

Adams O
Wei O
Yu O
, O
David O
Dohan O
, O
Minh O
- O
Thang O
Luong O
, O
Rui O
Zhao O
, O
Kai O
Chen O
, O
Mohammad O
Norouzi O
, O
and O
Quoc O
V O
Le O
. O

2018 O
. O

QANet O
: O

Combining O
local O
convolution O
with O
global O
self O
- O
attention O
for O
reading O
comprehen- O
sion O
. O

In O
ICLR B-MethodName
. O

Rowan B-MetricName
Zellers O
, O
Yonatan B-MetricName
Bisk O
, O
Roy O
Schwartz O
, O
and O
Yejin B-MetricName
Choi O
. O

2018 O
. O

Swag O
: O
A O
large O
- O
scale O
adversarial O
dataset O
for O
grounded O
commonsense O
inference O
. O

In O
Proceed- O
ings O
of O
the O
2018 O
Conference O
on I-MethodName
Empirical I-MethodName
Methods I-MethodName
in I-MethodName
Natural I-MethodName
Language I-MethodName
Processing I-MethodName
( O
EMNLP B-MethodName
) O
. O

Yukun B-MetricName
Zhu O
, O
Ryan B-MetricName
Kiros O
, O
Rich B-MetricName
Zemel O
, O
Ruslan B-MetricName
Salakhut- O
dinov O
, O
Raquel B-MetricName
Urtasun O
, O
Antonio O
Torralba O
, O
and O
Sanja B-MetricName
Fidler O
. O
2015 O
. O

Aligning O
books O
and O
movies O
: O
Towards O
story O
- O
like O
visual O
explanations O
by O
watching O
movies O
and O
reading O
books O
. O

In O
Proceedings O
of O
the O
IEEE B-MethodName
international O
conference O
on O
computer B-TaskName
vision I-TaskName
, O
pages O
19–27 O
. O

Appendix O
for O
“ O
BERT B-MethodName
: O
Pre O
- O
training O
of O
Deep B-MethodName
Bidirectional I-MethodName
Transformers I-MethodName
for O
Language B-TaskName
Understanding I-TaskName
” O
We O
organize O
the O
appendix O
into O
three O
sections O
: O

• O
Additional O
implementation O
details O
for O
BERT B-MethodName
are O
presented O
in O
Appendix O
A; O

• O
Additional O
details O
for O
our O
experiments O
are O
presented O
in O
Appendix O
B O
; O
and O

• O
Additional O
ablation O
studies O
are O
presented O
in O
Appendix O
C O
. O

We O
present O
additional O
ablation O
studies O
for O
BERT B-MethodName
including O
: O
– O
Effect O
of O
Number O
of O
Training O
Steps O
; O
and O
– O
Ablation O
for O
Different O
Masking O
Proce- O
dures O
. O

A O
Additional O
Details O
for O
BERT B-MethodName
A.1 I-MethodName
Illustration O
of O
the O
Pre O
- O
training O
Tasks O
We O
provide O
examples O
of O
the O
pre O
- O
training O
tasks O
in O
the O
following O
. O

Masked O
LM B-TaskName
and O
the O
Masking O
Procedure O
As- O
suming O
the O
unlabeled O
sentence O
is O
my O
dog O
is O
hairy O
, O
and O
during O
the O
random O
masking O
procedure O
we O
chose O
the O
4 O
- O
th O
token O
( O
which O
corresponding O
to O
hairy O
) O
, O
our O
masking O
procedure O
can O
be O
further O
il- O
lustrated O
by O
• O
80% B-HyperparameterValue
of O
the O
time O
: O

Replace O
the O
word O
with O
the O
[ O
MASK B-MetricName
] O
token O
, O
e.g. O
, O
my O
dog O
is O
hairy O
! O

my O
dog O
is O
[ O
MASK B-MetricName
] O
• O
10% B-HyperparameterValue
of O
the O
time O
: O

Replace O
the O
word O
with O
a O
random O
word O
, O
e.g. O
, O
my O
dog O
is O
hairy O
! O

my O
dog O
is O
apple O
• O
10% B-HyperparameterValue
of O
the O
time O
: O
Keep O
the O
word O
un- O
changed O
, O
e.g. O
, O
my O
dog O
is O
hairy O
! O

my O
dog O
is O
hairy O
. O

The O
purpose O
of O
this O
is O
to O
bias O
the O
representation O
towards O
the O
actual O
observed O
word O
. O

The O
advantage O
of O
this O
procedure O
is O
that O
the O
Transformer O
encoder O
does O
not O
know O
which O
words O
it O
will O
be O
asked O
to O
predict O
or O
which O
have O
been O
re- O
placed O
by O
random O
words O
, O
so O
it O
is O
forced O
to O
keep O
a O
distributional O
contextual O
representation O
of O
ev- O
eryinput O
token O
. O

Additionally O
, O
because O
random O
replacement O
only O
occurs O
for O
1.5% B-MetricValue
of O
all O
tokens O
( O
i.e. O
, O
10 B-MetricValue
% I-MetricValue
of O
15 B-MetricValue
% I-HyperparameterValue
) O
, O
this O
does O
not O
seem O
to O
harm O
the O
model O
’s O
language O
understanding O
capability O
. O

In O
Section O
C.2 O
, O
we O
evaluate O
the O
impact O
this O
proce- O
dure O
. O

Compared O
to O
standard O
langauge O
model O
training O
, O
the O
masked O
LM B-TaskName
only O
make O
predictions O
on O
15 B-HyperparameterValue
% O
of O
tokens O
in O
each O
batch O
, O
which O
suggests O
that O
more O
pre O
- O
training O
steps O
may O
be O
required O
for O
the O
model O
to O
converge O
. O

In O
Section O
C.1 O
we O
demonstrate O
that O
MLM B-MethodName
does O
converge O
marginally O
slower O
than O
a O
left- O
to O
- O
right O
model O
( O
which O
predicts O
every O
token O
) O
, O
but O
the O
empirical O
improvements O
of O
the O
MLM B-MethodName
model O
far O
outweigh O
the O
increased O
training O
cost O
. O

Next O
Sentence O
Prediction O
The O
next O
sentence O
prediction O
task O
can O
be O
illustrated O
in O
the O
following O
examples O
. O

Input O
=[ O
CLS O
] O
the O
man O
went O
to O
[ O
MASK O
] O
store O
[ O
SEP O
] O
he O
bought O
a O
gallon O
[ O
MASK O
] O
milk O

[ O
SEP O
] O
Label O
= O
IsNext O
Input O
=[ O
CLS O
] O
the O
man O
[ O
MASK O
] O
to O
the O
store O
[ O
SEP O
] O
penguin O

[ O
MASK B-MethodName
] O
are O
flight B-TaskName
# O
# O
less O
birds O

[ O
SEP O
] O
Label O
= O
NotNext O
A.2 O
Pre O
- O
training O
Procedure O
To O
generate O
each O
training O
input O
sequence O
, O
we O
sam- O
ple O
two O
spans O
of O
text O
from O
the O
corpus O
, O
which O
we O
refer O
to O
as O
“ O
sentences O
” O
even O
though O
they O
are O
typ- O
ically O
much O
longer O
than O
single O
sentences O
( O
but O
can O
be O
shorter O
also O
) O
. O

The O
ﬁrst O
sentence O
receives O
the O
A B-HyperparameterName
embedding O
and O
the O
second O
receives O
the O
Bembed- B-MetricName
ding O
. O

50% O
of O
the O
time O
Bis B-MetricName
the O
actual O
next O
sentence O
that O
follows O
Aand O
50% B-HyperparameterValue
of O
the O
time O
it O
is O
a O
random O
sentence O
, O
which O
is O
done O
for O
the O
“ O
next O
sentence O
pre- O
diction O
” O
task O
. O

They O
are O
sampled O
such O
that O
the O
com- B-HyperparameterName
bined I-HyperparameterName
length I-HyperparameterName
is O
<512 B-HyperparameterValue
tokens O
. O

The O
LM B-TaskName
masking O
is O
applied O
after O
WordPiece B-MethodName
tokenization O
with O
a O
uni- O
form O
masking O
rate O
of O
15% B-HyperparameterValue
, O
and O
no O
special O
consid- O
eration O
given O
to O
partial O
word O
pieces O
. O

We O
train O
with O
batch B-HyperparameterName
size I-HyperparameterName
of O
256 B-HyperparameterValue
sequences I-HyperparameterName
( O
256 B-HyperparameterValue
sequences I-HyperparameterName
* O
512 B-HyperparameterValue
tokens O
= O
128,000 B-HyperparameterValue
tokens O
/ O
batch B-HyperparameterName
) O
for O
1,000,000 B-HyperparameterValue
steps B-HyperparameterName
, O
which O
is O
approximately O
40 O
epochs O
over O
the O
3.3 B-HyperparameterValue
billion I-HyperparameterValue
word O
corpus O
. O

We O
use O
Adam O
with O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-4 B-HyperparameterValue
, O

1= O
0.9 B-HyperparameterValue
, O

2= O
0.999 B-HyperparameterValue
, O
L2 B-HyperparameterName
weight B-HyperparameterName
decay O
of O
0:01 B-HyperparameterValue
, O
learning B-HyperparameterName
rate I-HyperparameterName
warmup I-HyperparameterName
over O
the O
ﬁrst O
10,000 B-HyperparameterValue
steps B-HyperparameterName
, O
and O
linear O
decay O
of O
the O
learning B-HyperparameterName
rate I-HyperparameterName
. O

We O
use O
a O
dropout B-HyperparameterName
prob- I-HyperparameterName
ability O
of O
0.1 B-HyperparameterValue
on O
all O
layers O
. O

We O
use O
a O
gelu O
acti- O
vation O
( O
Hendrycks O
and O
Gimpel O
, O
2016 O
) O
rather O
than O
the O
standard O
relu O
, O
following O
OpenAI B-MethodName
GPT I-MethodName
. O

The O
training B-HyperparameterName
loss I-HyperparameterName
is O
the O
sum O
of O
the O
mean O
masked I-HyperparameterName
LM B-TaskName
likelihood I-HyperparameterName
and O
the O
mean O
next I-HyperparameterName
sentence O
prediction O
likelihood O
. O

Training O
of O
BERT B-MethodName
BASE I-MethodName
was O
performed O
on O
4 O
Cloud O
TPUs O
in O
Pod O
conﬁguration O
( O
16 O
TPU O
chips O
total).13Training O
of O
BERT B-MethodName
LARGE I-MethodName
was O
performed O
on O
16 O
Cloud O
TPUs O
( O
64 O
TPU O
chips O
total O
) O
. O

Each O
pre- O
training I-HyperparameterName
took O
4 B-HyperparameterValue
days I-HyperparameterName
to O
complete O
. O

Longer O
sequences O
are O
disproportionately O
expen- O
sive O
because O
attention O
is O
quadratic O
to O
the O
sequence O
length I-HyperparameterName
. O

To O
speed O
up O
pretraing O
in O
our O
experiments O
, O
we O
pre O
- O
train O
the O
model O
with O
sequence B-HyperparameterName
length I-HyperparameterName
of O
128 B-HyperparameterValue
for O
90% B-HyperparameterValue
of O
the O
steps B-HyperparameterName
. O

Then O
, O
we O
train O
the O
rest O
10% B-HyperparameterValue
of O
the O
steps O
of O
sequence O
of O
512 B-HyperparameterValue
to O
learn O
the O
positional O
embeddings O
. O

A.3 O
Fine O
- O
tuning O
Procedure O
For O
ﬁne B-TaskName
- O
tuning O
, O
most O
model O
hyperparameters O
are O
the O
same O
as O
in O
pre O
- O
training O
, O
with O
the O
exception O
of O
the O
batch B-HyperparameterName
size I-HyperparameterName
, O
learning B-HyperparameterName
rate I-HyperparameterName
, O
and O
number B-HyperparameterName
of I-HyperparameterName
train- B-HyperparameterName
ing I-HyperparameterName
epochs I-HyperparameterName
. O

The O
dropout B-HyperparameterName
probability I-HyperparameterName
was O
always O
kept O
at O
0.1 B-HyperparameterValue
. O

The O
optimal O
hyperparameter O
values O
are O
task O
- O
speciﬁc O
, O
but O
we O
found O
the O
following O
range O
of O
possible O
values O
to O
work O
well O
across O
all O
tasks O
: O
• O
Batch B-HyperparameterName
size I-HyperparameterName
: O
16 B-HyperparameterValue
, O
32 B-HyperparameterValue

• O
Learning B-HyperparameterName
rate I-HyperparameterName
( O
Adam O
) O
: O
5e-5 B-HyperparameterValue
, O
3e-5 B-HyperparameterValue
, O
2e-5 B-HyperparameterValue
• O
Number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
: O
2 B-HyperparameterValue
, O
3 B-HyperparameterValue
, O
4 B-HyperparameterValue
We O
also O
observed O
that O
large O
data O
sets O
( O
e.g. O
, O
100k+ B-HyperparameterValue
labeled O
training O
examples O
) O
were O
far O
less O
sensitive O
to O
hyperparameter O
choice O
than O
small O
data O
sets O
. O

Fine B-MetricName
- O
tuning O
is O
typically O
very O
fast O
, O
so O
it O
is O
rea- O
sonable O
to O
simply O
run O
an O
exhaustive O
search O
over O
the O
above O
parameters O
and O
choose O
the O
model O
that O
performs O
best O
on O
the O
development O
set O
. O

A.4 O
Comparison O
of O
BERT B-MethodName
, O
ELMo B-MethodName
, O
and O
OpenAI B-MethodName
GPT I-MethodName
Here O
we O
studies O
the O
differences O
in O
recent O
popular O
representation O
learning O
models O
including O
ELMo B-MethodName
, O
OpenAI B-MethodName
GPT I-MethodName
and O
BERT B-MethodName
. O

The O
comparisons O
be- O
tween O
the O
model O
architectures O
are O
shown O
visually O
in O
Figure O
3 O
. O

Note O
that O
in O
addition O
to O
the O
architec- O
ture O
differences O
, O
BERT B-MethodName
and O
OpenAI B-MethodName
GPT I-MethodName
are O
ﬁne- O
tuning O
approaches O
, O
while O
ELMo B-MethodName
is O
a O
feature O
- O
based O
approach O
. O

The O
most O
comparable O
existing O
pre O
- O
training O
method O
to O
BERT B-MethodName
is O
OpenAI B-MethodName
GPT I-MethodName
, O
which O
trains O
a O
left O
- O
to O
- O
right O
Transformer O
LM O
on O
a O
large O
text O
cor- O
pus O
. O

In O
fact O
, O
many O
of O
the O
design O
decisions O
in O
BERT B-MethodName
were O
intentionally O
made O
to O
make O
it O
as O
close O
to O
GPT B-MethodName
as O
possible O
so O
that O
the O
two O
methods O
could O
be O
minimally O
compared O
. O

The O
core O
argument O
of O
this O
work O
is O
that O
the O
bi O
- O
directionality O
and O
the O
two O
pre- O
training O
tasks O
presented O
in O
Section O
3.1 O
account O
for O
the O
majority O
of O
the O
empirical O
improvements O
, O
but O
we O
do O
note O
that O
there O
are O
several O
other O
differences O
between O
how O
BERT B-MethodName
and O
GPT B-MethodName
were O
trained O
: O

• B-MethodName
GPT I-MethodName
is O
trained O
on O
the O
BooksCorpus B-MethodName
( O
800 B-HyperparameterValue
M I-HyperparameterValue
words O
) O
; O
BERT B-MethodName
is O
trained O
on O
the O
BooksCor- B-MethodName
pus I-MethodName
( O
800 B-HyperparameterValue
M I-HyperparameterValue
words O
) O
and O
Wikipedia O
( O
2,500 B-HyperparameterValue
M I-HyperparameterValue
words O
) O
. O

• B-MethodName
GPT I-MethodName
uses O
a O
sentence O
separator O
( O
[ O
SEP O
] O
) O
and O
classiﬁer O
token O
( O
[ O
CLS O
] O
) O
which O
are O
only O
in- O
troduced O
at O
ﬁne O
- O
tuning O
time O
; O
BERT B-MethodName
learns O
[ O
SEP O
] O
, O
[ O
CLS O
] O
and O
sentence O
A O
/ O
Bembed- O

dings O
during O
pre O
- O
training O
. O

• B-MethodName
GPT I-MethodName
was O
trained O
for O
1M B-HyperparameterValue
steps B-HyperparameterName
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32,000 B-HyperparameterValue
words I-HyperparameterName
; O
BERT B-MethodName
was O
trained O
for O
1M B-HyperparameterValue
steps B-HyperparameterName
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
128,000 B-HyperparameterValue
words I-HyperparameterName
. O

• B-MethodName
GPT I-MethodName
used O
the O
same O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5e-5 B-HyperparameterValue
for O
all O
ﬁne O
- O
tuning O
experiments O
; O
BERT B-MethodName
chooses O
a O
task O
- O
speciﬁc O
ﬁne O
- O
tuning O
learning O
rate O
which O
performs O
the O
best O
on O
the O
development O
set O
. O

To O
isolate O
the O
effect O
of O
these O
differences O
, O
we O
per- O
form O
ablation O
experiments O
in O
Section O
5.1 O
which O
demonstrate O
that O
the O
majority O
of O
the O
improvements O
are O
in O
fact O
coming O
from O
the O
two O
pre O
- O
training O
tasks O
and O
the O
bidirectionality O
they O
enable O
. O

A.5 O
Illustrations O
of O
Fine O
- O
tuning O
on O
Different O
Tasks O
The O
illustration O
of O
ﬁne O
- O
tuning O
BERT B-MethodName
on O
different O
tasks O
can O
be O
seen O
in O
Figure O
4 O
. O

Our O
task O
- O
speciﬁc O
models O
are O
formed O
by O
incorporating O
BERT B-MethodName
with O
one O
additional O
output O
layer O
, O
so O
a O
minimal O
num- O
ber I-HyperparameterName
of O
parameters O
need O
to O
be O
learned O
from O
scratch O
. O

Among O
the O
tasks O
, O
( O
a O
) O
and O
( O
b O
) O
are O
sequence O
- O
level O
tasks O
while O
( O
c O
) O
and O
( O
d O
) O
are O
token O
- O
level O
tasks O
. O

In O
the O
ﬁgure O
, O
Erepresents O
the O
input O
embedding O
, O
Ti O
represents O
the O
contextual O
representation O
of O
token O
i O
, O

[ B-MethodName
CLS B-MethodName
] O
is O
the O
special O
symbol O
for O
classiﬁcation B-TaskName
out- O
put O
, O
and O
[ O
SEP O
] O
is O
the O
special O
symbol O
to O
separate O
non O
- O
consecutive O
token O
sequences O
. O

B O
Detailed O
Experimental O
Setup O
B.1 O
Detailed O
Descriptions O
for O
the O
GLUE B-TaskName
Benchmark O
Experiments O
. O

The O
GLUE B-DatasetName
benchmark O
includes O
the O
following O
datasets O
, O
the O
descriptions O
of O
which O
were O
originally O
summarized O
in O
Wang O
et O
al O
. O

( O
2018a O
): O
MNLI B-TaskName
Multi I-MethodName
- O
Genre O
Natural O
Language I-TaskName
Inference O
is O
a O
large O
- O
scale O
, O
crowdsourced O
entailment O
classiﬁ- I-TaskName
cation I-TaskName
task O
( O
Williams O
et O
al O
. O
, O
2018 O
) O
. O

Given O
a O
pair O
of O
sentences O
, O
the O
goal O
is O
to O
predict O
whether O
the O
sec- O
ond O
sentence O
is O
an O
entailment O
, O
contradiction O
, O
or O
neutral O
with O
respect O
to O
the O
ﬁrst O
one O
. O

QQP B-TaskName
Quora O
Question O
Pairs O
is O
a O
binary O
classiﬁ- I-TaskName
cation I-TaskName
task O
where O
the O
goal O
is O
to O
determine O
if O
two O
questions O
asked O
on O
Quora O
are O
semantically O
equiv- O
alent O
( O
Chen O
et O
al O
. O
, O
2018 O
) O
. O

QNLI B-MethodName

Question O
Natural I-TaskName
Language I-TaskName
Inference O
is O
a O
version O
of O
the O
Stanford B-MethodName
Question O
Answering I-MethodName
Dataset I-MethodName
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
which O
has O
been O
converted O
to O
a O
binary O
classiﬁcation O
task O
( O
Wang O
et O
al O
. O
, O
2018a O
) O
. O

The O
positive O
examples O
are O
( O
ques- B-TaskName
tion O
, O
sentence O
) O
pairs O
which O
do O
contain O
the O
correct O
answer O
, O
and O
the O
negative O
examples O
are O
( O
question O
, O
sentence O
) O
from O
the O
same O
paragraph O
which O
do O
not O
contain O
the O
answer O
. O

SST-2 B-DatasetName

The O
Stanford B-MethodName
Sentiment O
Treebank O
is O
a O
binary O
single O
- O
sentence O
classiﬁcation O
task O
consist- O
ing O
of O
sentences O
extracted O
from O
movie O
reviews O
with O
human O
annotations O
of O
their O
sentiment O
( O
Socher O
et O
al O
. O
, O
2013 O
) O
. O

CoLA B-MethodName

The O
Corpus B-MethodName
of I-MethodName
Linguistic I-MethodName
Acceptability I-MethodName
is O
a O
binary O
single O
- O
sentence O
classiﬁcation O
task O
, O
where O
the O
goal O
is O
to O
predict O
whether O
an O
English O
sentence O
is O
linguistically O
“ O
acceptable O
” O
or O
not O
( O
Warstadt O
et O
al O
. O
, O
2018 O
) O
. O

STS B-MethodName
- I-MethodName
B I-MethodName
The O
Semantic O
Textual O
Similarity I-TaskName
Bench- O
mark O
is O
a O
collection O
of O
sentence O
pairs O
drawn O
from O
news O
headlines O
and O
other O
sources O
( O
Cer O
et O
al O
. O
, O
2017 O
) O
. O

They O
were O
annotated O
with O
a O
score O
from O
1 B-HyperparameterValue
to O
5 B-HyperparameterValue
denoting O
how O
similar O
the O
two O
sentences O
are O
in O
terms O
of O
semantic O
meaning O
. O

MRPC B-MethodName
Microsoft I-MethodName
Research I-MethodName
Paraphrase I-MethodName
Corpus I-MethodName
consists O
of O
sentence O
pairs O
automatically O
extracted O
from O
online O
news O
sources O
, O
with O
human O
annotations O
for O
whether O
the O
sentences O
in O
the O
pair O
are O
semanti- O
cally O
equivalent O
( O
Dolan O
and O
Brockett O
, O
2005).RTE O

Recognizing O
Textual O
Entailment O
is O
a O
bi- O
nary O
entailment O
task O
similar O
to O
MNLI B-MethodName
, O
but O
with O
much O
less O
training O
data O
( O
Bentivogli O
et O
al O
. O
, O

2009).14 O
WNLI B-DatasetName
Winograd I-DatasetName
NLI I-DatasetName
is O
a O
small O
natural O
lan- O
guage O
inference O
dataset O
( O
Levesque O
et O
al O
. O
, O
2011 O
) O
. O

The O
GLUE B-MethodName
webpage O
notes O
that O
there O
are O
issues O
with O
the O
construction O
of O
this O
dataset,15and O
every O
trained O
system O
that O
’s O
been O
submitted O
to O
GLUE B-MethodName
has O
performed O
worse O
than O
the O
65.1 B-MetricValue
baseline O
accuracy O
of O
predicting O
the O
majority O
class O
. O

We O
therefore O
ex- O
clude O
this O
set O
to O
be O
fair O
to O
OpenAI B-MethodName
GPT I-MethodName
. O

For O
our O
GLUE B-TaskName
submission O
, O
we O
always O
predicted O
the O
ma- O
jority O
class O
. O

C O
Additional O
Ablation O
Studies O
C.1 O
Effect O
of O
Number O
of O
Training O
Steps O
Figure O
5 O
presents O
MNLI B-TaskName
Dev O
accuracy O
after O
ﬁne- O
tuning O
from O
a O
checkpoint O
that O
has O
been O
pre O
- O
trained O
forksteps O
. O

This O
allows O
us O
to O
answer O
the O
following O
questions O
: O
1 O
. O
Question O
: O
Does O
BERT B-MethodName
really O
need O
such O
a O
large O
amount O
of O
pre O
- O
training O
( O
128,000 B-HyperparameterValue
words O
/ O
batch O
* O
1,000,000 B-HyperparameterValue
steps O
) O
to O
achieve O
high O
ﬁne O
- O
tuning O
accuracy O
? O

Answer O
: O

Yes O
, O
BERT B-MethodName
BASE I-MethodName
achieves O
almost O
1.0% B-MetricValue
additional O
accuracy O
on O
MNLI B-TaskName
when O
trained O
on O
1M B-HyperparameterValue
steps B-HyperparameterName
compared O
to O
500k B-HyperparameterValue
steps B-HyperparameterName
. O

2 O
. O
Question O
: O
Does O
MLM B-MethodName
pre O
- O
training O
converge O
slower O
than O
LTR B-MethodName
pre O
- O
training O
, O
since O
only O
15% B-HyperparameterValue
of O
words O
are O
predicted O
in O
each O
batch O
rather O
than O
every O
word O
? O

Answer O
: O

The O
MLM B-MethodName
model O
does O
converge O
slightly O
slower O
than O
the O
LTR B-MethodName
model O
. O

How- O
ever O
, O
in O
terms O
of O
absolute O
accuracy O
the O
MLM B-MethodName
model O
begins O
to O
outperform O
the O
LTR B-MethodName
model O
almost O
immediately O
. O

C.2 O
Ablation O
for O
Different O
Masking O
Procedures O
In O
Section O
3.1 O
, O
we O
mention O
that O
BERT B-MethodName
uses O
a O
mixed O
strategy O
for O
masking O
the O
target O
tokens O
when O
pre O
- O
training O
with O
the O
masked O
language O
model O
( O
MLM B-MethodName
) O
objective O
. O

The O
following O
is O
an O
ablation O
study O
to O
evaluate O
the O
effect O
of O
different O
masking O
strategies O
. O

Note O
that O
the O
purpose O
of O
the O
masking O
strategies O
is O
to O
reduce O
the O
mismatch O
between O
pre O
- O
training O
and O
ﬁne O
- O
tuning O
, O
as O
the O
[ O
MASK B-MetricName
] O
symbol O
never O
ap- O
pears O
during O
the O
ﬁne O
- O
tuning O
stage O
. O

We O
report O
the O
Dev O
results O
for O
both O
MNLI B-TaskName
and O
NER B-TaskName
. O

For O
NER B-TaskName
, O
we O
report O
both O
ﬁne O
- O
tuning O
and O
feature O
- O
based O
ap- O
proaches O
, O
as O
we O
expect O
the O
mismatch O
will O
be O
am- O
pliﬁed O
for O
the O
feature O
- O
based O
approach O
as O
the O
model O
will O
not O
have O
the O
chance O
to O
adjust O
the O
representa- O
tions O
. O

The O
results O
are O
presented O
in O
Table O
8 O
. O

In O
the O
table O
, O
MASK B-MethodName
means O
that O
we O
replace O
the O
target O
token O
with O
the[MASK O
] O
symbol O
for O
MLM B-DatasetName
; O
S O
AME O
means O
that O
we O
keep O
the O
target O
token O
as O
is O
; O
R O
NDmeans O
that O
we O
replace O
the O
target O
token O
with O
another O
random O
token O
. O

The O
numbers O
in O
the O
left O
part O
of O
the O
table O
repre- O
sent O
the O
probabilities O
of O
the O
speciﬁc O
strategies O
used O
during O
MLM B-DatasetName
pre O
- O
training O
( O
BERT B-MethodName
uses O
80% B-MetricValue
, O
10% B-HyperparameterValue
, O
10% B-HyperparameterValue
) O
. O

The O
right O
part O
of O
the O
paper O
represents O
the O
Dev B-MetricName
set O
results O
. O

For O
the O
feature O
- O
based O
approach O
, O
we O
concatenate O
the O
last O
4 O
layers O
of O
BERT B-MethodName
as O
the O
features O
, O
which O
was O
shown O
to O
be O
the O
best O
approach O
in O
Section O
5.3 O
. O

From O
the O
table O
it O
can O
be O
seen O
that O
ﬁne O
- O
tuning O
is O
surprisingly O
robust O
to O
different O
masking O
strategies O
. O

However O
, O
as O
expected O
, O
using O
only O
the O
M B-MethodName
ASK I-MethodName
strat- O
egy O
was O
problematic O
when O
applying O
the O
feature- O
based O
approach O
to O
NER B-TaskName
. O

Interestingly O
, O
using O
only O
the O
R O
NDstrategy O
performs O
much O
worse O
than O
our O
strategy O
as O
well O
. O

Privacy B-TaskName
- I-TaskName
preserving I-TaskName
Neural I-TaskName
Representations I-TaskName
of O
Text O

Maximin O
Coavoux O
Shashi O
Narayan O
Shay O
B. O
Cohen O
Institute O
for O
Language O
, O
Cognition O
and O
Computation O
School O
of O
Informatics O
, O
University O
of O
Edinburgh I-MethodName
fmcoavoux O
, O
scohen O
g@inf.ed.ac.uk,shashi.narayan@ed.ac.uk O
Abstract O
This O
article O
deals O
with O
adversarial O
attacks O
to- O
wards O
deep O
learning O
systems O
for O
Natural B-TaskName
Lan- I-TaskName
guage I-TaskName
Processing I-TaskName
( O
NLP B-TaskName
) O
, O
in O
the O
context O
of O
pri- O
vacy I-TaskName
protection I-TaskName
. O

We O
study O
a O
speciﬁc O
type O
of O
at- O
tack O
: O
an O
attacker O
eavesdrops O
on O
the O
hidden O
rep- O
resentations O
of O
a O
neural O
text O
classiﬁer O
and O
tries O
to O
recover O
information O
about O
the O
input O
text O
. O

Such O
scenario O
may O
arise O
in O
situations O
when O
the O
computation O
of O
a O
neural O
network O
is O
shared O
across O
multiple O
devices O
, O
e.g. O
some O
hidden O
rep- O
resentation O
is O
computed O
by O
a O
user O
’s O
device O
and O
sent O
to O
a O
cloud O
- O
based O
model O
. O

We O
measure O
the O
privacy O
of O
a O
hidden O
representation O
by O
the O
abil- O
ity O
of O
an O
attacker O
to O
predict O
accurately O
speciﬁc O
private O
information O
from O
it O
and O
characterize O
the O
tradeoff O
between O
the O
privacy O
and O
the O
util- O
ity O
of O
neural O
representations O
. O

Finally O
, O
we O
pro- O
pose O
several O
defense O
methods O
based O
on O
modi- O
ﬁed O
training O
objectives O
and O
show O
that O
they O
im- O
prove O
the O
privacy O
of O
neural O
representations O
. O

1 O
Introduction O
This O
article O
presents O
an O
adversarial O
scenario O
meant O
at O
characterizing O
the O
privacy O
of O
neural O
representa- O
tions O
for O
NLP B-TaskName
tasks O
, O
as O
well O
as O
defense O
methods O
designed O
to O
improve O
the O
privacy O
of O
those O
represen- O
tations O
. O

A O
deep O
neural O
network O
constructs O
inter- O
mediate O
hidden O
representations O
to O
extract O
features O
from O
its O
input O
. O

Such O
representations O
are O
trained O
to O
predict O
a O
label O
, O
and O
therefore O
should O
contain O
use- O
ful O
features O
for O
the O
ﬁnal O
prediction O
. O

However O
, O
they O
might O
also O
encode O
information O
about O
the O
input O
that O
a O
user O
wants O
to O
keep O
private O
( O
e.g. O
personal O
data O
) O
and O
can O
be O
exploited O
for O
adversarial O
usages O
. O

We O
study O
a O
speciﬁc O
type O
of O
attack O
on O
neural O
rep- O
resentations O
: O
an O
attacker O
eavesdrops O
on O
the O
hidden O
representations O
of O
novel O
input O
examples O
( O
that O
are O
not O
in O
the O
training O
set O
) O
and O
tries O
to O
recover O
informa- O
tion O
about O
the O
content O
of O
the O
input O
text O
( O
Figure O
1 O
) O
. O

A O
typical O
scenario O
where O
such O
attacks O
would O
oc- O
cur O
is O
when O
the O
computation O
of O
a O
deep O
neural O
net O
Latent O
representation O
, O
sent O
over O
a O
channel O

zAttackery B-MethodName
x O
Private O
inputDesired O
OutputFigure O
1 O
: O
General O
setting O
illustration O
. O

The O
main O
classi- O
ﬁer O
predicts O
a O
label O
yfrom O
a O
textx O
, O
the O
attacker O
tries O
to O
recover O
some O
private O
information O
zcontained O
in O
xfrom O

the O
latent O
representation O
used O
by O
the O
main O
classiﬁer O
. O
is O
shared O
between O
several O
devices O
( O
Li O
et O
al O
. O
, O
2017 O
) O
. O

For O
example O
, O
a O
user O
’s O
device O
computes O
a O
represen- O
tation O
of O
a O
textual O
input O
, O
and O
sends O
it O
a O
to O
cloud- O
based O
neural O
network O
to O
obtain O
, O
e.g. O
the O
topic O
of O
the O
text O
or O
its O
sentiment O
. O

The O
scenario O
is O
illustrated O
in O
Figure O
1 O
. O

Private O
information O
can O
take O
the O
form O
of O
key O
phrases O
explicitly O
contained O
in O
the O
text O
. O

However O
, O
it O
can O
also O
be O
implicit O
. O

For O
example O
, O
demographic O
information O
about O
the O
author O
of O
a O
text O
can O
be O
pre- O
dicted O
with O
above O
chance O
accuracy O
from O
linguistic O
cues O
in O
the O
text O
itself O
( O
Rosenthal O
and O
McKeown O
, O
2011 O
; O
Preot O
¸iuc O
- O
Pietro O
et O
al O
. O
, O
2015 O
) O
. O

Independently O
of O
its O
explicitness O
, O
some O
of O
this O
private O
information O
correlates O
with O
the O
output O
la- O
bels O
, O
and O
therefore O
will O
be O
learned O
by O
the O
network O
. O

In O
such O
a O
case O
, O
there O
is O
a O
tradeoff O
between O
the O
util- O
ity O
of O
the O
representation O
( O
measured O
by O
the O
accu- B-MetricName
racy I-MetricName
of O
the O
network O
) O
and O
its O
privacy O
. O

It O
might O
be O

2necessary O
to O
sacriﬁce O
some O
accuracy B-MetricName
in O
order O
to O
satisfy O
privacy O
requirements O
. O

However O
, O
this O
is O
not O
the O
case O
of O
all O
private O
in- O
formation O
, O
since O
some O
of O
it O
is O
not O
relevant O
for O
the O
prediction O
of O
the O
text O
label O
. O

Still O
, O
private O
infor- O
mation O
might O
be O
learned O
incidentally O
. O

This O
non- O
intentional O
and O
incidental O
learning O
also O
raises O
pri- O
vacy O
concerns O
, O
since O
an O
attacker O
with O
an O
access O
to O
the O
hidden O
representations O
, O
may O
exploit O
them O
to O
re- O
cover O
information O
about O
the O
input O
. O

In O
this O
paper O
we O
explore O
the O
following O
situation O
: O
( O
i O
) O
amain O
classiﬁer O
uses O
a O
deep O
network O
to O
predict O
a O
label O
from O
textual O
data O
; O
( O
ii O
) O
an O
attacker O
eaves- O
drops O
on O
the O
hidden O
layers O
of O
the O
network O
and O
tries O
to O
recover O
information O
about O
the O
input O
text O
of O
un- O
seen O
examples O
. O

In O
contrast O
to O
previous O
work O
about O
neural O
networks O
and O
privacy O
( O
Papernot O
et O
al O
. O
, O
2016 O
; O
Carlini O
et O
al O
. O
, O
2018 O
) O
we O
do O
not O
protect O
the O
privacy O
of O
examples O
from O
the O
training O
set O
, O
but O
the O
privacy O
of O
unseen O
examples O
provided O
, O
e.g. O
, O
by O
a O
user O
. O

An O
example O
of O
a O
potential O
application O
would O
be O
a O
spam B-TaskName
detection O
service O
with O
the O
following O
con- O
straints O
: O
the O
service O
provider O
does O
not O
access O
ver- O

batim B-TaskName
emails O
sent O
to O
users O
, O
only O
their O
vector O
repre- O
sentations O
. O

Theses O
vector O
representations O
should O
not O
be O
usable O
to O
gather O
information O
about O
the O
user O
’s O
contacts O
or O
correspondents O
, O
i.e. O
protect O
the O
user O
from O
proﬁling O
. O

This O
paper O
makes O
the O
following O
contributions:1 O
We O
propose O
a O
metric O
to O
measure O
the O
privacy O
of O
the O
neural O
representation O
of O
an O
input O
for O
Natural B-TaskName
Language I-TaskName
Processing I-TaskName
tasks O
. O

The O
met- O
ric O
is O
based O
on O
the O
ability O
of O
an O
attacker O
to O
recover O
information O
about O
the O
input O
from O
the O
latent O
representation O
only O
. O

We O
present O
defense O
methods O
designed O
against O
this O
type O
of O
attack O
. O

The O
methods O
are O
based O
on O
modiﬁed O
training O
objectives O
and O
lead O
to O
an O
improved O
privacy B-MetricName
- O
accuracy I-MetricName
tradeoff O
. O

2 O
Adversarial O
Scenario O
In O
the O
scenario O
we O
propose O
, O
each O
example O
consists O
of O
a O
triple O
( O
x;y;z O
) O
, O
wherexis O
a O
natural O
language O
text O
, O
yis O
a O
single O
label O
( O
e.g. O
topic O
or O
sentiment O
) O
, O
andzis O
a O
vector O
of O
private O
information O
contained O
inx O
. O

Our O
base O
setting O
has O
two O
entities O
: O
( O
i O
) O
a O
main O
classiﬁer O
whose O
role O
is O
to O
learn O
to O
predict O
yfrom O
x O
, O
( O
ii O
) O
an O
attacker O
who O
learns O
to O
predict O
zfrom O
the O
latent O
representation O
of O
xused O
by O
the O
main O
classi- O
ﬁer O
. O

We O
illustrate O
this O
setting O
in O
Figure O
1 O
. O

In O
order O
to O
evaluate O
the O
utility O
and O
privacy O
of O
a O
speciﬁc O
model O
, O
we O
proceed O
in O
three O
phases O
: O
Phase O
1 O
. O

Training O
of O
the O
main O
classiﬁer O
on O
( O
x;y)pairs O
and O
evaluation O
of O
its O
accuracy O
; O
Phase O
2 O
. O
Generation O
of O
a O
dataset O
of O
pairs O
( O
r(x);z)for O
the O
attacker O
, O
ris O
the O
representation O
function O
of O
the O
main O
classiﬁer O
( O
ris O
deﬁned O
in O
Sec- O
tion I-TaskName
2.1 O
) O
; O
Phase O
3 O
. O

Training O
of O
the O
attacker O
’s O
network O
and O
evaluation O
of O
its O
performance O
for O
measuring O
pri- O
vacy O
. O

In O
the O
remainder O
of O
this O
section O
, O
we O
describe O
the O
main O
classiﬁer O
( O
Section O
2.1 O
) O
, O
and O
the O
attacker O
’s O
model O
( O
Section O
2.2 O
) O
. O

2.1 O
Text O
Classiﬁer O
As O
our O
base O
model O
, O
we O
chose O
a O
standard O
LSTM B-TaskName
architecture O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
for O
sequence B-TaskName
classiﬁcation I-TaskName
. O

LSTM B-TaskName
- O
based O
archi- O
tectures O
have O
been O
applied O
to O
many O
NLP B-TaskName
tasks O
, O
including O
sentiment B-TaskName
classiﬁcation I-TaskName
( O
Wang O
et O
al O
. O
, O
2016 O
) O
and O
text B-TaskName
classiﬁcation I-TaskName
( O
Zhou O
et O
al O
. O
, O
2016 O
) O
. O

First O
, O
an O
LSTM B-TaskName
encoder O
computes O
a O
ﬁxed O
- O
size O
representation O
r(x)from B-HyperparameterName
a O
sequence O
of O
tokens O
x= B-HyperparameterName
( O
x1;x2;:::;x O
n)projected O
to O
an O
embedding O
space O
. O

We O
use O
rto B-HyperparameterName
denote O
the O
parameters B-HyperparameterName
used I-HyperparameterName
to I-HyperparameterName
construct I-HyperparameterName
r. B-HyperparameterName

They O
include O
the O
parameters O
of O
the O
LSTM B-TaskName
, O
as O
well O
as O
the O
word O
embeddings O
. O

Then O
, O
the O
encoder O
output O
r(x)is B-HyperparameterName
fed O
as O
input O
to O
a O
feedfor- O
ward O
network O
with O
parameters O
pthat O
predicts O
the O
labelyof O
the O
text O
, O
with O
a O
softmax O
output O
activa- O
tion O
. O

In O
the O
standard O
setting O
, O
the O
model O
is O
trained O
to O
minimize O
the O
negative O
log O
- I-HyperparameterName
likelihood I-HyperparameterName
of I-HyperparameterName
ylabels I-HyperparameterName
: O
Lm(r;p O
) O

= O
NX O
i=1 logP(y(i)jx(i);r;p O
) O
; O
whereNis O
the O
number B-HyperparameterName
of I-HyperparameterName
training O
examples O
. O

2.2 O
Attacker O
’s O
Classiﬁer O
Once O
the O
main O
model O
has O
been O
trained O
, O
we O
assume O
that O
its O
parameters O
randpare O
ﬁxed O
. O

We O
gen- O
erate O
a O
new O
dataset O
made O
of O
pairs O
( O
r(x);z(x B-HyperparameterName
) O
) O
, O
where O
r(x)is B-HyperparameterName
the O
hidden O
representation O
used O
by O
the O
main O
model O
and O
z(x)is B-HyperparameterName
a O
vector O
of O
private O
cat- O
egorical O
variables O
. O

In O
practice O
, O
zis O
a O
vector O
of O
bi- O
nary O
variables O
, O
( O
representing O
e.g. O
demographic O
in- O
formation O
about O
the O
author O
) O
. O

In O
our O
experiments O
, O
we O
use O
the O
same O
training O
examples O
xfor O
the O
main O

3classiﬁer O
and O
the O
classiﬁer B-MetricName
of O
the O
attacker O
. O

How- O
ever O
, O
since O
the O
attacker O
has O
access O
to O
the O
repre- O
sentation O
function O
rparameterized O
by O
r B-HyperparameterName
, O
they O
can O
generate O
a O
dataset O
from O
any O
corpus O
containing O
the O
private O
variables O
they O
want O
to O
recover O
. O

In O
other O
words O
, O
it O
is O
not O
necessary O
that O
they O
have O
access O
to O
the O
original O
training O
corpus O
to O
train O
their O
classiﬁer O
. O

The O
attacker O
trains O
a O
second O
feedforward O
net- O
work O
on O
the O
new O
dataset O
f(r(x(i));z(i))giN. O

This O
classiﬁer O
uses O
a O
sigmoid O
output O
activation O
to O
com- O
pute O
the O
probabilities O
of O
each O
binary O
variable O
in O
z B-HyperparameterName
: O
P(zjr(x);a O
) O

= O
(FeedForward O
( O
r(x B-HyperparameterName
) O
) O
): O

It O
is O
trained O
to O
minimize O
the O
negative O
log- I-HyperparameterName
likelihood I-HyperparameterName
of I-HyperparameterName
z B-HyperparameterName
: O
La(a B-HyperparameterName
) O

= O
NX B-TaskName
i=1 logP(z(i)jr(x(i));a O
) O

= O
NX O
i=1KX O
j=1 logP(z(i O
) O
jjr(x(i));a O
) O
; O
assuming O
that O
the O
Kvariables O
in O
zare O
indepen- O
dent O
. O

Since O
the O
parameters O
used O
to O
construct O
rare O
ﬁxed O
, O
the O
attacker O
only O
acts O
upon O
its O
own O
parame- O
tersato O
optimize O
this O
loss O
. O

We O
use O
the O
performance O
of O
the O
attacker O
’s O
clas- O
siﬁer O
as O
a O
proxy O
for O
privacy O
. O

If O
its O
accuracy O
is O
high O
, O
then O
an O
eavesdropper O
can O
easily O
recover O
in- O
formation O
about O
the O
input O
document O
. O

In O
contrast O
, O
if O
its O
accuracy O
is O
low O
( O
i.e. O
close O
to O
that O
of O
a O
most- O
frequent O
label O
baseline O
) O
, O
then O
we O
may O
reasonably O
conclude O
that O
rdoes O
not O
encode O
enough O
informa- O
tion O
to O
reconstruct O
x O
, O
and O
mainly O
contains O
infor- O
mation O
that O
is O
useful O
to O
predict O
y. O
In O
general O
, O
the O
performance O
of O
a O
single O
attacker O
does O
not O
provide O
sufﬁcient O
evidence O
to O
conclude O
that O
the O
input O
representation O
ris O
robust O
to O
an O
at- O
tack O
. O

It O
should O
be O
robust O
to O
any O
type O
of O
reconstruc- B-TaskName
tion I-TaskName
method O
. O

In O
the O
scope O
of O
this O
paper O
though O
, O
we O
only O
experiment O
with O
a O
feedforward O
network O
reconstructor O
, O
i.e. O
a O
powerful O
learner O
. O

In O
the O
following O
sections O
, O
we O
propose O
several O
training O
method O
modiﬁcations O
aimed O
at O
obfuscat- O

ing O
private O
information O
from O
the O
hidden O
represen- O
tation O
r(x O
) O
. O

Intuitively O
, O
the O
aim O
of O
these O
modiﬁca- O
tions O
is O
to O
minimize O
some O
measure O
of O
information O
between O
randzto O
make O
the O
prediction O
of O
zhard O
. O

An O
obvious O
choice O
for O
that O
measure O
would O
be O
the O
Mutual B-MethodName
Information I-MethodName
( I-MethodName
MI I-MethodName
) I-MethodName
between I-MethodName
randz O
. O

How- O
ever O
, O
MI O
is O
hard O
to O
compute O
due O
to O
the O
continuous O
distribution O
of O
rand O
does O
not O
lend O
itself O
well O
to O
stochastic O
optimization. O

3 O
Defenses O
Against O
Adversarial O
Attacks O

In O
this O
section O
, O
we O
present O
three O
training O
methods O
designed O
as O
defenses O
against O
the O
type O
of O
attack O
we O
described O
in O
Section O
2.2 O
. O

The O
ﬁrst O
two O
methods O
are O
based O
on O
two O
neural O
networks O
with O
rival O
objective O
functions O
( O
Section O
3.1 O
) O
. O

The O
last O
method O
is O
meant O
at O
discouraging O
the O
model O
to O
cluster O
together O
train- O

ing O
examples O
with O
similar O
private O
variables O
z(Sec- B-HyperparameterName
tion O
3.2 O
) O
. O

3.1 O
Adversarial O
Training O
First O
, O
we O
propose O
to O
frame O
the O
training O
of O
the O
main O
classiﬁer O
as O
a O
two O
- O
agent O
process O
: O
the O
main O
agent O
and O
an O
adversarial O
generator O
, O
exploiting O
a O
set- O
ting O
similar O
to O
Generative B-MethodName
Adversarial I-MethodName
Networks I-MethodName
( O
GAN B-MethodName
, O
Goodfellow O
et O

al O
. O
, O
2014 O
) O
. O

The O
generator O
learns O
to O
reconstruct O
examples O
from O
the O
hidden O
representation O
, O
whereas O
the O
main O
agent O
learns O
( O
i O
) O
to O
perform O
its O
main O
task O
( O
ii O
) O
to O
make O
the O
task O
of O
the O
generator O
difﬁcult O
. O

We O
experiment O
with O
two O
types O
of O
generators O
: O
a O
classiﬁer O
that O
predicts O
the O
binary O
attributes O
z(x B-HyperparameterName
) O
used O
as O
a O
proxy O
for O
the O
reconstruction O
of O
x(Sec- B-HyperparameterName
tion O
3.1.1 O
) O
and O
a O
character O
- O
based O
language O
model O
that O
directly O
optimizes O
the O
likelihood O
of O
the O
train- O
ing O
examples O
( O
Section O
3.1.2 O
) O
. O

3.1.1 O
Adversarial O
Classiﬁcation O
: O

Multidetasking O
In O
order O
not O
to O
make O
r(x)a B-HyperparameterName
good O
representation O
for O
reconstructing O
z B-HyperparameterName
, O
we O
make O
two O
modiﬁcations O
to O
the O
training O
setup O
of O
the O
main O
model O
( O
Phase O
1 O
): O
We O
use O
a O
duplicate O
adversarial O
classiﬁer O
, O
with O
parameters O
0 O
a O
, O
that O
tries O
to O
predict O
z B-HyperparameterName
from O
r(x B-HyperparameterName
) O
. O

It O
is O
trained O
simultaneously O
with O
the O
main O
classiﬁer O
. O

Its O
training O
examples O
are O
generated O
on O
the O
ﬂy O
, O
and O
change O
overtime O
as O
the O
main O
classiﬁer O
updates O
its O
own O
parame- O
ters O
. O

This O
classiﬁer O
simulates O
an O
attack O
dur- O
ing O
training O
. O

We O
modify O
the O
objective O
function O
of O
the O
main O
classiﬁer O
to O
incorporate O
a O
penalty O
when O
the O
adversarial O
classiﬁer O
is O
good O
at O
reconstruct- O
ingz O
. O

In O
other O
words O
, O
the O
main O
classiﬁer O
tries O
to O
update O
its O
parameters O
so O
as O
to O
confuse O
the O
duplicate O
attacker O
. O

Formally O
, O
for O
a O
single O
data O
point O
( O
x;y;z O
) O
, O
the O
adversarial O
classiﬁer O
optimizes O
: O
La0(x;y;z;0 O
a)= logP(zjr(x);0 O
a O
) O
; O

4whereas O
the O
main O
classiﬁer O
optimizes O
: O
Lm(x;y;z;r;p)=  O

logP(yjx;r;p O
) O

  O

logP(:zjr(x);0 O
a O
): O
The O
ﬁrst O
term O
of O
this O
equation O
is O
the O
log B-HyperparameterName
- I-HyperparameterName
likelihood I-HyperparameterName
of O
theylabels I-HyperparameterName
. O

The O
second O
term O
is O
designed O
to O
de- O
ceive O
the O
adversary O
. O

The O
hyperparameters B-HyperparameterName

> B-HyperparameterValue
0 B-HyperparameterValue
and O

> O
0control B-HyperparameterValue
the O
relative O
importance O
of O
both O
terms O
. O

As O
in O
a O
GAN B-MethodName
, O
the O
losses O
of O
both O
classiﬁers O
are O
interdependent O
, O
but O
their O
parameters O
are O
distinct O
: O
the O
adversary O
can O
only O
update O
0 O
aand O
the O
main O
classiﬁer O
can O
only O
update O
randp O
. O

The O
duplicate O
adversarial O
classiﬁer O
is O
identical O
to O
the O
classiﬁer O
used O
to O
evaluate O
privacy O
after O
the O
main O
model O
has O
been O
trained O
and O
its O
parameters O
are O
ﬁxed O
. O

However O
, O
both O
classiﬁers O
are O
completely O
distinct O
: O
the O
former O
is O
used O
during O
the O
training O
of O
the O
main O
model O
( O
Phase O
1 O
) O
to O
take O
privacy O
into O
ac- O
count O
whereas O
the O
latter O
is O
used O
to O
evaluate O
the O
pri- O
vacy O
of O
the O
ﬁnal O
model O
( O
Phase O
3 O
) O
, O
as O
is O
described O
in O
Section O
2 O
. O

3.1.2 O
Adversarial O
Generation O

The O
second O
type O
of O
generator O
we O
use O
is O
a O
character- O
based O
LSTM B-TaskName
language O
model O
that O
is O
trained O
to O
re- O
construct O
full O
training O
examples O
. O

For O
a O
single O
ex- O
ample O
( O
x;y O
) O
, O
the O
hidden O
state O
of O
the O
LSTM B-TaskName
is O
ini- O
tialized O
with O
r(x B-HyperparameterName
) O
, O
computed O
by O
the O
main O
model O
. O

The O
generator O
optimizes O
: O
Lg(x;y;`;r B-HyperparameterName
) O

= O
 logP(xjr(x); O
` O
) O
= O
 CX O
i=1logP(xijxi 1 O
1;r(x); O
` O
) O
; O
where O
`is O
the O
set O
of O
parameters O
of O
the O
LSTM B-TaskName
generator O
, O
xiis O
theithcharacter O
in O
the O
document O
, O
and O
C B-HyperparameterName
is O
the B-HyperparameterName
length I-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
document I-HyperparameterName
in O
number I-HyperparameterName
of I-HyperparameterName
characters I-HyperparameterName
. O

The O
generator O
has O
no O
control O
over O
r(x O
) O
, O
and O
optimizes O
the O
objective O
only O
by O
updat- O
ing O
its O
own O
parameters O
 O
` O
. O

Conversely O
, O
the O
loss O
of O
the O
main O
model O
is O
modi- O
ﬁed O
as O
follows O
: O

Lm(x;y;r;p)=  B-HyperparameterName

logP(yjx;r;p O
) O

  O

Lg(x;y;`;r B-HyperparameterName
): O

The O
ﬁrst O
term O
maximizes O
the O
likelihood O
of O
the O
y B-HyperparameterName
labels O
whereas O
the O
second O
term O
is O
meant O
at O
mak- O
ing O
the O
reconstruction O
difﬁcult O
by O
maximizing O
the O
loss O
of O
the O
generator O
. O

As O
in O
the O
loss O
function O
de- O
scribed O
in O
the O
previous O
section O
, O

and O

controlthe O
relative O
importance O
of O
both O
terms O
. O

Once O
again O
, O
the O
main O
classiﬁer O
can O
optimize O
the O
second O
term O
only O
by O
updating O
r B-HyperparameterName
, O
since O
it O
has O
no O
control O
over O
the O
parameters O
of O
the O
adversarial O
generator O
. O

A O
key O
property O
of O
this O
defense O
method O
is O
that O
it O
has O
no O
awareness O
of O
what O
the O
private O
variables O
z B-HyperparameterName
are O
. O

Therefore O
, O
it O
has O
the O
potential O
to O
protect O
the O
neural O
representation O
against O
an O
attack O
on O
any O
pri- O
vate O
information O
. O

From O
a O
broader O
perspective O
, O
the O
goal O
of O
this O
defense O
method O
is O
to O
specialize O
the O
hid- O
den O
representation O
r(x)to O
the O
task O
at O
hand O
( O
sen- O
timent O
or O
topic O
prediction O
) O
and O
to O
avoid O
learning O
anything O
not O
relevant O
to O
it O
. O

3.2 B-HyperparameterValue

Declustering O
The O
last O
strategy O
we O
employ O
to O
make O
the O
task O
of O
the O
attacker O
harder O
is O
based O
on O
the O
intuition O
that O
pri- O
vate O
variables O
zare O
easier O
to O
predict O
from O
rwhen O
the O
main O
model O
learns O
implicitly O
to O
cluster O
exam- O
ples O
with O
similar O
zin O
the O
same O
regions O
of O
the O
rep- O
resentation O
space O
. O

In O
order O
to O
avoid O
such O
implicit O
clustering O
, O
we O
add O
a O
term O
to O
the O
training O
objective O
of O
the O
main O
model O
that O
penalizes O
pairs O
of O
examples O
( O
x;x0)that O
( O
i O
) O
have O
similar O
reconstructions O
z(x)z(x0)(ii O
) O
have O
hidden O
representations O
r(x)andr(x0)in O
the O
same O
region O
of O
space O
. O

We O
use O
the O
following O
modi- O
ﬁed O
loss O
for O
a O
single O
example O
: O
Lm(x;y;z;r;p O
) O
= O
 logP(yjx;r;p O
) O

+ O

( O
0:5 `(z;z0))jjr(x) r(x0)jj2 O
2 O
; O
where O
( O
x0;z0)is O
another O
example O
sampled O
uni- O
formly O
from O
the O
training O
set O
, O

is O
a O
hyperparame- O
ter O
controlling O
the O
importance O
of O
the O
second O
term O
, O
and`(;)2[0;1]is O
the O
normalized O
Hamming O
dis- O
tance O
. O

4 O
Experiments O
Our O
experiments O
are O
meant O
to O
characterize O
the O
privacy O
- O
utility O
tradeoff O
of O
neural O
representations O
on O
text O
classiﬁcation O
tasks O
, O
and O
evaluating O
if O
the O
proposed O
defense O
methods O
have O
a O
positive O
im- O
pact O
on O
it O
. O

We O
ﬁrst O
describe O
the O
datasets O
we O
used O
( O
Section O
4.1 O
) O
and O
the O
experimental O
protocol O
( O
Section O
4.2 O
) O
, O
then O
we O
discuss O
the O
results O
( O
Sec- O
tion O
4.3 O
) O
. O

We O
found O
that O
in O
the O
normal O
train- O
ing O
regime O
, O
where O
no O
defense O
is O
taken O
into O
ac- O
count O
, O
the O
adversary O
can O
recover O
private O
informa- O
tion O
with O
higher O
accuracy B-MetricName
than O
a O
most O
frequent O
class O
baseline O
. O

Furthermore O
, O
we O
found O
that O
the O
de- O
fenses O
we O
implemented O
have O
a O
positive O
effect O
on O
the O
accuracy B-MetricName
- O
privacy I-MetricName
tradeoff O
. O

4.1 O
Datasets O
We O
experiment O
with O
two O
text O
classiﬁcation O
tasks O
: O
sentiment B-TaskName
analysis I-TaskName
( O
Section O
4.1.1 O
) O
and O
topic B-TaskName
clas- I-TaskName
siﬁcation I-TaskName
( O
Section O
4.1.2 O
) O
. O

The O
sizes I-HyperparameterName
of I-HyperparameterName
each O
dataset O
are O
summarized O
in O
Table O
1 O
. O

4.1.1 O
Sentiment O
Analysis O
We O
use O
the O
Trustpilot B-DatasetName
dataset O
( O
Hovy O
et O
al O
. O
, O
2015 O
) O
for O
sentiment B-TaskName
analysis I-TaskName
. O

This O
corpus O
contains O
re- O
views O
associated O
with O
a O
sentiment B-MetricName
score O
on O
a O
ﬁve O
point O
scale O
, O
and O
self O
- O
reported O
information O
about O
the O
users O
. O

We O
use O
the O
ﬁve O
subcorpora O
correspond- O
ing O
to O
ﬁve O
areas O
( O
Denmark B-DatasetName
, O
France O
, O
Germany O
, O
United B-DatasetName
Kingdom I-DatasetName
, O
United B-DatasetName
States I-DatasetName
) O
. O

We O
ﬁlter O
examples O
containing O
both O
the O
birth O
year O
and O
gender O
of O
the O
author O
of O
the O
review O
and O
use O
these O
variables O
as O
the O
private O
information O
. O

As O
in O
previous O
work O
on O
this O
dataset O
( O
Hovy O
, O
2015 O
; O
Hovy O
and O
Søgaard O
, O
2015 O
) O
, O
we O
bin O
the O
age O
of O
the O
author O
into O
two O
categories O
( O
‘ O
under B-HyperparameterValue
35 B-HyperparameterValue
’ O
and O
‘ O
over O
45 B-HyperparameterValue
’ O
) O
. O

Fi- O
nally O
, O
we O
randomly O
split O
each O
subcorpus O
into O
a O
training B-HyperparameterName
set O
( O
80 B-HyperparameterValue
% I-HyperparameterValue
) O
, O
a O
development B-HyperparameterName
set I-HyperparameterName
( O
10 B-HyperparameterValue
% I-HyperparameterValue
) O
and O
a O
test O
( O
10 B-HyperparameterValue
% I-HyperparameterValue
) O
. O

As O
an O
additional O
experimental O
setting O
, O
we O
use O
both O
demographic O
variables O
( O
gender O
and O
age O
) O
as O
input O
to O
the O
main O
model O
. O

We O
do O
so O
by O
adding O
two O
additional O
tokens O
at O
the O
beginning O
of O
the O
input O
text O
, O
one O
for O
each O
variable O
. O

It O
has O
been O
shown O
that O
those O
variables O
can O
be O
used O
to O
improve O
text B-TaskName
classiﬁca- I-TaskName
tion I-TaskName
( O
Hovy O
, O
2015 O
) O
. O

Also O
, O
we O
would O
like O
to O
evalu- O
ate O
whether O
the O
attacker O
’s O
task O
is O
easier O
when O
the O
variables O
to O
predict O
are O
explicitly O
in O
the O
input O
, O
com- O
pared O
to O
when O
these O
information O
are O
only O
poten- O
tially O
and O
implicitly O
in O
the O
input O
. O

In O
other O
words O
, O
this O
setting O
simulates O
the O
case O
where O
private O
in O
- O
formation O
may O
be O
used O
by O
the O
model O
to O
improve O
classiﬁcation B-TaskName
, O
but O
should O
not O
be O
exposed O
too O
obvi- O
ously O
. O

In O
the O
rest O
of O
this O
section O
, O
we O
use O
RAW B-DatasetName
to O
denote O
the O
setting O
where O
only O
the O
raw O
text O
is O
used O
as O
input O
and O
+ B-DatasetName
DEMO I-DatasetName
, O
the O
setting O
where O
the O
demo- O
graphic O
variables O
are O
also O
used O
as O
input O
. O

4.1.2 O
Topic O
Classiﬁcation O
We O
perform O
topic O
classiﬁcation O
on O
two O
genres O
of O
documents O
: O
news O
articles O
and O
blog O
posts O
. O

News O
article O
For O
topic O
classiﬁcation O
of O
news O
ar- O
ticle O
, O
we O
use O
two O
datasets O
: O
the O
AG B-MethodName
news O
corpus O
( O
Del O
Corso O
et O
al O
. O
, O
2005 O
) O
and O
the O
English O
part O
of O
the O
Deutsche B-MethodName
Welle I-MethodName
( O
DW B-DatasetName
) O
news O
corpus O
( O
Pappas O
and O
Popescu O
- O
Belis O
, O
2017 O
) O
. O

For O
the O
AG B-MethodName
corpus O
, O
following O
Zhang O
et O
al O
. O
( O
2015 O
) O
, O
we O
construct O
the O
dataset O
by O
extracting O
doc- O
uments O
belonging O
to O
the O
four O
most O
frequent O
topics O
, O
and O
use O
the O
concatenation O
of O
the O
‘ O
title O
’ O
and O
‘ O
de- O
scription O
’ O
ﬁelds O
as O
the O
input O
to O
the O
classiﬁer O
. O

We O
randomly O
split O
the O
corpus B-HyperparameterName
into O
a O
training B-HyperparameterName
set I-HyperparameterName
( O
80 B-HyperparameterValue
% I-HyperparameterValue
) O
, O
a O
development B-HyperparameterName
set I-HyperparameterName
( O
10 B-HyperparameterValue
% I-HyperparameterValue
) O
and O
a O
test O
set I-HyperparameterName
( O
10 B-HyperparameterValue
% I-HyperparameterValue
) O
. O

For O
the O
DW B-DatasetName
dataset O
, O
we O
use O
the O
‘ O
text O
’ O
ﬁeld O
as O
input O
, O
and O
the O
standard O
split O
. O

We O
kept O
only O
documents O
belonging O
to O
the O
20 B-HyperparameterValue
most O
frequent O
topics O
. O

The O
attacker O
tries O
to O
detect O
which O
named O
enti- O
ties O
appear O
in O
the O
input O
text O
( O
each O
coefﬁcient O
in O
z(x)indicates B-HyperparameterName
whether O
a O
speciﬁc O
named O
entity O
oc- O
curs O
in O
the O
text O
) O
. O

For O
both O
datasets O
, O
we O
used O
the O
named O
entity O
recognition O
system O
from O
the O
NLTK B-MethodName
package O
( O
Bird O
et O
al O
. O
, O
2009 O
) O
to O
associate O
each O
ex- O
ample O
with O
the O
list O
of O
named O
entities O
that O
occur O
in O
it O
. O

We O
select O
the O
ﬁve O
most O
frequent O
named O
entities O
with O
type O
‘ O
person O
’ O
, O
and O
only O
keep O
examples O
con- O
taining O
at O
least O
one O
of O
these O
named O
entities O
. O

This O
ﬁltering O
is O
necessary O
to O
avoid O
a O
very O
unbalanced O
dataset O
( O
since O
each O
selected O
named O
entity O
appears O
usually O
in O
very O
few O
articles O
) O
. O

Blog O
posts O
We O
used O
the O
blog O
authorship O
corpus O
presented O
by O
Schler O
et O
al O
. O

( O
2006 O
) O
, O
a O
collection O
of O
blog O
posts O
associated O
with O
the O
age O
and O
gender O
of O
the O
authors O
, O
as O
provided O
by O
the O
authors O
themselves O
. O

Since O
the O
blog O
posts O
have O
no O
topic O
annotation O
, O
we O
ran O
the O
LDA B-MethodName
algorithm O
( O
Blei O
et O
al O
. O
, O
2003 O
) O
on O
the O
whole O
collection O
( O
with O
10 O
topics O
) O
. O

The O
LDA B-MethodName
out- O
puts O
a O
distribution O
on O
topics O
for O
each O
blog O
post O
. O

We O
selected O
posts O
with O
a O
single O
dominating O
topic O
( O
> O
80 B-HyperparameterValue
% I-HyperparameterValue
) O
and O
discarded O
the O
other O
posts O
. O

We O
binned O
age O
into O
two O
category O
( O
under O
20 B-HyperparameterValue
and O
over O
30 B-HyperparameterValue
) O
. O

We O
used O
the O
age B-HyperparameterName
and O
gender O
of I-HyperparameterName
the O
author O
as O
the O
private O
variables O
. O

These O
variables O
have O
a O
very O
unbalanced O
distribution O
in O
the O
dataset O
, O
we O
randomly O
select O
ex- O
amples O
to O
obtain O
uniform O
distributions O
of O
private O
variables O
. O

Finally O
, O
we O
split O
the O
corpus B-MetricName
into O
a O
train- B-HyperparameterName
ing I-HyperparameterName
set I-HyperparameterName
( O
80 B-HyperparameterValue
% I-HyperparameterValue
) O
, O
a O
validation B-HyperparameterName
set I-HyperparameterName
and O
a O
test O
set I-HyperparameterName
( O
10 B-HyperparameterValue
% I-HyperparameterValue
each O
) O
. O

4.2 O
Protocol O
Evaluation O
For O
the O
main O
task O
, O
we O
report O
a O
single O
accuracy O
measure O
. O

For O
measuring O
the O
privacy O
of O
a O
representation O
, O
we O
compute O
the O
following O
metrics O
: O
For O
demographic O
variables O
( O
sentiment B-TaskName
analy- I-TaskName
sis I-TaskName
and O
blog O
post O
topic O
classiﬁcation O
): O
1-X O
, O
where O
X B-HyperparameterName
is O
the O
average O
of O
the O
accuracy O
of O
the O
attacker O
on O
the O
prediction O
of O
gender O
and O
age O
; O
For O
named O
entities O
( O
news O
topic O
classiﬁca- O
tion O
): O
1-F O
, O
where O
F B-HyperparameterName
is O
an O
F B-HyperparameterName
- O
score O
computed O
over O
the O
set O
of O
binary O
variables O
in O
zthat O
in- O
dicate O
the O
presence O
of O
named O
entities O
in O
the O
input O
example O
. O

Training O
protocol O
We O
implemented O
our O
model O
using O
Dynet O
( O
Neubig O
et O
al O
. O
, O
2017 O
) O
. O

The O
feedfor- O
ward O
components O
( O
both O
of O
the O
main O
model O
and O
of O
the O
attacker O
) O
have O
a O
single O
hidden B-HyperparameterName
layer I-HyperparameterName
of O
64 B-HyperparameterValue
units I-HyperparameterValue
with O
a O
ReLU B-TaskName
activation O
. O

Word O
embeddings I-TaskName
have O
32 B-HyperparameterValue
units I-HyperparameterValue
. O

The O
LSTM B-TaskName
encoder O
has O
a O
single O
layer I-HyperparameterName
of O
varying O
sizes I-HyperparameterName
, O
since O
it O
is O
expected O
that O
the O
amount O
of O
information O
that O
can O
be O
learned O
depends O
on O
the O
size O
of O
these O
representations O
. O

We O
used O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
with O
the O
default O
learning B-HyperparameterName
rate I-HyperparameterName
, O
and O
0.2 B-HyperparameterValue
dropout B-HyperparameterName
rate I-HyperparameterName
for O
the O
LSTM B-TaskName
. O

We O
used O
= O
0.1 B-HyperparameterValue
for O
the O
declustering O
method O
, O
based O
on O
preliminary O
experiments O
. O

For O
the O
other O
defense O
methods O
, O
we O
used O
= O
= O
1 B-HyperparameterValue
and O
did O
not O
experiment O
with O
other O
values O
. O

For O
each O
dataset O
, O
and O
each O
LSTM B-TaskName
state O
di- O
mension O
( O
f B-HyperparameterName
8 B-HyperparameterValue
, O
16 B-HyperparameterValue
, O
32 B-HyperparameterValue
, O
64 B-HyperparameterValue
, O
128 B-HyperparameterValue
g I-HyperparameterValue
) O
, O
we O
train O
the O
main O
model O
for O
8 O
epochs O
( O
sentiment B-TaskName
classiﬁcation O
) O
or O
16 B-HyperparameterValue
epochs O
( O
topic B-TaskName
classiﬁcation O
) O
, O
and O
select O
the O
model O
with O
the O
best O
accuracy O
on O
the O
development O
set O
. O

Then O
, O
we O
generate O
the O
dataset O
for O
the O
attacker O
, O
train O
the O
adversarial O
model O
for O
16 O
epochs O
and O
se- O
lect O
the O
model O
with O
the O
worst O
privacy O
on O
the O
devel- O
opment O
set O
( O
i.e. O
the O
most O
successful O
attacker O
) O
. O

It O
has O
to O
be O
noted O
that O
we O
select O
the O
models O
that O
implement O
defenses O
on O
their O
accuracy O
, O
rather O
than O
their O
privacy O
or O
a O
combination O
thereof O
. O

In O
prac- O
tice O
, O
we O
could O
also O
base O
the O
selection O
strategy O
on O
a O
privacy O
budget O
: O
selecting O
the O
most O
accurate O
model O
with O
privacy O
above O
a O
certain O
threshold O
. O

4.3 O
Results O
This O
section O
discusses O
results O
for O
the O
sentiment B-TaskName
analysis I-TaskName
task O
( O
Section O
4.3.1 O
) O
and O
the O
topic B-TaskName
classi- I-TaskName
ﬁcation I-TaskName
task O
( O
Section O
4.3.2 O
) O
. O

4.3.1 O
Sentiment O
Analysis O
How O
private O
are O
neural O
representations O
? O

Be- O
fore O
discussing O
the O
effect O
of O
proposed O
defense O
methods O
, O
we O
motivate O
empirically O
our O
approach O
by O
showing O
that O
adversarial O
models O
can O
recover O
pri- O
vate O
information O
with O
reasonable O
accuracy O
when O
the O
attack O
is O
targeted O
towards O
a O
model O
that O
imple- O
ments O
none O
of O
the O
presented O
defense O
methods O
. O

To O
do O
so O
, O
we O
compare O
the O
accuracy O
of O
adversar- O
ial O
models O
to O
two O
types O
of O
baselines O
: O
As O
a O
lower O
bound O
, O
we O
use O
the O
most O
frequent O
class O
baseline O
. O

As O
an O
upper O
bound O
, O
we O
trained O
a O
classi- O
ﬁer O
that O
can O
optimize O
the O
hidden O
represen- O
tations O
( O
r O
) O
for O
the O
attacker O
’s O
tasks O
. O

In O
other O
words O
, O
this O
baseline O
is O
trained O
to O
predict O
de- O
mographic O
variables O
from O
x B-HyperparameterName
, O
as O
if O
it O
were O
the O
main O
task O
. O

In O
Table O
2 O
, O
we O
compare O
both O
baselines O
to O
the O
best O
adversary O
in O
the O
two O
settings O
( O
RAW B-DatasetName
and O
+ B-DatasetName
DEMO I-DatasetName
) O
among O
the O
models O
trained O
with O
no O
de- O
fenses O
. O

First O
of O
all O
, O
we O
observe O
that O
apart O
from O
gender O
on O
the O
German B-DatasetName
dataset O
, O
the O
trained O
baseline O
outperforms O
the O
most O
frequent O
class O
baseline O
by O
a O
wide O
margin O
( O
8 B-MetricValue
to O
25 B-HyperparameterValue
absolute O
difference O
) O
. O

Sec- O
ond O
of O
all O
, O
the O
attacker O
is O
able O
to O
outperform O
the O
most O
frequent O
class O
baseline O
overall O
, O
even O
in O
the O
RAW B-TaskName
setting O
. O

In O
more O
details O
, O
for O
age O
, O
the O
adver- O
sary O
is O
well O
over O
the O
baseline O
in O
all O
cases O
except O
US B-DatasetName
. O

On O
the O
other O
hand O
, O
gender O
seems O
harder O
to O
predict O
: O
the O
adversary O
outperforms O
the O
most O
fre- O
quent O
class O
baseline O
only O
in O
the O
+ B-DatasetName
DEMO I-DatasetName
setting O
. O

The O
same O
pattern O
is O
visible O
for O
the O
blog O
post O
dataset O
, O
also O
presented O
in O
the O
last O
line O
of O
Table O
2 O
: O
the O
best O
adversaries O
are O
14 B-HyperparameterValue
points O
over O
the O
base- O
line O
for O
gender O
and O
5 B-HyperparameterValue
points O
for O
age O
, O
i.e. O
almost O
as O
good O
as O
a O
model O
that O
can O
ﬁne O
tune O
the O
hidden O
representations O
. O

These O
results O
justify O
our O
approach O
, O
since O
they O
demonstrate O
that O
hidden O
representations O
learn O
pri- O
vate O
information O
about O
the O
input O
, O
and O
can O
be O
ex- O
ploited O
to O
recover O
this O
information O
with O
reasonable O
accuracy O
. O

Effect O
of O
defenses O
We O
report O
results O
for O
the O
main O
task O
accuracy O
and O
the O
representation O
privacy O
in O
Ta- B-DatasetName
ble O
3 O
for O
the O
+ B-DatasetName
DEMO I-DatasetName
setting O
and O
in O
Table O
4 O
for O
the O
RAW B-DatasetName
setting O
. O

Recall O
that O
the O
privacy O
measure O
( O
Priv O
. O
) O
is O
computed O
by O
1-X O
where O
X B-HyperparameterName
is O
the O
av- O
erage O
accuracy O
of O
the O
attacker O
on O
gender O
and O
age B-HyperparameterName
predictions O
. O

When O
this O
privacy O
metric O
is O
higher O
, O
it O
is O
more O
difﬁcult O
to O
exploit O
the O
hidden O
repre- O
sentation O
of O
the O
network O
to O
recover O
information O
about O
x O
. O

The O
‘ O
Standard B-MethodName
’ I-MethodName
columns O
contain O
the O
ac- O
curacy O
and O
privacy O
of O
the O
base O
model O
described O
in O
Section O
2 O
. O

The O
next O
columns O
present O
the O
abso- O
lute O
variation O
in O
accuracy O
and O
privacy O
for O
the O
three O
defense O
methods O
presented O
in O
Section O
3 O
: O
Multi- B-TaskName
detasking I-TaskName
, O
Adversarial B-TaskName
Generation I-TaskName
, O
and O
Decluster- B-TaskName
ing I-TaskName
. O

We O
also O
report O
for O
each O
corpus O
the O
most O
fre- O
quent O
class O
baseline O
for O
the O
main O
task O
accuracy O
, O
and O
the O
privacy O
of O
the O
most O
frequent O
class O
base- O
lines O
on O
private O
variables O
( O
i.e. O
the O
upper O
bound O
for O
privacy O
) O
. O

The O
three O
modiﬁed O
training O
methods O
designed O
as O
defenses O
have O
a O
positive O
effect O
on O
privacy O
. O

De- O
spite O
a O
model O
selection O
based O
on O
accuracy O
, O
they O
lead O
to O
an O
improvement O
in O
privacy O
on O
all O
datasets O
, O
except O
on O
the O
France B-DatasetName
subcorpus O
. O

In O
most O
cases O
, O
we O
observe O
only O
a O
small O
decrease O
in O
accuracy B-MetricName
, O
or O
even O
an O
improvement O
at O
times O
( O
e.g. O
multidetasking O
on O
the O
Germany B-DatasetName
dataset O
, O
RAW B-TaskName
setting O
) O
, O
thus O
improv- O
ing O
the O
tradeoff O
between O
the O
utility O
and O
the O
privacy O
of O
the O
text O
representations O
. O

84.3.2 O
Topic O
Classiﬁcation O
We O
report O
results O
on O
topic B-TaskName
classiﬁcation I-TaskName
in O
Table O
5 O
. O

News O
articles O
For O
the O
news O
corpora O
, O
the O
privacy O
metric O
is O
based O
on O
the O
F B-MetricName
- I-MetricName
score O
on O
the O
binary O
vari- O
ables O
zindicating O
the O
presence O
or O
absence O
of O
a O
named O
entity O
in O
the O
text O
. O

First O
of O
all O
, O
we O
ob- O
serve O
that O
defense O
methods O
that O
explicitly O
use O
z O
( O
i.e. O
multidetasking O
and O
declustering O
) O
, O
have O
a O
very O
positive O
effect O
on O
privacy O
, O
but O
also O
a O
detrimental O
effect O
on O
the O
main O
task O
. O

We O
hypothesize O
that O
this O
is O
due O
to O
the O
strong O
correlations O
between O
the O
main O
task O
labelsyand O
the O
private O
information O
z. O

As O
a O
result O
, O
improving O
the O
privacy O
of O
the O
neural O
repre- O
sentations O
comes O
at O
a O
cost O
in O
accuracy B-MetricName
. O

In O
contrast O
, O
the O
adversarial B-TaskName
generation I-TaskName
defense O
method O
lead O
to O
an O
improvement O
in O
accuracy B-MetricName
, O
that O
is O
quite O
substantial O
for O
the O
DW B-DatasetName
corpus O
. O

We O
specu- O
late O
that O
this O
is O
due O
to O
the O
secondary O
term O
in O
the O
ob- O
jective O
function O
of O
the O
main O
model O
( O
Section O
3.1.2 O
) O
that O
helps O
avoiding O
overﬁtting O
the O
main O
task O
or O
learning O
spurious O
features O
. O

Blog O
posts O
On O
the O
blog O
post O
dataset O
, O
the O
effects O
are O
smaller O
, O
which O
we O
attribute O
to O
the O
nature O
of O
the O
task O
of O
the O
attacker O
. O

The O
defense O
methods O
con- O
sistently O
improve O
privacy B-MetricName
and O
, O
in O
one O
case O
, O
accu- O
racy I-MetricName
. O

The O
best O
effects O
on O
the O
tradeoff O
are O
achieved O
with O
the O
multidetasking O
and O
adversarial O
generation I-TaskName
methods O
. O

5 O
Discussion O
The O
main O
result O
of O
our O
experiments O
is O
that O
the O
de- O
fenses O
we O
propose O
improve O
privacy O
with O
usually O
a O
small O
effect O
, O
either O
positive B-MetricValue
or O
negative B-MetricValue
, O
on O
accu- O
racy I-MetricName
, O
thus O
improving O
the O
tradeoff O
between O
the O
util- O
ity O
and O
the O
privacy O
of O
neural O
representations O
. O

An O
important O
direction O
for O
future O
work O
is O
the O
choice O
of O
a O
strategy O
for O
model O
selection O
. O

The O
tradeoff O
between O
utility O
and O
privacy O
can O
be O
con- O
trolled O
in O
many O
ways O
. O

For O
example O
, O
the O
impor- O
tance O
of O
both O
terms O
in O
the O
loss O
functions O
in O
Sec- B-DatasetName
tion I-TaskName
3.1 O
can O
be O
controlled O
to O
favor O
either O
privacy O
or O
utility O
. O

In O
the O
scope O
of O
this O
paper O
, O
we O
did O
not O
perform O
thorough O
hyperparameter O
tuning O
, O
but O
be- O
lieve O
that O
doing O
so O
is O
important O
for O
achieving O
better O
results O
, O
since O
the O
effects O
of O
defense O
method O
can O
be O
more O
drastic O
than O
desired O
in O
some O
cases O
, O
as O
exem- O
pliﬁed O
on O
the O
news O
corpora O
( O
Table O
5 O
) O
. O

Overall O
, O
we O
found O
that O
the O
multidetasking O
ap- I-MetricName
proach I-MetricName
lead O
to O
the O
more O
stable O
improvements O
and O
should O
be O
preferred O
in O
most O
cases O
, O
since O
it O
is O
alsothe O
less O
computationnally O
expensive O
defense O
. O

On O
the O
other O
hand O
, O
the O
adversarial B-TaskName
generation I-TaskName
method O
does O
not O
require O
the O
speciﬁcation O
of O
private O
vari- O
ables O
, O
and O
thus O
is O
a O
more O
general O
approach O
. O

6 O
Related O
Work O
The O
deployment O
of O
machine O
learning O
in O
both O
academic O
and O
industrial O
contexts O
raises O
concerns O
about O
adversarial O
uses O
of O
machine O
learning O
, O
as O
well O
as O
concerns O
about O
attacks O
speciﬁcally O
targeted O
at O
these O
algorithms O
that O
often O
rely O
on O
large O
amounts O
of O
data O
, O
including O
personal O
data O
. O

More O
generally O
, O
the O
framework O
of O
differential O
privacy O
( O
Dwork O
, O
2006 O
) O
provides O
privacy O
guaran- O
tees O
for O
the O
problem O
of O
releasing O
information O
with- O
out O
compromising O
conﬁdential O
data O
, O
and O
usually O
involves O
adding O
noise O
in O
the O
released O
information O
. O

It O
has O
been O
applied O
to O
the O
training O
of O
deep O
learning O
models O
( O
Abadi O
et O
al O
. O
, O
2016 O
; O

Papernot O
et O
al O
. O
, O
2016 O
; O
Papernot O
et O
al O
. O
, O
2018 O
) O
, O
and O
Bayesian O
topic O
models O
( O
Schein O
et O
al O
. O
, O
2018 O
) O
. O

The O
notion O
of O
privacy B-MetricName
is O
particularly O
crucial O
to O
NLP B-TaskName
, O
since O
it O
deals O
with O
textual O
data O
, O
oftentimes O
user O
- O
generated O
data O
, O
that O
contain O
a O
lot O
of O
private O
in- O
formation O
. O

For O
example O
, O
textual O
data O
contain O
a O
lot O
of O
signal O
about O
authors O
( O
Hovy O
and O
Spruit O
, O
2016 O
) O
. O

and O
can O
be O
leveraged O
to O
predict O
demographic O
vari- O
ables O
( O
Rosenthal O
and O
McKeown O
, O
2011 O
; O
Preot O
¸iuc- O
Pietro O
et O
al O
. O
, O
2015 O
) O
. O

Oftentimes O
, O
this O
information O
is O
not O
explicit O
in O
the O
text O
but O
latent O
and O
related O
to O
the O
usage O
of O
various O
linguistic O
traits O
. O

Our O
work O
is O
based O
on O
a O
stronger O
hypothesis O
: O
this O
latent O
infor- O
mation O
is O
still O
present O
in O
vectorial O
representations O
of O
texts O
, O
even O
if O
the O
representations O
have O
not O
been O
supervised O
by O
these O
latent O
variables O
. O

Li O
et O
al O
. O

( O
2017 O
) O
study O
the O
privacy O
of O
unsuper- O
vised O
representations O
of O
images O
, O
and O
measures O
their O
privacy O
with O
the O
peak O
signal O
to O
noise O
ratio O
between O
an O
original O
image O
and O
its O
reconstruction O
by O
an O
attacker O
. O

They O
ﬁnd O
a O
tradeoff O
between O
the O
privacy O
of O
the O
learned O
representations O
and O
the O
ac- O
curacy O
of O
an O
image O
classiﬁcation O
model O
that O
uses O
these O
representations O
as O
inputs O
. O

Our O
setting O
is O
complementary O
since O
it O
is O
applied O
to O
NLP B-TaskName
tasks O
, O
but O
explores O
a O
similar O
problem O
in O
the O
case O
of O
rep- O
resentations O
learned O
with O
a O
task O
supervision O
. O

A O
related O
problem O
is O
the O
unintended O
memoriza- O
tion O
of O
private O
data O
from O
the O
training O
set O
and O
has O
been O
addressed O
by O
Carlini O
et O
al O
. O

( O
2018 O
) O
. O

They O
tackle O
this O
problem O
in O
the O
context O
of O
text O
gener- O
ation O
( O
machine B-TaskName
translation I-TaskName
, O
language O
modelling I-TaskName
) O
. O

If O
an O
attacker O
has O
access O
to O
e.g. O
a O
trained O
language O
model O
, O
they O
are O
likely O
to O
be O
able O
to O
generate O
sen- O
tences O
from O
the O
training O
set O
, O
since O
the O
language O
model O
is O
trained O
to O
assign O
high O
probabilities O
to O
those O
sentences O
. O

Such O
memorization O
is O
problem- O
atic O
when O
the O
training O
data O
contains O
private O
infor- O
mation O
and O
personal O
data O
. O

The O
experimental O
set- O
ting O
we O
explore O
is O
different O
from O
these O
works O
: O
we O
assume O
that O
the O
attacker O
has O
access O
to O
a O
hidden O
layer O
of O
the O
network O
and O
tries O
to O
recover O
informa- O
tion O
about O
an O
input O
example O
that O
is O
not O
in O
the O
train- O
ing O
set O
. O

In O
a O
recent O
study O
, O
Li O
et O
al O
. O

( O
2018 O
) O
proposed O
a O
method O
based O
on O
GAN B-MethodName
designed O
to O
improve O
the O
robustness O
and O
privacy O
of O
neural O
representations O
, O
applied O
to O
part B-TaskName
- I-TaskName
of I-TaskName
- I-TaskName
speech I-TaskName
tagging I-TaskName
and O
sentiment B-TaskName
analysis I-TaskName
. O

They O
use O
a O
training O
scheme O
with O
two O
agents O
similar O
to O
our O
multidetasking O
strategy O
( O
Sec- B-TaskName
tion I-TaskName
3.1.1 O
) O
, O
and O
found O
that O
it O
made O
neural O
represen- O
tations O
more O
robust O
and O
accurate O
. O

However O
, O
they O
only O
use O
a O
single O
adversary O
to O
alter O
the O
training O
of O
the O
main O
model O
and O
to O
evaluate O
the O
privacy O
of O
the O
representations O
, O
with O
the O
risk O
of O
overestimat- O
ing O
privacy O
. O

In O
contrast O
, O
once O
the O
parameters O
of O
our O
main O
model O
are O
ﬁxed O
, O
we O
train O
a O
new O
classiﬁer O
from O
scratch O
to O
evaluate O
privacy O
. O

7 O
Conclusion O
We O
have O
presented O
an O
adversarial O
scenario O
and O
used O
it O
to O
measure O
the O
privacy O
of O
hidden O
repre- O
sentations O
in O
the O
context O
of O
two O
NLP B-TaskName
tasks O
: O
senti- O
ment O
analysis O
and O
topic O
classiﬁcation O
of O
news O
arti- O
cle O
and O
blog O
posts O
. O

We O
have O
shown O
that O
in O
general O
, O
it O
is O
possible O
for O
an O
attacker O
to O
recover O
private O
vari- O
ables O
with O
higher O
than O
chance O
accuracy O
, O
using O
only O
hidden O
representations O
. O

In O
order O
to O
improve O
the O
privacy O
of O
hidden O
representations O
, O
we O
have O
pro- O
posed O
defense O
methods O
based O
on O
modiﬁcations O
of O
the O
training O
objective O
of O
the O
main O
model O
. O

Empiri- O
cally O
, O
the O
proposed O
defenses O
lead O
to O
models O
with O
a O
better O
privacy O
. O

Acknowledgments O
We O
thank O
the O
anonymous O
reviewers O
and O
members O
of O
the O
Cohort O
for O
helpful O
feedback O
on O
previous O
ver- O
sions O
of O
the O
article O
. O

We O
gratefully O
acknowledge O
the O
support O
of O
the O
European B-MethodName
Union I-MethodName
under O
the O
Horizon B-MethodName
2020 I-MethodName
SUMMA I-MethodName
project O
( O
grant O
agreement O
688139 O
) O
, O
and O
the O
support O
of O
Huawei B-MethodName
Technologies I-MethodName
. O

References O
Martin O
Abadi O
, O
Andy O
Chu O
, O
Ian O
Goodfellow O
, O
H. O
Bren- O
dan O
McMahan O
, O
Ilya B-MetricName
Mironov O
, O
Kunal B-MetricName
Talwar O
, O
and O
Li O
Zhang O
. O

2016 O
. O

Deep O
learning O
with O
differential O
pri- O
vacy O
. O

In O
Proceedings O
of O
the O
2016 O
ACM B-TaskName
SIGSAC I-DatasetName
Conference O
on O
Computer B-TaskName
and I-MethodName
Communications I-TaskName
Se- I-MethodName
curity I-TaskName
, O
CCS B-TaskName
’ O
16 O
, O
pages O
308–318 O
, O
New B-TaskName
York I-DatasetName
, O
NY O
, O
USA O
. O
ACM B-MethodName
. O

Steven O
Bird O
, O
Ewan B-MetricName
Klein O
, O
and O
Edward O
Loper O
. O
2009 O
. O

Natural B-MethodName
Language I-TaskName
Processing I-TaskName
with O
Python O
, O
1st O
edi- O
tion O
. O

O’Reilly B-MethodName
Media I-MethodName
, I-MethodName
Inc. I-MethodName
David O
M. O
Blei O
, O
Andrew O
Y O
. O

Ng O
, O
and O
Michael O
I. O
Jordan O
. O

2003 O
. O

Latent O
dirichlet B-TaskName
allocation O
. O

Journal O
of O
Ma- O
chine O

Learning B-TaskName
Research I-TaskName
, O
3:993–1022 O
. O

Nicholas B-MetricName
Carlini O
, O
Chang B-MetricName
Liu O
, O
Jernej B-MetricName
Kos O
, O
´ O
Ulfar B-MetricName
Erlings- O
son O
, O
and O
Dawn B-MetricName
Song O
. O

2018 O
. O

The O
secret O
sharer O
: O
Mea- O
suring O
unintended O
neural O
network O
memorization O
& O
extracting O
secrets O
. O

CoRR B-TaskName
, O
abs/1802.08232 O
. O

Gianna B-MetricName
M. O
Del O
Corso O
, O
Antonio O
Gull O
´ O
ı O
, O
and O
Francesco O
Romani O
. O

2005 O
. O

Ranking O
a O
stream O
of O
news O
. O

In O
Pro- O
ceedings O
of O
the O
14th B-DatasetName
International I-DatasetName
Conference I-DatasetName
on I-DatasetName
World I-DatasetName
Wide I-DatasetName
Web I-DatasetName
, O
WWW B-DatasetName
’ I-DatasetName
05 I-DatasetName
, O
pages O
97–106 O
, O
New B-TaskName
York I-DatasetName
, O
NY B-DatasetName
, O
USA B-DatasetName
. O
ACM B-TaskName
. O

Cynthia O
Dwork O
. O

2006 O
. O

Differential B-TaskName
privacy I-TaskName
. O

In O
33rd O
International O
Colloquium I-DatasetName
on O
Automata O
, O
Languages O
and O
Programming O
, O
part O
II O
( O
ICALP B-MethodName
2006 O
) O
, O
volume O
4052 O
, O
pages O
1–12 O
, O
Venice B-TaskName
, O
Italy O
. O

Springer B-MethodName
Verlag I-MethodName
. O

Ian O
Goodfellow O
, O
Jean O
Pouget O
- O
Abadie O
, O
Mehdi B-MetricName
Mirza O
, O
Bing O
Xu O
, O
David O
Warde O
- O
Farley O
, O
Sherjil B-MetricName
Ozair O
, O
Aaron O
Courville O
, O
and O
Yoshua B-MetricName
Bengio O
. O

2014 O
. O

Generative B-TaskName
adversarial I-TaskName
nets I-TaskName
. O

In O
Z. O
Ghahramani O
, O
M. O
Welling O
, O
C. O
Cortes O
, O
N. O
D. O
Lawrence O
, O
and O
K. O
Q. O
Weinberger O
, O
editors O
, O
Advances B-MethodName
in O
Neural B-TaskName
Information I-TaskName
Processing I-TaskName
Systems I-TaskName
27 O
, O
pages O
2672–2680 O
. O

Curran B-MethodName
Associates I-MethodName
, I-MethodName
Inc. I-MethodName

Sepp B-MetricName
Hochreiter O
and O
J O
¨urgen O
Schmidhuber O
. O

1997 O
. O

Long O
short O
- O
term O
memory O
. O

Neural B-TaskName
computation I-TaskName
, O
9(8):1735–1780 O
. O

Dirk B-MetricName
Hovy O
. O
2015 O
. O

Demographic O
factors O
improve O
clas- B-TaskName
siﬁcation I-TaskName
performance O
. O

In O
Proceedings O
of O
the O
53rd O
Annual O
Meeting O
of O
the O
Association B-MethodName
for I-MethodName
Computa- I-MethodName
tional I-MethodName
Linguistics I-MethodName
and O
the O
7th O
International I-DatasetName
Joint I-MethodName
Conference I-MethodName
on I-MethodName
Natural I-MethodName
Language I-MethodName
Processing I-MethodName
( O
Vol- O
ume O
1 O
: O
Long O
Papers O
) O
, O
pages O
752–762 O
, O
Beijing O
, O
China O
. O

Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
. O

Dirk B-MetricName
Hovy O
, O
Anders O
Johannsen O
, O
and O
Anders O
Søgaard O
. O

2015 O
. O

User O
review O
sites O
as O
a O
resource O
for O
large- O
scale O
sociolinguistic O
studies O
. O

In O
Proceedings O
of O
the O
24th O
International I-DatasetName
Conference I-DatasetName
on I-DatasetName
World B-DatasetName
Wide I-DatasetName
Web I-DatasetName
, O
WWW B-DatasetName
’ I-DatasetName
15 O
, O
pages O
452–461 O
, O
Republic O
and O
Canton O
of O
Geneva B-DatasetName
, O
Switzerland O
. O

International B-DatasetName
World I-DatasetName
Wide I-DatasetName
Web I-DatasetName
Conferences I-DatasetName
Steering O
Committee O
. O

Dirk B-MetricName
Hovy O
and O
Anders O
Søgaard O
. O

2015 O
. O

Tagging O
perfor- O
mance O
correlates O
with O
author O
age O
. O

In O
Proceedings O
of O
the O
53rd O
Annual O
Meeting O
of O
the O
Association B-MethodName
for I-MethodName

10Computational O
Linguistics I-TaskName
and O
the O
7th O
International I-DatasetName
Joint I-DatasetName
Conference I-DatasetName
on O
Natural I-MethodName
Language I-TaskName
Processing I-TaskName
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
483–488 O
, O
Beijing O
, O
China O
. O
Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
. O

Dirk B-MetricName
Hovy O
and O
Shannon O
L. O
Spruit O
. O

2016 O
. O

The O
social O
impact O
of O
natural B-TaskName
language O
processing O
. O

In O
Proceed- O
ings O
of O
the O
54th O
Annual O
Meeting O
of O
the O
Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
( O
Volume O
2 O
: O
Short O
Pa- O
pers O
) O
, O
pages O
591–598 O
, O
Berlin O
, O
Germany O
. O

Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
. O

Diederik B-MetricName
P. O
Kingma O
and O
Jimmy O
Ba O
. O
2014 O
. O

Adam O
: O
A O
method O
for O
stochastic B-TaskName
optimization I-TaskName
. O

CoRR B-TaskName
, O
abs/1412.6980 O
. O

Meng O
Li O
, O
Liangzhen O
Lai O
, O
Naveen O
Suda O
, O
Vikas O
Chan- O
dra O
, O
and O
David O
Z. O
Pan O
. O
2017 O
. O
Privynet B-MethodName
: O
A O
ﬂexible O
framework O
for O
privacy O
- O
preserving O
deep O
neural O
net- O
work O
training O
with O
A O
ﬁne O
- O
grained O
privacy O
control O
. O

CoRR B-TaskName
, O
abs/1709.06161 O
. O

Yitong O
Li O
, O
Timothy O
Baldwin O
, O
and O
Trevor B-MetricName
Cohn O
. O

2018 O
. O

Towards O
robust O
and O
privacy O
- O
preserving O
text O
repre- O
sentations O
. O

In O
Proceedings O
of O
the O
56th O
Annual O
Meet- O
ing O
of O
the O
Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
25–30 O
, O
Melbourne O
, O
Australia O
. O

Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguis- I-MethodName
tics I-MethodName
. O

Graham O
Neubig O
, O
Chris O
Dyer O
, O
Yoav B-MetricName
Goldberg O
, O
Austin O
Matthews O
, O
Waleed O
Ammar O
, O
Antonios O
Anastasopou- O
los O
, O
Miguel O
Ballesteros O
, O
David O
Chiang O
, O
Daniel O
Clothiaux O
, O
Trevor O
Cohn O
, O
Kevin O
Duh O
, O
Manaal B-MetricName
Faruqui O
, O
Cynthia O
Gan O
, O
Dan O
Garrette O
, O
Yangfeng O
Ji O
, O
Lingpeng O
Kong O
, O
Adhiguna B-MetricName
Kuncoro O
, O
Gaurav O
Ku- O
mar O
, O
Chaitanya B-MetricName
Malaviya O
, O
Paul O
Michel O
, O
Yusuke O
Oda O
, O
Matthew O
Richardson O
, O
Naomi O
Saphra O
, O
Swabha B-MetricName
Swayamdipta O
, O
and O
Pengcheng O
Yin O
. O
2017 O
. O
Dynet O
: O
The O
dynamic O
neural I-MethodName
network I-MethodName
toolkit O
. O

arXiv O
preprint O
arXiv:1701.03980 O
. O

Nicolas O
Papernot O
, O
Mart O
´ O
ın O
Abadi O
, O
´ O
Ulfar O
Erlingsson O
, O
Ian O
J. O
Goodfellow O
, O
and O
Kunal B-MetricName
Talwar O
. O

2016 O
. O

Semi- O
supervised O
knowledge O
transfer O
for O
deep O
learning O
from O
private O
training O
data O
. O

CoRR B-TaskName
, O
abs/1610.05755 O
. O

Nicolas B-MetricName
Papernot O
, O
Shuang B-MetricName
Song O
, O
Ilya B-MetricName
Mironov O
, O
Ananth B-MetricName
Raghunathan O
, O
Kunal B-MetricName
Talwar O
, O
and O
´ O
Ulfar B-MetricName
Erlingsson O
. O

2018 O
. O

Scalable B-MethodName
Private I-MethodName
Learning I-MethodName
with I-MethodName
PATE B-MethodName
. O

ArXiv B-MethodName
e O

-prints O
, O
abs/1802.08908 O
. O

Nikolaos B-MetricName
Pappas O
and O
Andrei B-MetricName
Popescu O
- O
Belis O
. O

2017 O
. O

Multilingual O
hierarchical O
attention O
networks O
for O
doc- B-TaskName
ument I-TaskName
classiﬁcation I-TaskName
. O

In O
Proceedings O
of O
the O
Eighth O
International I-DatasetName
Joint I-DatasetName
Conference I-DatasetName
on O
Natural I-MethodName
Lan- I-MethodName
guage I-MethodName
Processing I-MethodName
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1015–1025 O
, O
Taipei O
, O
Taiwan O
. O

Asian B-MethodName
Federation I-MethodName
of I-MethodName
Natural I-MethodName
Language I-MethodName
Processing I-MethodName
. O

Daniel B-MetricName
Preot O
¸iuc O
- O
Pietro O
, O
Vasileios B-MetricName
Lampos O
, O
and O
Niko- B-MetricName
laos O
Aletras O
. O

2015 O
. O

An O
analysis O
of O
the O
user O
occupa- O
tional O
class O
through O
twitter O
content O
. O

In O
Proceedings O
of O
the O
53rd O
Annual O
Meeting O
of O
the O
Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
and O
the O
7th O
InternationalJoint B-DatasetName
Conference I-MethodName
on O
Natural I-MethodName
Language I-MethodName
Processing I-MethodName
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1754–1764 O
, O
Bei- O
jing O
, O
China O
. O

Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguis- I-MethodName
tics I-MethodName
. O

Sara O
Rosenthal O
and O
Kathleen O
McKeown O
. O
2011 O
. O

Age O
prediction O
in O
blogs O
: O
A O
study O
of O
style O
, O
content O
, O
and O
online O
behavior O
in O
pre- O
and O
post O
- O
social O
media O
gen- O
erations O
. O

In O
Proceedings O
of O
the O
49th O
Annual O
Meet- O
ing O
of O
the O
Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguis- I-MethodName

tics O
: O
Human B-MethodName
Language I-MethodName
Technologies I-MethodName
, O
pages O
763 O
– O
772 O
, O
Portland B-TaskName
, O
Oregon O
, O
USA O
. O
Association B-MethodName
for I-MethodName
Com- I-MethodName
putational I-MethodName
Linguistics I-MethodName
. O

Aaron B-MetricName
Schein O
, O
Zhiwei B-MetricName
Steven O
Wu O
, O
Mingyuan B-MetricName
Zhou O
, O
and O
Hanna B-MetricName
Wallach O
. O

2018 O
. O

Locally O
Private O
Bayesian O
Inference O
for O
Count O
Models O
. O

ArXiv B-MethodName
e O
- O
prints O
, O
abs/1803.08471 O
. O

Jonathan B-MetricName
Schler O
, O
Moshe B-MetricName
Koppel O
, O
Shlomo B-MetricName
Argamon O
, O
and O
James B-MetricName
Pennebaker O
. O

2006 O
. O

Effects O
of O
age O
and O
gender O
on O
blogging O
. O

In O
Computational O
Approaches O
to O
Analyzing O
Weblogs O
- O
Papers O
from O
the O
AAAI B-DatasetName
Spring O
Symposium O
, O
Technical O
Report O
, O
volume O
SS-06 O
- O
03 O
, O
pages O
191–197 O
. O

Yequan B-MetricName
Wang O
, O
Minlie B-MetricName
Huang O
, O
Xiaoyan B-MetricName
Zhu O
, O
and O
Li B-MetricName
Zhao O
. O

2016 O
. O

Attention O
- O
based O
LSTM B-TaskName
for O
aspect- B-TaskName
level I-TaskName
sentiment I-TaskName
classiﬁcation I-TaskName
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natu- B-TaskName
ral I-TaskName
Language I-TaskName
Processing I-TaskName
, O
pages O
606–615 O
, O
Austin B-TaskName
, O
Texas O
. O

Association B-MethodName
for I-MethodName
Computational I-MethodName
Linguistics I-MethodName
. O

Xiang B-MetricName
Zhang O
, O
Junbo B-MetricName
Zhao O
, O
and O
Yann O
LeCun O
. O
2015 O
. O

Character O
- O
level O
convolutional O
networks O
for O
text B-TaskName
clas- I-TaskName
siﬁcation I-TaskName
. O

In O
C. O
Cortes O
, O
N. O
D. O
Lawrence O
, O
D. O
D. O
Lee O
, O
M. O
Sugiyama O
, O
and O
R. O
Garnett O
, O
editors O
, O
Advances B-MethodName
in I-MethodName
Neural B-TaskName
Information I-TaskName
Processing I-TaskName
Systems I-MethodName
28 O
, O
pages O
649–657 O
. O

Curran B-MethodName
Associates I-MethodName
, I-MethodName
Inc. I-MethodName

Peng O
Zhou O
, O
Zhenyu B-MetricName
Qi O
, O
Suncong O
Zheng O
, O
Jiaming O
Xu O
, O
Hongyun O
Bao O
, O
and O
Bo O
Xu O
. O

2016 O
. O

Text O
classification O
improved O
by O
integrating O
bidirectional O
lstm O
with O
two- O
dimensional O
max O
pooling O
. O

In O
Proceedings O
of O
COL- B-MethodName
ING I-MethodName
2016 O
, O
the O
26th O
International I-DatasetName
Conference I-DatasetName
on O
Computational O
Linguistics I-MethodName
: O
Technical O
Papers O
, O
pages O
3485–3495 O
, O
Osaka O
, O
Japan O
. O

The O
COLING B-MethodName
2016 I-MethodName
Or- I-MethodName
ganizing I-MethodName
Committee I-MethodName
. O
