Language Model Augmented Monotonic Attention for Simultaneous Translation Sathish Indurthi§∗Mohd Abbas Zaidi‡ Beomseok Lee‡†Nikhil Kumar Lakumarapu‡†Sangha Kim‡ ‡Samsung Research , South Korea§Zoom AI Lab , Singapore sathishreddy.indurthi@zoom.us , { abbas.zaidi , bsgunn.lee , n07.kumar , sangha01.kim}@samsung.com 
 Abstract The state - of - the - art adaptive policies for simul- taneous neural machine translation ( SNMT ) use monotonic attention to perform read / write decisions based on the partial source and target sequences . 
 The lack of sufficient information might cause the monotonic attention to take poor read / write decisions , which in turn neg- atively affects the performance of the SNMT model . 
 On the other hand , human translators make better read / write decisions since they can anticipate the immediate future words using linguistic information and domain knowledge . 
 In this work , we propose a framework to aid monotonic attention with an external language model to improve its decisions . 
 Experiments on MuST-C English - German and English - French speech - to - text translation tasks show the future information from language model improves the state - of - the - art monotonic multi - head attention model further . 
 1 Introduction A typical application of simultaneous neural ma- chine translation ( SNMT ) is conversational speech or live video caption translation . 
 In order to achieve live translation , an SNMT model alternates be- tween performing read from source sequence and write to target sequence . 
 For a model to decide whether to read orwrite at certain moment , either a fixed or an adaptive read / write policy can be used . 
 Earlier approaches in simultaneous translation such as Ma et al . ( 2019a ) and Dalvi et al . 
 ( 2018 ) employ a fixed policy that alternate between read andwrite after the waiting period of ktokens . 
 To alleviate possible long delay of fixed polices , re- cent works such as monotonic infinite lookback attention ( MILk ) ( Arivazhagan et al . , 2019 ) , and monotonic multihead attention ( MMA ) ( Ma et al . , 2019c ) developed flexible policies using monotonic attention ( Raffel et al . , 2017 ) . 
 While these monotonic attention anticipates tar- get words using only available prefix source and target sequence , human translators anticipate the target words using their language expertise ( linguis- tic anticipation ) as well as contextual information ( extra - linguistic anticipation ) ( Vandepitte , 2001 ) . 
 Inspired by human translation experts , we aim to augment monotonic attention with future informa- tion using language models ( LM ) ( Devlin et al . , 2019 ; Conneau et al . , 2019 ) . 
 Integrating the external information effectively into text - to - text machine translation ( MT ) systems has been explored by several works ( Khandelwal et al . , 2020 ; Gulcehre et al . , 2015 , 2017 ; Stahlberg et al . , 2018 ) . 
 Also , integrating future information implicitly into SNMT system during training is ex- plored in Wu et al . 
 ( 2020 ) by simultaneously train- ing different wait-K SNMT systems . 
 However , no previous works make use of explicit future informa- tion both during training and inference . 
 To utilize explicit future information , we explored to inte- grate future information from LM directly into the output layer of the MMA model . 
 However , it did not provide any improvements ( refer to Appendix A ) , thus motivating us to explore a tighter integra- tion of the LM information into SNMT model . 
 In this work , we explicitly use plausible future information from LM during training by transform- ing the monotonic attention mechanism . 
 As shown in Figure 1 , at each step , the LM takes the prefix target ( and source , for cross - lingual LM ) sequence and predicts the probable future information . 
 We hypothesize that aiding the monotonic attention with this future information can improve MMA model ’s read / write policy , eventually leading to better translation with less delay . 
 Several experi- ments on MuST-C ( Di Gangi et al . , 2019 ) 
 English- 
 German and English - French speech - to - text translation tasks with our proposed approach show clear improvements of latency - quality trade - offs over the state - of - the - art MMA models . 
 2 Monotonic Attention with Future Information Model 2.1 Monotonic Attention In simultaneous machine translation ( SNMT ) mod- els , the probability of predicting the target token yi∈ydepends on the partial source and target sequences ( x≤j∈x , y < i∈y ) . 
 In sequence - to- sequence based SNMT model , each target token yi is generated as follows : 
 hj = E(x≤j ) ( 1 ) si = D(y < i , ci = A(si−1 , h≤j ) ) ( 2 ) yi = Output ( si ) ( 3 ) where E(.)andD(.)are the encoder and decoder layers , and ciis a context vector . 
 In monotonic attention based SNMT , the context vector is com- puted as follows : ei , j = MonotonicEnergy ( si−1 , hj)(4 ) pi , j = Sigmoid ( ei , j ) ( 5 ) zi , j∼Bernoulli ( pi , j ) ( 
 When generating a target token yi , the decoder chooses whether to read / write based on Bernoulli selection probability pi , j. When zi , j= 1 ( write ) , model sets ti = j , ci = hjand generates the target token yi . 
 Forzi , j= 0(read ) , it sets ti = j+ 1and repeats Eq . 4 to 6 . 
 Here tirefers to the index of the encoder when decoder needs to produce the ith target token . 
 Instead of hard alignment of ci = hj , Raffel et al . 
 ( 2017 ) compute an expected alignment in a recurrent manner and propose a closed - form parallel solution . 
 Arivazhagan et 
 al . 
 ( 2019 ) adopt monotonic attention into SNMT and later , Ma et al . 
 ( 2019c ) extend it to MMA to integrate it into the Transformer model ( Vaswani et al . , 2017 ) . 
 2.2 Monotonic Attention with Future Information 
 The monotonic attention described in Section 2.1 performs anticipation based only on the currently available source and target information . 
 To aug- ment this anticipation process using future informa- tion extracted using LMs , we propose the following modifications to the monotonic attention . 
 Future Representation Layer : 
 At every de- coding step i , the previous target token yi−1is equipped with a plausible future token ˆyias shown in the Figure 2 . 
 Since the token ˆyicomes from an LM possibly with a different tokenizer and vo- cabulary set , applying the model ’s tokenizer and vocabulary might split the token ˆyifurther into mul- tiple sub - tokens { ˆy1 i,ˆy2 i,···,ˆym i } . 
 To get a single future token representation ˜si∈ Rdfrom all the sub - tokens , we apply a sub - token summary layer : 
 ˜si= Γ({ˆy1 i,ˆy2 i,···,ˆym i } ) ( 7 ) TheΓrepresents a general sequence representation layer such as a Transformer encoder layer or a sim- ple normalized sum of sub - token representations . 
 We enrich ˜siat every layer lof the decoder block by applying a residual feed - forward network . 
 ˜sl i = FFN ( ˜yl−1 i ) ( 8) Monotonic Energy Layer with Future Informa- tion : Despite the fact that we can add the plau- sible future information to the output layer ( Ap- pendix A ) or append it to the target token represen- 
 tation yi−1 , the MMA read / write decisions happen in Eq . 
 4 . 
 Therefore , we integrate ˜siinto the Eq . 4 instead . 
 The integration is carried out by modifying Eq . 4 - Eq . 
 5 . 
 We compute the monotonic energy for future information using the enriched future token representation ˜siavailable at each layer : ˜ei , j = MonotonicEnergy ( ˜si , hj ) ( 9 ) We integrate the future monotonic energy function into Eq . 5 as follows : ˜pi , j = Sigmoid ( ei , j+ ˜ei , j ) ( 10 ) After computing ˜pi , j , we compute cisimilar to MMA model . 
 This way of integration of future information allows the model to condition the LM output us- age on the input sequence . 
 The model can control the relative weightage given to the LM output by varying the ˜ei , j. In case of insufficient source in- formation in the low latency regime , we expect the model ’s decision policy to rely more on ˜ei , j. Inference : 
 During inference , the start token does not contain any plausible information . 
 After pre- dicting the first target token , for every subsequent prediction of target token yi , we invoke the LM to predict the next plausible future token and integrate this new information into Eq . 
 10 . 3 Experiments and Results 3.1 Experimental Settings Datasets and Metrics : We conduct our experi- ments on the MuST-C English(En)-German(De ) and English(En)-French(Fr ) speech - to - text ( ST ) translation task . 
 The speech sequence is repre- sented using 80 - dimensional log - mel filter bank features . 
 The target sequence is represented as sub- words using a SentencePiece ( Kudo and Richard- son , 2018 ) model with a unigram vocabulary of size 10,000 . 
 We evaluate the performance of the models on both the latency and quality aspects . 
 We use Average Lagging ( AL ) as our latency metric and case - sensitive detokenized SacreBLEU ( Post , 2018 ) to measure the translation quality , similar to ( Ma et al . , 2020 ) . 
 The best models are chosen based on the dev set results and reported results are from the MuST-C test ( tst-COMMON ) sets . 
 Language Models We use two language mod- els to train our proposed modified MMA model . 
 Firstly , we use the pretrained XLM - RoBERTa ( Conneau et al . , 2019 ) model from Huggingface Transformers1model repository . 
 Since the LM out- put can be very open - ended and might not directly suit / cater to our task and dataset , we finetune the head of the model using the MuST-C target text data for each task . 
 We also train a smaller language model ( SLM ) , which contains 6 Transformer decoder layers , 512 hidden - states and 24 M parameters . 
 We use the MuST-C data along with additional data augmen- tation to reduce overfitting . 
 The SLM helps to remove the issues related to vocabulary mismatch as discussed in the Section 2.2 . 
 Implementation Details : Our base model is adopted from Ma et al . 
 ( 2020 ) . 
 We use a pre- decision ratio of 7 , which means that the simultane- ousread / write decisions are made after every seven encoder states . 
 We use λ or λlatency to refer to the hyperparameter corresponding to the weighted average ( λavg ) in MMA . 
 The values of this hyperpa- rameter λ are chosen from the set { 0.01,0.05,0.1 } . 
 TheΓlayer in Eq . 
 7 computes the normalized sum of the sub - token representations . 
 For SLM , it sim- ply finds the embedding since it shares the same vocabulary set . 
 All the models are trained on a NVIDIA v100 GPU with update_freq set to 8 . 
 Simultaneous Translation Models : Even though future information can be integrated explicitly into the fixed policy approaches such as Wait - K ( Ma et al . , 2019b ) , we choose monotonic attention as our baseline due to its superior performance ( Arivazhagan et al . , 2019 ; Ma et al . , 2019c ) . 
 We train a baseline based on Ma et al . 
 ( 2020 ) work , called as MMA model . 
 The MMA model encoder and decoder embedding dimensions are set to 392 , whereas our proposed model ’s encoder and decoder embeddings are set to 256 to have similar parameters ( ≈39 M ) for a fair comparison . 
 We train two models using the modified MMA based on two LMs ( XLM , SLM ) , referred as MMA - XLM and MMA - SLM . 
 3.2 Results We first analyze how the LM predictions are being utilized by the our model . 
 In order to measure the relative weight given to model ’s internal states ver- 
 sus the predictions from the LM , we compare the norm of the monotonic energies corresponding to the LM predictions epred(Eq . 
 9 ) and the previous output tokens eoutput ( Eq . 4 ) . 
 Let us define LM prediction weight as follows : LMpw=/parenleftbigg∥epred∥ ∥eoutput∥/parenrightbigg ( 11 ) In Figure 3 , we plot the variation of LMpw ( averaged ) vs. λ . 
 We use two additional values of λ ∈ { 0.005 0.001 }to obtain this plot . 
 We can observe that as the latency requirements become more and more strict , the model starts to give more weightage to the predictions coming from the LM . 
 This shows that the model learns to utilize the in- formation coming from LM predictions based on latency requirements . 
 Next , we discuss the performance improvements obtained from our proposed approach . 
 By vary- ing the λ , we train separate models for different latency regimes . 
 Moreover , the quality and latency for a particular model can also be varied by control- ling the speech segment size during the inference . 
 Speech segment size or step size refers to the du- ration of speech ( in ms ) processed corresponding to each read decision . 
 We vary these hyperparame- ters for all the three models , namely MMA , MMA- XLM and MMA - SLM . 
 The BLEU - AL curves for all the models have been provided in Figure 4 and BLEU - AL num- bers for all models are included in Appendix F for reference . 
 We vary the step sizes in intervals of 80ms from 120ms to 520ms in order to get performances corresponding to different latency regimes . 
 We can observe that the LM - based mod- els using both XLM and SLM provide a significant performance improvement over the baseline MMA model . 
 We observe improvements in the range of 1 - 2 BLEU scores consistently across all the latency regimes ( λ = 0.1 0.05 0.01 ) . 
 The MMA using SLM language model performs slightly better than MMA using XLM language model . 
 This is due to SLM ’s higher accuracy on the next token predic- tion task as compared to XLM , 30.15% vs. 21.5% for German & 31.65% vs. 18.45% for French . 
 The high accuracy of SLM is attributed to its training on in - domain data . 
 4 Conclusion In this work , we provide a generic framework to integrate the linguistic and extra - linguistic infor- mation into simultaneous models . 
 We rely on lan- guage models to extract this plausible future in- formation and propose a new monotonic attention mechanism to infuse this information . 
 Several ex- periments on speech - to - text translation tasks show the effectiveness of proposed approach on obtain- ing superior quality - latency trade - offs , compared to the state - of - the - art monotonic multihead attention . 
 References Naveen Arivazhagan , Colin Cherry , Wolfgang Macherey , Chung - Cheng Chiu , Semih Yavuz , Ruom- ing Pang , Wei Li , and Colin Raffel . 
 2019 . 
 Monotonic infinite lookback attention for simultaneous machine translation . 
 In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1313–1323 , Florence , Italy . Association for Computational Linguistics . 
 Alexis Conneau , Kartikay Khandelwal , Naman Goyal , Vishrav Chaudhary , Guillaume Wenzek , Francisco Guzmán , Edouard Grave , Myle Ott , Luke Zettle- moyer , and Veselin Stoyanov . 
 2019 . 
 Unsupervised cross - lingual representation learning at scale . 
 arXiv preprint arXiv:1911.02116 . 
 Fahim Dalvi , Nadir Durrani , Hassan Sajjad , and Stephan V ogel . 
 2018 . 
 Incremental decoding and training methods for simultaneous translation in neural machine translation . 
 In Proceedings of the 2018 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics : Human Lan- guage Technologies , Volume 2 ( Short Papers ) , pages 493–499 , New Orleans , Louisiana . Association for Computational Linguistics . 
 Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 . 
 BERT : Pre - training of deep bidirectional transformers for language under- standing . 
 In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Tech- nologies , Volume 1 ( Long and Short Papers ) , pages 4171–4186 , Minneapolis , Minnesota . 
 Association for Computational Linguistics . 
 Mattia A. Di Gangi , Roldano Cattoni , Luisa Bentivogli , Matteo Negri , and Marco Turchi . 
 2019 . 
 MuST-C : a Multilingual Speech Translation Corpus . 
 In Proceed- ings of the 2019 Conference of the North American Chapter of the Association for Computational Lin- guistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 2012–2017 , Min- neapolis , Minnesota . 
 Association for Computational Linguistics . 
 Caglar Gulcehre , Orhan Firat , Kelvin Xu , Kyunghyun Cho , Loic Barrault , Huei - Chi Lin , Fethi Bougares , Holger Schwenk , and Yoshua Bengio . 2015 . 
 On using monolingual corpora in neural machine trans- lation . 
 Caglar Gulcehre , Orhan Firat , Kelvin Xu , Kyunghyun Cho , and Yoshua Bengio . 2017 . 
 On integrating a lan- guage model into neural machine translation . 
 Com- puter Speech and Language , 45:137–148 . 
 Urvashi Khandelwal , Angela Fan , Dan Jurafsky , Luke Zettlemoyer , and Mike Lewis . 
 2020 . 
 Nearest neighbor machine translation . 
 arXiv preprint arXiv:2010.00710 . 
 Sosuke Kobayashi . 
 2018 . 
 Contextual augmentation : Data augmentation by words with paradigmatic re- lations . 
 In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Tech- nologies , Volume 2 ( Short Papers ) , pages 452–457 , New Orleans , Louisiana . 
 Association for Computa- tional Linguistics . 
 Taku Kudo and John Richardson . 
 2018 . 
 Sentencepiece : 
 A simple and language independent subword tok- enizer and detokenizer for neural text processing . 
 arXiv preprint arXiv:1808.06226 . 
 Mingbo Ma , Liang Huang , Hao Xiong , Renjie Zheng , Kaibo Liu , Baigong Zheng , Chuanqiang Zhang , Zhongjun He , Hairong Liu , Xing Li , Hua Wu , and Haifeng Wang . 2019a . 
 Stacl : 
 Simultaneous trans- lation with implicit anticipation and controllable la- tency using prefix - to - prefix framework . 
 Mingbo Ma , Liang Huang , Hao Xiong , Renjie Zheng , Kaibo Liu , Baigong Zheng , Chuanqiang Zhang , Zhongjun He , Hairong Liu , Xing Li , Hua Wu , and Haifeng Wang . 
 2019b . 
 STACL : 
 Simultaneous trans- lation with implicit anticipation and controllable la- tency using prefix - to - prefix framework . 
 In Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3025–3036 , Flo- rence , Italy . 
 Association for Computational Linguis- tics . 
 Xutai Ma , Juan Pino , James Cross , Liezl Puzon , and Jiatao Gu . 2019c . 
 Monotonic multihead attention . 
 Xutai Ma , Juan Pino , and Philipp Koehn . 2020 . 
 Simulmt to simulst : Adapting simultaneous text translation to end - to - end simultaneous speech trans- lation . 
 arXiv preprint arXiv:2011.02048 . 
 Matt Post . 2018 . 
 A call for clarity in reporting bleu scores . 
 arXiv preprint arXiv:1804.08771 . 
 Colin Raffel , Minh - Thang Luong , Peter J. Liu , Ron J. Weiss , and Douglas Eck . 2017 . 
 Online and linear- time attention by enforcing monotonic alignments . 
 InProceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research , pages 2837–2846 . PMLR . 
 Felix Stahlberg , James Cross , and Veselin Stoyanov . 
 2018 . 
 Simple fusion : Return of the language model . 
 InProceedings of the Third Conference on Machine Translation : Research Papers , pages 204–211 , Brus- sels , Belgium . 
 Association for Computational Lin- guistics.42 
 Sonia Vandepitte . 
 2001 . 
 Anticipation in conference interpreting : a cognitive process . 
 Alicante Journal of English Studies / Revista Alicantina de Estudios Ingleses , 0(14):323–335 . 
 Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Ł ukasz Kaiser , and Illia Polosukhin . 2017 . 
 Attention is all you need . 
 In Advances in Neural Information Pro- 
 cessing Systems 30 , pages 5998–6008 . 
 Curran Asso- ciates , 
 Inc. 
 Xueqing Wu , Yingce Xia , Lijun Wu , Shufang Xie , Weiqing Liu , Jiang Bian , Tao Qin , and Tie - Yan Liu . 2020 . 
 Learn to use future information in simultane- ous translation . 
 A LM at MMA Output Layer We explored a naive approach of integrating LM information into the MMA . 
 In this approach , we in- tegrate the future information obtained from the LM directly into the output layer of the MMA model . 
 We refer to this experiment as ‘ LM Rescor- ing(LMR ) ’ , and the corresponding model is called MMA - LMR . 
 As observed in Figure 5 , MMA - LMR has infe- rior performance compared to the MMA model . 
 Since the LM information integration is only done at the output layer of the model , the MMA model can not easily discard the incorrect information from LM . 
 This motivates us to tightly integrate the LM information into the simultaneous model . 
 B Language Models As mentioned earlier , we train two different lan- guage models ( LMs ) and use them to improve the anticipation in monotonic attention based Simulta- neous models . 
 B.1 XLM - RoBERTa - R ) XLM - R Large model2was trained on the 100 lan- guages Common Crawl corpora total size of 2.5 TB with 550 M parameters from 24 layers , 1024 hid- den states , 4096 feed - forward hidden - states , and 16 heads . 
 Total number of parameters is 558M. We finetune the head of the XLM - R LM model using the Masked Language Modeling objective which accounts for 0.23 % of the total model parameters , i.e. , 1.3 M parameters . 
 B.2 Smaller Language Model Since the LM predictions are computed serially during inference , the time taken to compute the token serves as a bottleneck to the latency re- quirements . 
 To reduce the LM computation time , we train a smaller Language Model ( SLM ) from scratch using the Causal Language Modeling ob- jective . 
 SLM is composed of 6 Transformer decoder blocks , 512 hidden - states , 2048 feed - forward hidden - states & 8 attention heads . 
 It alleviates the need for the sub - token summary layer since it shares the vocabulary and tokenization with the MMA models . 
 The train examples are at the sen- tence level , rather than forming a block out of multi- ple sentences(which is the usual case for Language Models ) . 
 Since the target texts contain lesser than 250k examples , we use additional data augmentation techniques to upsample the target data . 
 We also use additional data to avoid overfitting on the MuST-C target text . 
 Details have been provided in B.2.1 . 
 B.2.1 Data Augmentation Up - Sampling : To boost the LM performance and mitigate overfitting , we use contextual data augmentation ( Kobayashi , 2018 ) to upsample the MuST-C target text data by substituting and insert- ing words based on LM predictions . 
 We use the NLPAUG3package to get similar words based on contextual embeddings . 
 From the Hugging Face Repository , we use two different pretrained BERT ( Devlin et al . , 2019 ) models for German bert - base- german - dbmdz - cased & bert - base - german - dbmdz- uncased andbert - base - fr - cased for French . 
 We upsample German to 1.13 M examples and French to 1.38 M examples . 
 Additional Data : We also use additional data to avoid overfitting . 
 For German we use the Newscrawl ( WMT19 ) data which includes 58 M examples . 
 For French , we use Common Crawl and Europarl to augment 4 M extra training examples . 
 We observe that both upsampling and data aug- mentation help us to reduce the overfitting on the MuST-C dev set . 
 B.3 Token Prediction For each output token , the LM prediction is ob- tained by feeding the prefix upto that token to the LM model . 
 These predictions are pre - computed for training and validation sets . 
 This ensures par- allelization and avoids the overhead to run the LM simultaneously during the training process . 
 During inference , the LM model is called every time a new output token is written . 
 C Dataset 
 The MuST-C dataset comprises of English TED talks , the translations and transcriptions have been aligned with the speech at sentence level . 
 Dataset statistics have been provided in the Table 1 . 
 D Effect of LM Size on Latency - Quality We train several SLM models with varying sizes in our experiments and choose the best model based on the top-1 accuracy . 
 As we increase the number of layers in the LM model from 2 to 4 to 6 layers , the SLM and the proposed MMA with future infor- mation models have shown performance improve- ments . 
 However , increasing the number of layers greater than 6 does not yield any performance im- provements . 
 We also notice this degradation of performance with the XLM model while varying the number of hidden layers in the LM head . 
 E Training Details 
 We follow the training process similar to Ma et al . 
 ( 2020 ) training process . 
 We train an English ASR model using the source speech data . 
 Next , we train a simultaneous model without the latency loss ( setting λlatency = 0 ) after initializing the encoder from the English ASR model . 
 After this step , we finetune the simultaneous model for different λs . 
 This training process is repeated for all the reported models and for each task . 
 The details regarding the hyperparameters for the model have been provided in Table 2 . 
 F BLEU - AL Numbers As mentioned in the results section of the main pa- per , we vary the latency weight hyperparameter ( λ ) to train different models to obtain different latency regimes . 
 We also vary the step - size / speech seg- ment size during inference . 
 In total , we obtain 18 different data points corresponding to each model . 
 In Table 3 , we compare the results obtained using MMA , MMA - XLM and MMA - SLM under similar hyperparameter settings . 
 It will help the reader to quantify the benefits obtained from our proposed approach .
BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming - Wei Chang Kenton Lee Kristina Toutanova Google AI Language fjacobdevlin , mingweichang , kentonl , kristout g@google.com 
 Abstract We introduce a new language representa- tion model called BERT , which stands for Bidirectional Encoder Representations from Transformers . 
 Unlike recent language repre- sentation models ( Peters et al . , 2018a ; Rad- ford et al . , 2018 ) , BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . 
 As a re- sult , the pre - trained BERT model can be ﬁne- tuned with just one additional output layer to create state - of - the - art models for a wide range of tasks , such as question answering and language inference , without substantial task- speciﬁc architecture modiﬁcations . 
 BERT is conceptually simple and empirically powerful . 
 It obtains new state - of - the - art re- sults on eleven natural language processing tasks , including pushing the GLUE score to 80.5% ( 7.7% point absolute improvement ) , MultiNLI accuracy to 86.7% ( 4.6% absolute improvement ) , SQuAD v1.1 question answering Test F1 to 93.2 ( 1.5 point absolute im- provement ) and SQuAD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) . 
 1 Introduction Language model pre - training has been shown to be effective for improving many natural language processing tasks ( Dai and Le , 2015 ; Peters et al . , 2018a ; Radford et al . , 2018 ; Howard and Ruder , 2018 ) . 
 These include sentence - level tasks such as natural language inference ( Bowman et al . , 2015 ; Williams et al . , 2018 ) and paraphrasing ( Dolan and Brockett , 2005 ) , which aim to predict the re- lationships between sentences by analyzing them holistically , as well as token - level tasks such as named entity recognition and question answering , where models are required to produce ﬁne - grained output at the token level ( Tjong Kim Sang and De Meulder , 2003 ; 
 Rajpurkar et al . , 2016). 
 There are two existing strategies for apply- ing pre - trained language representations to down- stream tasks : feature - based andﬁne - tuning . 
 The feature - based approach , such as ELMo ( Peters et al . , 2018a ) , uses task - speciﬁc architectures that include the pre - trained representations as addi- tional features . 
 The ﬁne - tuning approach , such as the Generative Pre - trained Transformer ( OpenAI GPT ) ( Radford et al . , 2018 ) , introduces minimal task - speciﬁc parameters , and is trained on the downstream tasks by simply ﬁne - tuning allpre- trained parameters . 
 The two approaches share the same objective function during pre - training , where they use unidirectional language models to learn general language representations . 
 We argue that current techniques restrict the power of the pre - trained representations , espe- cially for the ﬁne - tuning approaches . 
 The ma- jor limitation is that standard language models are unidirectional , and this limits the choice of archi- tectures that can be used during pre - training . 
 For example , in OpenAI GPT , the authors use a left - to- right architecture , where every token can only at- tend to previous tokens in the self - attention layers of the Transformer ( Vaswani et 
 al . , 2017 ) . 
 Such re- strictions are sub - optimal for sentence - level tasks , and could be very harmful when applying ﬁne- tuning based approaches to token - level tasks such as question answering , where it is crucial to incor- porate context from both directions . 
 In this paper , we improve the ﬁne - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers . 
 BERT alleviates the previously mentioned unidi- rectionality constraint by using a “ masked lan- guage model ” ( MLM ) pre - training objective , in- spired by the Cloze task ( Taylor , 1953 ) . 
 The masked language model randomly masks some of the tokens from the input , and the objective is to predict the original vocabulary i d of the masked word based only on its context . 
 Unlike left - to- right language model pre - training , the MLM ob- jective enables the representation to fuse the left and the right context , which allows us to pre- train a deep bidirectional Transformer . 
 In addi- tion to the masked language model , we also use a “ next sentence prediction ” task that jointly pre- trains text - pair representations . 
 The contributions of our paper are as follows : • We demonstrate the importance of bidirectional pre - training for language representations . 
 Un- like Radford et al . 
 ( 2018 ) , which uses unidirec- tional language models for pre - training , BERT uses masked language models to enable pre- trained deep bidirectional representations . 
 This is also in contrast to Peters et al . ( 2018a ) , which uses a shallow concatenation of independently trained left - to - right and right - to - left LMs . 
 • 
 We show that pre - trained representations reduce the need for many heavily - engineered task- speciﬁc architectures . 
 BERT is the ﬁrst ﬁne- tuning based representation model that achieves state - of - the - art performance on a large suite of sentence - level andtoken - level tasks , outper- forming many task - speciﬁc architectures . 
 • BERT advances the state of the art for eleven NLP tasks . 
 The code and pre - trained mod- els are available at https://github.com/ google - research / bert . 
 2 Related Work There is a long history of pre - training general lan- guage representations , and we brieﬂy review the most widely - used approaches in this section . 
 2.1 Unsupervised Feature - based Approaches Learning widely applicable representations of words has been an active area of research for decades , including non - neural ( Brown et al . , 1992 ; Ando and Zhang , 2005 ; Blitzer et al . , 2006 ) and neural ( Mikolov et al . , 2013 ; Pennington et al . , 2014 ) methods . 
 Pre - trained word embeddings are an integral part of modern NLP systems , of- fering signiﬁcant improvements over embeddings learned from scratch ( Turian et 
 al . , 2010 ) . 
 To pre- train word embedding vectors , left - to - right lan- guage modeling objectives have been used ( Mnih and Hinton , 2009 ) , as well as objectives to dis- criminate correct from incorrect words in left and right context ( Mikolov et al . , 2013).These approaches have been generalized to coarser granularities , such as sentence embed- dings ( Kiros et al . , 2015 ; Logeswaran and Lee , 2018 ) or paragraph embeddings ( Le and Mikolov , 2014 ) . 
 To train sentence representations , prior work has used objectives to rank candidate next sentences ( Jernite et al . , 2017 ; Logeswaran and Lee , 2018 ) , left - to - right generation of next sen- tence words given a representation of the previous sentence ( Kiros et al . , 2015 ) , or denoising auto- encoder derived objectives ( Hill et al . , 2016 ) . 
 ELMo and its predecessor ( Peters et al . , 2017 , 2018a ) generalize traditional word embedding re- search along a different dimension . 
 They extract context - sensitive features from a left - to - right and a right - to - left language model . 
 The contextual rep- resentation of each token is the concatenation of the left - to - right and right - to - left representations . 
 When integrating contextual word embeddings with existing task - speciﬁc architectures , ELMo advances the state of the art for several major NLP benchmarks ( Peters et al . , 2018a ) including ques- tion answering ( Rajpurkar et al . , 2016 ) , sentiment analysis ( Socher et al . , 2013 ) , and named entity recognition ( Tjong Kim Sang and De Meulder , 2003 ) . 
 Melamud et 
 al . 
 ( 2016 ) proposed learning contextual representations through a task to pre- dict a single word from both left and right context using LSTMs . 
 Similar to ELMo , their model is feature - based and not deeply bidirectional . 
 Fedus et 
 al . 
 ( 2018 ) shows that the cloze task can be used to improve the robustness of text generation mod- els . 
 2.2 Unsupervised Fine - tuning Approaches As with the feature - based approaches , the ﬁrst works in this direction only pre - trained word em- bedding parameters from unlabeled text ( Col- lobert and Weston , 2008 ) . 
 More recently , sentence or document encoders which produce contextual token representations have been pre - trained from unlabeled text and ﬁne - tuned for a supervised downstream task ( Dai and Le , 2015 ; Howard and Ruder , 2018 ; Radford et al . , 2018 ) . 
 The advantage of these approaches is that few parameters need to be learned from scratch . 
 At least partly due to this advantage , OpenAI GPT ( Radford et al . , 2018 ) achieved pre- viously state - of - the - art results on many sentence- level tasks from the GLUE benchmark ( Wang et al . , 2018a ) . 
 Left - to - right language model- ing and auto - encoder objectives have been used for pre - training such models ( Howard and Ruder , 2018 ; Radford et 
 al . , 2018 ; Dai and Le , 2015 ) 
 2.3 Transfer Learning from Supervised Data There has also been work showing effective trans- fer from supervised tasks with large datasets , such as natural language inference ( Conneau et al . , 2017 ) and machine translation ( McCann et al . , 2017 ) . 
 Computer vision research has also demon- strated the importance of transfer learning from large pre - trained models , where an effective recipe is to ﬁne - tune models pre - trained with Ima- geNet ( Deng et al . , 2009 ; Yosinski et al . , 2014 ) . 
 3 BERT We introduce BERT and its detailed implementa- tion in this section . 
 There are two steps in our framework : pre - training and ﬁne - tuning . 
 Dur- ing pre - training , the model is trained on unlabeled data over different pre - training tasks . 
 For ﬁne- tuning , the BERT model is ﬁrst initialized with the pre - trained parameters , and all of the param- eters are ﬁne - tuned using labeled data from the downstream tasks . 
 Each downstream task has sep- arate ﬁne - tuned models , even though they are ini- tialized with the same pre - trained parameters . 
 The question - answering example in Figure 1 will serve as a running example for this section . 
 A distinctive feature of BERT is its uniﬁed ar- chitecture across different tasks . 
 There is mini - mal difference between the pre - trained architec- ture and the ﬁnal downstream architecture . 
 Model Architecture BERT ’s model architec- ture is a multi - layer bidirectional Transformer en- coder based on the original implementation de- scribed in Vaswani 
 et 
 al . ( 2017 ) and released in thetensor2tensor library.1Because the use of Transformers has become common and our im- plementation is almost identical to the original , we will omit an exhaustive background descrip- tion of the model architecture and refer readers to Vaswani et 
 al . ( 2017 ) as well as excellent guides such as “ The Annotated Transformer . ”2 
 In this work , we denote the number of layers ( i.e. , Transformer blocks ) as L , the hidden size as H , and the number of self - attention heads as A . 
 We primarily report results on two model sizes : BERT BASE ( L = 12 , H = 768 , A = 12 , Total Param- eters=110 M ) and BERT LARGE ( L = 24 , H = 1024 , A = 16 , Total Parameters=340 M ) . 
 BERT BASE was chosen to have the same model size as OpenAI GPT for comparison purposes . 
 Critically , however , the BERT Transformer uses bidirectional self - attention , while the GPT Trans- former uses constrained self - attention where every token can only attend to context to its left . 
 Input / Output Representations To make BERT handle a variety of down - stream tasks , our input representation is able to unambiguously represent both a single sentence and a pair of sentences ( e.g. ,hQuestion , Answeri ) in one token sequence . 
 Throughout this work , a “ sentence ” can be an arbi- trary span of contiguous text , rather than an actual linguistic sentence . 
 A “ sequence ” refers to the in- put token sequence to BERT , which may be a sin- gle sentence or two sentences packed together . 
 We use WordPiece embeddings ( Wu et al . , 2016 ) with a 30,000 token vocabulary . 
 The ﬁrst token of every sequence is always a special clas- siﬁcation token 
 ( [ CLS ] ) . 
 The ﬁnal hidden state corresponding to this token is used as the ag- gregate sequence representation for classiﬁcation tasks . 
 Sentence pairs are packed together into a single sequence . 
 We differentiate the sentences in two ways . 
 First , we separate them with a special token ( [ SEP ] ) . 
 Second , we add a learned embed- ding to every token indicating whether it belongs to sentence Aor sentence B. As shown in Figure 1 , we denote input embedding as E , the ﬁnal hidden vector of the special [ CLS ] token asC2RH , and the ﬁnal hidden vector for the ithinput token asTi2RH . 
 For a given token , its input representation is constructed by summing the corresponding token , segment , and position embeddings . 
 A visualiza- tion of this construction can be seen in Figure 2 . 
 3.1 Pre - training BERT Unlike Peters et al . ( 2018a ) and Radford et 
 al . 
 ( 2018 ) , we do not use traditional left - to - right or right - to - left language models to pre - train BERT . 
 Instead , we pre - train BERT using two unsuper- vised tasks , described in this section . 
 This step is presented in the left part of Figure 1 . 
 Task # 1 : Masked LM 
 Intuitively , it is reason- able to believe that a deep bidirectional model is strictly more powerful than either a left - to - right model or the shallow concatenation of a left - to- right and a right - to - left model . 
 Unfortunately , standard conditional language models can only be trained left - to - right orright - to - left , since bidirec- tional conditioning would allow each word to in- 
 directly “ see itself ” , and the model could trivially predict the target word in a multi - layered context . 
 former is often referred to as a “ Transformer encoder ” while the left - context - only version is referred to as a “ Transformer decoder ” since it can be used for text generation . 
 In order to train a deep bidirectional representa- tion , we simply mask some percentage of the input tokens at random , and then predict those masked tokens . 
 We refer to this procedure as a “ masked LM ” ( MLM ) , although it is often referred to as a Cloze task in the literature ( Taylor , 1953 ) . 
 In this case , the ﬁnal hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary , as in a standard LM . 
 In all of our experiments , we mask 15% of all WordPiece to- kens in each sequence at random . 
 In contrast to denoising auto - encoders ( Vincent et al . , 2008 ) , we only predict the masked words rather than recon- structing the entire input . 
 Although this allows us to obtain a bidirec- tional pre - trained model , a downside is that we are creating a mismatch between pre - training and ﬁne - tuning , since the [ MASK ] token does not ap- pear during ﬁne - tuning . 
 To mitigate this , we do not always replace “ masked ” words with the ac- 
 tual[MASK ] token . 
 The training data generator chooses 15% of the token positions at random for prediction . 
 If the i - th token is chosen , we replace thei - th token with ( 1 ) the [ MASK ] token 80% of the time ( 2 ) a random token 10% of the time ( 3 ) the unchanged i - th token 10% of the time . 
 Then , Tiwill be used to predict the original token with cross entropy loss . 
 We compare variations of this procedure in Appendix C.2 . 
 Task # 2 : Next Sentence Prediction ( NSP ) Many important downstream tasks such as Ques- tion Answering ( QA ) and Natural Language Infer- ence ( NLI ) are based on understanding the rela- tionship between two sentences , which is not di- rectly captured by language modeling . 
 In order to train a model that understands sentence rela- tionships , we pre - train for a binarized next sen- tence prediction task that can be trivially gener- ated from any monolingual corpus . 
 Speciﬁcally , when choosing the sentences AandBfor each pre- training example , 50% of the time Bis the actual next sentence that follows A(labeled as IsNext ) , and 50% of the time it is a random sentence from the corpus ( labeled as NotNext ) . 
 As we show in Figure 1 , Cis used for next sentence predic- tion ( NSP).5Despite its simplicity , we demon- strate in Section 5.1 that pre - training towards this task is very beneﬁcial to both QA and NLI . 
 The NSP task is closely related to representation- learning objectives used in Jernite et al . 
 ( 2017 ) and Logeswaran and Lee ( 2018 ) . 
 However , in prior work , only sentence embeddings are transferred to down - stream tasks , where BERT transfers all pa- rameters to initialize end - task model parameters . 
 Pre - training data The pre - training procedure largely follows the existing literature on language model pre - training . 
 For the pre - training corpus we use the BooksCorpus ( 800 M words ) 
 ( Zhu et al . , 2015 ) and English Wikipedia ( 2,500 M words ) . 
 For Wikipedia we extract only the text passages and ignore lists , tables , and headers . 
 It is criti- cal to use a document - level corpus rather than a shufﬂed sentence - level corpus such as the Billion Word Benchmark ( Chelba et al . , 2013 ) in order to extract long contiguous sequences . 
 3.2 Fine - tuning BERT Fine - tuning is straightforward since the self- attention mechanism in the Transformer al- lows BERT to model many downstream tasks — whether they involve single text or text pairs — by swapping out the appropriate inputs and outputs . 
 For applications involving text pairs , a common pattern is to independently encode text pairs be- 
 fore applying bidirectional cross attention , such as Parikh et al . 
 ( 2016 ) ; 
 Seo et al . ( 2017 ) . 
 BERT instead uses the self - attention mechanism to unify these two stages , as encoding a concatenated text pair with self - attention effectively includes bidi- rectional cross attention between two sentences . 
 For each task , we simply plug in the task- speciﬁc inputs and outputs into BERT and ﬁne- tune all the parameters end - to - end . 
 At the in- put , sentence Aand sentence Bfrom pre - training are analogous to ( 1 ) sentence pairs in paraphras- ing , ( 2 ) hypothesis - premise pairs in entailment , ( 3 ) question - passage pairs in question answering , and(4 ) a degenerate text- ? 
 pair in text classiﬁcation or sequence tagging . 
 At the output , the token rep- resentations are fed into an output layer for token- level tasks , such as sequence tagging or question answering , and the [ CLS ] representation is fed into an output layer for classiﬁcation , such as en- tailment or sentiment analysis . 
 Compared to pre - training , ﬁne - tuning is rela- tively inexpensive . 
 All of the results in the pa- per can be replicated in at most 1 hour on a sin- gle Cloud TPU , or a few hours on a GPU , starting from the exact same pre - trained model.7We de- scribe the task - speciﬁc details in the correspond- ing subsections of Section 4 . 
 More details can be found in Appendix A.5 . 
 4 Experiments In this section , we present BERT ﬁne - tuning re- sults on 11 NLP tasks . 
 4.1 GLUE The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al . , 2018a ) is a col- lection of diverse natural language understanding tasks . 
 Detailed descriptions of GLUE datasets are included in Appendix B.1 . 
 To ﬁne - tune on GLUE , we represent the input sequence ( for single sentence or sentence pairs ) as described in Section 3 , and use the ﬁnal hid- den vectorC2RHcorresponding to the ﬁrst input token ( [ CLS ] ) as the aggregate representa- tion . 
 The only new parameters introduced during ﬁne - tuning are classiﬁcation layer weights W RKH , where K is the number of labels . 
 We com- pute a standard classiﬁcation loss with CandW , i.e. ,log(softmax ( CWT ) ) . 
 We use a batch size of 32 and ﬁne - tune for 3 epochs over the data for all GLUE tasks . 
 For each task , we selected the best ﬁne - tuning learning rate ( among 5e-5 , 4e-5 , 3e-5 , and 2e-5 ) on the Dev set . 
 Additionally , for BERT LARGE we found that ﬁne- tuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set . 
 With random restarts , we use the same pre - trained checkpoint but per- form different ﬁne - tuning data shufﬂing and 
 clas- siﬁer layer initialization.9 Results are presented in Table 1 . 
 Both BERT BASE and BERT LARGE outperform all sys- tems on all tasks by a substantial margin , obtaining 4.5% and 7.0% respective average accuracy im- provement over the prior state of the art . 
 Note that BERT BASE and OpenAI GPT are nearly identical in terms of model architecture apart from the at- tention masking . 
 For the largest and most widely reported GLUE task , MNLI , BERT obtains a 4.6% absolute accuracy improvement . 
 On the ofﬁcial GLUE leaderboard10 , BERT LARGE obtains a score of 80.5 , compared to OpenAI GPT , which obtains 72.8 as of the date of writing . 
 We ﬁnd that BERT LARGE signiﬁcantly outper- forms BERT BASE across all tasks , especially those with very little training data . 
 The effect of model size is explored more thoroughly in Section 5.2 . 
 4.2 SQuAD v1.1 The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100k crowd- sourced question / answer pairs ( Rajpurkar et al . , 2016 ) . 
 Given a question and a passage from Wikipedia containing the answer , the task is to predict the answer text span in the passage . 
 As shown in Figure 1 , in the question answer- ing task , we represent the input question and pas- sage as a single packed sequence , with the ques- tion using the Aembedding and the passage using theBembedding . 
 We only introduce a start vec- torS2RHand an end vector E2RHduring ﬁne - tuning . 
 The probability of word ibeing the start of the answer span is computed as a dot prod- uct between TiandSfollowed by a softmax over all of the words in the paragraph : Pi = eSTiP jeSTj . 
 The analogous formula is used for the end of the answer span . 
 The score of a candidate span from positionito positionjis deﬁned as STi+ETj , and the maximum scoring span where jiis used as a prediction . 
 The training objective is the sum of the log - likelihoods of the correct start and end positions . 
 We ﬁne - tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32 . 
 Table 2 shows top leaderboard entries as well as results from top published systems ( Seo et al . , 2017 ; Clark and Gardner , 2018 ; Peters et al . , 2018a ; Hu et al . , 2018 ) . 
 The top results from the SQuAD leaderboard do not have up - to - date public system descriptions available,11and are allowed to use any public data when training their systems . 
 We therefore use modest data augmentation in our system by ﬁrst ﬁne - tuning on TriviaQA ( Joshi et al . , 2017 ) befor ﬁne - tuning on SQuAD . 
 Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system . 
 In fact , our single BERT model outperforms the top ensemble sys- tem in terms of F1 score . 
 Without TriviaQA ﬁne- tuning data , we only lose 0.1 - 0.4 F1 , still outper- forming all existing systems by a wide margin . 
 4.3 SQuAD v2.0 
 The SQuAD 2.0 task extends the SQuAD 1.1 problem deﬁnition by allowing for the possibility that no short answer exists in the provided para- graph , making the problem more realistic . 
 We use a simple approach to extend the SQuAD v1.1 BERT model for this task . 
 We treat ques- tions that do not have an answer as having an an- swer span with start and end at the [ CLS ] to- ken . 
 The probability space for the start and end answer span positions is extended to include the position of the [ CLS ] token . 
 For prediction , we compare the score of the no - answer span : snull= SC+ECto the score of the best non - null span ^si;j = maxjiSTi+ETj . 
 We predict a non - null answer when ^si;j > s null+ 
 , where the thresh- old 
 is selected on the dev set to maximize F1 . 
 We did not use TriviaQA data for this model . 
 We ﬁne - tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48 . 
 The results compared to prior leaderboard en- 
 tries and top published work ( Sun et al . , 2018 ; Wang et al . , 2018b ) are shown in Table 3 , exclud- ing systems that use BERT as one of their com- ponents . 
 We observe a +5.1 F1 improvement over the previous best system . 
 4.4 SWAG The Situations With Adversarial Generations ( SWAG ) dataset contains 113k sentence - pair com- pletion examples that evaluate grounded common- sense inference ( Zellers et al . , 2018 ) . 
 Given a sen- tence , the task is to choose the most plausible con- tinuation among four choices . 
 When ﬁne - tuning on the SWAG dataset , we construct four input sequences , each containing the concatenation of the given sentence ( sentence A ) and a possible continuation ( sentence B ) . 
 The only task - speciﬁc parameters introduced is a vec- tor whose dot product with the [ CLS ] token rep- 
 resentation Cdenotes a score for each choice which is normalized with a softmax layer . 
 We ﬁne - tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16 . 
 Re- sults are presented in Table 4 . 
 BERT LARGE out- performs the authors ’ baseline ESIM+ELMo sys- tem by +27.1% and OpenAI GPT by 8.3% . 
 5 Ablation Studies 
 In this section , we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance . 
 Additional ablation studies can be found in Appendix C . 
 5.1 Effect of Pre - training Tasks We demonstrate the importance of the deep bidi- rectionality of BERT by evaluating two pre- training objectives using exactly the same pre- training data , ﬁne - tuning scheme , and hyperpa- rameters as BERT BASE : No NSP : A bidirectional model which is trained using the “ masked LM ” ( MLM ) but without the “ next sentence prediction ” ( NSP ) task . 
 LTR & No NSP : A left - context - only model which is trained using a standard Left - to - Right ( LTR ) LM , rather than an MLM . 
 The left - only constraint was also applied at ﬁne - tuning , because removing it introduced a pre - train/ﬁne - tune mismatch that degraded downstream performance . 
 Additionally , this model was pre - trained without the NSP task . 
 This is directly comparable to OpenAI GPT , but using our larger training dataset , our input repre- sentation , and our ﬁne - tuning scheme . 
 We ﬁrst examine the impact brought by the NSP task . 
 In Table 5 , we show that removing NSP hurts performance signiﬁcantly on QNLI , MNLI , and SQuAD 1.1 . 
 Next , we evaluate the impact of training bidirectional representations by com- paring “ No NSP ” to “ LTR & No NSP ” . 
 The LTR model performs worse than the MLM model on all tasks , with large drops on MRPC and SQuAD . 
 For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions , since the token - level hidden states have no right- side context . 
 In order to make a good faith at- tempt at strengthening the LTR system , we added a randomly initialized BiLSTM on top . 
 This does signiﬁcantly improve results on SQuAD , but theresults are still far worse than those of the pre- trained bidirectional models . 
 The BiLSTM hurts performance on the GLUE tasks . 
 We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two mod- els , as ELMo does . 
 However : ( a ) this is twice as expensive as a single bidirectional model ; ( b ) this is non - intuitive for tasks like QA , since the RTL model would not be able to condition the answer on the question ; ( c ) this it is strictly less powerful than a deep bidirectional model , since it can use both left and right context at every layer . 
 5.2 Effect of Model Size In this section , we explore the effect of model size on ﬁne - tuning task accuracy . 
 We trained a number of BERT models with a differing number of layers , hidden units , and attention heads , while otherwise using the same hyperparameters and training pro- cedure as described previously . 
 Results on selected GLUE tasks are shown in Table 6 . 
 In this table , we report the average Dev Set accuracy from 5 random restarts of ﬁne - tuning . 
 We can see that larger models lead to a strict ac- curacy improvement across all four datasets , even for MRPC which only has 3,600 labeled train- ing examples , and is substantially different from the pre - training tasks . 
 It is also perhaps surpris- ing that we are able to achieve such signiﬁcant improvements on top of models which are al- ready quite large relative to the existing literature . 
 For example , the largest Transformer explored in Vaswani et al . 
 ( 2017 ) is ( L = 6 , H = 1024 , A = 16 ) with 100M parameters for the encoder , and the largest Transformer we have found in the literature is ( L = 64 , H = 512 , A = 2 ) with 235M parameters ( Al - Rfou et al . , 2018 ) . 
 By contrast , BERT BASE contains 110M parameters and BERT LARGE con- tains 340M parameters . 
 It has long been known that increasing the model size will lead to continual improvements on large - scale tasks such as machine translation and language modeling , which is demonstrated by the LM perplexity of held - out training data shown in Table 6 . 
 However , we believe that this is the ﬁrst work to demonstrate convinc- ingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks , provided that the model has been sufﬁ- ciently pre - trained . 
 Peters et al . ( 2018b ) presented mixed results on the downstream task impact of increasing the pre - trained bi - LM size from two to four layers and Melamud et al . 
 ( 2016 ) 
 men- tioned in passing that increasing hidden dimen- sion size from 200 to 600 helped , but increasing further to 1,000 did not bring further improve- 
 ments . 
 Both of these prior works used a feature- based approach — we hypothesize that when the model is ﬁne - tuned directly on the downstream tasks and uses only a very small number of ran- domly initialized additional parameters , the task- speciﬁc models can beneﬁt from the larger , more expressive pre - trained representations even when downstream task data is very small . 
 5.3 Feature - based Approach with BERT All of the BERT results presented so far have used the ﬁne - tuning approach , where a simple classiﬁ- cation layer is added to the pre - trained model , and all parameters are jointly ﬁne - tuned on a down- stream task . 
 However , the feature - based approach , where ﬁxed features are extracted from the pre- trained model , has certain advantages . 
 First , not all tasks can be easily represented by a Trans- former encoder architecture , and therefore require a task - speciﬁc model architecture to be added . 
 Second , there are major computational beneﬁts to pre - compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation . 
 In this section , we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition ( NER ) task ( Tjong Kim Sang and De Meulder , 2003 ) . 
 In the input to BERT , we use a case - preserving WordPiece model , and we include the maximal document context provided by the data . 
 Following standard practice , we for- mulate this as a tagging task but do not use a CRF layer in the output . 
 We use the representation of the ﬁrst sub - token as the input to the token - level classiﬁer over the NER label set . 
 To ablate the ﬁne - tuning approach , we apply the feature - based approach by extracting the activa- tions from one or more layers without ﬁne - tuning any parameters of BERT . 
 These contextual em- beddings are used as input to a randomly initial- ized two - layer 768 - dimensional BiLSTM before the classiﬁcation layer . 
 Results are presented in Table 7 . 
 BERT LARGE performs competitively with state - of - the - art meth- ods . 
 The best performing method concatenates the token representations from the top four hidden lay- ers of the pre - trained Transformer , which is only 0.3 F1 behind ﬁne - tuning the entire model . 
 This demonstrates that BERT is effective for both ﬁne- tuning and feature - based approaches . 
 6 Conclusion Recent empirical improvements due to transfer learning with language models have demonstrated that rich , unsupervised pre - training is an integral part of many language understanding systems . 
 In particular , these results enable even low - resource tasks to beneﬁt from deep unidirectional architec- tures . 
 Our major contribution is further general- izing these ﬁndings to deep bidirectional architec- tures , allowing the same pre - trained model to suc- cessfully tackle a broad set of NLP tasks . 
 References Alan Akbik , Duncan Blythe , and Roland V ollgraf . 
 2018 . 
 Contextual string embeddings for sequence labeling . 
 In Proceedings of the 27th International Conference on Computational Linguistics , pages 1638–1649 . 
 Rami Al - Rfou , Dokook Choe , Noah Constant , Mandy Guo , and Llion Jones . 
 2018 . 
 Character - level lan- guage modeling with deeper self - attention . 
 arXiv preprint arXiv:1808.04444 . 
 Rie Kubota Ando and Tong Zhang . 
 2005 . 
 A framework for learning predictive structures from multiple tasks and unlabeled data . 
 Journal of Machine Learning Research , 6(Nov):1817–1853 . 
 Luisa Bentivogli , Bernardo Magnini , Ido Dagan , Hoa Trang Dang , and Danilo Giampiccolo . 
 2009 . 
 The ﬁfth PASCAL recognizing textual entailment challenge . 
 In TAC . 
 NIST . 
 John Blitzer , Ryan McDonald , and Fernando Pereira . 2006 . 
 Domain adaptation with structural correspon- dence learning . 
 In Proceedings of the 2006 confer- ence on empirical methods in natural language pro- 
 cessing , pages 120–128 . 
 Association for Computa- tional Linguistics . 
 Samuel R. Bowman , Gabor Angeli , Christopher Potts , and Christopher D. Manning . 
 2015 . 
 A large anno- tated corpus for learning natural language inference . 
 InEMNLP . 
 Association for Computational Linguis- tics . 
 Peter F Brown , Peter V Desouza , Robert L Mercer , Vincent J Della Pietra , and Jenifer C Lai . 
 1992 . 
 Class - based n - gram models of natural language . 
 Computational linguistics , 18(4):467–479 . 
 Daniel Cer , Mona Diab , Eneko Agirre , Inigo Lopez- Gazpio , and Lucia Specia . 
 2017 . 
 Semeval-2017 task 1 : Semantic textual similarity multilingual and crosslingual focused evaluation . 
 In Proceedings of the 11th International Workshop on Semantic Evaluation ( SemEval-2017 ) , pages 1–14 , Vancou- ver , Canada . 
 Association for Computational Lin- guistics . 
 Ciprian Chelba , Tomas Mikolov , Mike Schuster , Qi Ge , Thorsten Brants , Phillipp Koehn , and Tony Robin- son . 2013 . 
 One billion word benchmark for measur- ing progress in statistical language modeling . 
 arXiv preprint arXiv:1312.3005 . 
 Z. Chen , H. Zhang , X. Zhang , and L. Zhao . 
 2018 . 
 Quora question pairs . 
 Christopher Clark and Matt Gardner . 
 2018 . 
 Simple and effective multi - paragraph reading comprehen- sion . 
 In ACL.Kevin Clark , Minh - Thang Luong , Christopher D Man- ning , and Quoc Le . 2018 . 
 Semi - supervised se- quence modeling with cross - view training . 
 In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing , pages 1914 – 1925 . 
 Ronan Collobert and Jason Weston . 
 2008 . 
 A uniﬁed architecture for natural language processing : 
 Deep neural networks with multitask learning . 
 In Pro- ceedings of the 25th international conference on Machine learning , pages 160–167 . ACM . 
 Alexis Conneau , Douwe Kiela , Holger Schwenk , Lo ¨ıc Barrault , and Antoine Bordes . 
 2017 . 
 Supervised learning of universal sentence representations from natural language inference data . 
 In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing , pages 670–680 , Copen- hagen , Denmark . 
 Association for Computational Linguistics . 
 Andrew M Dai and Quoc V Le . 2015 . 
 Semi - supervised sequence learning . 
 In Advances in neural informa- tion processing systems , pages 3079–3087 . 
 J. Deng , W. Dong , R. Socher , L.-J. Li , K. Li , and L. Fei- Fei . 2009 . 
 ImageNet : 
 A Large - Scale Hierarchical Image Database . 
 In CVPR09 . 
 William B Dolan and Chris Brockett . 
 2005 . 
 Automati- cally constructing a corpus of sentential paraphrases . 
 InProceedings of the Third International Workshop on Paraphrasing ( IWP2005 ) . 
 William Fedus , Ian Goodfellow , and Andrew M Dai . 2018 . 
 Maskgan : Better text generation via ﬁlling in the.arXiv preprint arXiv:1801.07736 . 
 Dan Hendrycks and Kevin Gimpel . 
 2016 . 
 Bridging nonlinearities and stochastic regularizers with gaus- sian error linear units . 
 CoRR , abs/1606.08415 . 
 Felix Hill , Kyunghyun Cho , and Anna Korhonen . 
 2016 . 
 Learning distributed representations of sentences from unlabelled data . 
 In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies . 
 Association for Computa- tional Linguistics . 
 Jeremy Howard and Sebastian Ruder . 
 2018 . 
 Universal language model ﬁne - tuning for text classiﬁcation . 
 In ACL . 
 Association for Computational Linguistics . 
 Minghao Hu , Yuxing Peng , Zhen Huang , Xipeng Qiu , Furu Wei , and Ming Zhou . 
 2018 . 
 Reinforced mnemonic reader for machine reading comprehen- sion . 
 In IJCAI . 
 Yacine Jernite , Samuel R. Bowman , and David Son- tag . 
 2017 . 
 Discourse - based objectives for fast un- supervised sentence representation learning . 
 CoRR , abs/1705.00557 . 
 4181Mandar Joshi , Eunsol Choi , Daniel S Weld , and Luke Zettlemoyer . 2017 . 
 Triviaqa : 
 A large scale distantly supervised challenge dataset for reading comprehen- sion . 
 In ACL . 
 Ryan Kiros , Yukun Zhu , Ruslan R Salakhutdinov , Richard Zemel , Raquel Urtasun , Antonio Torralba , and Sanja Fidler . 
 2015 . 
 Skip - thought vectors . 
 In Advances in neural information processing systems , pages 3294–3302 . 
 Quoc Le and Tomas Mikolov . 
 2014 . 
 Distributed rep- resentations of sentences and documents . 
 In Inter- national Conference on Machine Learning , pages 1188–1196 . 
 Hector J Levesque , Ernest Davis , and Leora Morgen- stern . 
 2011 . 
 The winograd schema challenge . 
 In Aaai spring symposium : Logical formalizations of commonsense reasoning , volume 46 , page 47 . 
 Lajanugen Logeswaran and Honglak Lee . 2018 . 
 An efﬁcient framework for learning sentence represen- 
 tations . 
 In International Conference on Learning Representations . 
 Bryan McCann , James Bradbury , Caiming Xiong , and Richard Socher . 
 2017 . 
 Learned in translation : Con- textualized word vectors . 
 In NIPS . 
 Oren Melamud , Jacob Goldberger , and Ido Dagan . 
 2016 . 
 context2vec : Learning generic context em- bedding with bidirectional LSTM . 
 In CoNLL . 
 Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S Cor- rado , and Jeff Dean . 
 2013 . 
 Distributed representa- tions of words and phrases and their compositional- ity . 
 In Advances in Neural Information Processing Systems 26 , pages 3111–3119 . 
 Curran Associates , Inc. Andriy Mnih and Geoffrey E Hinton . 
 2009 . 
 A scal- able hierarchical distributed language model . 
 In D. Koller , D. Schuurmans , Y . 
 Bengio , and L. Bot- tou , editors , Advances in Neural Information Pro- 
 cessing Systems 21 , pages 1081–1088 . 
 Curran As- sociates , Inc. 
 Ankur P Parikh , Oscar T ¨ackstr ¨om , Dipanjan Das , and Jakob Uszkoreit . 
 2016 . 
 A decomposable attention model for natural language inference . 
 In EMNLP . 
 Jeffrey Pennington , Richard Socher , and Christo- pher D. Manning . 
 2014 . 
 Glove : Global vectors for word representation . 
 In Empirical Methods in Nat- ural Language Processing ( EMNLP ) , pages 1532 – 1543 . 
 Matthew Peters , Waleed Ammar , Chandra Bhagavat- ula , and Russell Power . 2017 . 
 Semi - supervised se- quence tagging with bidirectional language models . 
 InACL . 
 Matthew Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer . 
 2018a . 
 Deep contextualized word rep- resentations . 
 In NAACL .Matthew 
 Peters , Mark Neumann , Luke Zettlemoyer , and Wen - tau Yih . 2018b . 
 Dissecting contextual word embeddings : Architecture and representation . 
 InProceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing , pages 1499–1509 . 
 Alec Radford , Karthik Narasimhan , Tim Salimans , and Ilya Sutskever . 2018 . 
 Improving language under- standing with unsupervised learning . 
 Technical re- port , OpenAI . 
 Pranav Rajpurkar , Jian Zhang , Konstantin Lopyrev , and Percy Liang . 
 2016 . 
 Squad : 100,000 + questions for machine comprehension of text . 
 In Proceedings of the 2016 Conference on Empirical Methods in Nat- ural Language Processing , pages 2383–2392 . 
 Minjoon Seo , Aniruddha Kembhavi , Ali Farhadi , and Hannaneh Hajishirzi . 2017 . 
 Bidirectional attention ﬂow for machine comprehension . 
 In ICLR . 
 Richard Socher , Alex Perelygin , Jean Wu , Jason Chuang , Christopher D Manning , Andrew Ng , and Christopher Potts . 2013 . 
 Recursive deep models for semantic compositionality over a sentiment tree- bank . 
 In Proceedings of the 2013 conference on empirical methods in natural language processing , pages 1631–1642 . 
 Fu Sun , Linyang Li , Xipeng Qiu , and Yang Liu . 
 2018 . 
 U - net : Machine reading comprehension with unanswerable questions . 
 arXiv preprint arXiv:1810.06638 . 
 Wilson L Taylor . 
 1953 . 
 Cloze procedure : A new tool for measuring readability . 
 Journalism Bulletin , 30(4):415–433 . 
 Erik F Tjong Kim Sang and Fien De Meulder . 
 2003 . 
 Introduction to the conll-2003 shared task : Language - independent named entity recognition . 
 In CoNLL . 
 Joseph Turian , Lev Ratinov , and Yoshua Bengio . 
 2010 . 
 Word representations : A simple and general method for semi - supervised learning . 
 In Proceedings of the 48th Annual Meeting of the Association for Compu- tational Linguistics , ACL ’ 10 , pages 384–394 . 
 Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 . 
 Attention is all you need . 
 In Advances in Neural Information Pro- cessing Systems , pages 6000–6010 . 
 Pascal Vincent , Hugo Larochelle , Yoshua Bengio , and Pierre - Antoine Manzagol . 2008 . 
 Extracting and composing robust features with denoising autoen- coders . 
 In Proceedings of the 25th international conference on Machine learning , pages 1096–1103 . ACM . 
 Alex Wang , Amanpreet Singh , Julian Michael , Fe- lix Hill , Omer Levy , and Samuel Bowman . 
 2018a . 
 Glue : A multi - task benchmark and analysis platform 
 4182for natural language understanding . 
 In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : An- alyzing and Interpreting Neural Networks for NLP , pages 353–355 . 
 Wei Wang , Ming Yan , and Chen Wu . 2018b . 
 Multi- 
 granularity hierarchical attention fusion networks for reading comprehension and question answering . 
 InProceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics ( Volume 1 : Long Papers ) . 
 Association for Computational Lin- guistics . 
 Alex Warstadt , Amanpreet Singh , and Samuel R Bow- man . 
 2018 . 
 Neural network acceptability judg- ments . 
 arXiv preprint arXiv:1805.12471 . 
 Adina Williams , Nikita Nangia , and Samuel R Bow- man . 
 2018 . 
 A broad - coverage challenge corpus for sentence understanding through inference . 
 In NAACL . 
 Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc V Le , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , Klaus Macherey , et al . 2016 . 
 Google ’s neural ma- chine translation system : Bridging the gap between human and machine translation . 
 arXiv preprint arXiv:1609.08144 . 
 Jason Yosinski , Jeff Clune , Yoshua Bengio , and Hod Lipson . 
 2014 . 
 How transferable are features in deep neural networks ? 
 In Advances in neural information processing systems , pages 3320–3328 . 
 Adams Wei Yu , David Dohan , Minh - Thang Luong , Rui Zhao , Kai Chen , Mohammad Norouzi , and Quoc V Le . 
 2018 . 
 QANet : 
 Combining local convolution with global self - attention for reading comprehen- sion . 
 In ICLR . 
 Rowan Zellers , Yonatan Bisk , Roy Schwartz , and Yejin Choi . 
 2018 . 
 Swag : A large - scale adversarial dataset for grounded commonsense inference . 
 In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) . 
 Yukun Zhu , Ryan Kiros , Rich Zemel , Ruslan Salakhut- dinov , Raquel Urtasun , Antonio Torralba , and Sanja Fidler . 2015 . 
 Aligning books and movies : Towards story - like visual explanations by watching movies and reading books . 
 In Proceedings of the IEEE international conference on computer vision , pages 19–27 . 
 Appendix for “ BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding ” We organize the appendix into three sections : 
 • Additional implementation details for BERT are presented in Appendix A; 
 • Additional details for our experiments are presented in Appendix B ; and 
 • Additional ablation studies are presented in Appendix C . 
 We present additional ablation studies for BERT including : – Effect of Number of Training Steps ; and – Ablation for Different Masking Proce- dures . 
 A Additional Details for BERT A.1 Illustration of the Pre - training Tasks We provide examples of the pre - training tasks in the following . 
 Masked LM and the Masking Procedure As- suming the unlabeled sentence is my dog is hairy , and during the random masking procedure we chose the 4 - th token ( which corresponding to hairy ) , our masking procedure can be further il- lustrated by • 80% of the time : 
 Replace the word with the [ MASK ] token , e.g. , my dog is hairy ! 
 my dog is [ MASK ] • 10% of the time : 
 Replace the word with a random word , e.g. , my dog is hairy ! 
 my dog is apple • 10% of the time : Keep the word un- changed , e.g. , my dog is hairy ! 
 my dog is hairy . 
 The purpose of this is to bias the representation towards the actual observed word . 
 The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been re- placed by random words , so it is forced to keep a distributional contextual representation of ev- eryinput token . 
 Additionally , because random replacement only occurs for 1.5% of all tokens ( i.e. , 10 % of 15 % ) , this does not seem to harm the model ’s language understanding capability . 
 In Section C.2 , we evaluate the impact this proce- dure . 
 Compared to standard langauge model training , the masked LM only make predictions on 15 % of tokens in each batch , which suggests that more pre - training steps may be required for the model to converge . 
 In Section C.1 we demonstrate that MLM does converge marginally slower than a left- to - right model ( which predicts every token ) , but the empirical improvements of the MLM model far outweigh the increased training cost . 
 Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples . 
 Input =[ CLS ] the man went to [ MASK ] store [ SEP ] he bought a gallon [ MASK ] milk 
 [ SEP ] Label = IsNext Input =[ CLS ] the man [ MASK ] to the store [ SEP ] penguin 
 [ MASK ] are flight # # less birds 
 [ SEP ] Label = NotNext A.2 Pre - training Procedure To generate each training input sequence , we sam- ple two spans of text from the corpus , which we refer to as “ sentences ” even though they are typ- ically much longer than single sentences ( but can be shorter also ) . 
 The ﬁrst sentence receives the A embedding and the second receives the Bembed- ding . 
 50% of the time Bis the actual next sentence that follows Aand 50% of the time it is a random sentence , which is done for the “ next sentence pre- diction ” task . 
 They are sampled such that the com- bined length is <512 tokens . 
 The LM masking is applied after WordPiece tokenization with a uni- form masking rate of 15% , and no special consid- eration given to partial word pieces . 
 We train with batch size of 256 sequences ( 256 sequences * 512 tokens = 128,000 tokens / batch ) for 1,000,000 steps , which is approximately 40 epochs over the 3.3 billion word corpus . 
 We use Adam with learning rate of 1e-4 , 
 1= 0.9 , 
 2= 0.999 , L2 weight decay of 0:01 , learning rate warmup over the ﬁrst 10,000 steps , and linear decay of the learning rate . 
 We use a dropout prob- ability of 0.1 on all layers . 
 We use a gelu acti- vation ( Hendrycks and Gimpel , 2016 ) rather than the standard relu , following OpenAI GPT . 
 The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood . 
 Training of BERT BASE was performed on 4 Cloud TPUs in Pod conﬁguration ( 16 TPU chips total).13Training of BERT LARGE was performed on 16 Cloud TPUs ( 64 TPU chips total ) . 
 Each pre- training took 4 days to complete . 
 Longer sequences are disproportionately expen- sive because attention is quadratic to the sequence length . 
 To speed up pretraing in our experiments , we pre - train the model with sequence length of 128 for 90% of the steps . 
 Then , we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings . 
 A.3 Fine - tuning Procedure For ﬁne - tuning , most model hyperparameters are the same as in pre - training , with the exception of the batch size , learning rate , and number of train- ing epochs . 
 The dropout probability was always kept at 0.1 . 
 The optimal hyperparameter values are task - speciﬁc , but we found the following range of possible values to work well across all tasks : • Batch size : 16 , 32 
 • Learning rate ( Adam ) : 5e-5 , 3e-5 , 2e-5 • Number of epochs : 2 , 3 , 4 We also observed that large data sets ( e.g. , 100k+ labeled training examples ) were far less sensitive to hyperparameter choice than small data sets . 
 Fine - tuning is typically very fast , so it is rea- sonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set . 
 A.4 Comparison of BERT , ELMo , and OpenAI GPT Here we studies the differences in recent popular representation learning models including ELMo , OpenAI GPT and BERT . 
 The comparisons be- tween the model architectures are shown visually in Figure 3 . 
 Note that in addition to the architec- ture differences , BERT and OpenAI GPT are ﬁne- tuning approaches , while ELMo is a feature - based approach . 
 The most comparable existing pre - training method to BERT is OpenAI GPT , which trains a left - to - right Transformer LM on a large text cor- pus . 
 In fact , many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared . 
 The core argument of this work is that the bi - directionality and the two pre- training tasks presented in Section 3.1 account for the majority of the empirical improvements , but we do note that there are several other differences between how BERT and GPT were trained : 
 • GPT is trained on the BooksCorpus ( 800 M words ) ; BERT is trained on the BooksCor- pus ( 800 M words ) and Wikipedia ( 2,500 M words ) . 
 • GPT uses a sentence separator ( [ SEP ] ) and classiﬁer token ( [ CLS ] ) which are only in- troduced at ﬁne - tuning time ; BERT learns [ SEP ] , [ CLS ] and sentence A / Bembed- 
 dings during pre - training . 
 • GPT was trained for 1M steps with a batch size of 32,000 words ; BERT was trained for 1M steps with a batch size of 128,000 words . 
 • GPT used the same learning rate of 5e-5 for all ﬁne - tuning experiments ; BERT chooses a task - speciﬁc ﬁne - tuning learning rate which performs the best on the development set . 
 To isolate the effect of these differences , we per- form ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre - training tasks and the bidirectionality they enable . 
 A.5 Illustrations of Fine - tuning on Different Tasks The illustration of ﬁne - tuning BERT on different tasks can be seen in Figure 4 . 
 Our task - speciﬁc models are formed by incorporating BERT with one additional output layer , so a minimal num- ber of parameters need to be learned from scratch . 
 Among the tasks , ( a ) and ( b ) are sequence - level tasks while ( c ) and ( d ) are token - level tasks . 
 In the ﬁgure , Erepresents the input embedding , Ti represents the contextual representation of token i , 
 [ CLS ] is the special symbol for classiﬁcation out- put , and [ SEP ] is the special symbol to separate non - consecutive token sequences . 
 B Detailed Experimental Setup B.1 Detailed Descriptions for the GLUE Benchmark Experiments . 
 The GLUE benchmark includes the following datasets , the descriptions of which were originally summarized in Wang et al . 
 ( 2018a ): MNLI Multi - Genre Natural Language Inference is a large - scale , crowdsourced entailment classiﬁ- cation task ( Williams et al . , 2018 ) . 
 Given a pair of sentences , the goal is to predict whether the sec- ond sentence is an entailment , contradiction , or neutral with respect to the ﬁrst one . 
 QQP Quora Question Pairs is a binary classiﬁ- cation task where the goal is to determine if two questions asked on Quora are semantically equiv- alent ( Chen et al . , 2018 ) . 
 QNLI 
 Question Natural Language Inference is a version of the Stanford Question Answering Dataset ( Rajpurkar et al . , 2016 ) which has been converted to a binary classiﬁcation task ( Wang et al . , 2018a ) . 
 The positive examples are ( ques- tion , sentence ) pairs which do contain the correct answer , and the negative examples are ( question , sentence ) from the same paragraph which do not contain the answer . 
 SST-2 
 The Stanford Sentiment Treebank is a binary single - sentence classiﬁcation task consist- ing of sentences extracted from movie reviews with human annotations of their sentiment ( Socher et al . , 2013 ) . 
 CoLA 
 The Corpus of Linguistic Acceptability is a binary single - sentence classiﬁcation task , where the goal is to predict whether an English sentence is linguistically “ acceptable ” or not ( Warstadt et al . , 2018 ) . 
 STS - B The Semantic Textual Similarity Bench- mark is a collection of sentence pairs drawn from news headlines and other sources ( Cer et al . , 2017 ) . 
 They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning . 
 MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources , with human annotations for whether the sentences in the pair are semanti- cally equivalent ( Dolan and Brockett , 2005).RTE 
 Recognizing Textual Entailment is a bi- nary entailment task similar to MNLI , but with much less training data ( Bentivogli et al . , 
 2009).14 WNLI Winograd NLI is a small natural lan- guage inference dataset ( Levesque et al . , 2011 ) . 
 The GLUE webpage notes that there are issues with the construction of this dataset,15and every trained system that ’s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class . 
 We therefore ex- clude this set to be fair to OpenAI GPT . 
 For our GLUE submission , we always predicted the ma- jority class . 
 C Additional Ablation Studies C.1 Effect of Number of Training Steps Figure 5 presents MNLI Dev accuracy after ﬁne- tuning from a checkpoint that has been pre - trained forksteps . 
 This allows us to answer the following questions : 1 . Question : Does BERT really need such a large amount of pre - training ( 128,000 words / batch * 1,000,000 steps ) to achieve high ﬁne - tuning accuracy ? 
 Answer : 
 Yes , BERT BASE achieves almost 1.0% additional accuracy on MNLI when trained on 1M steps compared to 500k steps . 
 2 . Question : Does MLM pre - training converge slower than LTR pre - training , since only 15% of words are predicted in each batch rather than every word ? 
 Answer : 
 The MLM model does converge slightly slower than the LTR model . 
 How- ever , in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately . 
 C.2 Ablation for Different Masking Procedures In Section 3.1 , we mention that BERT uses a mixed strategy for masking the target tokens when pre - training with the masked language model ( MLM ) objective . 
 The following is an ablation study to evaluate the effect of different masking strategies . 
 Note that the purpose of the masking strategies is to reduce the mismatch between pre - training and ﬁne - tuning , as the [ MASK ] symbol never ap- pears during the ﬁne - tuning stage . 
 We report the Dev results for both MNLI and NER . 
 For NER , we report both ﬁne - tuning and feature - based ap- proaches , as we expect the mismatch will be am- pliﬁed for the feature - based approach as the model will not have the chance to adjust the representa- tions . 
 The results are presented in Table 8 . 
 In the table , MASK means that we replace the target token with the[MASK ] symbol for MLM ; S AME means that we keep the target token as is ; R NDmeans that we replace the target token with another random token . 
 The numbers in the left part of the table repre- sent the probabilities of the speciﬁc strategies used during MLM pre - training ( BERT uses 80% , 10% , 10% ) . 
 The right part of the paper represents the Dev set results . 
 For the feature - based approach , we concatenate the last 4 layers of BERT as the features , which was shown to be the best approach in Section 5.3 . 
 From the table it can be seen that ﬁne - tuning is surprisingly robust to different masking strategies . 
 However , as expected , using only the M ASK strat- egy was problematic when applying the feature- based approach to NER . 
 Interestingly , using only the R NDstrategy performs much worse than our strategy as well .
Privacy - preserving Neural Representations of Text 
 Maximin Coavoux Shashi Narayan Shay B. Cohen Institute for Language , Cognition and Computation School of Informatics , University of Edinburgh fmcoavoux , scohen g@inf.ed.ac.uk,shashi.narayan@ed.ac.uk Abstract This article deals with adversarial attacks to- wards deep learning systems for Natural Lan- guage Processing ( NLP ) , in the context of pri- vacy protection . 
 We study a speciﬁc type of at- tack : an attacker eavesdrops on the hidden rep- resentations of a neural text classiﬁer and tries to recover information about the input text . 
 Such scenario may arise in situations when the computation of a neural network is shared across multiple devices , e.g. some hidden rep- resentation is computed by a user ’s device and sent to a cloud - based model . 
 We measure the privacy of a hidden representation by the abil- ity of an attacker to predict accurately speciﬁc private information from it and characterize the tradeoff between the privacy and the util- ity of neural representations . 
 Finally , we pro- pose several defense methods based on modi- ﬁed training objectives and show that they im- prove the privacy of neural representations . 
 1 Introduction This article presents an adversarial scenario meant at characterizing the privacy of neural representa- tions for NLP tasks , as well as defense methods designed to improve the privacy of those represen- tations . 
 A deep neural network constructs inter- mediate hidden representations to extract features from its input . 
 Such representations are trained to predict a label , and therefore should contain use- ful features for the ﬁnal prediction . 
 However , they might also encode information about the input that a user wants to keep private ( e.g. personal data ) and can be exploited for adversarial usages . 
 We study a speciﬁc type of attack on neural rep- resentations : an attacker eavesdrops on the hidden representations of novel input examples ( that are not in the training set ) and tries to recover informa- tion about the content of the input text ( Figure 1 ) . 
 A typical scenario where such attacks would oc- cur is when the computation of a deep neural net Latent representation , sent over a channel 
 zAttackery x Private inputDesired OutputFigure 1 : General setting illustration . 
 The main classi- ﬁer predicts a label yfrom a textx , the attacker tries to recover some private information zcontained in xfrom 
 the latent representation used by the main classiﬁer . is shared between several devices ( Li et al . , 2017 ) . 
 For example , a user ’s device computes a represen- tation of a textual input , and sends it a to cloud- based neural network to obtain , e.g. the topic of the text or its sentiment . 
 The scenario is illustrated in Figure 1 . 
 Private information can take the form of key phrases explicitly contained in the text . 
 However , it can also be implicit . 
 For example , demographic information about the author of a text can be pre- dicted with above chance accuracy from linguistic cues in the text itself ( Rosenthal and McKeown , 2011 ; Preot ¸iuc - Pietro et al . , 2015 ) . 
 Independently of its explicitness , some of this private information correlates with the output la- bels , and therefore will be learned by the network . 
 In such a case , there is a tradeoff between the util- ity of the representation ( measured by the accu- racy of the network ) and its privacy . 
 It might be 
 2necessary to sacriﬁce some accuracy in order to satisfy privacy requirements . 
 However , this is not the case of all private in- formation , since some of it is not relevant for the prediction of the text label . 
 Still , private infor- mation might be learned incidentally . 
 This non- intentional and incidental learning also raises pri- vacy concerns , since an attacker with an access to the hidden representations , may exploit them to re- cover information about the input . 
 In this paper we explore the following situation : ( i ) amain classiﬁer uses a deep network to predict a label from textual data ; ( ii ) an attacker eaves- drops on the hidden layers of the network and tries to recover information about the input text of un- seen examples . 
 In contrast to previous work about neural networks and privacy ( Papernot et al . , 2016 ; Carlini et al . , 2018 ) we do not protect the privacy of examples from the training set , but the privacy of unseen examples provided , e.g. , by a user . 
 An example of a potential application would be a spam detection service with the following con- straints : the service provider does not access ver- 
 batim emails sent to users , only their vector repre- sentations . 
 Theses vector representations should not be usable to gather information about the user ’s contacts or correspondents , i.e. protect the user from proﬁling . 
 This paper makes the following contributions:1 We propose a metric to measure the privacy of the neural representation of an input for Natural Language Processing tasks . 
 The met- ric is based on the ability of an attacker to recover information about the input from the latent representation only . 
 We present defense methods designed against this type of attack . 
 The methods are based on modiﬁed training objectives and lead to an improved privacy - accuracy tradeoff . 
 2 Adversarial Scenario In the scenario we propose , each example consists of a triple ( x;y;z ) , wherexis a natural language text , yis a single label ( e.g. topic or sentiment ) , andzis a vector of private information contained inx . 
 Our base setting has two entities : ( i ) a main classiﬁer whose role is to learn to predict yfrom x , ( ii ) an attacker who learns to predict zfrom the latent representation of xused by the main classi- ﬁer . 
 We illustrate this setting in Figure 1 . 
 In order to evaluate the utility and privacy of a speciﬁc model , we proceed in three phases : Phase 1 . 
 Training of the main classiﬁer on ( x;y)pairs and evaluation of its accuracy ; Phase 2 . Generation of a dataset of pairs ( r(x);z)for the attacker , ris the representation function of the main classiﬁer ( ris deﬁned in Sec- tion 2.1 ) ; Phase 3 . 
 Training of the attacker ’s network and evaluation of its performance for measuring pri- vacy . 
 In the remainder of this section , we describe the main classiﬁer ( Section 2.1 ) , and the attacker ’s model ( Section 2.2 ) . 
 2.1 Text Classiﬁer As our base model , we chose a standard LSTM architecture ( Hochreiter and Schmidhuber , 1997 ) for sequence classiﬁcation . 
 LSTM - based archi- tectures have been applied to many NLP tasks , including sentiment classiﬁcation ( Wang et al . , 2016 ) and text classiﬁcation ( Zhou et al . , 2016 ) . 
 First , an LSTM encoder computes a ﬁxed - size representation r(x)from a sequence of tokens x= ( x1;x2;:::;x n)projected to an embedding space . 
 We use rto denote the parameters used to construct r. 
 They include the parameters of the LSTM , as well as the word embeddings . 
 Then , the encoder output r(x)is fed as input to a feedfor- ward network with parameters pthat predicts the labelyof the text , with a softmax output activa- tion . 
 In the standard setting , the model is trained to minimize the negative log - likelihood of ylabels : Lm(r;p ) 
 = NX i=1 logP(y(i)jx(i);r;p ) ; whereNis the number of training examples . 
 2.2 Attacker ’s Classiﬁer Once the main model has been trained , we assume that its parameters randpare ﬁxed . 
 We gen- erate a new dataset made of pairs ( r(x);z(x ) ) , where r(x)is the hidden representation used by the main model and z(x)is a vector of private cat- egorical variables . 
 In practice , zis a vector of bi- nary variables , ( representing e.g. demographic in- formation about the author ) . 
 In our experiments , we use the same training examples xfor the main 
 3classiﬁer and the classiﬁer of the attacker . 
 How- ever , since the attacker has access to the repre- sentation function rparameterized by r , they can generate a dataset from any corpus containing the private variables they want to recover . 
 In other words , it is not necessary that they have access to the original training corpus to train their classiﬁer . 
 The attacker trains a second feedforward net- work on the new dataset f(r(x(i));z(i))giN. 
 This classiﬁer uses a sigmoid output activation to com- pute the probabilities of each binary variable in z : P(zjr(x);a ) 
 = (FeedForward ( r(x ) ) ): 
 It is trained to minimize the negative log- likelihood of z : La(a ) 
 = NX i=1 logP(z(i)jr(x(i));a ) 
 = NX i=1KX j=1 logP(z(i ) jjr(x(i));a ) ; assuming that the Kvariables in zare indepen- dent . 
 Since the parameters used to construct rare ﬁxed , the attacker only acts upon its own parame- tersato optimize this loss . 
 We use the performance of the attacker ’s clas- siﬁer as a proxy for privacy . 
 If its accuracy is high , then an eavesdropper can easily recover in- formation about the input document . 
 In contrast , if its accuracy is low ( i.e. close to that of a most- frequent label baseline ) , then we may reasonably conclude that rdoes not encode enough informa- tion to reconstruct x , and mainly contains infor- mation that is useful to predict y. In general , the performance of a single attacker does not provide sufﬁcient evidence to conclude that the input representation ris robust to an at- tack . 
 It should be robust to any type of reconstruc- tion method . 
 In the scope of this paper though , we only experiment with a feedforward network reconstructor , i.e. a powerful learner . 
 In the following sections , we propose several training method modiﬁcations aimed at obfuscat- 
 ing private information from the hidden represen- tation r(x ) . 
 Intuitively , the aim of these modiﬁca- tions is to minimize some measure of information between randzto make the prediction of zhard . 
 An obvious choice for that measure would be the Mutual Information ( MI ) between randz . 
 How- ever , MI is hard to compute due to the continuous distribution of rand does not lend itself well to stochastic optimization. 
 3 Defenses Against Adversarial Attacks 
 In this section , we present three training methods designed as defenses against the type of attack we described in Section 2.2 . 
 The ﬁrst two methods are based on two neural networks with rival objective functions ( Section 3.1 ) . 
 The last method is meant at discouraging the model to cluster together train- 
 ing examples with similar private variables z(Sec- tion 3.2 ) . 
 3.1 Adversarial Training First , we propose to frame the training of the main classiﬁer as a two - agent process : the main agent and an adversarial generator , exploiting a set- ting similar to Generative Adversarial Networks ( GAN , Goodfellow et 
 al . , 2014 ) . 
 The generator learns to reconstruct examples from the hidden representation , whereas the main agent learns ( i ) to perform its main task ( ii ) to make the task of the generator difﬁcult . 
 We experiment with two types of generators : a classiﬁer that predicts the binary attributes z(x ) used as a proxy for the reconstruction of x(Sec- tion 3.1.1 ) and a character - based language model that directly optimizes the likelihood of the train- ing examples ( Section 3.1.2 ) . 
 3.1.1 Adversarial Classiﬁcation : 
 Multidetasking In order not to make r(x)a good representation for reconstructing z , we make two modiﬁcations to the training setup of the main model ( Phase 1 ): We use a duplicate adversarial classiﬁer , with parameters 0 a , that tries to predict z from r(x ) . 
 It is trained simultaneously with the main classiﬁer . 
 Its training examples are generated on the ﬂy , and change overtime as the main classiﬁer updates its own parame- ters . 
 This classiﬁer simulates an attack dur- ing training . 
 We modify the objective function of the main classiﬁer to incorporate a penalty when the adversarial classiﬁer is good at reconstruct- ingz . 
 In other words , the main classiﬁer tries to update its parameters so as to confuse the duplicate attacker . 
 Formally , for a single data point ( x;y;z ) , the adversarial classiﬁer optimizes : La0(x;y;z;0 a)= logP(zjr(x);0 a ) ; 
 4whereas the main classiﬁer optimizes : Lm(x;y;z;r;p)=  
 logP(yjx;r;p ) 
   
 logP(:zjr(x);0 a ): The ﬁrst term of this equation is the log - likelihood of theylabels . 
 The second term is designed to de- ceive the adversary . 
 The hyperparameters 
 > 0 and 
 > 0control the relative importance of both terms . 
 As in a GAN , the losses of both classiﬁers are interdependent , but their parameters are distinct : the adversary can only update 0 aand the main classiﬁer can only update randp . 
 The duplicate adversarial classiﬁer is identical to the classiﬁer used to evaluate privacy after the main model has been trained and its parameters are ﬁxed . 
 However , both classiﬁers are completely distinct : the former is used during the training of the main model ( Phase 1 ) to take privacy into ac- count whereas the latter is used to evaluate the pri- vacy of the ﬁnal model ( Phase 3 ) , as is described in Section 2 . 
 3.1.2 Adversarial Generation 
 The second type of generator we use is a character- based LSTM language model that is trained to re- construct full training examples . 
 For a single ex- ample ( x;y ) , the hidden state of the LSTM is ini- tialized with r(x ) , computed by the main model . 
 The generator optimizes : Lg(x;y;`;r ) 
 =  logP(xjr(x); ` ) =  CX i=1logP(xijxi 1 1;r(x); ` ) ; where `is the set of parameters of the LSTM generator , xiis theithcharacter in the document , and C is the length of the document in number of characters . 
 The generator has no control over r(x ) , and optimizes the objective only by updat- ing its own parameters  ` . 
 Conversely , the loss of the main model is modi- ﬁed as follows : 
 Lm(x;y;r;p)=  
 logP(yjx;r;p ) 
   
 Lg(x;y;`;r ): 
 The ﬁrst term maximizes the likelihood of the y labels whereas the second term is meant at mak- ing the reconstruction difﬁcult by maximizing the loss of the generator . 
 As in the loss function de- scribed in the previous section , 
 and 
 controlthe relative importance of both terms . 
 Once again , the main classiﬁer can optimize the second term only by updating r , since it has no control over the parameters of the adversarial generator . 
 A key property of this defense method is that it has no awareness of what the private variables z are . 
 Therefore , it has the potential to protect the neural representation against an attack on any pri- vate information . 
 From a broader perspective , the goal of this defense method is to specialize the hid- den representation r(x)to the task at hand ( sen- timent or topic prediction ) and to avoid learning anything not relevant to it . 
 3.2 
 Declustering The last strategy we employ to make the task of the attacker harder is based on the intuition that pri- vate variables zare easier to predict from rwhen the main model learns implicitly to cluster exam- ples with similar zin the same regions of the rep- resentation space . 
 In order to avoid such implicit clustering , we add a term to the training objective of the main model that penalizes pairs of examples ( x;x0)that ( i ) have similar reconstructions z(x)z(x0)(ii ) have hidden representations r(x)andr(x0)in the same region of space . 
 We use the following modi- ﬁed loss for a single example : Lm(x;y;z;r;p ) =  logP(yjx;r;p ) 
 + 
 ( 0:5 `(z;z0))jjr(x) r(x0)jj2 2 ; where ( x0;z0)is another example sampled uni- formly from the training set , 
 is a hyperparame- ter controlling the importance of the second term , and`(;)2[0;1]is the normalized Hamming dis- tance . 
 4 Experiments Our experiments are meant to characterize the privacy - utility tradeoff of neural representations on text classiﬁcation tasks , and evaluating if the proposed defense methods have a positive im- pact on it . 
 We ﬁrst describe the datasets we used ( Section 4.1 ) and the experimental protocol ( Section 4.2 ) , then we discuss the results ( Sec- tion 4.3 ) . 
 We found that in the normal train- ing regime , where no defense is taken into ac- count , the adversary can recover private informa- tion with higher accuracy than a most frequent class baseline . 
 Furthermore , we found that the de- fenses we implemented have a positive effect on the accuracy - privacy tradeoff . 
 4.1 Datasets We experiment with two text classiﬁcation tasks : sentiment analysis ( Section 4.1.1 ) and topic clas- siﬁcation ( Section 4.1.2 ) . 
 The sizes of each dataset are summarized in Table 1 . 
 4.1.1 Sentiment Analysis We use the Trustpilot dataset ( Hovy et al . , 2015 ) for sentiment analysis . 
 This corpus contains re- views associated with a sentiment score on a ﬁve point scale , and self - reported information about the users . 
 We use the ﬁve subcorpora correspond- ing to ﬁve areas ( Denmark , France , Germany , United Kingdom , United States ) . 
 We ﬁlter examples containing both the birth year and gender of the author of the review and use these variables as the private information . 
 As in previous work on this dataset ( Hovy , 2015 ; Hovy and Søgaard , 2015 ) , we bin the age of the author into two categories ( ‘ under 35 ’ and ‘ over 45 ’ ) . 
 Fi- nally , we randomly split each subcorpus into a training set ( 80 % ) , a development set ( 10 % ) and a test ( 10 % ) . 
 As an additional experimental setting , we use both demographic variables ( gender and age ) as input to the main model . 
 We do so by adding two additional tokens at the beginning of the input text , one for each variable . 
 It has been shown that those variables can be used to improve text classiﬁca- tion ( Hovy , 2015 ) . 
 Also , we would like to evalu- ate whether the attacker ’s task is easier when the variables to predict are explicitly in the input , com- pared to when these information are only poten- tially and implicitly in the input . 
 In other words , this setting simulates the case where private in - formation may be used by the model to improve classiﬁcation , but should not be exposed too obvi- ously . 
 In the rest of this section , we use RAW to denote the setting where only the raw text is used as input and + DEMO , the setting where the demo- graphic variables are also used as input . 
 4.1.2 Topic Classiﬁcation We perform topic classiﬁcation on two genres of documents : news articles and blog posts . 
 News article For topic classiﬁcation of news ar- ticle , we use two datasets : the AG news corpus ( Del Corso et al . , 2005 ) and the English part of the Deutsche Welle ( DW ) news corpus ( Pappas and Popescu - Belis , 2017 ) . 
 For the AG corpus , following Zhang et al . ( 2015 ) , we construct the dataset by extracting doc- uments belonging to the four most frequent topics , and use the concatenation of the ‘ title ’ and ‘ de- scription ’ ﬁelds as the input to the classiﬁer . 
 We randomly split the corpus into a training set ( 80 % ) , a development set ( 10 % ) and a test set ( 10 % ) . 
 For the DW dataset , we use the ‘ text ’ ﬁeld as input , and the standard split . 
 We kept only documents belonging to the 20 most frequent topics . 
 The attacker tries to detect which named enti- ties appear in the input text ( each coefﬁcient in z(x)indicates whether a speciﬁc named entity oc- curs in the text ) . 
 For both datasets , we used the named entity recognition system from the NLTK package ( Bird et al . , 2009 ) to associate each ex- ample with the list of named entities that occur in it . 
 We select the ﬁve most frequent named entities with type ‘ person ’ , and only keep examples con- taining at least one of these named entities . 
 This ﬁltering is necessary to avoid a very unbalanced dataset ( since each selected named entity appears usually in very few articles ) . 
 Blog posts We used the blog authorship corpus presented by Schler et al . 
 ( 2006 ) , a collection of blog posts associated with the age and gender of the authors , as provided by the authors themselves . 
 Since the blog posts have no topic annotation , we ran the LDA algorithm ( Blei et al . , 2003 ) on the whole collection ( with 10 topics ) . 
 The LDA out- puts a distribution on topics for each blog post . 
 We selected posts with a single dominating topic ( > 80 % ) and discarded the other posts . 
 We binned age into two category ( under 20 and over 30 ) . 
 We used the age and gender of the author as the private variables . 
 These variables have a very unbalanced distribution in the dataset , we randomly select ex- amples to obtain uniform distributions of private variables . 
 Finally , we split the corpus into a train- ing set ( 80 % ) , a validation set and a test set ( 10 % each ) . 
 4.2 Protocol Evaluation For the main task , we report a single accuracy measure . 
 For measuring the privacy of a representation , we compute the following metrics : For demographic variables ( sentiment analy- sis and blog post topic classiﬁcation ): 1-X , where X is the average of the accuracy of the attacker on the prediction of gender and age ; For named entities ( news topic classiﬁca- tion ): 1-F , where F is an F - score computed over the set of binary variables in zthat in- dicate the presence of named entities in the input example . 
 Training protocol We implemented our model using Dynet ( Neubig et al . , 2017 ) . 
 The feedfor- ward components ( both of the main model and of the attacker ) have a single hidden layer of 64 units with a ReLU activation . 
 Word embeddings have 32 units . 
 The LSTM encoder has a single layer of varying sizes , since it is expected that the amount of information that can be learned depends on the size of these representations . 
 We used the Adam optimizer ( Kingma and Ba , 2014 ) with the default learning rate , and 0.2 dropout rate for the LSTM . 
 We used = 0.1 for the declustering method , based on preliminary experiments . 
 For the other defense methods , we used = = 1 and did not experiment with other values . 
 For each dataset , and each LSTM state di- mension ( f 8 , 16 , 32 , 64 , 128 g ) , we train the main model for 8 epochs ( sentiment classiﬁcation ) or 16 epochs ( topic classiﬁcation ) , and select the model with the best accuracy on the development set . 
 Then , we generate the dataset for the attacker , train the adversarial model for 16 epochs and se- lect the model with the worst privacy on the devel- opment set ( i.e. the most successful attacker ) . 
 It has to be noted that we select the models that implement defenses on their accuracy , rather than their privacy or a combination thereof . 
 In prac- tice , we could also base the selection strategy on a privacy budget : selecting the most accurate model with privacy above a certain threshold . 
 4.3 Results This section discusses results for the sentiment analysis task ( Section 4.3.1 ) and the topic classi- ﬁcation task ( Section 4.3.2 ) . 
 4.3.1 Sentiment Analysis How private are neural representations ? 
 Be- fore discussing the effect of proposed defense methods , we motivate empirically our approach by showing that adversarial models can recover pri- vate information with reasonable accuracy when the attack is targeted towards a model that imple- ments none of the presented defense methods . 
 To do so , we compare the accuracy of adversar- ial models to two types of baselines : As a lower bound , we use the most frequent class baseline . 
 As an upper bound , we trained a classi- ﬁer that can optimize the hidden represen- tations ( r ) for the attacker ’s tasks . 
 In other words , this baseline is trained to predict de- mographic variables from x , as if it were the main task . 
 In Table 2 , we compare both baselines to the best adversary in the two settings ( RAW and + DEMO ) among the models trained with no de- fenses . 
 First of all , we observe that apart from gender on the German dataset , the trained baseline outperforms the most frequent class baseline by a wide margin ( 8 to 25 absolute difference ) . 
 Sec- ond of all , the attacker is able to outperform the most frequent class baseline overall , even in the RAW setting . 
 In more details , for age , the adver- sary is well over the baseline in all cases except US . 
 On the other hand , gender seems harder to predict : the adversary outperforms the most fre- quent class baseline only in the + DEMO setting . 
 The same pattern is visible for the blog post dataset , also presented in the last line of Table 2 : the best adversaries are 14 points over the base- line for gender and 5 points for age , i.e. almost as good as a model that can ﬁne tune the hidden representations . 
 These results justify our approach , since they demonstrate that hidden representations learn pri- vate information about the input , and can be ex- ploited to recover this information with reasonable accuracy . 
 Effect of defenses We report results for the main task accuracy and the representation privacy in Ta- ble 3 for the + DEMO setting and in Table 4 for the RAW setting . 
 Recall that the privacy measure ( Priv . ) is computed by 1-X where X is the av- erage accuracy of the attacker on gender and age predictions . 
 When this privacy metric is higher , it is more difﬁcult to exploit the hidden repre- sentation of the network to recover information about x . 
 The ‘ Standard ’ columns contain the ac- curacy and privacy of the base model described in Section 2 . 
 The next columns present the abso- lute variation in accuracy and privacy for the three defense methods presented in Section 3 : Multi- detasking , Adversarial Generation , and Decluster- ing . 
 We also report for each corpus the most fre- quent class baseline for the main task accuracy , and the privacy of the most frequent class base- lines on private variables ( i.e. the upper bound for privacy ) . 
 The three modiﬁed training methods designed as defenses have a positive effect on privacy . 
 De- spite a model selection based on accuracy , they lead to an improvement in privacy on all datasets , except on the France subcorpus . 
 In most cases , we observe only a small decrease in accuracy , or even an improvement at times ( e.g. multidetasking on the Germany dataset , RAW setting ) , thus improv- ing the tradeoff between the utility and the privacy of the text representations . 
 84.3.2 Topic Classiﬁcation We report results on topic classiﬁcation in Table 5 . 
 News articles For the news corpora , the privacy metric is based on the F - score on the binary vari- ables zindicating the presence or absence of a named entity in the text . 
 First of all , we ob- serve that defense methods that explicitly use z ( i.e. multidetasking and declustering ) , have a very positive effect on privacy , but also a detrimental effect on the main task . 
 We hypothesize that this is due to the strong correlations between the main task labelsyand the private information z. 
 As a result , improving the privacy of the neural repre- sentations comes at a cost in accuracy . 
 In contrast , the adversarial generation defense method lead to an improvement in accuracy , that is quite substantial for the DW corpus . 
 We specu- late that this is due to the secondary term in the ob- jective function of the main model ( Section 3.1.2 ) that helps avoiding overﬁtting the main task or learning spurious features . 
 Blog posts On the blog post dataset , the effects are smaller , which we attribute to the nature of the task of the attacker . 
 The defense methods con- sistently improve privacy and , in one case , accu- racy . 
 The best effects on the tradeoff are achieved with the multidetasking and adversarial generation methods . 
 5 Discussion The main result of our experiments is that the de- fenses we propose improve privacy with usually a small effect , either positive or negative , on accu- racy , thus improving the tradeoff between the util- ity and the privacy of neural representations . 
 An important direction for future work is the choice of a strategy for model selection . 
 The tradeoff between utility and privacy can be con- trolled in many ways . 
 For example , the impor- tance of both terms in the loss functions in Sec- tion 3.1 can be controlled to favor either privacy or utility . 
 In the scope of this paper , we did not perform thorough hyperparameter tuning , but be- lieve that doing so is important for achieving better results , since the effects of defense method can be more drastic than desired in some cases , as exem- pliﬁed on the news corpora ( Table 5 ) . 
 Overall , we found that the multidetasking ap- proach lead to the more stable improvements and should be preferred in most cases , since it is alsothe less computationnally expensive defense . 
 On the other hand , the adversarial generation method does not require the speciﬁcation of private vari- ables , and thus is a more general approach . 
 6 Related Work The deployment of machine learning in both academic and industrial contexts raises concerns about adversarial uses of machine learning , as well as concerns about attacks speciﬁcally targeted at these algorithms that often rely on large amounts of data , including personal data . 
 More generally , the framework of differential privacy ( Dwork , 2006 ) provides privacy guaran- tees for the problem of releasing information with- out compromising conﬁdential data , and usually involves adding noise in the released information . 
 It has been applied to the training of deep learning models ( Abadi et al . , 2016 ; 
 Papernot et al . , 2016 ; Papernot et al . , 2018 ) , and Bayesian topic models ( Schein et al . , 2018 ) . 
 The notion of privacy is particularly crucial to NLP , since it deals with textual data , oftentimes user - generated data , that contain a lot of private in- formation . 
 For example , textual data contain a lot of signal about authors ( Hovy and Spruit , 2016 ) . 
 and can be leveraged to predict demographic vari- ables ( Rosenthal and McKeown , 2011 ; Preot ¸iuc- Pietro et al . , 2015 ) . 
 Oftentimes , this information is not explicit in the text but latent and related to the usage of various linguistic traits . 
 Our work is based on a stronger hypothesis : this latent infor- mation is still present in vectorial representations of texts , even if the representations have not been supervised by these latent variables . 
 Li et al . 
 ( 2017 ) study the privacy of unsuper- vised representations of images , and measures their privacy with the peak signal to noise ratio between an original image and its reconstruction by an attacker . 
 They ﬁnd a tradeoff between the privacy of the learned representations and the ac- curacy of an image classiﬁcation model that uses these representations as inputs . 
 Our setting is complementary since it is applied to NLP tasks , but explores a similar problem in the case of rep- resentations learned with a task supervision . 
 A related problem is the unintended memoriza- tion of private data from the training set and has been addressed by Carlini et al . 
 ( 2018 ) . 
 They tackle this problem in the context of text gener- ation ( machine translation , language modelling ) . 
 If an attacker has access to e.g. a trained language model , they are likely to be able to generate sen- tences from the training set , since the language model is trained to assign high probabilities to those sentences . 
 Such memorization is problem- atic when the training data contains private infor- mation and personal data . 
 The experimental set- ting we explore is different from these works : we assume that the attacker has access to a hidden layer of the network and tries to recover informa- tion about an input example that is not in the train- ing set . 
 In a recent study , Li et al . 
 ( 2018 ) proposed a method based on GAN designed to improve the robustness and privacy of neural representations , applied to part - of - speech tagging and sentiment analysis . 
 They use a training scheme with two agents similar to our multidetasking strategy ( Sec- tion 3.1.1 ) , and found that it made neural represen- tations more robust and accurate . 
 However , they only use a single adversary to alter the training of the main model and to evaluate the privacy of the representations , with the risk of overestimat- ing privacy . 
 In contrast , once the parameters of our main model are ﬁxed , we train a new classiﬁer from scratch to evaluate privacy . 
 7 Conclusion We have presented an adversarial scenario and used it to measure the privacy of hidden repre- sentations in the context of two NLP tasks : senti- ment analysis and topic classiﬁcation of news arti- cle and blog posts . 
 We have shown that in general , it is possible for an attacker to recover private vari- ables with higher than chance accuracy , using only hidden representations . 
 In order to improve the privacy of hidden representations , we have pro- posed defense methods based on modiﬁcations of the training objective of the main model . 
 Empiri- cally , the proposed defenses lead to models with a better privacy . 
 Acknowledgments We thank the anonymous reviewers and members of the Cohort for helpful feedback on previous ver- sions of the article . 
 We gratefully acknowledge the support of the European Union under the Horizon 2020 SUMMA project ( grant agreement 688139 ) , and the support of Huawei Technologies . 
 References Martin Abadi , Andy Chu , Ian Goodfellow , H. Bren- dan McMahan , Ilya Mironov , Kunal Talwar , and Li Zhang . 
 2016 . 
 Deep learning with differential pri- vacy . 
 In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Se- curity , CCS ’ 16 , pages 308–318 , New York , NY , USA . ACM . 
 Steven Bird , Ewan Klein , and Edward Loper . 2009 . 
 Natural Language Processing with Python , 1st edi- tion . 
 O’Reilly Media , Inc. David M. Blei , Andrew Y . 
 Ng , and Michael I. Jordan . 
 2003 . 
 Latent dirichlet allocation . 
 Journal of Ma- chine 
 Learning Research , 3:993–1022 . 
 Nicholas Carlini , Chang Liu , Jernej Kos , ´ Ulfar Erlings- son , and Dawn Song . 
 2018 . 
 The secret sharer : Mea- suring unintended neural network memorization & extracting secrets . 
 CoRR , abs/1802.08232 . 
 Gianna M. Del Corso , Antonio Gull ´ ı , and Francesco Romani . 
 2005 . 
 Ranking a stream of news . 
 In Pro- ceedings of the 14th International Conference on World Wide Web , WWW ’ 05 , pages 97–106 , New York , NY , USA . ACM . 
 Cynthia Dwork . 
 2006 . 
 Differential privacy . 
 In 33rd International Colloquium on Automata , Languages and Programming , part II ( ICALP 2006 ) , volume 4052 , pages 1–12 , Venice , Italy . 
 Springer Verlag . 
 Ian Goodfellow , Jean Pouget - Abadie , Mehdi Mirza , Bing Xu , David Warde - Farley , Sherjil Ozair , Aaron Courville , and Yoshua Bengio . 
 2014 . 
 Generative adversarial nets . 
 In Z. Ghahramani , M. Welling , C. Cortes , N. D. Lawrence , and K. Q. Weinberger , editors , Advances in Neural Information Processing Systems 27 , pages 2672–2680 . 
 Curran Associates , Inc. 
 Sepp Hochreiter and J ¨urgen Schmidhuber . 
 1997 . 
 Long short - term memory . 
 Neural computation , 9(8):1735–1780 . 
 Dirk Hovy . 2015 . 
 Demographic factors improve clas- siﬁcation performance . 
 In Proceedings of the 53rd Annual Meeting of the Association for Computa- tional Linguistics and the 7th International Joint Conference on Natural Language Processing ( Vol- ume 1 : Long Papers ) , pages 752–762 , Beijing , China . 
 Association for Computational Linguistics . 
 Dirk Hovy , Anders Johannsen , and Anders Søgaard . 
 2015 . 
 User review sites as a resource for large- scale sociolinguistic studies . 
 In Proceedings of the 24th International Conference on World Wide Web , WWW ’ 15 , pages 452–461 , Republic and Canton of Geneva , Switzerland . 
 International World Wide Web Conferences Steering Committee . 
 Dirk Hovy and Anders Søgaard . 
 2015 . 
 Tagging perfor- mance correlates with author age . 
 In Proceedings of the 53rd Annual Meeting of the Association for 
 10Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 2 : Short Papers ) , pages 483–488 , Beijing , China . Association for Computational Linguistics . 
 Dirk Hovy and Shannon L. Spruit . 
 2016 . 
 The social impact of natural language processing . 
 In Proceed- ings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Pa- pers ) , pages 591–598 , Berlin , Germany . 
 Association for Computational Linguistics . 
 Diederik P. Kingma and Jimmy Ba . 2014 . 
 Adam : A method for stochastic optimization . 
 CoRR , abs/1412.6980 . 
 Meng Li , Liangzhen Lai , Naveen Suda , Vikas Chan- dra , and David Z. Pan . 2017 . Privynet : A ﬂexible framework for privacy - preserving deep neural net- work training with A ﬁne - grained privacy control . 
 CoRR , abs/1709.06161 . 
 Yitong Li , Timothy Baldwin , and Trevor Cohn . 
 2018 . 
 Towards robust and privacy - preserving text repre- sentations . 
 In Proceedings of the 56th Annual Meet- ing of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 25–30 , Melbourne , Australia . 
 Association for Computational Linguis- tics . 
 Graham Neubig , Chris Dyer , Yoav Goldberg , Austin Matthews , Waleed Ammar , Antonios Anastasopou- los , Miguel Ballesteros , David Chiang , Daniel Clothiaux , Trevor Cohn , Kevin Duh , Manaal Faruqui , Cynthia Gan , Dan Garrette , Yangfeng Ji , Lingpeng Kong , Adhiguna Kuncoro , Gaurav Ku- mar , Chaitanya Malaviya , Paul Michel , Yusuke Oda , Matthew Richardson , Naomi Saphra , Swabha Swayamdipta , and Pengcheng Yin . 2017 . Dynet : The dynamic neural network toolkit . 
 arXiv preprint arXiv:1701.03980 . 
 Nicolas Papernot , Mart ´ ın Abadi , ´ Ulfar Erlingsson , Ian J. Goodfellow , and Kunal Talwar . 
 2016 . 
 Semi- supervised knowledge transfer for deep learning from private training data . 
 CoRR , abs/1610.05755 . 
 Nicolas Papernot , Shuang Song , Ilya Mironov , Ananth Raghunathan , Kunal Talwar , and ´ Ulfar Erlingsson . 
 2018 . 
 Scalable Private Learning with PATE . 
 ArXiv e 
 -prints , abs/1802.08908 . 
 Nikolaos Pappas and Andrei Popescu - Belis . 
 2017 . 
 Multilingual hierarchical attention networks for doc- ument classiﬁcation . 
 In Proceedings of the Eighth International Joint Conference on Natural Lan- guage Processing ( Volume 1 : Long Papers ) , pages 1015–1025 , Taipei , Taiwan . 
 Asian Federation of Natural Language Processing . 
 Daniel Preot ¸iuc - Pietro , Vasileios Lampos , and Niko- laos Aletras . 
 2015 . 
 An analysis of the user occupa- tional class through twitter content . 
 In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th InternationalJoint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 1754–1764 , Bei- jing , China . 
 Association for Computational Linguis- tics . 
 Sara Rosenthal and Kathleen McKeown . 2011 . 
 Age prediction in blogs : A study of style , content , and online behavior in pre- and post - social media gen- erations . 
 In Proceedings of the 49th Annual Meet- ing of the Association for Computational Linguis- 
 tics : Human Language Technologies , pages 763 – 772 , Portland , Oregon , USA . Association for Com- putational Linguistics . 
 Aaron Schein , Zhiwei Steven Wu , Mingyuan Zhou , and Hanna Wallach . 
 2018 . 
 Locally Private Bayesian Inference for Count Models . 
 ArXiv e - prints , abs/1803.08471 . 
 Jonathan Schler , Moshe Koppel , Shlomo Argamon , and James Pennebaker . 
 2006 . 
 Effects of age and gender on blogging . 
 In Computational Approaches to Analyzing Weblogs - Papers from the AAAI Spring Symposium , Technical Report , volume SS-06 - 03 , pages 191–197 . 
 Yequan Wang , Minlie Huang , Xiaoyan Zhu , and Li Zhao . 
 2016 . 
 Attention - based LSTM for aspect- level sentiment classiﬁcation . 
 In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing , pages 606–615 , Austin , Texas . 
 Association for Computational Linguistics . 
 Xiang Zhang , Junbo Zhao , and Yann LeCun . 2015 . 
 Character - level convolutional networks for text clas- siﬁcation . 
 In C. Cortes , N. D. Lawrence , D. D. Lee , M. Sugiyama , and R. Garnett , editors , Advances in Neural Information Processing Systems 28 , pages 649–657 . 
 Curran Associates , Inc. 
 Peng Zhou , Zhenyu Qi , Suncong Zheng , Jiaming Xu , Hongyun Bao , and Bo Xu . 
 2016 . 
 Text classification improved by integrating bidirectional lstm with two- dimensional max pooling . 
 In Proceedings of COL- ING 2016 , the 26th International Conference on Computational Linguistics : Technical Papers , pages 3485–3495 , Osaka , Japan . 
 The COLING 2016 Or- ganizing Committee .